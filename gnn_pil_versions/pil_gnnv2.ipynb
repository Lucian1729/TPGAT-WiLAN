{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-06T10:08:11.655094Z",
          "iopub.execute_input": "2023-07-06T10:08:11.655513Z",
          "iopub.status.idle": "2023-07-06T10:10:02.592058Z",
          "shell.execute_reply.started": "2023-07-06T10:08:11.655481Z",
          "shell.execute_reply": "2023-07-06T10:10:02.590896Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vHlenqqcoB8",
        "outputId": "aba174d2-588f-4b88-e595-85ef442ce3e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu117\n",
            "Collecting torch==1.13.1+cu117\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torch-1.13.1%2Bcu117-cp310-cp310-linux_x86_64.whl (1801.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m727.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.14.1+cu117\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torchvision-0.14.1%2Bcu117-cp310-cp310-linux_x86_64.whl (24.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.3/24.3 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.13.1\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torchaudio-0.13.1%2Bcu117-cp310-cp310-linux_x86_64.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1+cu117) (4.6.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1+cu117) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1+cu117) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1+cu117) (8.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (3.4)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.15.2+cu118\n",
            "    Uninstalling torchvision-0.15.2+cu118:\n",
            "      Successfully uninstalled torchvision-0.15.2+cu118\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.0.2+cu118\n",
            "    Uninstalling torchaudio-2.0.2+cu118:\n",
            "      Successfully uninstalled torchaudio-2.0.2+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.13.1+cu117 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.13.1+cu117 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.13.1+cu117 torchaudio-0.13.1+cu117 torchvision-0.14.1+cu117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdata==0.5.1 torchtext==0.14.1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-06T10:10:02.595858Z",
          "iopub.execute_input": "2023-07-06T10:10:02.596163Z",
          "iopub.status.idle": "2023-07-06T10:10:17.089867Z",
          "shell.execute_reply.started": "2023-07-06T10:10:02.596135Z",
          "shell.execute_reply": "2023-07-06T10:10:17.088572Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZgyOuL7coCG",
        "outputId": "e6d4f428-3729-4a7e-e450-319686cf56fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchdata==0.5.1\n",
            "  Downloading torchdata-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchtext==0.14.1\n",
            "  Downloading torchtext-0.14.1-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.5.1) (1.26.16)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata==0.5.1) (2.27.1)\n",
            "Collecting portalocker>=2.0.0 (from torchdata==0.5.1)\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.5.1) (1.13.1+cu117)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.14.1) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.14.1) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->torchdata==0.5.1) (4.6.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.5.1) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.5.1) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.5.1) (3.4)\n",
            "Installing collected packages: portalocker, torchtext, torchdata\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.15.2\n",
            "    Uninstalling torchtext-0.15.2:\n",
            "      Successfully uninstalled torchtext-0.15.2\n",
            "  Attempting uninstall: torchdata\n",
            "    Found existing installation: torchdata 0.6.1\n",
            "    Uninstalling torchdata-0.6.1:\n",
            "      Successfully uninstalled torchdata-0.6.1\n",
            "Successfully installed portalocker-2.7.0 torchdata-0.5.1 torchtext-0.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.13.1+cu117.html"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-06T10:10:17.095299Z",
          "iopub.execute_input": "2023-07-06T10:10:17.097502Z",
          "iopub.status.idle": "2023-07-06T10:10:31.268254Z",
          "shell.execute_reply.started": "2023-07-06T10:10:17.097463Z",
          "shell.execute_reply": "2023-07-06T10:10:31.267128Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgIfr5w2coCI",
        "outputId": "d4a79685-8bd4-4780-d740-be1bf9775590"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-1.13.1+cu117.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu117/torch_scatter-2.1.1%2Bpt113cu117-cp310-cp310-linux_x86_64.whl (10.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.1+pt113cu117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-06T10:10:31.271551Z",
          "iopub.execute_input": "2023-07-06T10:10:31.271935Z",
          "iopub.status.idle": "2023-07-06T10:10:56.721495Z",
          "shell.execute_reply.started": "2023-07-06T10:10:31.271896Z",
          "shell.execute_reply": "2023-07-06T10:10:56.720245Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgVl6k43coCK",
        "outputId": "ee8cbf1f-bd33-46db-f7a1-5f3e85520dcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/661.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch_geometric\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910459 sha256=9c61f81dc122d23f2bea3082da99b9222e644d20dcbd1f698ca4816d57e3459f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "Successfully built torch_geometric\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GCNConv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_scatter import scatter_mean\n",
        "from torch_geometric.data import InMemoryDataset, download_url, extract_zip\n",
        "from torch_geometric.nn import MetaLayer"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-06T10:10:56.723782Z",
          "iopub.execute_input": "2023-07-06T10:10:56.724219Z",
          "iopub.status.idle": "2023-07-06T10:10:59.243881Z",
          "shell.execute_reply.started": "2023-07-06T10:10:56.724166Z",
          "shell.execute_reply": "2023-07-06T10:10:59.242716Z"
        },
        "trusted": true,
        "id": "yN1FByVgcoCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UCLVQJjrfGZ",
        "outputId": "04b2481d-3711-4347-f04a-36faa47e01da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available()  else \"cpu\" )\n",
        "device"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-06T10:20:24.422651Z",
          "iopub.execute_input": "2023-07-06T10:20:24.423030Z",
          "iopub.status.idle": "2023-07-06T10:20:24.430269Z",
          "shell.execute_reply.started": "2023-07-06T10:20:24.423000Z",
          "shell.execute_reply": "2023-07-06T10:20:24.429163Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Pzb-vSNcoCV",
        "outputId": "14f7caea-f4ff-4836-c6ae-d553320da175"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_int_map(dep):\n",
        "    dep = df.loc[df[\"deployment\"]==dep]\n",
        "    dep = dep.reset_index(drop=True)\n",
        "    return dep"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-06T10:13:51.804545Z",
          "iopub.execute_input": "2023-07-06T10:13:51.804937Z",
          "iopub.status.idle": "2023-07-06T10:13:51.811195Z",
          "shell.execute_reply.started": "2023-07-06T10:13:51.804907Z",
          "shell.execute_reply": "2023-07-06T10:13:51.810142Z"
        },
        "trusted": true,
        "id": "_Pj8h3HecoCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = get_int_map(100)"
      ],
      "metadata": {
        "id": "k3RVPftNcoCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/required_format.csv\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-06T10:10:59.245394Z",
          "iopub.execute_input": "2023-07-06T10:10:59.246051Z",
          "iopub.status.idle": "2023-07-06T10:11:00.452826Z",
          "shell.execute_reply.started": "2023-07-06T10:10:59.246013Z",
          "shell.execute_reply": "2023-07-06T10:11:00.451791Z"
        },
        "trusted": true,
        "id": "R2o8JJX7coCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-06T10:11:00.454402Z",
          "iopub.execute_input": "2023-07-06T10:11:00.454755Z",
          "iopub.status.idle": "2023-07-06T10:11:00.497714Z",
          "shell.execute_reply.started": "2023-07-06T10:11:00.454723Z",
          "shell.execute_reply": "2023-07-06T10:11:00.496795Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "K26GfkcfcoCa",
        "outputId": "ab0c01ab-f6b2-4c38-90a8-452d9a30b616"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0    0      1      2       3      4     5      6       7      8  \\\n",
              "0           0  0.0 -79.65 -93.86 -109.01 -78.68 -85.3 -98.24 -108.79 -97.21   \n",
              "1           1  0.0 -79.65 -93.86 -109.01 -78.68 -85.3 -98.24 -108.79 -97.21   \n",
              "2           2  0.0 -79.65 -93.86 -109.01 -78.68 -85.3 -98.24 -108.79 -97.21   \n",
              "3           3  0.0 -79.65 -93.86 -109.01 -78.68 -85.3 -98.24 -108.79 -97.21   \n",
              "4           4  0.0 -79.65 -93.86 -109.01 -78.68 -85.3 -98.24 -108.79 -97.21   \n",
              "\n",
              "   ...  primary_channel  min_channel_allowed  max_channel_allowed   rssi  \\\n",
              "0  ...                0                    0                    3 -55.42   \n",
              "1  ...                0                    0                    3 -62.31   \n",
              "2  ...                0                    0                    3 -55.42   \n",
              "3  ...                0                    0                    3 -58.23   \n",
              "4  ...                0                    0                    3 -66.64   \n",
              "\n",
              "   node_type   sinr  air_time_mean  deployment  channel_bonding_model  \\\n",
              "0          0  35.68          25.15         0.0                      4   \n",
              "1          1  26.99          25.15         0.0                      4   \n",
              "2          1  35.68          25.15         0.0                      4   \n",
              "3          1  33.42          25.15         0.0                      4   \n",
              "4          1  26.83          25.15         0.0                      4   \n",
              "\n",
              "   throughput  \n",
              "0      104.96  \n",
              "1        7.68  \n",
              "2       11.09  \n",
              "3       14.51  \n",
              "4       11.95  \n",
              "\n",
              "[5 rows x 27 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c01d6cf5-9d1c-4957-b004-4acc652c3660\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>primary_channel</th>\n",
              "      <th>min_channel_allowed</th>\n",
              "      <th>max_channel_allowed</th>\n",
              "      <th>rssi</th>\n",
              "      <th>node_type</th>\n",
              "      <th>sinr</th>\n",
              "      <th>air_time_mean</th>\n",
              "      <th>deployment</th>\n",
              "      <th>channel_bonding_model</th>\n",
              "      <th>throughput</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-79.65</td>\n",
              "      <td>-93.86</td>\n",
              "      <td>-109.01</td>\n",
              "      <td>-78.68</td>\n",
              "      <td>-85.3</td>\n",
              "      <td>-98.24</td>\n",
              "      <td>-108.79</td>\n",
              "      <td>-97.21</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>-55.42</td>\n",
              "      <td>0</td>\n",
              "      <td>35.68</td>\n",
              "      <td>25.15</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>104.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-79.65</td>\n",
              "      <td>-93.86</td>\n",
              "      <td>-109.01</td>\n",
              "      <td>-78.68</td>\n",
              "      <td>-85.3</td>\n",
              "      <td>-98.24</td>\n",
              "      <td>-108.79</td>\n",
              "      <td>-97.21</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>-62.31</td>\n",
              "      <td>1</td>\n",
              "      <td>26.99</td>\n",
              "      <td>25.15</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>7.68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-79.65</td>\n",
              "      <td>-93.86</td>\n",
              "      <td>-109.01</td>\n",
              "      <td>-78.68</td>\n",
              "      <td>-85.3</td>\n",
              "      <td>-98.24</td>\n",
              "      <td>-108.79</td>\n",
              "      <td>-97.21</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>-55.42</td>\n",
              "      <td>1</td>\n",
              "      <td>35.68</td>\n",
              "      <td>25.15</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>11.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-79.65</td>\n",
              "      <td>-93.86</td>\n",
              "      <td>-109.01</td>\n",
              "      <td>-78.68</td>\n",
              "      <td>-85.3</td>\n",
              "      <td>-98.24</td>\n",
              "      <td>-108.79</td>\n",
              "      <td>-97.21</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>-58.23</td>\n",
              "      <td>1</td>\n",
              "      <td>33.42</td>\n",
              "      <td>25.15</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>14.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-79.65</td>\n",
              "      <td>-93.86</td>\n",
              "      <td>-109.01</td>\n",
              "      <td>-78.68</td>\n",
              "      <td>-85.3</td>\n",
              "      <td>-98.24</td>\n",
              "      <td>-108.79</td>\n",
              "      <td>-97.21</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>-66.64</td>\n",
              "      <td>1</td>\n",
              "      <td>26.83</td>\n",
              "      <td>25.15</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>11.95</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 27 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c01d6cf5-9d1c-4957-b004-4acc652c3660')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c01d6cf5-9d1c-4957-b004-4acc652c3660 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c01d6cf5-9d1c-4957-b004-4acc652c3660');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6jgFIMTzcoCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating individual graphs\n",
        "# This assumes all APs and STAs are connected to each other\n",
        "def create_graph(split,split_y,deployment):\n",
        "    dep = get_int_map(deployment)\n",
        "    dep_y = dep[\"throughput\"]\n",
        "    dep_x = dep[['0', '1', '2', '3', '4', '5', '6', '7','8', '9', '10', '11', 'wlan_code_index', 'x(m)', 'y(m)','z(m)',\n",
        "            'primary_channel', 'min_channel_allowed', 'max_channel_allowed', 'rssi', 'node_type',\n",
        "            'sinr', 'air_time_mean', 'deployment',\"channel_bonding_model\"]]\n",
        "    #print(dep_x)\n",
        "    dep_reset = dep.reset_index(drop=True)\n",
        "    ap_index = {}\n",
        "    out = dep_reset[dep_reset[\"node_type\"] == 0]\n",
        "    for i in range(len(out)):\n",
        "        ap_index[out.index[i]] = i\n",
        "    #print(ap_index)\n",
        "    node_features = dep_x.iloc[:,12:].values\n",
        "    # print(node_features)\n",
        "    #edge_features = dep.iloc[:,:12].values - here each node has been given an edge feature\n",
        "    # need to give each edge an edge feature\n",
        "    node_targets = dep_y.values\n",
        "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
        "    print(node_features.shape)\n",
        "    #edge_features = torch.tensor(edge_features, dtype=torch.float)\n",
        "    node_targets = torch.tensor(node_targets, dtype=torch.float)\n",
        "    # Add edges here for each deployment\n",
        "    edges = []\n",
        "    edge_features = []\n",
        "    edge_index = []\n",
        "    for i in range(len(dep)):\n",
        "        for j in range(len(dep)):\n",
        "            if (i != j and (dep[\"node_type\"].iloc[i] == 0 and dep[\"node_type\"].iloc[j] == 0)) or (i !=j and (dep[\"node_type\"].iloc[i] == 1 and dep[\"node_type\"].iloc[j] == 0)):\n",
        "                edges.append([i,j])\n",
        "    #print(edges)\n",
        "    edges2=edges\n",
        "    edges = torch.tensor(edges, dtype=torch.float)\n",
        "    #print(\"Edges: \", edges, edges.shape)\n",
        "    # edge_index = torch.tensor(edges, dtype=torch.long)\n",
        "    edge_index = torch.tensor(edges,dtype=torch.long)\n",
        "    edge_index = edge_index.t().contiguous()\n",
        "    #print(edges.detach(), edges.shape)\n",
        "    print(edges.shape[0])\n",
        "    for i in range(edges.shape[0]):\n",
        "        # print(dep.iloc[int(edges[i][0]), ap_index[int(edges[i][1])]])\n",
        "        i_pos = np.asarray([dep.at[edges2[i][0],\"x(m)\"],dep.at[edges2[i][0],\"y(m)\"],dep.at[edges2[i][0],\"z(m)\"]])\n",
        "        j_pos = np.asarray([dep.at[edges2[i][1],\"x(m)\"],dep.at[edges2[i][1],\"y(m)\"],dep.at[edges2[i][1],\"z(m)\"]])\n",
        "        distance = np.linalg.norm(i_pos - j_pos)\n",
        "        #print(i)\n",
        "        # edge_features.append([distance,dep.iloc[int(edges[i][0]), ap_index[int(edges[i][1])]]])\n",
        "        edge_features.append([distance,dep.at[edges2[i][0],\"rssi\"],dep.iloc[int(edges[i][0]), ap_index[int(edges[i][1])]]])\n",
        "    # edge_features = np.array(edge_features)\n",
        "    # print(edge_features)\n",
        "    edge_features = torch.tensor(edge_features, dtype=torch.float)\n",
        "    #print(edge_features, edge_features.shape)\n",
        "    graph = {\n",
        "        \"edges\": edges,\n",
        "        \"edge_index\": edge_index,\n",
        "        \"node_features\": node_features,\n",
        "        \"edge_features\": edge_features,\n",
        "        \"node_targets\": node_targets\n",
        "    }\n",
        "#     print(\"*\"*10)\n",
        "#     print(edges.shape)\n",
        "#     print(edge_index.shape)\n",
        "#     print(node_features.shape)\n",
        "#     print(edge_features.shape)\n",
        "#     print(node_targets.shape)\n",
        "    return graph"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-06T10:28:27.630364Z",
          "iopub.execute_input": "2023-07-06T10:28:27.630716Z",
          "iopub.status.idle": "2023-07-06T10:28:27.648692Z",
          "shell.execute_reply.started": "2023-07-06T10:28:27.630687Z",
          "shell.execute_reply": "2023-07-06T10:28:27.647731Z"
        },
        "trusted": true,
        "id": "-bXLwx5ycoCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_graph(0,0,0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-06T10:19:03.496147Z",
          "iopub.execute_input": "2023-07-06T10:19:03.496718Z",
          "iopub.status.idle": "2023-07-06T10:19:05.076427Z",
          "shell.execute_reply.started": "2023-07-06T10:19:03.496690Z",
          "shell.execute_reply": "2023-07-06T10:19:05.075388Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFkij5z5coCd",
        "outputId": "70ac9078-be3d-4617-c115-7b0cb0817bec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([170, 13])\n",
            "2028\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'edges': tensor([[  0.,  11.],\n",
              "         [  0.,  27.],\n",
              "         [  0.,  44.],\n",
              "         ...,\n",
              "         [169., 128.],\n",
              "         [169., 139.],\n",
              "         [169., 156.]]),\n",
              " 'edge_index': tensor([[  0,   0,   0,  ..., 169, 169, 169],\n",
              "         [ 11,  27,  44,  ..., 128, 139, 156]]),\n",
              " 'node_features': tensor([[ 0.0000,  7.5000,  8.3333,  ..., 25.1500,  0.0000,  4.0000],\n",
              "         [ 0.0000, 12.0627,  4.6918,  ..., 25.1500,  0.0000,  4.0000],\n",
              "         [ 0.0000,  8.2712,  4.8383,  ..., 25.1500,  0.0000,  4.0000],\n",
              "         ...,\n",
              "         [11.0000, 56.2321, 45.8161,  ..., 25.7725,  0.0000,  4.0000],\n",
              "         [11.0000, 50.4659, 43.8516,  ..., 25.7725,  0.0000,  4.0000],\n",
              "         [11.0000, 46.8457, 36.8127,  ..., 25.7725,  0.0000,  4.0000]]),\n",
              " 'edge_features': tensor([[  15.5222,  -55.4200,    0.0000],\n",
              "         [  30.1687,  -55.4200,  -79.6500],\n",
              "         [  45.1525,  -55.4200,  -93.8600],\n",
              "         ...,\n",
              "         [  24.8402,  -64.6900, -108.8900],\n",
              "         [  10.7666,  -64.6900,  -96.9100],\n",
              "         [   7.8846,  -64.6900,  -76.9500]]),\n",
              " 'node_targets': tensor([104.9600,   7.6800,  11.0900,  14.5100,  11.9500,  10.2400,  11.0900,\n",
              "           9.3900,   3.4100,  13.6500,  11.9500,  52.0500,   5.9700,   0.8500,\n",
              "           5.1200,   0.8500,   5.9700,   5.1200,   7.6800,   5.1200,   0.0000,\n",
              "           7.6800,   0.0000,   0.8500,   3.4100,   1.7100,   1.7100,  40.1100,\n",
              "           0.0000,   3.4100,   1.7100,   1.7100,   7.6800,   1.7100,   2.5600,\n",
              "           1.7100,   2.5600,   0.0000,   1.7100,   3.4100,   1.7100,   3.4100,\n",
              "           3.4100,   3.4100, 125.4400,  10.2400,  16.2100,  11.0900,  14.5100,\n",
              "           5.1200,   2.5600,   7.6800,  10.2400,  12.8000,   7.6800,  15.3600,\n",
              "          11.9500,  34.1300,   0.0000,   5.9700,   0.8500,   3.4100,   0.8500,\n",
              "           2.5600,   2.5600,   8.5300,   4.2700,   0.8500,   0.0000,   4.2700,\n",
              "          18.7700,   0.0000,   2.5600,   1.7100,   2.5600,   2.5600,   0.0000,\n",
              "           4.2700,   0.8500,   1.7100,   0.0000,   2.5600,  44.3700,   2.5600,\n",
              "           2.5600,   2.5600,   3.4100,   5.9700,   0.0000,   1.7100,  11.0900,\n",
              "           0.0000,   0.0000,  10.2400,   0.8500,   0.8500,   2.5600,  27.1700,\n",
              "           1.3300,   0.0000,   0.6700,   0.6700,   0.5100,   3.0400,   0.7600,\n",
              "           0.0000,   0.0000,   2.5600,   0.0000,   4.2700,   5.1200,   2.2800,\n",
              "           5.9700,  90.4500,   5.9700,   9.3900,   5.9700,   8.5300,  11.9500,\n",
              "           2.5600,   6.8300,   8.5300,   5.9700,   2.5600,   1.7100,   6.8300,\n",
              "           6.8300,   6.8300,  74.2400,   3.4100,  14.5100,   6.8300,   1.7100,\n",
              "           3.4100,  14.5100,   8.5300,   5.9700,   6.8300,   8.5300,  32.2800,\n",
              "           0.8500,   1.7100,   2.5600,   0.7100,   0.8500,   0.8500,   0.8500,\n",
              "           3.4100,   1.7100,   4.2700,   5.1200,   4.2700,   1.7100,   0.8500,\n",
              "           1.7100,   0.8500,  78.5100,   2.5600,   0.0000,   1.7100,  10.2400,\n",
              "           3.4100,   8.5300,  12.8000,   1.7100,   8.5300,  17.0700,   6.8300,\n",
              "           5.1200,   0.0000])}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_geometric_graph(graph):\n",
        "    data = Data(\n",
        "        # Input graph.\n",
        "        x=graph[\"node_features\"],\n",
        "        #pos=pos,\n",
        "        edge_index=graph[\"edge_index\"],\n",
        "        edge_attr=graph[\"edge_features\"],\n",
        "        # Output node targets.\n",
        "        y=graph[\"node_targets\"],\n",
        "        num_nodes = len(graph[\"node_features\"])\n",
        "\n",
        "    )\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-06T10:19:05.078366Z",
          "iopub.execute_input": "2023-07-06T10:19:05.079352Z",
          "iopub.status.idle": "2023-07-06T10:19:05.085230Z",
          "shell.execute_reply.started": "2023-07-06T10:19:05.079300Z",
          "shell.execute_reply": "2023-07-06T10:19:05.083975Z"
        },
        "trusted": true,
        "id": "ZO3ljwu7coCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(create_geometric_graph(create_graph(0,0,0)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-06T10:19:05.086872Z",
          "iopub.execute_input": "2023-07-06T10:19:05.087282Z",
          "iopub.status.idle": "2023-07-06T10:19:06.732846Z",
          "shell.execute_reply.started": "2023-07-06T10:19:05.087249Z",
          "shell.execute_reply": "2023-07-06T10:19:06.731610Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kr7yzgfocoCf",
        "outputId": "cffacd4b-936b-45b5-cc96-6f69504caac0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([170, 13])\n",
            "2028\n",
            "Data(x=[170, 13], edge_index=[2, 2028], edge_attr=[2028, 3], y=[170], num_nodes=170)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_scatter import scatter_mean\n",
        "from torch_geometric.nn import MetaLayer\n",
        "\n",
        "class EdgeModel(torch.nn.Module):\n",
        "    def __init__(self, n_node_features, n_edge_features, hiddens, n_targets):\n",
        "        super().__init__()\n",
        "        self.edge_mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(2 * n_node_features + n_edge_features, hiddens),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hiddens, n_targets),\n",
        "        )\n",
        "\n",
        "    def forward(self, src, dest, edge_attr, u=None, batch=None):\n",
        "        #print(\"In edge model\")\n",
        "        #print(src, src.shape)\n",
        "        #print(dest, dest.shape)\n",
        "        #print(edge_attr, edge_attr.shape)\n",
        "        out = torch.cat([src, dest, edge_attr], 1)\n",
        "        out = self.edge_mlp(out)\n",
        "        #print(\"Exit edge model\")\n",
        "        return out\n",
        "\n",
        "\n",
        "class NodeModel(torch.nn.Module):\n",
        "    def __init__(self, n_node_features, hiddens, n_targets):\n",
        "        super(NodeModel, self).__init__()\n",
        "        self.node_mlp_1 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(n_node_features + hiddens, hiddens),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hiddens, hiddens),\n",
        "        )\n",
        "        self.node_mlp_2 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(n_node_features + hiddens, hiddens),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hiddens, n_targets),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
        "        #print(\"In node model\")\n",
        "        row, col = edge_index\n",
        "        out = torch.cat([x[col], edge_attr], dim=1)\n",
        "        out = self.node_mlp_1(out)\n",
        "        out = scatter_mean(out, row, dim=0, dim_size=x.size(0))\n",
        "        out = torch.cat([x, out], dim=1)\n",
        "        out = self.node_mlp_2(out)\n",
        "        #print(\"Exit node model\")\n",
        "        return out\n",
        "\n",
        "\n",
        "class MetaNet(torch.nn.Module):\n",
        "    def __init__(self, n_node_features, n_edge_features, num_hidden):\n",
        "        super(MetaNet, self).__init__()\n",
        "\n",
        "        # Input Layer\n",
        "        self.input = MetaLayer(\n",
        "            edge_model=EdgeModel(\n",
        "                n_node_features=n_node_features, n_edge_features=n_edge_features,\n",
        "                hiddens=num_hidden, n_targets=num_hidden),\n",
        "            node_model=NodeModel(n_node_features=n_node_features, hiddens=num_hidden, n_targets=num_hidden)\n",
        "            )\n",
        "\n",
        "        # Output Layer\n",
        "        self.output = MetaLayer(\n",
        "            edge_model=EdgeModel(\n",
        "                n_node_features=num_hidden, n_edge_features=num_hidden,\n",
        "                hiddens=num_hidden, n_targets=num_hidden),\n",
        "            node_model=NodeModel(n_node_features=num_hidden, hiddens=num_hidden, n_targets=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_attr, y = data.x, data.edge_index, data.edge_attr, data.y\n",
        "        #print(\"In meta model\")\n",
        "        x, edge_attr, _ = self.input(x, edge_index, edge_attr)\n",
        "        x = F.relu(x)\n",
        "        #x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x, edge_attr, _ = self.output(x, edge_index, edge_attr)\n",
        "        #x = F.dropout(x, p=0.5, training=self.training)\n",
        "        #print(\"Exit meta model\")\n",
        "        return x\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, num_input, num_hidden):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.name = \"Net\"\n",
        "\n",
        "        # Input GCN layer.\n",
        "        self.conv1 = GCNConv(num_input, num_hidden)\n",
        "        self.conv2 = GCNConv(num_hidden, num_hidden)\n",
        "        self.conv3 = GCNConv(num_hidden, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, y = data.x, data.edge_index, data.y\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = F.dropout(x, training=self.training)\n",
        "\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        return x\n",
        "\n",
        "class AttentionNet(torch.nn.Module):\n",
        "    def __init__(self, num_input, num_hidden):\n",
        "        super(AttentionNet, self).__init__()\n",
        "\n",
        "        self.name = \"AttentionNet\"\n",
        "\n",
        "        # Input GCN layer.\n",
        "        self.conv1 = GATv2Conv(num_input, num_hidden)\n",
        "        self.conv2 = GATv2Conv(num_hidden, num_hidden)\n",
        "        self.conv3 = GATv2Conv(num_hidden, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, y = data.x, data.edge_index, data.y\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = F.dropout(x, training=self.training)\n",
        "\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-06T10:20:04.103933Z",
          "iopub.execute_input": "2023-07-06T10:20:04.104351Z",
          "iopub.status.idle": "2023-07-06T10:20:04.129439Z",
          "shell.execute_reply.started": "2023-07-06T10:20:04.104294Z",
          "shell.execute_reply": "2023-07-06T10:20:04.128365Z"
        },
        "trusted": true,
        "id": "lD4l3XlKcoCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_node_features = 13\n",
        "num_edge_features = 3\n",
        "num_hidden = 128"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-06T10:20:13.785196Z",
          "iopub.execute_input": "2023-07-06T10:20:13.786261Z",
          "iopub.status.idle": "2023-07-06T10:20:13.791675Z",
          "shell.execute_reply.started": "2023-07-06T10:20:13.786213Z",
          "shell.execute_reply": "2023-07-06T10:20:13.790368Z"
        },
        "trusted": true,
        "id": "bPRRP8jlcoCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MetaNet(num_node_features, num_edge_features, num_hidden).to(device)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-06T10:20:32.677457Z",
          "iopub.execute_input": "2023-07-06T10:20:32.677829Z",
          "iopub.status.idle": "2023-07-06T10:20:34.309011Z",
          "shell.execute_reply.started": "2023-07-06T10:20:32.677800Z",
          "shell.execute_reply": "2023-07-06T10:20:34.308022Z"
        },
        "trusted": true,
        "id": "u_oMx_LLcoCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(lr=1e-4,params=model.parameters())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-06T10:20:46.697337Z",
          "iopub.execute_input": "2023-07-06T10:20:46.697896Z",
          "iopub.status.idle": "2023-07-06T10:20:46.703235Z",
          "shell.execute_reply.started": "2023-07-06T10:20:46.697865Z",
          "shell.execute_reply": "2023-07-06T10:20:46.702092Z"
        },
        "trusted": true,
        "id": "tyIe36JGcoCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataset):\n",
        "    # Monitor training.\n",
        "    losses = []\n",
        "\n",
        "    # Put model in training mode!\n",
        "    model.train()\n",
        "    i=0\n",
        "    for i, batch in enumerate(dataset):\n",
        "        #print(\"misaa\")\n",
        "        # Training step.\n",
        "\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch)\n",
        "        loss = torch.sqrt(F.mse_loss(out.squeeze(), batch.y.squeeze()))\n",
        "        #print(f\"Training oss for {i}: {loss}\")\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Monitoring\n",
        "        losses.append(loss.item())\n",
        "        if(i == 1151): break\n",
        "    # Return training metrics.\n",
        "    return losses\n",
        "\n",
        "\n",
        "def evaluate(dataset):\n",
        "    # Monitor evaluation.\n",
        "    losses = []\n",
        "\n",
        "    # Validation (1)\n",
        "    model.eval()\n",
        "    i = 0\n",
        "    for i, batch in enumerate(dataset):\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        # Calculate validation losses.\n",
        "        out = model(batch)\n",
        "        loss = torch.sqrt(F.mse_loss(out.squeeze(), batch.y.squeeze()))\n",
        "\n",
        "        # Metric logging.\n",
        "        losses.append(loss.item())\n",
        "        if(i == 383): break\n",
        "    return losses"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-06T10:20:46.966338Z",
          "iopub.execute_input": "2023-07-06T10:20:46.966933Z",
          "iopub.status.idle": "2023-07-06T10:20:46.975716Z",
          "shell.execute_reply.started": "2023-07-06T10:20:46.966903Z",
          "shell.execute_reply": "2023-07-06T10:20:46.974673Z"
        },
        "trusted": true,
        "id": "kOdrIldDcoCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "round(0.6*(df.shape[0]))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-06T10:20:47.888727Z",
          "iopub.execute_input": "2023-07-06T10:20:47.889076Z",
          "iopub.status.idle": "2023-07-06T10:20:47.896289Z",
          "shell.execute_reply.started": "2023-07-06T10:20:47.889048Z",
          "shell.execute_reply": "2023-07-06T10:20:47.895351Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsvn2_gWcoCk",
        "outputId": "f463eb59-9778-4602-e067-fea4a06c0949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "183854"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "round(0.2*(df.shape[0]))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-06T10:20:48.794968Z",
          "iopub.execute_input": "2023-07-06T10:20:48.796016Z",
          "iopub.status.idle": "2023-07-06T10:20:48.803443Z",
          "shell.execute_reply.started": "2023-07-06T10:20:48.795974Z",
          "shell.execute_reply": "2023-07-06T10:20:48.802259Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQXi6QNhcoCm",
        "outputId": "f0f626ea-ed9d-4025-da74-5a12c2514abd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "61285"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape[0]-round(0.6*(df.shape[0]))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-06T10:20:48.875480Z",
          "iopub.execute_input": "2023-07-06T10:20:48.876369Z",
          "iopub.status.idle": "2023-07-06T10:20:48.882691Z",
          "shell.execute_reply.started": "2023-07-06T10:20:48.876334Z",
          "shell.execute_reply": "2023-07-06T10:20:48.881607Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-lqESyccoCn",
        "outputId": "4c2d7798-9ba5-4e67-88e7-5d8cf3008587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "122570"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "245139+61285"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-06T10:20:49.709189Z",
          "iopub.execute_input": "2023-07-06T10:20:49.709847Z",
          "iopub.status.idle": "2023-07-06T10:20:49.716009Z",
          "shell.execute_reply.started": "2023-07-06T10:20:49.709812Z",
          "shell.execute_reply": "2023-07-06T10:20:49.715062Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxdebFj0coCn",
        "outputId": "386ee441-9ac7-4bd9-a53e-c2eb8c4625f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "306424"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-06T10:20:51.966651Z",
          "iopub.execute_input": "2023-07-06T10:20:51.967254Z",
          "iopub.status.idle": "2023-07-06T10:20:51.972521Z",
          "shell.execute_reply.started": "2023-07-06T10:20:51.967213Z",
          "shell.execute_reply": "2023-07-06T10:20:51.971278Z"
        },
        "trusted": true,
        "id": "wxbz_k7dcoCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# With train, validation and test data.\n",
        "import random\n",
        "import os\n",
        "from torch_geometric.data import InMemoryDataset, download_url, extract_zip\n",
        "# divide into training and testing points\n",
        "class CustomDataset(InMemoryDataset):\n",
        "    def __init__(self, split=\"train\", transform=None):\n",
        "        self.data = pd.read_csv(\"/content/drive/MyDrive/required_format.csv\")\n",
        "        self.split = split\n",
        "        super(CustomDataset, self).__init__( split, transform)\n",
        "        #self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "        #self.data = pd.read_csv(\"deployment_with_int_map.csv\")\n",
        "        #self.data, self.slices = pd.read_csv(\"deployment_with_int_map.csv\")\n",
        "\n",
        "        # print(\"In init\")\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        # print(\"In raw_file_names\")\n",
        "        return [\"/content/drive/MyDrive/required_format.csv\"]\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        # print(\"In processed_file_names\")\n",
        "        li = ['data_train_' + str(i) + '.pt' for i in range(1152)]+ ['data_valid_' + str(j) + '.pt' for j in range(1152, 1536)] + ['data_test_' + str(k) + '.pt' for k in range(1536, 1920)]\n",
        "        #print(li)\n",
        "        return ['data_train_' + str(i) + '.pt' for i in range(1152)]+ ['data_valid_' + str(j) + '.pt' for j in range(1152,1536)] + ['data_test_' + str(k) + '.pt' for k in range(384)]\n",
        "\n",
        "    def _download(self):\n",
        "        '''\n",
        "        print(\"In download\")\n",
        "        path = download_url(self.url, self.raw_dir)\n",
        "        extract_zip(path, self.raw_dir)\n",
        "        # The zip file is removed\n",
        "        os.unlink(path)\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "    def process(self):\n",
        "        print(\"In process\")\n",
        "        #df = pd.read_csv(self.raw_paths[0])\n",
        "        X = self.data[['0', '1', '2', '3', '4', '5', '6', '7','8', '9', '10', '11', 'wlan_code_index', 'x(m)', 'y(m)','z(m)',\n",
        "            'primary_channel', 'min_channel_allowed', 'max_channel_allowed', 'node_type','rssi',\n",
        "            'sinr', 'air_time_mean','channel_bonding_model','deployment']]\n",
        "        y = self.data.loc[:, [\"throughput\", \"deployment\"]]\n",
        "#         X_train = X.iloc[:183854, :]\n",
        "#         X_valid = X.iloc[183854:245139, :]\n",
        "#         X_test = X.iloc[245139:,:]\n",
        "#         print(X_test.columns)\n",
        "#         y_train = y.iloc[:183854, :]\n",
        "#         y_valid = y.iloc[183854:245139, :]\n",
        "#         y_test = y.iloc[245139:,:]\n",
        "        graphs = []\n",
        "        # print(\"Here\")\n",
        "        l = [i for i in range(1920)]\n",
        "        self.l_train = random.sample(l, 1152)\n",
        "        l = [x for x in l if x not in self.l_train]\n",
        "        self.l_valid = random.sample(l, 384)\n",
        "        l = [x for x in l if x not in self.l_valid]\n",
        "        self.l_test = l\n",
        "        count = 0\n",
        "        if(self.split == \"train\"):\n",
        "\n",
        "            for i in self.l_train:\n",
        "\n",
        "                #X_train = torch.tensor(X_train.to_numpy(), dtype=torch.float)\n",
        "                #y_train = torch.tensor(y_train.to_numpy(), dtype=torch.float)\n",
        "                graph = create_graph(X, y, i)\n",
        "\n",
        "                graph = create_geometric_graph(graph)\n",
        "                graphs.append(graph)\n",
        "\n",
        "                torch.save(graph, os.path.join(self.processed_dir, f'data_train_{count}.pt'))\n",
        "                count += 1\n",
        "        elif(self.split == \"valid\"):\n",
        "            for i in self.l_valid:\n",
        "                #X_test = torch.tensor(X_test.to_numpy(), dtype=torch.float)\n",
        "                #y_test = torch.tensor(y_test.to_numpy(), dtype=torch.float)\n",
        "                graph = create_graph(X, y, i)\n",
        "                graph = create_geometric_graph(graph)\n",
        "                graphs.append(graph)\n",
        "\n",
        "                torch.save(graph, os.path.join(self.processed_dir, f'data_valid_{count}.pt'))\n",
        "                count += 1\n",
        "        else:\n",
        "            for i in self.l_test:\n",
        "                #X_test = torch.tensor(X_test.to_numpy(), dtype=torch.float)\n",
        "                #y_test = torch.tensor(y_test.to_numpy(), dtype=torch.float)\n",
        "                graph = create_graph(X, y, i)\n",
        "                graph = create_geometric_graph(graph)\n",
        "                graphs.append(graph)\n",
        "\n",
        "                torch.save(graph, os.path.join(self.processed_dir, f'data_test_{count}.pt'))\n",
        "                count += 1\n",
        "        #return graphs[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        if(self.split == \"train\"):\n",
        "            #return len(self.processed_file_names[0])\n",
        "            return 1152\n",
        "        elif self.split == \"valid\":\n",
        "            return 384\n",
        "        else:\n",
        "            return 384\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #print(\"Part: \", self.processed_file_names[1])\n",
        "\n",
        "        if(self.split == \"train\"):\n",
        "            data = torch.load(os.path.join(self.processed_dir, f'data_train_{idx}.pt'))\n",
        "        elif(self.split == \"valid\"):\n",
        "            data = torch.load(os.path.join(self.processed_dir, f'data_valid_{idx}.pt'))\n",
        "        elif (self.split==\"test\"):\n",
        "            data = torch.load(os.path.join(self.processed_dir, f'data_test_{idx}.pt'))\n",
        "        return data"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-06T10:26:27.926425Z",
          "iopub.execute_input": "2023-07-06T10:26:27.926786Z",
          "iopub.status.idle": "2023-07-06T10:26:27.948100Z",
          "shell.execute_reply.started": "2023-07-06T10:26:27.926757Z",
          "shell.execute_reply": "2023-07-06T10:26:27.947207Z"
        },
        "trusted": true,
        "id": "y-oS27FycoCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = CustomDataset( split='train')\n",
        "dataset_valid = CustomDataset( split='valid')\n",
        "dataset_test = CustomDataset( split='test')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-06T10:28:36.315907Z",
          "iopub.execute_input": "2023-07-06T10:28:36.317595Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXdnYhegcoCq",
        "outputId": "f4dc11f5-34f8-413a-dd71-ce2c51620658"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In process\n",
            "torch.Size([147, 13])\n",
            "1460\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([122, 13])\n",
            "968\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([151, 13])\n",
            "1500\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([148, 13])\n",
            "1470\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([146, 13])\n",
            "1450\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([137, 13])\n",
            "1088\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([151, 13])\n",
            "1500\n",
            "torch.Size([111, 13])\n",
            "880\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([122, 13])\n",
            "968\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([201, 13])\n",
            "2400\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([151, 13])\n",
            "1500\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([178, 13])\n",
            "1770\n",
            "torch.Size([173, 13])\n",
            "2064\n",
            "torch.Size([145, 13])\n",
            "1440\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([181, 13])\n",
            "2160\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([122, 13])\n",
            "968\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([176, 13])\n",
            "1750\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([149, 13])\n",
            "1480\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([174, 13])\n",
            "2076\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([145, 13])\n",
            "1440\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([144, 13])\n",
            "1430\n",
            "torch.Size([179, 13])\n",
            "1780\n",
            "torch.Size([146, 13])\n",
            "1450\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([198, 13])\n",
            "2364\n",
            "torch.Size([141, 13])\n",
            "1120\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([110, 13])\n",
            "872\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([147, 13])\n",
            "1460\n",
            "torch.Size([176, 13])\n",
            "1750\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([171, 13])\n",
            "2040\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([202, 13])\n",
            "2412\n",
            "torch.Size([108, 13])\n",
            "856\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([116, 13])\n",
            "920\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([199, 13])\n",
            "2376\n",
            "torch.Size([115, 13])\n",
            "912\n",
            "torch.Size([173, 13])\n",
            "1720\n",
            "torch.Size([221, 13])\n",
            "2640\n",
            "torch.Size([116, 13])\n",
            "920\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([131, 13])\n",
            "1040\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([178, 13])\n",
            "1770\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([227, 13])\n",
            "2712\n",
            "torch.Size([108, 13])\n",
            "856\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([168, 13])\n",
            "1670\n",
            "torch.Size([212, 13])\n",
            "2532\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([142, 13])\n",
            "1128\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([173, 13])\n",
            "2064\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([169, 13])\n",
            "2016\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([213, 13])\n",
            "2544\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([139, 13])\n",
            "1380\n",
            "torch.Size([179, 13])\n",
            "2136\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([170, 13])\n",
            "1690\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([131, 13])\n",
            "1040\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([197, 13])\n",
            "2352\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([201, 13])\n",
            "2400\n",
            "torch.Size([199, 13])\n",
            "2376\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([143, 13])\n",
            "1136\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([168, 13])\n",
            "1670\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([179, 13])\n",
            "2136\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([148, 13])\n",
            "1470\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([221, 13])\n",
            "2640\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([198, 13])\n",
            "2364\n",
            "torch.Size([148, 13])\n",
            "1470\n",
            "torch.Size([174, 13])\n",
            "1730\n",
            "torch.Size([206, 13])\n",
            "2460\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([142, 13])\n",
            "1410\n",
            "torch.Size([173, 13])\n",
            "1720\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([207, 13])\n",
            "2472\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([144, 13])\n",
            "1144\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([145, 13])\n",
            "1440\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([179, 13])\n",
            "1780\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([164, 13])\n",
            "1956\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([141, 13])\n",
            "1120\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([116, 13])\n",
            "920\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([180, 13])\n",
            "1790\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([149, 13])\n",
            "1480\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([198, 13])\n",
            "2364\n",
            "torch.Size([219, 13])\n",
            "2616\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([115, 13])\n",
            "912\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([181, 13])\n",
            "2160\n",
            "torch.Size([201, 13])\n",
            "2400\n",
            "torch.Size([203, 13])\n",
            "2424\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([174, 13])\n",
            "1730\n",
            "torch.Size([141, 13])\n",
            "1120\n",
            "torch.Size([146, 13])\n",
            "1450\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([212, 13])\n",
            "2532\n",
            "torch.Size([199, 13])\n",
            "2376\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([109, 13])\n",
            "864\n",
            "torch.Size([140, 13])\n",
            "1112\n",
            "torch.Size([109, 13])\n",
            "864\n",
            "torch.Size([174, 13])\n",
            "2076\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([149, 13])\n",
            "1480\n",
            "torch.Size([201, 13])\n",
            "2400\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([113, 13])\n",
            "896\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([145, 13])\n",
            "1152\n",
            "torch.Size([110, 13])\n",
            "872\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([176, 13])\n",
            "1750\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([180, 13])\n",
            "1790\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([208, 13])\n",
            "2484\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([122, 13])\n",
            "968\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([180, 13])\n",
            "2148\n",
            "torch.Size([208, 13])\n",
            "2484\n",
            "torch.Size([137, 13])\n",
            "1088\n",
            "torch.Size([183, 13])\n",
            "2184\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([151, 13])\n",
            "1500\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([175, 13])\n",
            "2088\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([143, 13])\n",
            "1136\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([115, 13])\n",
            "912\n",
            "torch.Size([206, 13])\n",
            "2460\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([207, 13])\n",
            "2472\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([137, 13])\n",
            "1088\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([157, 13])\n",
            "1560\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([181, 13])\n",
            "2160\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([117, 13])\n",
            "928\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([135, 13])\n",
            "1340\n",
            "torch.Size([192, 13])\n",
            "2292\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([175, 13])\n",
            "2088\n",
            "torch.Size([168, 13])\n",
            "1670\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([201, 13])\n",
            "2400\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([179, 13])\n",
            "2136\n",
            "torch.Size([197, 13])\n",
            "2352\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([151, 13])\n",
            "1500\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([117, 13])\n",
            "928\n",
            "torch.Size([196, 13])\n",
            "2340\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([182, 13])\n",
            "2172\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([113, 13])\n",
            "896\n",
            "torch.Size([178, 13])\n",
            "2124\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([176, 13])\n",
            "2100\n",
            "torch.Size([197, 13])\n",
            "2352\n",
            "torch.Size([206, 13])\n",
            "2460\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([139, 13])\n",
            "1380\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([206, 13])\n",
            "2460\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([145, 13])\n",
            "1440\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([183, 13])\n",
            "2184\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([157, 13])\n",
            "1560\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([211, 13])\n",
            "2520\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([202, 13])\n",
            "2412\n",
            "torch.Size([144, 13])\n",
            "1430\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([168, 13])\n",
            "1670\n",
            "torch.Size([170, 13])\n",
            "1690\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([199, 13])\n",
            "2376\n",
            "torch.Size([176, 13])\n",
            "2100\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([149, 13])\n",
            "1480\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([143, 13])\n",
            "1420\n",
            "torch.Size([173, 13])\n",
            "1720\n",
            "torch.Size([151, 13])\n",
            "1500\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([212, 13])\n",
            "2532\n",
            "torch.Size([196, 13])\n",
            "2340\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([176, 13])\n",
            "1750\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([174, 13])\n",
            "2076\n",
            "torch.Size([164, 13])\n",
            "1956\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([175, 13])\n",
            "2088\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([146, 13])\n",
            "1450\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([114, 13])\n",
            "904\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([149, 13])\n",
            "1480\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([108, 13])\n",
            "856\n",
            "torch.Size([169, 13])\n",
            "2016\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([183, 13])\n",
            "1820\n",
            "torch.Size([179, 13])\n",
            "2136\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([115, 13])\n",
            "912\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([202, 13])\n",
            "2412\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([116, 13])\n",
            "920\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([151, 13])\n",
            "1500\n",
            "torch.Size([170, 13])\n",
            "1690\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([208, 13])\n",
            "2484\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([182, 13])\n",
            "1810\n",
            "torch.Size([170, 13])\n",
            "1690\n",
            "torch.Size([131, 13])\n",
            "1040\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([206, 13])\n",
            "2460\n",
            "torch.Size([108, 13])\n",
            "856\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([143, 13])\n",
            "1136\n",
            "torch.Size([122, 13])\n",
            "968\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([142, 13])\n",
            "1128\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([110, 13])\n",
            "872\n",
            "torch.Size([170, 13])\n",
            "1690\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([201, 13])\n",
            "2400\n",
            "torch.Size([199, 13])\n",
            "2376\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([178, 13])\n",
            "2124\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([140, 13])\n",
            "1390\n",
            "torch.Size([180, 13])\n",
            "2148\n",
            "torch.Size([223, 13])\n",
            "2664\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([146, 13])\n",
            "1450\n",
            "torch.Size([145, 13])\n",
            "1440\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([201, 13])\n",
            "2400\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([141, 13])\n",
            "1120\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([180, 13])\n",
            "2148\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([221, 13])\n",
            "2640\n",
            "torch.Size([179, 13])\n",
            "2136\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([109, 13])\n",
            "864\n",
            "torch.Size([115, 13])\n",
            "912\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([168, 13])\n",
            "1670\n",
            "torch.Size([122, 13])\n",
            "968\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([216, 13])\n",
            "2580\n",
            "torch.Size([140, 13])\n",
            "1390\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([142, 13])\n",
            "1410\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([142, 13])\n",
            "1410\n",
            "torch.Size([198, 13])\n",
            "2364\n",
            "torch.Size([221, 13])\n",
            "2640\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([210, 13])\n",
            "2508\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([139, 13])\n",
            "1380\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([180, 13])\n",
            "2148\n",
            "torch.Size([209, 13])\n",
            "2496\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([144, 13])\n",
            "1430\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([139, 13])\n",
            "1104\n",
            "torch.Size([172, 13])\n",
            "2052\n",
            "torch.Size([196, 13])\n",
            "2340\n",
            "torch.Size([151, 13])\n",
            "1500\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([122, 13])\n",
            "968\n",
            "torch.Size([146, 13])\n",
            "1450\n",
            "torch.Size([173, 13])\n",
            "1720\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([144, 13])\n",
            "1144\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([148, 13])\n",
            "1470\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([197, 13])\n",
            "2352\n",
            "torch.Size([108, 13])\n",
            "856\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([148, 13])\n",
            "1470\n",
            "torch.Size([142, 13])\n",
            "1128\n",
            "torch.Size([202, 13])\n",
            "2412\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([146, 13])\n",
            "1160\n",
            "torch.Size([209, 13])\n",
            "2496\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([199, 13])\n",
            "2376\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([182, 13])\n",
            "2172\n",
            "torch.Size([216, 13])\n",
            "2580\n",
            "torch.Size([196, 13])\n",
            "2340\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([201, 13])\n",
            "2400\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([174, 13])\n",
            "2076\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([197, 13])\n",
            "2352\n",
            "torch.Size([170, 13])\n",
            "1690\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([172, 13])\n",
            "2052\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([140, 13])\n",
            "1112\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([144, 13])\n",
            "1430\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([102, 13])\n",
            "808\n",
            "torch.Size([211, 13])\n",
            "2520\n",
            "torch.Size([122, 13])\n",
            "968\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([141, 13])\n",
            "1120\n",
            "torch.Size([117, 13])\n",
            "928\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([139, 13])\n",
            "1380\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([131, 13])\n",
            "1040\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([151, 13])\n",
            "1500\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([143, 13])\n",
            "1420\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([148, 13])\n",
            "1470\n",
            "torch.Size([142, 13])\n",
            "1128\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([144, 13])\n",
            "1144\n",
            "torch.Size([192, 13])\n",
            "2292\n",
            "torch.Size([117, 13])\n",
            "928\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([157, 13])\n",
            "1560\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([175, 13])\n",
            "1740\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([182, 13])\n",
            "1810\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([112, 13])\n",
            "888\n",
            "torch.Size([149, 13])\n",
            "1480\n",
            "torch.Size([137, 13])\n",
            "1088\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([157, 13])\n",
            "1560\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([173, 13])\n",
            "1720\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([174, 13])\n",
            "1730\n",
            "torch.Size([145, 13])\n",
            "1152\n",
            "torch.Size([145, 13])\n",
            "1440\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([141, 13])\n",
            "1120\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([149, 13])\n",
            "1480\n",
            "torch.Size([209, 13])\n",
            "2496\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([178, 13])\n",
            "1770\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([148, 13])\n",
            "1470\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([182, 13])\n",
            "2172\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([206, 13])\n",
            "2460\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([180, 13])\n",
            "2148\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([146, 13])\n",
            "1450\n",
            "torch.Size([131, 13])\n",
            "1040\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([115, 13])\n",
            "912\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([203, 13])\n",
            "2424\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([168, 13])\n",
            "2004\n",
            "torch.Size([115, 13])\n",
            "912\n",
            "torch.Size([166, 13])\n",
            "1980\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([167, 13])\n",
            "1992\n",
            "torch.Size([202, 13])\n",
            "2412\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([172, 13])\n",
            "2052\n",
            "torch.Size([137, 13])\n",
            "1088\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([139, 13])\n",
            "1380\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([179, 13])\n",
            "2136\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([148, 13])\n",
            "1470\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([142, 13])\n",
            "1128\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([170, 13])\n",
            "2028\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([148, 13])\n",
            "1470\n",
            "torch.Size([157, 13])\n",
            "1560\n",
            "torch.Size([180, 13])\n",
            "2148\n",
            "torch.Size([214, 13])\n",
            "2556\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([168, 13])\n",
            "1670\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([144, 13])\n",
            "1430\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([138, 13])\n",
            "1370\n",
            "torch.Size([192, 13])\n",
            "2292\n",
            "torch.Size([183, 13])\n",
            "2184\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([174, 13])\n",
            "1730\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([140, 13])\n",
            "1390\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([131, 13])\n",
            "1040\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([213, 13])\n",
            "2544\n",
            "torch.Size([170, 13])\n",
            "1690\n",
            "torch.Size([168, 13])\n",
            "1670\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([115, 13])\n",
            "912\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([179, 13])\n",
            "2136\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([122, 13])\n",
            "968\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([143, 13])\n",
            "1420\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([198, 13])\n",
            "2364\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([220, 13])\n",
            "2628\n",
            "torch.Size([106, 13])\n",
            "840\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([192, 13])\n",
            "2292\n",
            "torch.Size([192, 13])\n",
            "2292\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([182, 13])\n",
            "2172\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([137, 13])\n",
            "1088\n",
            "torch.Size([114, 13])\n",
            "904\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([192, 13])\n",
            "2292\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([182, 13])\n",
            "2172\n",
            "torch.Size([173, 13])\n",
            "2064\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([179, 13])\n",
            "1780\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([146, 13])\n",
            "1450\n",
            "torch.Size([110, 13])\n",
            "872\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([131, 13])\n",
            "1040\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([151, 13])\n",
            "1500\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([103, 13])\n",
            "816\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([182, 13])\n",
            "1810\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([206, 13])\n",
            "2460\n",
            "torch.Size([201, 13])\n",
            "2400\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([146, 13])\n",
            "1450\n",
            "torch.Size([151, 13])\n",
            "1500\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([116, 13])\n",
            "920\n",
            "torch.Size([168, 13])\n",
            "1670\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([178, 13])\n",
            "2124\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([192, 13])\n",
            "2292\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([167, 13])\n",
            "1992\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([117, 13])\n",
            "928\n",
            "torch.Size([197, 13])\n",
            "2352\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([208, 13])\n",
            "2484\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([178, 13])\n",
            "2124\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([116, 13])\n",
            "920\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([149, 13])\n",
            "1480\n",
            "torch.Size([113, 13])\n",
            "896\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([116, 13])\n",
            "920\n",
            "torch.Size([192, 13])\n",
            "2292\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([234, 13])\n",
            "2796\n",
            "torch.Size([144, 13])\n",
            "1144\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([170, 13])\n",
            "2028\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([157, 13])\n",
            "1560\n",
            "torch.Size([179, 13])\n",
            "2136\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([196, 13])\n",
            "2340\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([131, 13])\n",
            "1040\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([224, 13])\n",
            "2676\n",
            "torch.Size([181, 13])\n",
            "2160\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([139, 13])\n",
            "1104\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([179, 13])\n",
            "2136\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([174, 13])\n",
            "1730\n",
            "torch.Size([112, 13])\n",
            "888\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([146, 13])\n",
            "1160\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([144, 13])\n",
            "1430\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([206, 13])\n",
            "2460\n",
            "torch.Size([203, 13])\n",
            "2424\n",
            "torch.Size([198, 13])\n",
            "2364\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([178, 13])\n",
            "2124\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([141, 13])\n",
            "1400\n",
            "torch.Size([180, 13])\n",
            "2148\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([157, 13])\n",
            "1560\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([151, 13])\n",
            "1500\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([215, 13])\n",
            "2568\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([135, 13])\n",
            "1340\n",
            "torch.Size([141, 13])\n",
            "1120\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([115, 13])\n",
            "912\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([170, 13])\n",
            "2028\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([148, 13])\n",
            "1176\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([202, 13])\n",
            "2412\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([142, 13])\n",
            "1410\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([180, 13])\n",
            "2148\n",
            "torch.Size([180, 13])\n",
            "2148\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([210, 13])\n",
            "2508\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([183, 13])\n",
            "2184\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([145, 13])\n",
            "1152\n",
            "torch.Size([199, 13])\n",
            "2376\n",
            "torch.Size([175, 13])\n",
            "1740\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([146, 13])\n",
            "1160\n",
            "torch.Size([122, 13])\n",
            "968\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([111, 13])\n",
            "880\n",
            "torch.Size([183, 13])\n",
            "2184\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([170, 13])\n",
            "1690\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([220, 13])\n",
            "2628\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([115, 13])\n",
            "912\n",
            "torch.Size([157, 13])\n",
            "1560\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([157, 13])\n",
            "1560\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([220, 13])\n",
            "2628\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([210, 13])\n",
            "2508\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([192, 13])\n",
            "2292\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([183, 13])\n",
            "2184\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([149, 13])\n",
            "1480\n",
            "torch.Size([157, 13])\n",
            "1560\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([116, 13])\n",
            "920\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([131, 13])\n",
            "1040\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([181, 13])\n",
            "2160\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([212, 13])\n",
            "2532\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([145, 13])\n",
            "1440\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([137, 13])\n",
            "1088\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([175, 13])\n",
            "1740\n",
            "torch.Size([198, 13])\n",
            "2364\n",
            "torch.Size([163, 13])\n",
            "1944\n",
            "torch.Size([178, 13])\n",
            "2124\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([150, 13])\n",
            "1192\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([131, 13])\n",
            "1040\n",
            "torch.Size([198, 13])\n",
            "2364\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([198, 13])\n",
            "2364\n",
            "torch.Size([212, 13])\n",
            "2532\n",
            "torch.Size([140, 13])\n",
            "1112\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([129, 13])\n",
            "1280\n",
            "torch.Size([178, 13])\n",
            "2124\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([211, 13])\n",
            "2520\n",
            "torch.Size([116, 13])\n",
            "920\n",
            "torch.Size([216, 13])\n",
            "2580\n",
            "torch.Size([198, 13])\n",
            "2364\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([115, 13])\n",
            "912\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([175, 13])\n",
            "2088\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([169, 13])\n",
            "2016\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([196, 13])\n",
            "2340\n",
            "torch.Size([140, 13])\n",
            "1112\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([201, 13])\n",
            "2400\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([141, 13])\n",
            "1400\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([182, 13])\n",
            "2172\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([169, 13])\n",
            "2016\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([171, 13])\n",
            "2040\n",
            "torch.Size([166, 13])\n",
            "1650\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In process\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([198, 13])\n",
            "2364\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([142, 13])\n",
            "1128\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([169, 13])\n",
            "2016\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([166, 13])\n",
            "1980\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([108, 13])\n",
            "856\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([215, 13])\n",
            "2568\n",
            "torch.Size([112, 13])\n",
            "888\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([141, 13])\n",
            "1400\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([143, 13])\n",
            "1136\n",
            "torch.Size([140, 13])\n",
            "1390\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([117, 13])\n",
            "928\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([170, 13])\n",
            "1690\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([175, 13])\n",
            "1740\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([137, 13])\n",
            "1088\n",
            "torch.Size([157, 13])\n",
            "1560\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([157, 13])\n",
            "1560\n",
            "torch.Size([142, 13])\n",
            "1128\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([174, 13])\n",
            "1730\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([202, 13])\n",
            "2412\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([201, 13])\n",
            "2400\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([198, 13])\n",
            "2364\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([143, 13])\n",
            "1420\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([146, 13])\n",
            "1160\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([203, 13])\n",
            "2424\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([146, 13])\n",
            "1450\n",
            "torch.Size([199, 13])\n",
            "2376\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([143, 13])\n",
            "1420\n",
            "torch.Size([206, 13])\n",
            "2460\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([198, 13])\n",
            "2364\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([206, 13])\n",
            "2460\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([196, 13])\n",
            "2340\n",
            "torch.Size([115, 13])\n",
            "912\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([122, 13])\n",
            "968\n",
            "torch.Size([169, 13])\n",
            "2016\n",
            "torch.Size([145, 13])\n",
            "1440\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([180, 13])\n",
            "2148\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([168, 13])\n",
            "1670\n",
            "torch.Size([197, 13])\n",
            "2352\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([192, 13])\n",
            "2292\n",
            "torch.Size([179, 13])\n",
            "2136\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([172, 13])\n",
            "2052\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([110, 13])\n",
            "872\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([175, 13])\n",
            "2088\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([212, 13])\n",
            "2532\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([131, 13])\n",
            "1040\n",
            "torch.Size([192, 13])\n",
            "2292\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([196, 13])\n",
            "2340\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([192, 13])\n",
            "2292\n",
            "torch.Size([168, 13])\n",
            "1670\n",
            "torch.Size([139, 13])\n",
            "1104\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([181, 13])\n",
            "1800\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([117, 13])\n",
            "928\n",
            "torch.Size([202, 13])\n",
            "2412\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([183, 13])\n",
            "1820\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([157, 13])\n",
            "1560\n",
            "torch.Size([151, 13])\n",
            "1500\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([181, 13])\n",
            "2160\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([212, 13])\n",
            "2532\n",
            "torch.Size([172, 13])\n",
            "2052\n",
            "torch.Size([173, 13])\n",
            "1720\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([208, 13])\n",
            "2484\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([182, 13])\n",
            "1810\n",
            "torch.Size([206, 13])\n",
            "2460\n",
            "torch.Size([116, 13])\n",
            "920\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([174, 13])\n",
            "1730\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([179, 13])\n",
            "2136\n",
            "torch.Size([174, 13])\n",
            "2076\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([131, 13])\n",
            "1040\n",
            "torch.Size([141, 13])\n",
            "1120\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([227, 13])\n",
            "2712\n",
            "torch.Size([168, 13])\n",
            "1670\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([206, 13])\n",
            "2460\n",
            "torch.Size([202, 13])\n",
            "2412\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([170, 13])\n",
            "2028\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([202, 13])\n",
            "2412\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([170, 13])\n",
            "1690\n",
            "torch.Size([179, 13])\n",
            "2136\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([142, 13])\n",
            "1128\n",
            "torch.Size([176, 13])\n",
            "1750\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([192, 13])\n",
            "2292\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([198, 13])\n",
            "2364\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([197, 13])\n",
            "2352\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([144, 13])\n",
            "1430\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([180, 13])\n",
            "1790\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([113, 13])\n",
            "896\n",
            "torch.Size([199, 13])\n",
            "2376\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([182, 13])\n",
            "2172\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([178, 13])\n",
            "2124\n",
            "torch.Size([141, 13])\n",
            "1400\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([141, 13])\n",
            "1120\n",
            "torch.Size([178, 13])\n",
            "2124\n",
            "torch.Size([197, 13])\n",
            "2352\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([151, 13])\n",
            "1500\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([151, 13])\n",
            "1500\n",
            "torch.Size([145, 13])\n",
            "1440\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([141, 13])\n",
            "1400\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([148, 13])\n",
            "1470\n",
            "torch.Size([117, 13])\n",
            "928\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([142, 13])\n",
            "1128\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([180, 13])\n",
            "2148\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([199, 13])\n",
            "2376\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([156, 13])\n",
            "1860\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([157, 13])\n",
            "1560\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([147, 13])\n",
            "1460\n",
            "torch.Size([177, 13])\n",
            "1760\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([173, 13])\n",
            "1720\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([199, 13])\n",
            "2376\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([178, 13])\n",
            "1770\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([139, 13])\n",
            "1104\n",
            "torch.Size([180, 13])\n",
            "2148\n",
            "torch.Size([139, 13])\n",
            "1380\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([140, 13])\n",
            "1112\n",
            "torch.Size([149, 13])\n",
            "1480\n",
            "torch.Size([215, 13])\n",
            "2568\n",
            "torch.Size([197, 13])\n",
            "2352\n",
            "torch.Size([211, 13])\n",
            "2520\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([174, 13])\n",
            "1730\n",
            "torch.Size([142, 13])\n",
            "1410\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([192, 13])\n",
            "2292\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([175, 13])\n",
            "1740\n",
            "torch.Size([141, 13])\n",
            "1120\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([183, 13])\n",
            "2184\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([157, 13])\n",
            "1560\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([197, 13])\n",
            "2352\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([168, 13])\n",
            "1670\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([176, 13])\n",
            "1750\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([113, 13])\n",
            "896\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([178, 13])\n",
            "2124\n",
            "torch.Size([145, 13])\n",
            "1152\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([110, 13])\n",
            "872\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([114, 13])\n",
            "904\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([168, 13])\n",
            "1670\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([131, 13])\n",
            "1040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In process\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([234, 13])\n",
            "2796\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([202, 13])\n",
            "2412\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([178, 13])\n",
            "2124\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([207, 13])\n",
            "2472\n",
            "torch.Size([226, 13])\n",
            "2700\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([181, 13])\n",
            "2160\n",
            "torch.Size([202, 13])\n",
            "2412\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([201, 13])\n",
            "2400\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([215, 13])\n",
            "2568\n",
            "torch.Size([208, 13])\n",
            "2484\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([172, 13])\n",
            "2052\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([181, 13])\n",
            "2160\n",
            "torch.Size([179, 13])\n",
            "2136\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([197, 13])\n",
            "2352\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([192, 13])\n",
            "2292\n",
            "torch.Size([203, 13])\n",
            "2424\n",
            "torch.Size([174, 13])\n",
            "2076\n",
            "torch.Size([182, 13])\n",
            "2172\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([209, 13])\n",
            "2496\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([198, 13])\n",
            "2364\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([198, 13])\n",
            "2364\n",
            "torch.Size([220, 13])\n",
            "2628\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([179, 13])\n",
            "2136\n",
            "torch.Size([183, 13])\n",
            "2184\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([181, 13])\n",
            "2160\n",
            "torch.Size([171, 13])\n",
            "2040\n",
            "torch.Size([170, 13])\n",
            "2028\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([175, 13])\n",
            "2088\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([179, 13])\n",
            "2136\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([192, 13])\n",
            "2292\n",
            "torch.Size([179, 13])\n",
            "2136\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([202, 13])\n",
            "2412\n",
            "torch.Size([199, 13])\n",
            "2376\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([169, 13])\n",
            "2016\n",
            "torch.Size([179, 13])\n",
            "2136\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([207, 13])\n",
            "2472\n",
            "torch.Size([202, 13])\n",
            "2412\n",
            "torch.Size([216, 13])\n",
            "2580\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([179, 13])\n",
            "2136\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([199, 13])\n",
            "2376\n",
            "torch.Size([183, 13])\n",
            "2184\n",
            "torch.Size([201, 13])\n",
            "2400\n",
            "torch.Size([229, 13])\n",
            "2736\n",
            "torch.Size([210, 13])\n",
            "2508\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([198, 13])\n",
            "2364\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([169, 13])\n",
            "2016\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([206, 13])\n",
            "2460\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([176, 13])\n",
            "2100\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([175, 13])\n",
            "2088\n",
            "torch.Size([216, 13])\n",
            "2580\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([196, 13])\n",
            "2340\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([209, 13])\n",
            "2496\n",
            "torch.Size([213, 13])\n",
            "2544\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([168, 13])\n",
            "2004\n",
            "torch.Size([192, 13])\n",
            "2292\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([180, 13])\n",
            "2148\n",
            "torch.Size([183, 13])\n",
            "2184\n",
            "torch.Size([196, 13])\n",
            "2340\n",
            "torch.Size([181, 13])\n",
            "2160\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([134, 13])\n",
            "1330\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([168, 13])\n",
            "1670\n",
            "torch.Size([145, 13])\n",
            "1440\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([146, 13])\n",
            "1450\n",
            "torch.Size([151, 13])\n",
            "1500\n",
            "torch.Size([179, 13])\n",
            "1780\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([170, 13])\n",
            "1690\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([178, 13])\n",
            "1770\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([149, 13])\n",
            "1480\n",
            "torch.Size([176, 13])\n",
            "1750\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([175, 13])\n",
            "1740\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([144, 13])\n",
            "1430\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([139, 13])\n",
            "1380\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([168, 13])\n",
            "1670\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([148, 13])\n",
            "1470\n",
            "torch.Size([141, 13])\n",
            "1400\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([174, 13])\n",
            "1730\n",
            "torch.Size([157, 13])\n",
            "1560\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([170, 13])\n",
            "1690\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([145, 13])\n",
            "1440\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([168, 13])\n",
            "1670\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([142, 13])\n",
            "1410\n",
            "torch.Size([139, 13])\n",
            "1380\n",
            "torch.Size([148, 13])\n",
            "1470\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([145, 13])\n",
            "1440\n",
            "torch.Size([173, 13])\n",
            "1720\n",
            "torch.Size([146, 13])\n",
            "1450\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([183, 13])\n",
            "1820\n",
            "torch.Size([157, 13])\n",
            "1560\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([185, 13])\n",
            "1840\n",
            "torch.Size([182, 13])\n",
            "1810\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([178, 13])\n",
            "1770\n",
            "torch.Size([173, 13])\n",
            "1720\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([140, 13])\n",
            "1390\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([144, 13])\n",
            "1144\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([137, 13])\n",
            "1088\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([141, 13])\n",
            "1120\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([145, 13])\n",
            "1152\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([116, 13])\n",
            "920\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([111, 13])\n",
            "880\n",
            "torch.Size([116, 13])\n",
            "920\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([137, 13])\n",
            "1088\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([144, 13])\n",
            "1144\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([131, 13])\n",
            "1040\n",
            "torch.Size([131, 13])\n",
            "1040\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([143, 13])\n",
            "1136\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([122, 13])\n",
            "968\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([139, 13])\n",
            "1104\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([114, 13])\n",
            "904\n",
            "torch.Size([139, 13])\n",
            "1104\n",
            "torch.Size([137, 13])\n",
            "1088\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([108, 13])\n",
            "856\n",
            "torch.Size([131, 13])\n",
            "1040\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([131, 13])\n",
            "1040\n",
            "torch.Size([111, 13])\n",
            "880\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([117, 13])\n",
            "928\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([143, 13])\n",
            "1136\n",
            "torch.Size([110, 13])\n",
            "872\n",
            "torch.Size([110, 13])\n",
            "872\n",
            "torch.Size([117, 13])\n",
            "928\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([117, 13])\n",
            "928\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([146, 13])\n",
            "1160\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([151, 13])\n",
            "1200\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([109, 13])\n",
            "864\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([141, 13])\n",
            "1120\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([110, 13])\n",
            "872\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([110, 13])\n",
            "872\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([140, 13])\n",
            "1112\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([116, 13])\n",
            "920\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([128, 13])\n",
            "1016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "from torch_geometric.data import DataLoader\n",
        "train_loader = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(dataset_valid, batch_size=3, shuffle=True)\n",
        "test_loader = DataLoader(dataset_test, batch_size=1, shuffle=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "jrn8HVXecoCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in valid_loader:\n",
        "    print(batch)\n",
        "    break"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lip8xSKacoCr",
        "outputId": "f6d11ae8-44f5-450c-81a9-6d0339aca9c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataBatch(x=[449, 13], edge_index=[2, 4548], edge_attr=[4548, 3], y=[449], num_nodes=449, batch=[449], ptr=[4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Metrics recorder per epoch.\n",
        "train_losses = []\n",
        "\n",
        "valid_losses = []\n",
        "valid_losses_corrected = []\n",
        "\n",
        "# Training loop.\n",
        "model.train()\n",
        "for epoch in range(2000):\n",
        "    # Train.\n",
        "    train_epoch_losses = train(train_loader)\n",
        "    print(f\"Epoch: {epoch}, Len of Training loss: {len(train_epoch_losses)}, Average loss: {float(np.sum(train_epoch_losses))/len(train_epoch_losses)}\")\n",
        "    train_losses.append(np.mean(train_epoch_losses))\n",
        "\n",
        "    valid_epoch_losses= evaluate(valid_loader)\n",
        "    print(f\"Len of Validation loss: {len(valid_epoch_losses)}, Average loss: {float(np.sum(valid_epoch_losses))/len(valid_epoch_losses)}\")\n",
        "    valid_losses.append(np.mean(valid_epoch_losses))"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeVD_pAqcoCr",
        "outputId": "7a07e481-a528-4512-8cb0-4f36e7bac360"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Len of Training loss: 36, Average loss: 9.441449191835192\n",
            "Len of Validation loss: 128, Average loss: 8.3044736944139\n",
            "Epoch: 1, Len of Training loss: 36, Average loss: 9.12962798277537\n",
            "Len of Validation loss: 128, Average loss: 8.166568905115128\n",
            "Epoch: 2, Len of Training loss: 36, Average loss: 8.84437788857354\n",
            "Len of Validation loss: 128, Average loss: 8.201223265379667\n",
            "Epoch: 3, Len of Training loss: 36, Average loss: 8.72490151723226\n",
            "Len of Validation loss: 128, Average loss: 7.73916500993073\n",
            "Epoch: 4, Len of Training loss: 36, Average loss: 8.511919140815735\n",
            "Len of Validation loss: 128, Average loss: 7.585057363845408\n",
            "Epoch: 5, Len of Training loss: 36, Average loss: 8.316455708609688\n",
            "Len of Validation loss: 128, Average loss: 7.539754600264132\n",
            "Epoch: 6, Len of Training loss: 36, Average loss: 8.19070061047872\n",
            "Len of Validation loss: 128, Average loss: 7.199761721305549\n",
            "Epoch: 7, Len of Training loss: 36, Average loss: 7.81824775536855\n",
            "Len of Validation loss: 128, Average loss: 6.830237757414579\n",
            "Epoch: 8, Len of Training loss: 36, Average loss: 7.315089835060967\n",
            "Len of Validation loss: 128, Average loss: 6.688946611247957\n",
            "Epoch: 9, Len of Training loss: 36, Average loss: 7.531958884663052\n",
            "Len of Validation loss: 128, Average loss: 6.36193479411304\n",
            "Epoch: 10, Len of Training loss: 36, Average loss: 6.785008351008098\n",
            "Len of Validation loss: 128, Average loss: 5.842606854625046\n",
            "Epoch: 11, Len of Training loss: 36, Average loss: 6.599755783875783\n",
            "Len of Validation loss: 128, Average loss: 5.967514547519386\n",
            "Epoch: 12, Len of Training loss: 36, Average loss: 6.383917782041761\n",
            "Len of Validation loss: 128, Average loss: 5.547972583211958\n",
            "Epoch: 13, Len of Training loss: 36, Average loss: 6.419665190908644\n",
            "Len of Validation loss: 128, Average loss: 5.770788098685443\n",
            "Epoch: 14, Len of Training loss: 36, Average loss: 6.225241568353441\n",
            "Len of Validation loss: 128, Average loss: 5.498103756457567\n",
            "Epoch: 15, Len of Training loss: 36, Average loss: 5.9821045597394304\n",
            "Len of Validation loss: 128, Average loss: 5.686498047783971\n",
            "Epoch: 16, Len of Training loss: 36, Average loss: 5.887717273500231\n",
            "Len of Validation loss: 128, Average loss: 5.056454003788531\n",
            "Epoch: 17, Len of Training loss: 36, Average loss: 5.860933634969923\n",
            "Len of Validation loss: 128, Average loss: 5.1368649285286665\n",
            "Epoch: 18, Len of Training loss: 36, Average loss: 5.696771456135644\n",
            "Len of Validation loss: 128, Average loss: 4.908490682952106\n",
            "Epoch: 19, Len of Training loss: 36, Average loss: 5.553302082750532\n",
            "Len of Validation loss: 128, Average loss: 5.328960003331304\n",
            "Epoch: 20, Len of Training loss: 36, Average loss: 5.56877123647266\n",
            "Len of Validation loss: 128, Average loss: 5.04401923250407\n",
            "Epoch: 21, Len of Training loss: 36, Average loss: 5.38543258772956\n",
            "Len of Validation loss: 128, Average loss: 4.699087858200073\n",
            "Epoch: 22, Len of Training loss: 36, Average loss: 5.215459181202783\n",
            "Len of Validation loss: 128, Average loss: 4.977684075944126\n",
            "Epoch: 23, Len of Training loss: 36, Average loss: 5.251242988639408\n",
            "Len of Validation loss: 128, Average loss: 4.591733627952635\n",
            "Epoch: 24, Len of Training loss: 36, Average loss: 5.109031630886926\n",
            "Len of Validation loss: 128, Average loss: 4.474792959168553\n",
            "Epoch: 25, Len of Training loss: 36, Average loss: 5.254225982560052\n",
            "Len of Validation loss: 128, Average loss: 4.471713898703456\n",
            "Epoch: 26, Len of Training loss: 36, Average loss: 4.874104373984867\n",
            "Len of Validation loss: 128, Average loss: 4.3710453333333135\n",
            "Epoch: 27, Len of Training loss: 36, Average loss: 4.8152928948402405\n",
            "Len of Validation loss: 128, Average loss: 4.395888169761747\n",
            "Epoch: 28, Len of Training loss: 36, Average loss: 4.807807743549347\n",
            "Len of Validation loss: 128, Average loss: 4.236795493401587\n",
            "Epoch: 29, Len of Training loss: 36, Average loss: 4.876510077052647\n",
            "Len of Validation loss: 128, Average loss: 4.485891736112535\n",
            "Epoch: 30, Len of Training loss: 36, Average loss: 4.974380168649885\n",
            "Len of Validation loss: 128, Average loss: 4.062609666027129\n",
            "Epoch: 31, Len of Training loss: 36, Average loss: 4.464456902609931\n",
            "Len of Validation loss: 128, Average loss: 4.012927292846143\n",
            "Epoch: 32, Len of Training loss: 36, Average loss: 4.499907930692037\n",
            "Len of Validation loss: 128, Average loss: 4.170119462534785\n",
            "Epoch: 33, Len of Training loss: 36, Average loss: 4.537651439507802\n",
            "Len of Validation loss: 128, Average loss: 3.912421975750476\n",
            "Epoch: 34, Len of Training loss: 36, Average loss: 4.419910569985707\n",
            "Len of Validation loss: 128, Average loss: 3.8752151327207685\n",
            "Epoch: 35, Len of Training loss: 36, Average loss: 4.364058779345618\n",
            "Len of Validation loss: 128, Average loss: 3.6320092864334583\n",
            "Epoch: 36, Len of Training loss: 36, Average loss: 4.221219897270203\n",
            "Len of Validation loss: 128, Average loss: 4.063303737901151\n",
            "Epoch: 37, Len of Training loss: 36, Average loss: 4.098327186372545\n",
            "Len of Validation loss: 128, Average loss: 3.84758588578552\n",
            "Epoch: 38, Len of Training loss: 36, Average loss: 3.977324512269762\n",
            "Len of Validation loss: 128, Average loss: 4.119033564813435\n",
            "Epoch: 39, Len of Training loss: 36, Average loss: 4.13809953795539\n",
            "Len of Validation loss: 128, Average loss: 3.481015793979168\n",
            "Epoch: 40, Len of Training loss: 36, Average loss: 3.879770212703281\n",
            "Len of Validation loss: 128, Average loss: 3.5007130200974643\n",
            "Epoch: 41, Len of Training loss: 36, Average loss: 4.190245310465495\n",
            "Len of Validation loss: 128, Average loss: 3.3575105695053935\n",
            "Epoch: 42, Len of Training loss: 36, Average loss: 3.6194914711846247\n",
            "Len of Validation loss: 128, Average loss: 3.3561249980702996\n",
            "Epoch: 43, Len of Training loss: 36, Average loss: 3.467912779914008\n",
            "Len of Validation loss: 128, Average loss: 3.196569535881281\n",
            "Epoch: 44, Len of Training loss: 36, Average loss: 3.3863430652353497\n",
            "Len of Validation loss: 128, Average loss: 3.182468408718705\n",
            "Epoch: 45, Len of Training loss: 36, Average loss: 3.480035682519277\n",
            "Len of Validation loss: 128, Average loss: 3.451246208511293\n",
            "Epoch: 46, Len of Training loss: 36, Average loss: 3.291158629788293\n",
            "Len of Validation loss: 128, Average loss: 2.97785581368953\n",
            "Epoch: 47, Len of Training loss: 36, Average loss: 3.2391768826378717\n",
            "Len of Validation loss: 128, Average loss: 3.2965157441794872\n",
            "Epoch: 48, Len of Training loss: 36, Average loss: 3.529911802874671\n",
            "Len of Validation loss: 128, Average loss: 3.288839959539473\n",
            "Epoch: 49, Len of Training loss: 36, Average loss: 3.062648528152042\n",
            "Len of Validation loss: 128, Average loss: 3.1311860540881753\n",
            "Epoch: 50, Len of Training loss: 36, Average loss: 3.011833952532874\n",
            "Len of Validation loss: 128, Average loss: 2.9327842723578215\n",
            "Epoch: 51, Len of Training loss: 36, Average loss: 3.0515566600693598\n",
            "Len of Validation loss: 128, Average loss: 3.1971983537077904\n",
            "Epoch: 52, Len of Training loss: 36, Average loss: 2.9577021068996854\n",
            "Len of Validation loss: 128, Average loss: 2.74905461166054\n",
            "Epoch: 53, Len of Training loss: 36, Average loss: 2.958914624320136\n",
            "Len of Validation loss: 128, Average loss: 2.5512717198580503\n",
            "Epoch: 54, Len of Training loss: 36, Average loss: 2.9886680708991156\n",
            "Len of Validation loss: 128, Average loss: 2.6575933545827866\n",
            "Epoch: 55, Len of Training loss: 36, Average loss: 2.9316178957621255\n",
            "Len of Validation loss: 128, Average loss: 2.61796538811177\n",
            "Epoch: 56, Len of Training loss: 36, Average loss: 2.9143882195154824\n",
            "Len of Validation loss: 128, Average loss: 2.5704081640578806\n",
            "Epoch: 57, Len of Training loss: 36, Average loss: 2.9407207800282373\n",
            "Len of Validation loss: 128, Average loss: 2.8042887267656624\n",
            "Epoch: 58, Len of Training loss: 36, Average loss: 2.8132011559274464\n",
            "Len of Validation loss: 128, Average loss: 2.6276514502242208\n",
            "Epoch: 59, Len of Training loss: 36, Average loss: 2.7443386912345886\n",
            "Len of Validation loss: 128, Average loss: 2.6976556731387973\n",
            "Epoch: 60, Len of Training loss: 36, Average loss: 2.8902791208691068\n",
            "Len of Validation loss: 128, Average loss: 3.1046426817774773\n",
            "Epoch: 61, Len of Training loss: 36, Average loss: 2.8887181414498224\n",
            "Len of Validation loss: 128, Average loss: 2.8416416607797146\n",
            "Epoch: 62, Len of Training loss: 36, Average loss: 2.839152329497867\n",
            "Len of Validation loss: 128, Average loss: 2.5856025139801204\n",
            "Epoch: 63, Len of Training loss: 36, Average loss: 2.7279820442199707\n",
            "Len of Validation loss: 128, Average loss: 2.6772658303380013\n",
            "Epoch: 64, Len of Training loss: 36, Average loss: 2.871774693330129\n",
            "Len of Validation loss: 128, Average loss: 2.532980258576572\n",
            "Epoch: 65, Len of Training loss: 36, Average loss: 2.666498839855194\n",
            "Len of Validation loss: 128, Average loss: 2.4544069576077163\n",
            "Epoch: 66, Len of Training loss: 36, Average loss: 2.8357329103681774\n",
            "Len of Validation loss: 128, Average loss: 2.7083797892555594\n",
            "Epoch: 67, Len of Training loss: 36, Average loss: 2.679690056376987\n",
            "Len of Validation loss: 128, Average loss: 2.4262415198609233\n",
            "Epoch: 68, Len of Training loss: 36, Average loss: 2.614500039153629\n",
            "Len of Validation loss: 128, Average loss: 2.5616919696331024\n",
            "Epoch: 69, Len of Training loss: 36, Average loss: 2.7618025872442455\n",
            "Len of Validation loss: 128, Average loss: 2.863338802009821\n",
            "Epoch: 70, Len of Training loss: 36, Average loss: 2.782554202609592\n",
            "Len of Validation loss: 128, Average loss: 2.4133166265673935\n",
            "Epoch: 71, Len of Training loss: 36, Average loss: 2.623380091455248\n",
            "Len of Validation loss: 128, Average loss: 2.4190146275795996\n",
            "Epoch: 72, Len of Training loss: 36, Average loss: 2.7513681915071277\n",
            "Len of Validation loss: 128, Average loss: 2.633071579039097\n",
            "Epoch: 73, Len of Training loss: 36, Average loss: 2.814935326576233\n",
            "Len of Validation loss: 128, Average loss: 2.443482860457152\n",
            "Epoch: 74, Len of Training loss: 36, Average loss: 2.569459617137909\n",
            "Len of Validation loss: 128, Average loss: 2.3897766531445086\n",
            "Epoch: 75, Len of Training loss: 36, Average loss: 2.571051061153412\n",
            "Len of Validation loss: 128, Average loss: 2.3680861755274236\n",
            "Epoch: 76, Len of Training loss: 36, Average loss: 2.5259018076790705\n",
            "Len of Validation loss: 128, Average loss: 2.4157867496833205\n",
            "Epoch: 77, Len of Training loss: 36, Average loss: 2.676309969690111\n",
            "Len of Validation loss: 128, Average loss: 2.5479397224262357\n",
            "Epoch: 78, Len of Training loss: 36, Average loss: 2.5583758850892386\n",
            "Len of Validation loss: 128, Average loss: 2.336129338014871\n",
            "Epoch: 79, Len of Training loss: 36, Average loss: 2.558421426349216\n",
            "Len of Validation loss: 128, Average loss: 2.4617543753702193\n",
            "Epoch: 80, Len of Training loss: 36, Average loss: 2.6482105520036487\n",
            "Len of Validation loss: 128, Average loss: 2.406425244640559\n",
            "Epoch: 81, Len of Training loss: 36, Average loss: 2.587119711769952\n",
            "Len of Validation loss: 128, Average loss: 2.326269870158285\n",
            "Epoch: 82, Len of Training loss: 36, Average loss: 2.5370874371793537\n",
            "Len of Validation loss: 128, Average loss: 2.4644590434618294\n",
            "Epoch: 83, Len of Training loss: 36, Average loss: 2.5263379083739386\n",
            "Len of Validation loss: 128, Average loss: 2.3051559892483056\n",
            "Epoch: 84, Len of Training loss: 36, Average loss: 2.502324793073866\n",
            "Len of Validation loss: 128, Average loss: 2.298929997254163\n",
            "Epoch: 85, Len of Training loss: 36, Average loss: 2.4890922407309213\n",
            "Len of Validation loss: 128, Average loss: 2.3959638252854347\n",
            "Epoch: 86, Len of Training loss: 36, Average loss: 2.5847485760847726\n",
            "Len of Validation loss: 128, Average loss: 2.3632315690629184\n",
            "Epoch: 87, Len of Training loss: 36, Average loss: 2.601901570955912\n",
            "Len of Validation loss: 128, Average loss: 2.5257234959863126\n",
            "Epoch: 88, Len of Training loss: 36, Average loss: 2.6579388313823276\n",
            "Len of Validation loss: 128, Average loss: 2.3955149156972766\n",
            "Epoch: 89, Len of Training loss: 36, Average loss: 2.578035169177585\n",
            "Len of Validation loss: 128, Average loss: 2.6000787960365415\n",
            "Epoch: 90, Len of Training loss: 36, Average loss: 2.5660552183787027\n",
            "Len of Validation loss: 128, Average loss: 2.4660818711854517\n",
            "Epoch: 91, Len of Training loss: 36, Average loss: 2.571752690606647\n",
            "Len of Validation loss: 128, Average loss: 2.7601818977855146\n",
            "Epoch: 92, Len of Training loss: 36, Average loss: 2.6542834440867105\n",
            "Len of Validation loss: 128, Average loss: 2.4276986103504896\n",
            "Epoch: 93, Len of Training loss: 36, Average loss: 2.4772241380479603\n",
            "Len of Validation loss: 128, Average loss: 2.4469557185657322\n",
            "Epoch: 94, Len of Training loss: 36, Average loss: 2.488322893778483\n",
            "Len of Validation loss: 128, Average loss: 2.3505851342342794\n",
            "Epoch: 95, Len of Training loss: 36, Average loss: 2.4391641914844513\n",
            "Len of Validation loss: 128, Average loss: 2.270354662556201\n",
            "Epoch: 96, Len of Training loss: 36, Average loss: 2.4716216458214655\n",
            "Len of Validation loss: 128, Average loss: 2.2614562017843127\n",
            "Epoch: 97, Len of Training loss: 36, Average loss: 2.4542998572190604\n",
            "Len of Validation loss: 128, Average loss: 2.2808704008348286\n",
            "Epoch: 98, Len of Training loss: 36, Average loss: 2.492107000615862\n",
            "Len of Validation loss: 128, Average loss: 2.468632302712649\n",
            "Epoch: 99, Len of Training loss: 36, Average loss: 2.550857037305832\n",
            "Len of Validation loss: 128, Average loss: 2.5234236903488636\n",
            "Epoch: 100, Len of Training loss: 36, Average loss: 2.5937147041161857\n",
            "Len of Validation loss: 128, Average loss: 2.4886629194952548\n",
            "Epoch: 101, Len of Training loss: 36, Average loss: 2.4242863953113556\n",
            "Len of Validation loss: 128, Average loss: 2.365830644965172\n",
            "Epoch: 102, Len of Training loss: 36, Average loss: 2.425796260436376\n",
            "Len of Validation loss: 128, Average loss: 2.276322024408728\n",
            "Epoch: 103, Len of Training loss: 36, Average loss: 2.40837252802319\n",
            "Len of Validation loss: 128, Average loss: 2.2763012726791203\n",
            "Epoch: 104, Len of Training loss: 36, Average loss: 2.392230984237459\n",
            "Len of Validation loss: 128, Average loss: 2.456198733765632\n",
            "Epoch: 105, Len of Training loss: 36, Average loss: 2.439659767680698\n",
            "Len of Validation loss: 128, Average loss: 2.636715207248926\n",
            "Epoch: 106, Len of Training loss: 36, Average loss: 2.477684997849994\n",
            "Len of Validation loss: 128, Average loss: 2.2045523254200816\n",
            "Epoch: 107, Len of Training loss: 36, Average loss: 2.442125658194224\n",
            "Len of Validation loss: 128, Average loss: 2.2791063133627176\n",
            "Epoch: 108, Len of Training loss: 36, Average loss: 2.506729506784015\n",
            "Len of Validation loss: 128, Average loss: 2.347310184966773\n",
            "Epoch: 109, Len of Training loss: 36, Average loss: 2.4196790125634937\n",
            "Len of Validation loss: 128, Average loss: 2.2347833551466465\n",
            "Epoch: 110, Len of Training loss: 36, Average loss: 2.327900913026598\n",
            "Len of Validation loss: 128, Average loss: 2.2256759773008525\n",
            "Epoch: 111, Len of Training loss: 36, Average loss: 2.39580100774765\n",
            "Len of Validation loss: 128, Average loss: 2.218286284711212\n",
            "Epoch: 112, Len of Training loss: 36, Average loss: 2.398736755053202\n",
            "Len of Validation loss: 128, Average loss: 2.5567392772063613\n",
            "Epoch: 113, Len of Training loss: 36, Average loss: 2.4993486834896936\n",
            "Len of Validation loss: 128, Average loss: 2.243081948021427\n",
            "Epoch: 114, Len of Training loss: 36, Average loss: 2.469888402356042\n",
            "Len of Validation loss: 128, Average loss: 2.377083702944219\n",
            "Epoch: 115, Len of Training loss: 36, Average loss: 2.388408021794425\n",
            "Len of Validation loss: 128, Average loss: 2.1351521448232234\n",
            "Epoch: 116, Len of Training loss: 36, Average loss: 2.317261050144831\n",
            "Len of Validation loss: 128, Average loss: 2.361947914585471\n",
            "Epoch: 117, Len of Training loss: 36, Average loss: 2.405109233326382\n",
            "Len of Validation loss: 128, Average loss: 2.2977289794944227\n",
            "Epoch: 118, Len of Training loss: 36, Average loss: 2.3742977347638874\n",
            "Len of Validation loss: 128, Average loss: 2.198903874028474\n",
            "Epoch: 119, Len of Training loss: 36, Average loss: 2.664614872799979\n",
            "Len of Validation loss: 128, Average loss: 2.3129182951524854\n",
            "Epoch: 120, Len of Training loss: 36, Average loss: 2.395082116127014\n",
            "Len of Validation loss: 128, Average loss: 2.148441694211215\n",
            "Epoch: 121, Len of Training loss: 36, Average loss: 2.416236254904005\n",
            "Len of Validation loss: 128, Average loss: 2.211016974877566\n",
            "Epoch: 122, Len of Training loss: 36, Average loss: 2.4205796950393252\n",
            "Len of Validation loss: 128, Average loss: 2.276184451300651\n",
            "Epoch: 123, Len of Training loss: 36, Average loss: 2.4036291340986886\n",
            "Len of Validation loss: 128, Average loss: 2.3447871399112046\n",
            "Epoch: 124, Len of Training loss: 36, Average loss: 2.384866495927175\n",
            "Len of Validation loss: 128, Average loss: 2.1272864970378578\n",
            "Epoch: 125, Len of Training loss: 36, Average loss: 2.3543942040867276\n",
            "Len of Validation loss: 128, Average loss: 2.188850097823888\n",
            "Epoch: 126, Len of Training loss: 36, Average loss: 2.303589019510481\n",
            "Len of Validation loss: 128, Average loss: 2.1090233074501157\n",
            "Epoch: 127, Len of Training loss: 36, Average loss: 2.390131139092975\n",
            "Len of Validation loss: 128, Average loss: 2.8511245669797063\n",
            "Epoch: 128, Len of Training loss: 36, Average loss: 2.4809090163972645\n",
            "Len of Validation loss: 128, Average loss: 2.4118900960311294\n",
            "Epoch: 129, Len of Training loss: 36, Average loss: 2.3150097264183893\n",
            "Len of Validation loss: 128, Average loss: 2.1890186253003776\n",
            "Epoch: 130, Len of Training loss: 36, Average loss: 2.3312553928958044\n",
            "Len of Validation loss: 128, Average loss: 2.222414292860776\n",
            "Epoch: 131, Len of Training loss: 36, Average loss: 2.3385335173871784\n",
            "Len of Validation loss: 128, Average loss: 2.3017737208865583\n",
            "Epoch: 132, Len of Training loss: 36, Average loss: 2.3551387389500937\n",
            "Len of Validation loss: 128, Average loss: 2.225067840423435\n",
            "Epoch: 133, Len of Training loss: 36, Average loss: 2.3060203823778362\n",
            "Len of Validation loss: 128, Average loss: 2.198135936167091\n",
            "Epoch: 134, Len of Training loss: 36, Average loss: 2.2742162346839905\n",
            "Len of Validation loss: 128, Average loss: 2.3034321260638535\n",
            "Epoch: 135, Len of Training loss: 36, Average loss: 2.290332224633959\n",
            "Len of Validation loss: 128, Average loss: 2.151457973755896\n",
            "Epoch: 136, Len of Training loss: 36, Average loss: 2.292569465107388\n",
            "Len of Validation loss: 128, Average loss: 2.166650605853647\n",
            "Epoch: 137, Len of Training loss: 36, Average loss: 2.3538940615124173\n",
            "Len of Validation loss: 128, Average loss: 2.18850154383108\n",
            "Epoch: 138, Len of Training loss: 36, Average loss: 2.3434113992585077\n",
            "Len of Validation loss: 128, Average loss: 2.149766034912318\n",
            "Epoch: 139, Len of Training loss: 36, Average loss: 2.3486772080262504\n",
            "Len of Validation loss: 128, Average loss: 2.114118206780404\n",
            "Epoch: 140, Len of Training loss: 36, Average loss: 2.3414748079246945\n",
            "Len of Validation loss: 128, Average loss: 2.2054785990621895\n",
            "Epoch: 141, Len of Training loss: 36, Average loss: 2.2908642258909016\n",
            "Len of Validation loss: 128, Average loss: 2.091593414079398\n",
            "Epoch: 142, Len of Training loss: 36, Average loss: 2.309148543410831\n",
            "Len of Validation loss: 128, Average loss: 2.1370279463008046\n",
            "Epoch: 143, Len of Training loss: 36, Average loss: 2.351521220472124\n",
            "Len of Validation loss: 128, Average loss: 2.2314527267590165\n",
            "Epoch: 144, Len of Training loss: 36, Average loss: 2.4240363207128315\n",
            "Len of Validation loss: 128, Average loss: 2.1500832228921354\n",
            "Epoch: 145, Len of Training loss: 36, Average loss: 2.262530359956953\n",
            "Len of Validation loss: 128, Average loss: 2.0885639800690114\n",
            "Epoch: 146, Len of Training loss: 36, Average loss: 2.262753771411048\n",
            "Len of Validation loss: 128, Average loss: 2.229310696013272\n",
            "Epoch: 147, Len of Training loss: 36, Average loss: 2.3036455346478357\n",
            "Len of Validation loss: 128, Average loss: 2.0879067820496857\n",
            "Epoch: 148, Len of Training loss: 36, Average loss: 2.3111645910474987\n",
            "Len of Validation loss: 128, Average loss: 2.0764306485652924\n",
            "Epoch: 149, Len of Training loss: 36, Average loss: 2.223570429616504\n",
            "Len of Validation loss: 128, Average loss: 2.2952223303727806\n",
            "Epoch: 150, Len of Training loss: 36, Average loss: 2.2955192824204764\n",
            "Len of Validation loss: 128, Average loss: 2.5436018395703286\n",
            "Epoch: 151, Len of Training loss: 36, Average loss: 2.427568346261978\n",
            "Len of Validation loss: 128, Average loss: 2.330989827401936\n",
            "Epoch: 152, Len of Training loss: 36, Average loss: 2.31393262412813\n",
            "Len of Validation loss: 128, Average loss: 2.194264112273231\n",
            "Epoch: 153, Len of Training loss: 36, Average loss: 2.332167125410504\n",
            "Len of Validation loss: 128, Average loss: 2.1161996074952185\n",
            "Epoch: 154, Len of Training loss: 36, Average loss: 2.3167334695657096\n",
            "Len of Validation loss: 128, Average loss: 2.111084514996037\n",
            "Epoch: 155, Len of Training loss: 36, Average loss: 2.326828976472219\n",
            "Len of Validation loss: 128, Average loss: 2.134098317939788\n",
            "Epoch: 156, Len of Training loss: 36, Average loss: 2.2997713022761874\n",
            "Len of Validation loss: 128, Average loss: 2.1245182268321514\n",
            "Epoch: 157, Len of Training loss: 36, Average loss: 2.241973807414373\n",
            "Len of Validation loss: 128, Average loss: 2.18473539641127\n",
            "Epoch: 158, Len of Training loss: 36, Average loss: 2.216631535026762\n",
            "Len of Validation loss: 128, Average loss: 2.2712855739519\n",
            "Epoch: 159, Len of Training loss: 36, Average loss: 2.2666575014591217\n",
            "Len of Validation loss: 128, Average loss: 2.0639289738610387\n",
            "Epoch: 160, Len of Training loss: 36, Average loss: 2.264702879720264\n",
            "Len of Validation loss: 128, Average loss: 2.2374990032985806\n",
            "Epoch: 161, Len of Training loss: 36, Average loss: 2.2810399962796106\n",
            "Len of Validation loss: 128, Average loss: 2.163561788853258\n",
            "Epoch: 162, Len of Training loss: 36, Average loss: 2.246679166952769\n",
            "Len of Validation loss: 128, Average loss: 2.233125594910234\n",
            "Epoch: 163, Len of Training loss: 36, Average loss: 2.224691735373603\n",
            "Len of Validation loss: 128, Average loss: 2.2143870815634727\n",
            "Epoch: 164, Len of Training loss: 36, Average loss: 2.2939166327317557\n",
            "Len of Validation loss: 128, Average loss: 2.138649492058903\n",
            "Epoch: 165, Len of Training loss: 36, Average loss: 2.192590286334356\n",
            "Len of Validation loss: 128, Average loss: 2.0251600157935172\n",
            "Epoch: 166, Len of Training loss: 36, Average loss: 2.222677747408549\n",
            "Len of Validation loss: 128, Average loss: 2.0663420651108027\n",
            "Epoch: 167, Len of Training loss: 36, Average loss: 2.245969216028849\n",
            "Len of Validation loss: 128, Average loss: 2.3510756832547486\n",
            "Epoch: 168, Len of Training loss: 36, Average loss: 2.313952558570438\n",
            "Len of Validation loss: 128, Average loss: 2.1476254276931286\n",
            "Epoch: 169, Len of Training loss: 36, Average loss: 2.2406825257672205\n",
            "Len of Validation loss: 128, Average loss: 2.0774707100354135\n",
            "Epoch: 170, Len of Training loss: 36, Average loss: 2.2220868468284607\n",
            "Len of Validation loss: 128, Average loss: 2.0737625332549214\n",
            "Epoch: 171, Len of Training loss: 36, Average loss: 2.1958038409550986\n",
            "Len of Validation loss: 128, Average loss: 2.0143600329756737\n",
            "Epoch: 172, Len of Training loss: 36, Average loss: 2.2341966893937855\n",
            "Len of Validation loss: 128, Average loss: 2.202382877469063\n",
            "Epoch: 173, Len of Training loss: 36, Average loss: 2.203483564986123\n",
            "Len of Validation loss: 128, Average loss: 2.065372619777918\n",
            "Epoch: 174, Len of Training loss: 36, Average loss: 2.191979547341665\n",
            "Len of Validation loss: 128, Average loss: 2.164757897146046\n",
            "Epoch: 175, Len of Training loss: 36, Average loss: 2.2612605128023358\n",
            "Len of Validation loss: 128, Average loss: 2.0979735888540745\n",
            "Epoch: 176, Len of Training loss: 36, Average loss: 2.2957257164849176\n",
            "Len of Validation loss: 128, Average loss: 2.0928884772583842\n",
            "Epoch: 177, Len of Training loss: 36, Average loss: 2.182484438021978\n",
            "Len of Validation loss: 128, Average loss: 2.0750471386127174\n",
            "Epoch: 178, Len of Training loss: 36, Average loss: 2.2326455811659494\n",
            "Len of Validation loss: 128, Average loss: 2.07779625011608\n",
            "Epoch: 179, Len of Training loss: 36, Average loss: 2.1843557722038693\n",
            "Len of Validation loss: 128, Average loss: 2.1109444350004196\n",
            "Epoch: 180, Len of Training loss: 36, Average loss: 2.157794972260793\n",
            "Len of Validation loss: 128, Average loss: 2.0007264758460224\n",
            "Epoch: 181, Len of Training loss: 36, Average loss: 2.213521440823873\n",
            "Len of Validation loss: 128, Average loss: 2.303076345473528\n",
            "Epoch: 182, Len of Training loss: 36, Average loss: 2.226752993133333\n",
            "Len of Validation loss: 128, Average loss: 2.0755390971899033\n",
            "Epoch: 183, Len of Training loss: 36, Average loss: 2.1208275225427418\n",
            "Len of Validation loss: 128, Average loss: 2.0993884527124465\n",
            "Epoch: 184, Len of Training loss: 36, Average loss: 2.202226208315955\n",
            "Len of Validation loss: 128, Average loss: 2.0389722432009876\n",
            "Epoch: 185, Len of Training loss: 36, Average loss: 2.165961249007119\n",
            "Len of Validation loss: 128, Average loss: 2.0538664287887514\n",
            "Epoch: 186, Len of Training loss: 36, Average loss: 2.179709169599745\n",
            "Len of Validation loss: 128, Average loss: 2.1254414790309966\n",
            "Epoch: 187, Len of Training loss: 36, Average loss: 2.2433812618255615\n",
            "Len of Validation loss: 128, Average loss: 2.1131985783576965\n",
            "Epoch: 188, Len of Training loss: 36, Average loss: 2.2265393204159207\n",
            "Len of Validation loss: 128, Average loss: 2.217440165579319\n",
            "Epoch: 189, Len of Training loss: 36, Average loss: 2.163648873567581\n",
            "Len of Validation loss: 128, Average loss: 2.038375376025215\n",
            "Epoch: 190, Len of Training loss: 36, Average loss: 2.166377776198917\n",
            "Len of Validation loss: 128, Average loss: 2.0617622735444456\n",
            "Epoch: 191, Len of Training loss: 36, Average loss: 2.128625293572744\n",
            "Len of Validation loss: 128, Average loss: 2.0594798563979566\n",
            "Epoch: 192, Len of Training loss: 36, Average loss: 2.2988542715708413\n",
            "Len of Validation loss: 128, Average loss: 2.161003263667226\n",
            "Epoch: 193, Len of Training loss: 36, Average loss: 2.2442251907454596\n",
            "Len of Validation loss: 128, Average loss: 2.288357330020517\n",
            "Epoch: 194, Len of Training loss: 36, Average loss: 2.3270433677567377\n",
            "Len of Validation loss: 128, Average loss: 2.2818882474675775\n",
            "Epoch: 195, Len of Training loss: 36, Average loss: 2.203237702449163\n",
            "Len of Validation loss: 128, Average loss: 2.1666591726243496\n",
            "Epoch: 196, Len of Training loss: 36, Average loss: 2.172315329313278\n",
            "Len of Validation loss: 128, Average loss: 2.1518300431780517\n",
            "Epoch: 197, Len of Training loss: 36, Average loss: 2.1367661787403955\n",
            "Len of Validation loss: 128, Average loss: 1.953586344840005\n",
            "Epoch: 198, Len of Training loss: 36, Average loss: 2.0905129081673093\n",
            "Len of Validation loss: 128, Average loss: 2.0224193511530757\n",
            "Epoch: 199, Len of Training loss: 36, Average loss: 2.1703869700431824\n",
            "Len of Validation loss: 128, Average loss: 2.057229903060943\n",
            "Epoch: 200, Len of Training loss: 36, Average loss: 2.1431668798128762\n",
            "Len of Validation loss: 128, Average loss: 2.020766797941178\n",
            "Epoch: 201, Len of Training loss: 36, Average loss: 2.1182249552673764\n",
            "Len of Validation loss: 128, Average loss: 1.9797291737049818\n",
            "Epoch: 202, Len of Training loss: 36, Average loss: 2.163688931200239\n",
            "Len of Validation loss: 128, Average loss: 1.9791495902463794\n",
            "Epoch: 203, Len of Training loss: 36, Average loss: 2.21898180908627\n",
            "Len of Validation loss: 128, Average loss: 2.0565159348770976\n",
            "Epoch: 204, Len of Training loss: 36, Average loss: 2.147493749856949\n",
            "Len of Validation loss: 128, Average loss: 1.9750641519203782\n",
            "Epoch: 205, Len of Training loss: 36, Average loss: 2.113601323631075\n",
            "Len of Validation loss: 128, Average loss: 2.0167311006225646\n",
            "Epoch: 206, Len of Training loss: 36, Average loss: 2.1942780117193856\n",
            "Len of Validation loss: 128, Average loss: 2.2033397895283997\n",
            "Epoch: 207, Len of Training loss: 36, Average loss: 2.1949731243981256\n",
            "Len of Validation loss: 128, Average loss: 2.002212272025645\n",
            "Epoch: 208, Len of Training loss: 36, Average loss: 2.1293073859479694\n",
            "Len of Validation loss: 128, Average loss: 1.9804183379746974\n",
            "Epoch: 209, Len of Training loss: 36, Average loss: 2.129587640364965\n",
            "Len of Validation loss: 128, Average loss: 2.124971837271005\n",
            "Epoch: 210, Len of Training loss: 36, Average loss: 2.188517779111862\n",
            "Len of Validation loss: 128, Average loss: 1.964663787279278\n",
            "Epoch: 211, Len of Training loss: 36, Average loss: 2.1529171334372625\n",
            "Len of Validation loss: 128, Average loss: 2.1466658781282604\n",
            "Epoch: 212, Len of Training loss: 36, Average loss: 2.1321205033196344\n",
            "Len of Validation loss: 128, Average loss: 1.9292329717427492\n",
            "Epoch: 213, Len of Training loss: 36, Average loss: 2.1457626389132605\n",
            "Len of Validation loss: 128, Average loss: 2.1308260548394173\n",
            "Epoch: 214, Len of Training loss: 36, Average loss: 2.13775967558225\n",
            "Len of Validation loss: 128, Average loss: 1.949948047986254\n",
            "Epoch: 215, Len of Training loss: 36, Average loss: 2.0935842759079404\n",
            "Len of Validation loss: 128, Average loss: 2.1027034213766456\n",
            "Epoch: 216, Len of Training loss: 36, Average loss: 2.160532057285309\n",
            "Len of Validation loss: 128, Average loss: 2.0566128683276474\n",
            "Epoch: 217, Len of Training loss: 36, Average loss: 2.0807959702279835\n",
            "Len of Validation loss: 128, Average loss: 2.1115685449913144\n",
            "Epoch: 218, Len of Training loss: 36, Average loss: 2.1013381017578974\n",
            "Len of Validation loss: 128, Average loss: 1.983206451870501\n",
            "Epoch: 219, Len of Training loss: 36, Average loss: 2.125935822725296\n",
            "Len of Validation loss: 128, Average loss: 1.9315453479066491\n",
            "Epoch: 220, Len of Training loss: 36, Average loss: 2.1123175190554724\n",
            "Len of Validation loss: 128, Average loss: 2.0884999185800552\n",
            "Epoch: 221, Len of Training loss: 36, Average loss: 2.0734737283653684\n",
            "Len of Validation loss: 128, Average loss: 1.9591016727499664\n",
            "Epoch: 222, Len of Training loss: 36, Average loss: 2.1263431012630463\n",
            "Len of Validation loss: 128, Average loss: 1.9751847749575973\n",
            "Epoch: 223, Len of Training loss: 36, Average loss: 2.0936753981643252\n",
            "Len of Validation loss: 128, Average loss: 2.1285891463048756\n",
            "Epoch: 224, Len of Training loss: 36, Average loss: 2.1387703087594776\n",
            "Len of Validation loss: 128, Average loss: 2.052109057549387\n",
            "Epoch: 225, Len of Training loss: 36, Average loss: 2.207856274313397\n",
            "Len of Validation loss: 128, Average loss: 2.2668612799607217\n",
            "Epoch: 226, Len of Training loss: 36, Average loss: 2.1854357884989843\n",
            "Len of Validation loss: 128, Average loss: 2.133506545331329\n",
            "Epoch: 227, Len of Training loss: 36, Average loss: 2.0775387585163116\n",
            "Len of Validation loss: 128, Average loss: 1.9788537505082786\n",
            "Epoch: 228, Len of Training loss: 36, Average loss: 2.119674974017673\n",
            "Len of Validation loss: 128, Average loss: 2.031281976029277\n",
            "Epoch: 229, Len of Training loss: 36, Average loss: 2.0733095738622875\n",
            "Len of Validation loss: 128, Average loss: 2.009759495500475\n",
            "Epoch: 230, Len of Training loss: 36, Average loss: 2.093931350443098\n",
            "Len of Validation loss: 128, Average loss: 2.002141870558262\n",
            "Epoch: 231, Len of Training loss: 36, Average loss: 2.0425997508896723\n",
            "Len of Validation loss: 128, Average loss: 2.006026326213032\n",
            "Epoch: 232, Len of Training loss: 36, Average loss: 2.0660236974557242\n",
            "Len of Validation loss: 128, Average loss: 2.046462985686958\n",
            "Epoch: 233, Len of Training loss: 36, Average loss: 2.1039056976636252\n",
            "Len of Validation loss: 128, Average loss: 1.9522748840972781\n",
            "Epoch: 234, Len of Training loss: 36, Average loss: 2.0527178843816123\n",
            "Len of Validation loss: 128, Average loss: 1.970880405511707\n",
            "Epoch: 235, Len of Training loss: 36, Average loss: 2.084470169411765\n",
            "Len of Validation loss: 128, Average loss: 1.9241324686445296\n",
            "Epoch: 236, Len of Training loss: 36, Average loss: 2.0536143514845104\n",
            "Len of Validation loss: 128, Average loss: 2.0147912027314305\n",
            "Epoch: 237, Len of Training loss: 36, Average loss: 2.1133527093463473\n",
            "Len of Validation loss: 128, Average loss: 1.92792560136877\n",
            "Epoch: 238, Len of Training loss: 36, Average loss: 2.108781291378869\n",
            "Len of Validation loss: 128, Average loss: 1.934321902692318\n",
            "Epoch: 239, Len of Training loss: 36, Average loss: 2.1363381379180484\n",
            "Len of Validation loss: 128, Average loss: 1.9510836307890713\n",
            "Epoch: 240, Len of Training loss: 36, Average loss: 2.1065992878543005\n",
            "Len of Validation loss: 128, Average loss: 2.1137036504223943\n",
            "Epoch: 241, Len of Training loss: 36, Average loss: 2.234839608271917\n",
            "Len of Validation loss: 128, Average loss: 2.018228649161756\n",
            "Epoch: 242, Len of Training loss: 36, Average loss: 2.122087988588545\n",
            "Len of Validation loss: 128, Average loss: 1.9997154343873262\n",
            "Epoch: 243, Len of Training loss: 36, Average loss: 2.103441725174586\n",
            "Len of Validation loss: 128, Average loss: 2.096886585932225\n",
            "Epoch: 244, Len of Training loss: 36, Average loss: 2.057305372423596\n",
            "Len of Validation loss: 128, Average loss: 1.9786836276762187\n",
            "Epoch: 245, Len of Training loss: 36, Average loss: 2.0597687628534107\n",
            "Len of Validation loss: 128, Average loss: 2.079282635822892\n",
            "Epoch: 246, Len of Training loss: 36, Average loss: 2.046494570043352\n",
            "Len of Validation loss: 128, Average loss: 1.9900785745121539\n",
            "Epoch: 247, Len of Training loss: 36, Average loss: 2.066888411839803\n",
            "Len of Validation loss: 128, Average loss: 1.9254911236930639\n",
            "Epoch: 248, Len of Training loss: 36, Average loss: 2.064836710691452\n",
            "Len of Validation loss: 128, Average loss: 1.950284599326551\n",
            "Epoch: 249, Len of Training loss: 36, Average loss: 2.044563912683063\n",
            "Len of Validation loss: 128, Average loss: 1.8948321021161973\n",
            "Epoch: 250, Len of Training loss: 36, Average loss: 2.0459761056635113\n",
            "Len of Validation loss: 128, Average loss: 1.9722190452739596\n",
            "Epoch: 251, Len of Training loss: 36, Average loss: 2.058190663655599\n",
            "Len of Validation loss: 128, Average loss: 1.9231099120806903\n",
            "Epoch: 252, Len of Training loss: 36, Average loss: 2.0467675659391613\n",
            "Len of Validation loss: 128, Average loss: 1.9595386569853872\n",
            "Epoch: 253, Len of Training loss: 36, Average loss: 2.049849188990063\n",
            "Len of Validation loss: 128, Average loss: 1.9235890824347734\n",
            "Epoch: 254, Len of Training loss: 36, Average loss: 2.1804519130123987\n",
            "Len of Validation loss: 128, Average loss: 2.008606686722487\n",
            "Epoch: 255, Len of Training loss: 36, Average loss: 2.0933483805921345\n",
            "Len of Validation loss: 128, Average loss: 2.1599624436348677\n",
            "Epoch: 256, Len of Training loss: 36, Average loss: 2.156676063934962\n",
            "Len of Validation loss: 128, Average loss: 2.1143607143312693\n",
            "Epoch: 257, Len of Training loss: 36, Average loss: 2.078778816594018\n",
            "Len of Validation loss: 128, Average loss: 1.9324637148529291\n",
            "Epoch: 258, Len of Training loss: 36, Average loss: 2.0654041469097137\n",
            "Len of Validation loss: 128, Average loss: 1.939845443237573\n",
            "Epoch: 259, Len of Training loss: 36, Average loss: 2.027287403742472\n",
            "Len of Validation loss: 128, Average loss: 1.8925880338065326\n",
            "Epoch: 260, Len of Training loss: 36, Average loss: 2.011756479740143\n",
            "Len of Validation loss: 128, Average loss: 1.8990002279169858\n",
            "Epoch: 261, Len of Training loss: 36, Average loss: 2.058068742354711\n",
            "Len of Validation loss: 128, Average loss: 1.9191911448724568\n",
            "Epoch: 262, Len of Training loss: 36, Average loss: 2.1026775969399347\n",
            "Len of Validation loss: 128, Average loss: 2.1333060460165143\n",
            "Epoch: 263, Len of Training loss: 36, Average loss: 2.060334119531843\n",
            "Len of Validation loss: 128, Average loss: 1.9347271085716784\n",
            "Epoch: 264, Len of Training loss: 36, Average loss: 2.049152543147405\n",
            "Len of Validation loss: 128, Average loss: 1.8805679946672171\n",
            "Epoch: 265, Len of Training loss: 36, Average loss: 2.0324528912703195\n",
            "Len of Validation loss: 128, Average loss: 1.8945297617465258\n",
            "Epoch: 266, Len of Training loss: 36, Average loss: 2.046756668223275\n",
            "Len of Validation loss: 128, Average loss: 2.063082746230066\n",
            "Epoch: 267, Len of Training loss: 36, Average loss: 2.0726760890748768\n",
            "Len of Validation loss: 128, Average loss: 1.8985313510056585\n",
            "Epoch: 268, Len of Training loss: 36, Average loss: 2.029369523127874\n",
            "Len of Validation loss: 128, Average loss: 1.9025612585246563\n",
            "Epoch: 269, Len of Training loss: 36, Average loss: 2.087554785940382\n",
            "Len of Validation loss: 128, Average loss: 1.895201899111271\n",
            "Epoch: 270, Len of Training loss: 36, Average loss: 2.0273438228501215\n",
            "Len of Validation loss: 128, Average loss: 1.962954721879214\n",
            "Epoch: 271, Len of Training loss: 36, Average loss: 2.025596665011512\n",
            "Len of Validation loss: 128, Average loss: 1.881006128154695\n",
            "Epoch: 272, Len of Training loss: 36, Average loss: 1.9977008402347565\n",
            "Len of Validation loss: 128, Average loss: 1.998232218902558\n",
            "Epoch: 273, Len of Training loss: 36, Average loss: 2.0201717217763266\n",
            "Len of Validation loss: 128, Average loss: 1.9848489058203995\n",
            "Epoch: 274, Len of Training loss: 36, Average loss: 2.0560138358010187\n",
            "Len of Validation loss: 128, Average loss: 2.0482504623942077\n",
            "Epoch: 275, Len of Training loss: 36, Average loss: 2.060654991202884\n",
            "Len of Validation loss: 128, Average loss: 1.9005446466617286\n",
            "Epoch: 276, Len of Training loss: 36, Average loss: 2.0025147828790875\n",
            "Len of Validation loss: 128, Average loss: 2.0862135444767773\n",
            "Epoch: 277, Len of Training loss: 36, Average loss: 2.007460468345218\n",
            "Len of Validation loss: 128, Average loss: 1.9247549150604755\n",
            "Epoch: 278, Len of Training loss: 36, Average loss: 1.985929611656401\n",
            "Len of Validation loss: 128, Average loss: 1.859613592736423\n",
            "Epoch: 279, Len of Training loss: 36, Average loss: 2.0416617393493652\n",
            "Len of Validation loss: 128, Average loss: 1.9848102983087301\n",
            "Epoch: 280, Len of Training loss: 36, Average loss: 1.9909487234221563\n",
            "Len of Validation loss: 128, Average loss: 1.9181334148161113\n",
            "Epoch: 281, Len of Training loss: 36, Average loss: 2.075473510556751\n",
            "Len of Validation loss: 128, Average loss: 2.012317524291575\n",
            "Epoch: 282, Len of Training loss: 36, Average loss: 2.001110467645857\n",
            "Len of Validation loss: 128, Average loss: 1.8333176113665104\n",
            "Epoch: 283, Len of Training loss: 36, Average loss: 2.17929278810819\n",
            "Len of Validation loss: 128, Average loss: 2.1925290040671825\n",
            "Epoch: 284, Len of Training loss: 36, Average loss: 2.211168987883462\n",
            "Len of Validation loss: 128, Average loss: 1.9339075526222587\n",
            "Epoch: 285, Len of Training loss: 36, Average loss: 2.2046881119410195\n",
            "Len of Validation loss: 128, Average loss: 2.357141749933362\n",
            "Epoch: 286, Len of Training loss: 36, Average loss: 2.1605365541246204\n",
            "Len of Validation loss: 128, Average loss: 2.0181790264323354\n",
            "Epoch: 287, Len of Training loss: 36, Average loss: 2.050656043820911\n",
            "Len of Validation loss: 128, Average loss: 2.001783687621355\n",
            "Epoch: 288, Len of Training loss: 36, Average loss: 2.0859325329462686\n",
            "Len of Validation loss: 128, Average loss: 1.9491096027195454\n",
            "Epoch: 289, Len of Training loss: 36, Average loss: 2.039206802845001\n",
            "Len of Validation loss: 128, Average loss: 1.9307962623424828\n",
            "Epoch: 290, Len of Training loss: 36, Average loss: 2.0423177944289312\n",
            "Len of Validation loss: 128, Average loss: 1.9503042111173272\n",
            "Epoch: 291, Len of Training loss: 36, Average loss: 2.0327218108707004\n",
            "Len of Validation loss: 128, Average loss: 2.029429629445076\n",
            "Epoch: 292, Len of Training loss: 36, Average loss: 2.0783451166417866\n",
            "Len of Validation loss: 128, Average loss: 2.003713697893545\n",
            "Epoch: 293, Len of Training loss: 36, Average loss: 2.045352680815591\n",
            "Len of Validation loss: 128, Average loss: 1.9394979625940323\n",
            "Epoch: 294, Len of Training loss: 36, Average loss: 2.0083508094151816\n",
            "Len of Validation loss: 128, Average loss: 1.9064462939277291\n",
            "Epoch: 295, Len of Training loss: 36, Average loss: 2.0063102145989737\n",
            "Len of Validation loss: 128, Average loss: 1.8906557196751237\n",
            "Epoch: 296, Len of Training loss: 36, Average loss: 2.0087305307388306\n",
            "Len of Validation loss: 128, Average loss: 1.8187033724971116\n",
            "Epoch: 297, Len of Training loss: 36, Average loss: 2.04673989282714\n",
            "Len of Validation loss: 128, Average loss: 2.07181977853179\n",
            "Epoch: 298, Len of Training loss: 36, Average loss: 2.0638435151841907\n",
            "Len of Validation loss: 128, Average loss: 1.8895637597888708\n",
            "Epoch: 299, Len of Training loss: 36, Average loss: 2.0686944557560816\n",
            "Len of Validation loss: 128, Average loss: 1.9416011793073267\n",
            "Epoch: 300, Len of Training loss: 36, Average loss: 2.0212652020984225\n",
            "Len of Validation loss: 128, Average loss: 1.891382704488933\n",
            "Epoch: 301, Len of Training loss: 36, Average loss: 2.0373070306248136\n",
            "Len of Validation loss: 128, Average loss: 1.8618128986563534\n",
            "Epoch: 302, Len of Training loss: 36, Average loss: 1.972874595059289\n",
            "Len of Validation loss: 128, Average loss: 1.8928141465876251\n",
            "Epoch: 303, Len of Training loss: 36, Average loss: 1.9716115891933441\n",
            "Len of Validation loss: 128, Average loss: 1.8639614610001445\n",
            "Epoch: 304, Len of Training loss: 36, Average loss: 2.0045480496353574\n",
            "Len of Validation loss: 128, Average loss: 1.9174935203045607\n",
            "Epoch: 305, Len of Training loss: 36, Average loss: 2.001227322551939\n",
            "Len of Validation loss: 128, Average loss: 1.9068684719968587\n",
            "Epoch: 306, Len of Training loss: 36, Average loss: 2.0146297779348163\n",
            "Len of Validation loss: 128, Average loss: 1.8358847750350833\n",
            "Epoch: 307, Len of Training loss: 36, Average loss: 2.0288844075467853\n",
            "Len of Validation loss: 128, Average loss: 1.9227643748745322\n",
            "Epoch: 308, Len of Training loss: 36, Average loss: 1.9759534663624234\n",
            "Len of Validation loss: 128, Average loss: 2.0164829902350903\n",
            "Epoch: 309, Len of Training loss: 36, Average loss: 1.9497637384467654\n",
            "Len of Validation loss: 128, Average loss: 1.8573968505952507\n",
            "Epoch: 310, Len of Training loss: 36, Average loss: 1.9788048565387726\n",
            "Len of Validation loss: 128, Average loss: 1.8693265467882156\n",
            "Epoch: 311, Len of Training loss: 36, Average loss: 2.0113041202227273\n",
            "Len of Validation loss: 128, Average loss: 1.844238978344947\n",
            "Epoch: 312, Len of Training loss: 36, Average loss: 1.970085213581721\n",
            "Len of Validation loss: 128, Average loss: 1.8714426278602332\n",
            "Epoch: 313, Len of Training loss: 36, Average loss: 1.9392760462231107\n",
            "Len of Validation loss: 128, Average loss: 1.8401037557050586\n",
            "Epoch: 314, Len of Training loss: 36, Average loss: 1.9781169427765741\n",
            "Len of Validation loss: 128, Average loss: 2.0013506182003766\n",
            "Epoch: 315, Len of Training loss: 36, Average loss: 2.0223701463805304\n",
            "Len of Validation loss: 128, Average loss: 1.889491240028292\n",
            "Epoch: 316, Len of Training loss: 36, Average loss: 1.9511914286348555\n",
            "Len of Validation loss: 128, Average loss: 1.8470931090414524\n",
            "Epoch: 317, Len of Training loss: 36, Average loss: 2.0220539735423193\n",
            "Len of Validation loss: 128, Average loss: 2.3311456707306206\n",
            "Epoch: 318, Len of Training loss: 36, Average loss: 2.102823038895925\n",
            "Len of Validation loss: 128, Average loss: 1.9368348480202258\n",
            "Epoch: 319, Len of Training loss: 36, Average loss: 2.0864950915177665\n",
            "Len of Validation loss: 128, Average loss: 1.8964725201949477\n",
            "Epoch: 320, Len of Training loss: 36, Average loss: 1.9890521400504642\n",
            "Len of Validation loss: 128, Average loss: 1.9537211691495031\n",
            "Epoch: 321, Len of Training loss: 36, Average loss: 1.9956989453898535\n",
            "Len of Validation loss: 128, Average loss: 1.9007547737564892\n",
            "Epoch: 322, Len of Training loss: 36, Average loss: 2.0285806722111173\n",
            "Len of Validation loss: 128, Average loss: 1.8397873903159052\n",
            "Epoch: 323, Len of Training loss: 36, Average loss: 1.9683143662081823\n",
            "Len of Validation loss: 128, Average loss: 2.0413288311101496\n",
            "Epoch: 324, Len of Training loss: 36, Average loss: 2.058450847864151\n",
            "Len of Validation loss: 128, Average loss: 1.8588250286411494\n",
            "Epoch: 325, Len of Training loss: 36, Average loss: 2.0085015926096172\n",
            "Len of Validation loss: 128, Average loss: 1.9007841446436942\n",
            "Epoch: 326, Len of Training loss: 36, Average loss: 1.989033086432351\n",
            "Len of Validation loss: 128, Average loss: 1.8572783530689776\n",
            "Epoch: 327, Len of Training loss: 36, Average loss: 1.963163286447525\n",
            "Len of Validation loss: 128, Average loss: 1.9389312686398625\n",
            "Epoch: 328, Len of Training loss: 36, Average loss: 1.995998289850023\n",
            "Len of Validation loss: 128, Average loss: 1.90411417465657\n",
            "Epoch: 329, Len of Training loss: 36, Average loss: 1.9619360930389829\n",
            "Len of Validation loss: 128, Average loss: 1.8428520299494267\n",
            "Epoch: 330, Len of Training loss: 36, Average loss: 1.943768497970369\n",
            "Len of Validation loss: 128, Average loss: 1.933184009976685\n",
            "Epoch: 331, Len of Training loss: 36, Average loss: 1.9624590939945645\n",
            "Len of Validation loss: 128, Average loss: 1.854513212107122\n",
            "Epoch: 332, Len of Training loss: 36, Average loss: 1.9416120284133487\n",
            "Len of Validation loss: 128, Average loss: 1.8457125974819064\n",
            "Epoch: 333, Len of Training loss: 36, Average loss: 1.9378063082695007\n",
            "Len of Validation loss: 128, Average loss: 1.822323402389884\n",
            "Epoch: 334, Len of Training loss: 36, Average loss: 1.9374861584769354\n",
            "Len of Validation loss: 128, Average loss: 1.8696766295470297\n",
            "Epoch: 335, Len of Training loss: 36, Average loss: 1.9925048682424757\n",
            "Len of Validation loss: 128, Average loss: 1.9636728349141777\n",
            "Epoch: 336, Len of Training loss: 36, Average loss: 1.97600457072258\n",
            "Len of Validation loss: 128, Average loss: 1.8175464957021177\n",
            "Epoch: 337, Len of Training loss: 36, Average loss: 2.0083798368771872\n",
            "Len of Validation loss: 128, Average loss: 1.8776390277780592\n",
            "Epoch: 338, Len of Training loss: 36, Average loss: 1.9403208361731634\n",
            "Len of Validation loss: 128, Average loss: 1.884249735623598\n",
            "Epoch: 339, Len of Training loss: 36, Average loss: 2.0095915132098727\n",
            "Len of Validation loss: 128, Average loss: 1.9440799006260931\n",
            "Epoch: 340, Len of Training loss: 36, Average loss: 1.9703340927759807\n",
            "Len of Validation loss: 128, Average loss: 1.8737358509097248\n",
            "Epoch: 341, Len of Training loss: 36, Average loss: 1.9588591390185885\n",
            "Len of Validation loss: 128, Average loss: 2.0104362750425935\n",
            "Epoch: 342, Len of Training loss: 36, Average loss: 2.0269707805580564\n",
            "Len of Validation loss: 128, Average loss: 1.9169567227363586\n",
            "Epoch: 343, Len of Training loss: 36, Average loss: 1.9353289769755468\n",
            "Len of Validation loss: 128, Average loss: 1.7897728697862476\n",
            "Epoch: 344, Len of Training loss: 36, Average loss: 1.9301502638392978\n",
            "Len of Validation loss: 128, Average loss: 1.812402622308582\n",
            "Epoch: 345, Len of Training loss: 36, Average loss: 1.9365749624040391\n",
            "Len of Validation loss: 128, Average loss: 1.8999337926506996\n",
            "Epoch: 346, Len of Training loss: 36, Average loss: 1.926869769891103\n",
            "Len of Validation loss: 128, Average loss: 1.8427131967619061\n",
            "Epoch: 347, Len of Training loss: 36, Average loss: 1.9118849999374814\n",
            "Len of Validation loss: 128, Average loss: 1.8697210480459034\n",
            "Epoch: 348, Len of Training loss: 36, Average loss: 1.9712616900602977\n",
            "Len of Validation loss: 128, Average loss: 1.851548426086083\n",
            "Epoch: 349, Len of Training loss: 36, Average loss: 1.9027690125836267\n",
            "Len of Validation loss: 128, Average loss: 1.8285513813607395\n",
            "Epoch: 350, Len of Training loss: 36, Average loss: 1.9489648871951633\n",
            "Len of Validation loss: 128, Average loss: 1.910578881856054\n",
            "Epoch: 351, Len of Training loss: 36, Average loss: 1.960580845673879\n",
            "Len of Validation loss: 128, Average loss: 1.912882638629526\n",
            "Epoch: 352, Len of Training loss: 36, Average loss: 1.9448245002163782\n",
            "Len of Validation loss: 128, Average loss: 1.9346875241026282\n",
            "Epoch: 353, Len of Training loss: 36, Average loss: 1.9475258025858138\n",
            "Len of Validation loss: 128, Average loss: 1.9037029310129583\n",
            "Epoch: 354, Len of Training loss: 36, Average loss: 1.9237683415412903\n",
            "Len of Validation loss: 128, Average loss: 1.7998209297657013\n",
            "Epoch: 355, Len of Training loss: 36, Average loss: 1.95998701122072\n",
            "Len of Validation loss: 128, Average loss: 1.8994974121451378\n",
            "Epoch: 356, Len of Training loss: 36, Average loss: 1.9215285844273038\n",
            "Len of Validation loss: 128, Average loss: 1.8938261084258556\n",
            "Epoch: 357, Len of Training loss: 36, Average loss: 1.9695314069588978\n",
            "Len of Validation loss: 128, Average loss: 1.9567068074829876\n",
            "Epoch: 358, Len of Training loss: 36, Average loss: 1.926341046889623\n",
            "Len of Validation loss: 128, Average loss: 1.8289759133476764\n",
            "Epoch: 359, Len of Training loss: 36, Average loss: 1.9123861359225378\n",
            "Len of Validation loss: 128, Average loss: 1.7862830770900473\n",
            "Epoch: 360, Len of Training loss: 36, Average loss: 2.002568440304862\n",
            "Len of Validation loss: 128, Average loss: 2.1266891341656446\n",
            "Epoch: 361, Len of Training loss: 36, Average loss: 1.9328107237815857\n",
            "Len of Validation loss: 128, Average loss: 1.8814188605174422\n",
            "Epoch: 362, Len of Training loss: 36, Average loss: 1.9134730796019237\n",
            "Len of Validation loss: 128, Average loss: 1.8563406609464437\n",
            "Epoch: 363, Len of Training loss: 36, Average loss: 1.9407251609696283\n",
            "Len of Validation loss: 128, Average loss: 1.9099399303086102\n",
            "Epoch: 364, Len of Training loss: 36, Average loss: 1.9465538726912603\n",
            "Len of Validation loss: 128, Average loss: 1.8036648554261774\n",
            "Epoch: 365, Len of Training loss: 36, Average loss: 1.9255904754002888\n",
            "Len of Validation loss: 128, Average loss: 1.9245939296670258\n",
            "Epoch: 366, Len of Training loss: 36, Average loss: 1.929204050037596\n",
            "Len of Validation loss: 128, Average loss: 1.8915454782545567\n",
            "Epoch: 367, Len of Training loss: 36, Average loss: 1.928625116745631\n",
            "Len of Validation loss: 128, Average loss: 1.8627767572179437\n",
            "Epoch: 368, Len of Training loss: 36, Average loss: 1.92452934384346\n",
            "Len of Validation loss: 128, Average loss: 1.8863180833868682\n",
            "Epoch: 369, Len of Training loss: 36, Average loss: 1.9103783170382183\n",
            "Len of Validation loss: 128, Average loss: 1.7948608198203146\n",
            "Epoch: 370, Len of Training loss: 36, Average loss: 1.889019850227568\n",
            "Len of Validation loss: 128, Average loss: 1.791529495967552\n",
            "Epoch: 371, Len of Training loss: 36, Average loss: 1.8899803757667542\n",
            "Len of Validation loss: 128, Average loss: 1.8157834224402905\n",
            "Epoch: 372, Len of Training loss: 36, Average loss: 1.9483017093605466\n",
            "Len of Validation loss: 128, Average loss: 1.885633117519319\n",
            "Epoch: 373, Len of Training loss: 36, Average loss: 1.9225931333170996\n",
            "Len of Validation loss: 128, Average loss: 1.81653215829283\n",
            "Epoch: 374, Len of Training loss: 36, Average loss: 1.8876042465368907\n",
            "Len of Validation loss: 128, Average loss: 1.828262388240546\n",
            "Epoch: 375, Len of Training loss: 36, Average loss: 1.9216104414727952\n",
            "Len of Validation loss: 128, Average loss: 1.8676149998791516\n",
            "Epoch: 376, Len of Training loss: 36, Average loss: 1.892153607474433\n",
            "Len of Validation loss: 128, Average loss: 1.9321474628522992\n",
            "Epoch: 377, Len of Training loss: 36, Average loss: 1.9774342046843634\n",
            "Len of Validation loss: 128, Average loss: 1.9425680451095104\n",
            "Epoch: 378, Len of Training loss: 36, Average loss: 1.923925389846166\n",
            "Len of Validation loss: 128, Average loss: 1.939306286163628\n",
            "Epoch: 379, Len of Training loss: 36, Average loss: 1.960732423596912\n",
            "Len of Validation loss: 128, Average loss: 1.809755478054285\n",
            "Epoch: 380, Len of Training loss: 36, Average loss: 1.975340382920371\n",
            "Len of Validation loss: 128, Average loss: 1.876024947501719\n",
            "Epoch: 381, Len of Training loss: 36, Average loss: 1.9239070018132527\n",
            "Len of Validation loss: 128, Average loss: 1.8366705542430282\n",
            "Epoch: 382, Len of Training loss: 36, Average loss: 1.935339583290948\n",
            "Len of Validation loss: 128, Average loss: 1.8605712172575295\n",
            "Epoch: 383, Len of Training loss: 36, Average loss: 1.9298380182849035\n",
            "Len of Validation loss: 128, Average loss: 1.783794459886849\n",
            "Epoch: 384, Len of Training loss: 36, Average loss: 1.9775337643093533\n",
            "Len of Validation loss: 128, Average loss: 1.8607999021187425\n",
            "Epoch: 385, Len of Training loss: 36, Average loss: 1.9326380458143022\n",
            "Len of Validation loss: 128, Average loss: 1.8155457386747003\n",
            "Epoch: 386, Len of Training loss: 36, Average loss: 1.9568515751096938\n",
            "Len of Validation loss: 128, Average loss: 1.792295736959204\n",
            "Epoch: 387, Len of Training loss: 36, Average loss: 1.887428383032481\n",
            "Len of Validation loss: 128, Average loss: 1.7850141762755811\n",
            "Epoch: 388, Len of Training loss: 36, Average loss: 1.9090763198004828\n",
            "Len of Validation loss: 128, Average loss: 1.891646547242999\n",
            "Epoch: 389, Len of Training loss: 36, Average loss: 1.9876573582490284\n",
            "Len of Validation loss: 128, Average loss: 1.9092435115016997\n",
            "Epoch: 390, Len of Training loss: 36, Average loss: 1.913825402657191\n",
            "Len of Validation loss: 128, Average loss: 1.8216738421469927\n",
            "Epoch: 391, Len of Training loss: 36, Average loss: 1.9097396499580808\n",
            "Len of Validation loss: 128, Average loss: 1.8232407397590578\n",
            "Epoch: 392, Len of Training loss: 36, Average loss: 1.8835695915751987\n",
            "Len of Validation loss: 128, Average loss: 1.8236475044395775\n",
            "Epoch: 393, Len of Training loss: 36, Average loss: 1.8659572071499295\n",
            "Len of Validation loss: 128, Average loss: 1.7856201552785933\n",
            "Epoch: 394, Len of Training loss: 36, Average loss: 1.9520844717820485\n",
            "Len of Validation loss: 128, Average loss: 1.8985661757178605\n",
            "Epoch: 395, Len of Training loss: 36, Average loss: 1.9023487468560536\n",
            "Len of Validation loss: 128, Average loss: 1.8692182069644332\n",
            "Epoch: 396, Len of Training loss: 36, Average loss: 1.9602033860153623\n",
            "Len of Validation loss: 128, Average loss: 1.9226824454963207\n",
            "Epoch: 397, Len of Training loss: 36, Average loss: 1.9218216372860804\n",
            "Len of Validation loss: 128, Average loss: 1.8556266285013407\n",
            "Epoch: 398, Len of Training loss: 36, Average loss: 1.9181855552726321\n",
            "Len of Validation loss: 128, Average loss: 1.909006194677204\n",
            "Epoch: 399, Len of Training loss: 36, Average loss: 1.9005878302786086\n",
            "Len of Validation loss: 128, Average loss: 1.7492768606171012\n",
            "Epoch: 400, Len of Training loss: 36, Average loss: 1.9057800273100536\n",
            "Len of Validation loss: 128, Average loss: 1.870625926880166\n",
            "Epoch: 401, Len of Training loss: 36, Average loss: 1.9561273687415652\n",
            "Len of Validation loss: 128, Average loss: 2.194024018011987\n",
            "Epoch: 402, Len of Training loss: 36, Average loss: 1.9386427866088018\n",
            "Len of Validation loss: 128, Average loss: 1.8072926360182464\n",
            "Epoch: 403, Len of Training loss: 36, Average loss: 1.8577144973807864\n",
            "Len of Validation loss: 128, Average loss: 1.8006394002586603\n",
            "Epoch: 404, Len of Training loss: 36, Average loss: 1.9036053352885776\n",
            "Len of Validation loss: 128, Average loss: 1.9735382900107652\n",
            "Epoch: 405, Len of Training loss: 36, Average loss: 1.9108806086911097\n",
            "Len of Validation loss: 128, Average loss: 1.8248438779264688\n",
            "Epoch: 406, Len of Training loss: 36, Average loss: 1.8625310858090718\n",
            "Len of Validation loss: 128, Average loss: 1.7473922411445528\n",
            "Epoch: 407, Len of Training loss: 36, Average loss: 1.9274378087785509\n",
            "Len of Validation loss: 128, Average loss: 1.8982660067267716\n",
            "Epoch: 408, Len of Training loss: 36, Average loss: 1.9732080830468073\n",
            "Len of Validation loss: 128, Average loss: 1.8275794661603868\n",
            "Epoch: 409, Len of Training loss: 36, Average loss: 1.8820252451631758\n",
            "Len of Validation loss: 128, Average loss: 1.9756873801816255\n",
            "Epoch: 410, Len of Training loss: 36, Average loss: 1.9230096009042528\n",
            "Len of Validation loss: 128, Average loss: 1.7750665650237352\n",
            "Epoch: 411, Len of Training loss: 36, Average loss: 1.8849288523197174\n",
            "Len of Validation loss: 128, Average loss: 1.8248508465476334\n",
            "Epoch: 412, Len of Training loss: 36, Average loss: 1.9225547578599718\n",
            "Len of Validation loss: 128, Average loss: 1.8206993162166327\n",
            "Epoch: 413, Len of Training loss: 36, Average loss: 1.9050881034798093\n",
            "Len of Validation loss: 128, Average loss: 1.8578378884121776\n",
            "Epoch: 414, Len of Training loss: 36, Average loss: 1.8728343281481001\n",
            "Len of Validation loss: 128, Average loss: 1.7896678007673472\n",
            "Epoch: 415, Len of Training loss: 36, Average loss: 1.891586230860816\n",
            "Len of Validation loss: 128, Average loss: 1.804978137370199\n",
            "Epoch: 416, Len of Training loss: 36, Average loss: 1.8814399705992804\n",
            "Len of Validation loss: 128, Average loss: 1.7977327445987612\n",
            "Epoch: 417, Len of Training loss: 36, Average loss: 1.9190689225991566\n",
            "Len of Validation loss: 128, Average loss: 1.8737675799056888\n",
            "Epoch: 418, Len of Training loss: 36, Average loss: 1.8669376240836248\n",
            "Len of Validation loss: 128, Average loss: 1.7699511307291687\n",
            "Epoch: 419, Len of Training loss: 36, Average loss: 1.8757166895601485\n",
            "Len of Validation loss: 128, Average loss: 1.79380161059089\n",
            "Epoch: 420, Len of Training loss: 36, Average loss: 1.8821871711148157\n",
            "Len of Validation loss: 128, Average loss: 1.7839966053143144\n",
            "Epoch: 421, Len of Training loss: 36, Average loss: 1.9312844773133595\n",
            "Len of Validation loss: 128, Average loss: 1.8001348089892417\n",
            "Epoch: 422, Len of Training loss: 36, Average loss: 1.9050090114275615\n",
            "Len of Validation loss: 128, Average loss: 1.8898261529393494\n",
            "Epoch: 423, Len of Training loss: 36, Average loss: 1.8899644116560619\n",
            "Len of Validation loss: 128, Average loss: 1.818144291639328\n",
            "Epoch: 424, Len of Training loss: 36, Average loss: 1.8374037676387363\n",
            "Len of Validation loss: 128, Average loss: 1.7609839646611363\n",
            "Epoch: 425, Len of Training loss: 36, Average loss: 1.865920967525906\n",
            "Len of Validation loss: 128, Average loss: 1.7549171284772456\n",
            "Epoch: 426, Len of Training loss: 36, Average loss: 1.8443567156791687\n",
            "Len of Validation loss: 128, Average loss: 1.770316261332482\n",
            "Epoch: 427, Len of Training loss: 36, Average loss: 1.8839874201350741\n",
            "Len of Validation loss: 128, Average loss: 1.8975070281885564\n",
            "Epoch: 428, Len of Training loss: 36, Average loss: 1.887716488705741\n",
            "Len of Validation loss: 128, Average loss: 1.7944816909730434\n",
            "Epoch: 429, Len of Training loss: 36, Average loss: 1.8608752257294126\n",
            "Len of Validation loss: 128, Average loss: 1.7931778610218316\n",
            "Epoch: 430, Len of Training loss: 36, Average loss: 1.9223664436075423\n",
            "Len of Validation loss: 128, Average loss: 1.8405598951503634\n",
            "Epoch: 431, Len of Training loss: 36, Average loss: 1.908900519212087\n",
            "Len of Validation loss: 128, Average loss: 1.80045848316513\n",
            "Epoch: 432, Len of Training loss: 36, Average loss: 1.9189300305313535\n",
            "Len of Validation loss: 128, Average loss: 1.80646184226498\n",
            "Epoch: 433, Len of Training loss: 36, Average loss: 1.8791305422782898\n",
            "Len of Validation loss: 128, Average loss: 1.775659203529358\n",
            "Epoch: 434, Len of Training loss: 36, Average loss: 1.8627615438567267\n",
            "Len of Validation loss: 128, Average loss: 1.807900790590793\n",
            "Epoch: 435, Len of Training loss: 36, Average loss: 1.8593720992406209\n",
            "Len of Validation loss: 128, Average loss: 1.8096528502646834\n",
            "Epoch: 436, Len of Training loss: 36, Average loss: 1.8988822003205617\n",
            "Len of Validation loss: 128, Average loss: 1.8367883870378137\n",
            "Epoch: 437, Len of Training loss: 36, Average loss: 1.903135488430659\n",
            "Len of Validation loss: 128, Average loss: 1.7291408786550164\n",
            "Epoch: 438, Len of Training loss: 36, Average loss: 1.900397800736957\n",
            "Len of Validation loss: 128, Average loss: 1.7985024272929877\n",
            "Epoch: 439, Len of Training loss: 36, Average loss: 1.8678869042131636\n",
            "Len of Validation loss: 128, Average loss: 1.7362094521522522\n",
            "Epoch: 440, Len of Training loss: 36, Average loss: 1.8728744056489732\n",
            "Len of Validation loss: 128, Average loss: 1.7710992807988077\n",
            "Epoch: 441, Len of Training loss: 36, Average loss: 1.88717473215527\n",
            "Len of Validation loss: 128, Average loss: 1.866009653545916\n",
            "Epoch: 442, Len of Training loss: 36, Average loss: 1.8737830387221441\n",
            "Len of Validation loss: 128, Average loss: 1.7603084836155176\n",
            "Epoch: 443, Len of Training loss: 36, Average loss: 1.851281225681305\n",
            "Len of Validation loss: 128, Average loss: 1.8221771959215403\n",
            "Epoch: 444, Len of Training loss: 36, Average loss: 1.8639149599605136\n",
            "Len of Validation loss: 128, Average loss: 1.8072535258252174\n",
            "Epoch: 445, Len of Training loss: 36, Average loss: 1.8739168776406183\n",
            "Len of Validation loss: 128, Average loss: 1.7847001715563238\n",
            "Epoch: 446, Len of Training loss: 36, Average loss: 1.9068130519655015\n",
            "Len of Validation loss: 128, Average loss: 1.7675712211057544\n",
            "Epoch: 447, Len of Training loss: 36, Average loss: 1.8238947921329074\n",
            "Len of Validation loss: 128, Average loss: 1.8314814504701644\n",
            "Epoch: 448, Len of Training loss: 36, Average loss: 1.9312366611427731\n",
            "Len of Validation loss: 128, Average loss: 1.8972420156933367\n",
            "Epoch: 449, Len of Training loss: 36, Average loss: 1.8661633266343012\n",
            "Len of Validation loss: 128, Average loss: 1.7948576691560447\n",
            "Epoch: 450, Len of Training loss: 36, Average loss: 1.8396611909071605\n",
            "Len of Validation loss: 128, Average loss: 1.7526025197003037\n",
            "Epoch: 451, Len of Training loss: 36, Average loss: 1.830388651953803\n",
            "Len of Validation loss: 128, Average loss: 1.7740009389817715\n",
            "Epoch: 452, Len of Training loss: 36, Average loss: 1.8630605803595648\n",
            "Len of Validation loss: 128, Average loss: 1.7901046513579786\n",
            "Epoch: 453, Len of Training loss: 36, Average loss: 1.8933536973264482\n",
            "Len of Validation loss: 128, Average loss: 1.8453635177575052\n",
            "Epoch: 454, Len of Training loss: 36, Average loss: 1.8300911254352994\n",
            "Len of Validation loss: 128, Average loss: 1.7845649849623442\n",
            "Epoch: 455, Len of Training loss: 36, Average loss: 1.850243283642663\n",
            "Len of Validation loss: 128, Average loss: 1.9024572405032814\n",
            "Epoch: 456, Len of Training loss: 36, Average loss: 1.8782423834005992\n",
            "Len of Validation loss: 128, Average loss: 1.8401624343823642\n",
            "Epoch: 457, Len of Training loss: 36, Average loss: 1.8858206338352628\n",
            "Len of Validation loss: 128, Average loss: 1.7779476381838322\n",
            "Epoch: 458, Len of Training loss: 36, Average loss: 1.8389196064737108\n",
            "Len of Validation loss: 128, Average loss: 1.7778244521468878\n",
            "Epoch: 459, Len of Training loss: 36, Average loss: 1.8320889241165585\n",
            "Len of Validation loss: 128, Average loss: 1.8549779392778873\n",
            "Epoch: 460, Len of Training loss: 36, Average loss: 1.8650855190224118\n",
            "Len of Validation loss: 128, Average loss: 1.876047242200002\n",
            "Epoch: 461, Len of Training loss: 36, Average loss: 1.8611214955647786\n",
            "Len of Validation loss: 128, Average loss: 1.716629604343325\n",
            "Epoch: 462, Len of Training loss: 36, Average loss: 1.8019419097238116\n",
            "Len of Validation loss: 128, Average loss: 1.7421473450958729\n",
            "Epoch: 463, Len of Training loss: 36, Average loss: 1.815311547782686\n",
            "Len of Validation loss: 128, Average loss: 1.8835105726029724\n",
            "Epoch: 464, Len of Training loss: 36, Average loss: 1.852192461490631\n",
            "Len of Validation loss: 128, Average loss: 1.818579437211156\n",
            "Epoch: 465, Len of Training loss: 36, Average loss: 1.8656698597802057\n",
            "Len of Validation loss: 128, Average loss: 1.7873869445174932\n",
            "Epoch: 466, Len of Training loss: 36, Average loss: 1.8403976526525285\n",
            "Len of Validation loss: 128, Average loss: 1.8244313527829945\n",
            "Epoch: 467, Len of Training loss: 36, Average loss: 1.953595330317815\n",
            "Len of Validation loss: 128, Average loss: 1.960026691434905\n",
            "Epoch: 468, Len of Training loss: 36, Average loss: 1.83686696489652\n",
            "Len of Validation loss: 128, Average loss: 1.7681330339983106\n",
            "Epoch: 469, Len of Training loss: 36, Average loss: 1.8156055410703023\n",
            "Len of Validation loss: 128, Average loss: 1.7402319223619998\n",
            "Epoch: 470, Len of Training loss: 36, Average loss: 1.8307830459541745\n",
            "Len of Validation loss: 128, Average loss: 1.7918283762410283\n",
            "Epoch: 471, Len of Training loss: 36, Average loss: 1.8256268964873419\n",
            "Len of Validation loss: 128, Average loss: 1.7498701051808894\n",
            "Epoch: 472, Len of Training loss: 36, Average loss: 1.8081090483400557\n",
            "Len of Validation loss: 128, Average loss: 1.7178865645546466\n",
            "Epoch: 473, Len of Training loss: 36, Average loss: 1.8270936906337738\n",
            "Len of Validation loss: 128, Average loss: 1.7621822860091925\n",
            "Epoch: 474, Len of Training loss: 36, Average loss: 1.835707661178377\n",
            "Len of Validation loss: 128, Average loss: 1.7462900888640434\n",
            "Epoch: 475, Len of Training loss: 36, Average loss: 1.8339648246765137\n",
            "Len of Validation loss: 128, Average loss: 1.790701820049435\n",
            "Epoch: 476, Len of Training loss: 36, Average loss: 1.8347978327009413\n",
            "Len of Validation loss: 128, Average loss: 1.7529116161167622\n",
            "Epoch: 477, Len of Training loss: 36, Average loss: 1.8180280526479085\n",
            "Len of Validation loss: 128, Average loss: 1.7914912328124046\n",
            "Epoch: 478, Len of Training loss: 36, Average loss: 1.8271265195475683\n",
            "Len of Validation loss: 128, Average loss: 1.7534479482565075\n",
            "Epoch: 479, Len of Training loss: 36, Average loss: 1.8305759065681033\n",
            "Len of Validation loss: 128, Average loss: 1.7609469469171017\n",
            "Epoch: 480, Len of Training loss: 36, Average loss: 1.8074781894683838\n",
            "Len of Validation loss: 128, Average loss: 1.734593512956053\n",
            "Epoch: 481, Len of Training loss: 36, Average loss: 1.7953735126389399\n",
            "Len of Validation loss: 128, Average loss: 1.7365728495642543\n",
            "Epoch: 482, Len of Training loss: 36, Average loss: 1.8409292929702334\n",
            "Len of Validation loss: 128, Average loss: 1.8453943002969027\n",
            "Epoch: 483, Len of Training loss: 36, Average loss: 1.8725008401605818\n",
            "Len of Validation loss: 128, Average loss: 1.7912761340849102\n",
            "Epoch: 484, Len of Training loss: 36, Average loss: 1.8037441538439856\n",
            "Len of Validation loss: 128, Average loss: 1.8202802245505154\n",
            "Epoch: 485, Len of Training loss: 36, Average loss: 1.842370675669776\n",
            "Len of Validation loss: 128, Average loss: 1.790440387558192\n",
            "Epoch: 486, Len of Training loss: 36, Average loss: 1.873548020919164\n",
            "Len of Validation loss: 128, Average loss: 1.8093275972642004\n",
            "Epoch: 487, Len of Training loss: 36, Average loss: 1.8594526019361284\n",
            "Len of Validation loss: 128, Average loss: 1.8485980653204024\n",
            "Epoch: 488, Len of Training loss: 36, Average loss: 1.7977500591013167\n",
            "Len of Validation loss: 128, Average loss: 1.7634820574894547\n",
            "Epoch: 489, Len of Training loss: 36, Average loss: 1.8337888022263844\n",
            "Len of Validation loss: 128, Average loss: 1.809056896949187\n",
            "Epoch: 490, Len of Training loss: 36, Average loss: 1.8522835241423712\n",
            "Len of Validation loss: 128, Average loss: 1.7236153967678547\n",
            "Epoch: 491, Len of Training loss: 36, Average loss: 1.8705088595549266\n",
            "Len of Validation loss: 128, Average loss: 1.7806608425453305\n",
            "Epoch: 492, Len of Training loss: 36, Average loss: 1.8399019605583615\n",
            "Len of Validation loss: 128, Average loss: 1.7474872912280262\n",
            "Epoch: 493, Len of Training loss: 36, Average loss: 1.8177321586343977\n",
            "Len of Validation loss: 128, Average loss: 1.7966225068084896\n",
            "Epoch: 494, Len of Training loss: 36, Average loss: 1.853744192255868\n",
            "Len of Validation loss: 128, Average loss: 1.845480412710458\n",
            "Epoch: 495, Len of Training loss: 36, Average loss: 1.8644525640540652\n",
            "Len of Validation loss: 128, Average loss: 1.8057520240545273\n",
            "Epoch: 496, Len of Training loss: 36, Average loss: 1.8368701901700761\n",
            "Len of Validation loss: 128, Average loss: 1.7708344091661274\n",
            "Epoch: 497, Len of Training loss: 36, Average loss: 1.8353956474198236\n",
            "Len of Validation loss: 128, Average loss: 1.7914073639549315\n",
            "Epoch: 498, Len of Training loss: 36, Average loss: 1.8223637011316087\n",
            "Len of Validation loss: 128, Average loss: 1.7761726723983884\n",
            "Epoch: 499, Len of Training loss: 36, Average loss: 1.8604471484820049\n",
            "Len of Validation loss: 128, Average loss: 1.8404152998700738\n",
            "Epoch: 500, Len of Training loss: 36, Average loss: 1.8444350361824036\n",
            "Len of Validation loss: 128, Average loss: 1.8481351367663592\n",
            "Epoch: 501, Len of Training loss: 36, Average loss: 1.8340456551975675\n",
            "Len of Validation loss: 128, Average loss: 1.91227547544986\n",
            "Epoch: 502, Len of Training loss: 36, Average loss: 1.9632509152094524\n",
            "Len of Validation loss: 128, Average loss: 1.8376100840978324\n",
            "Epoch: 503, Len of Training loss: 36, Average loss: 1.895966798067093\n",
            "Len of Validation loss: 128, Average loss: 1.7283357526175678\n",
            "Epoch: 504, Len of Training loss: 36, Average loss: 1.853754586643643\n",
            "Len of Validation loss: 128, Average loss: 1.7398438432719558\n",
            "Epoch: 505, Len of Training loss: 36, Average loss: 1.8152053720421262\n",
            "Len of Validation loss: 128, Average loss: 1.7510408605448902\n",
            "Epoch: 506, Len of Training loss: 36, Average loss: 1.774581578042772\n",
            "Len of Validation loss: 128, Average loss: 1.7840305655263364\n",
            "Epoch: 507, Len of Training loss: 36, Average loss: 1.803029351764255\n",
            "Len of Validation loss: 128, Average loss: 1.7346697179600596\n",
            "Epoch: 508, Len of Training loss: 36, Average loss: 1.841342740588718\n",
            "Len of Validation loss: 128, Average loss: 1.7706616707146168\n",
            "Epoch: 509, Len of Training loss: 36, Average loss: 1.8059127397007413\n",
            "Len of Validation loss: 128, Average loss: 1.8111050841398537\n",
            "Epoch: 510, Len of Training loss: 36, Average loss: 1.8143133057488336\n",
            "Len of Validation loss: 128, Average loss: 1.7463399290572852\n",
            "Epoch: 511, Len of Training loss: 36, Average loss: 1.8076768153243594\n",
            "Len of Validation loss: 128, Average loss: 1.8512147678993642\n",
            "Epoch: 512, Len of Training loss: 36, Average loss: 1.819098734193378\n",
            "Len of Validation loss: 128, Average loss: 1.778189821401611\n",
            "Epoch: 513, Len of Training loss: 36, Average loss: 1.797281609641181\n",
            "Len of Validation loss: 128, Average loss: 1.8279552976600826\n",
            "Epoch: 514, Len of Training loss: 36, Average loss: 1.8090689546532102\n",
            "Len of Validation loss: 128, Average loss: 1.752606582827866\n",
            "Epoch: 515, Len of Training loss: 36, Average loss: 1.801346133152644\n",
            "Len of Validation loss: 128, Average loss: 1.8089702329598367\n",
            "Epoch: 516, Len of Training loss: 36, Average loss: 1.8345252441035376\n",
            "Len of Validation loss: 128, Average loss: 1.736050422070548\n",
            "Epoch: 517, Len of Training loss: 36, Average loss: 1.8193197349707286\n",
            "Len of Validation loss: 128, Average loss: 1.7463027688208967\n",
            "Epoch: 518, Len of Training loss: 36, Average loss: 1.7988325589232974\n",
            "Len of Validation loss: 128, Average loss: 1.796759549062699\n",
            "Epoch: 519, Len of Training loss: 36, Average loss: 1.825589097208447\n",
            "Len of Validation loss: 128, Average loss: 1.73168889968656\n",
            "Epoch: 520, Len of Training loss: 36, Average loss: 1.7981849246554904\n",
            "Len of Validation loss: 128, Average loss: 1.7785310307517648\n",
            "Epoch: 521, Len of Training loss: 36, Average loss: 1.8239577147695754\n",
            "Len of Validation loss: 128, Average loss: 1.8691462809219956\n",
            "Epoch: 522, Len of Training loss: 36, Average loss: 1.7877776126066844\n",
            "Len of Validation loss: 128, Average loss: 1.6877398896031082\n",
            "Epoch: 523, Len of Training loss: 36, Average loss: 1.8147942158910964\n",
            "Len of Validation loss: 128, Average loss: 1.7323384443297982\n",
            "Epoch: 524, Len of Training loss: 36, Average loss: 1.8066433005862765\n",
            "Len of Validation loss: 128, Average loss: 1.8528538318350911\n",
            "Epoch: 525, Len of Training loss: 36, Average loss: 1.7977501584423914\n",
            "Len of Validation loss: 128, Average loss: 1.7094942010007799\n",
            "Epoch: 526, Len of Training loss: 36, Average loss: 1.8100360698170133\n",
            "Len of Validation loss: 128, Average loss: 1.71181978052482\n",
            "Epoch: 527, Len of Training loss: 36, Average loss: 1.7954241434733074\n",
            "Len of Validation loss: 128, Average loss: 1.7371206912212074\n",
            "Epoch: 528, Len of Training loss: 36, Average loss: 1.7910968363285065\n",
            "Len of Validation loss: 128, Average loss: 1.7875330769456923\n",
            "Epoch: 529, Len of Training loss: 36, Average loss: 1.8401386704709795\n",
            "Len of Validation loss: 128, Average loss: 2.0627466021105647\n",
            "Epoch: 530, Len of Training loss: 36, Average loss: 1.8102871113353305\n",
            "Len of Validation loss: 128, Average loss: 1.7254103156737983\n",
            "Epoch: 531, Len of Training loss: 36, Average loss: 1.7899944815370772\n",
            "Len of Validation loss: 128, Average loss: 1.749218477634713\n",
            "Epoch: 532, Len of Training loss: 36, Average loss: 1.8180474672052596\n",
            "Len of Validation loss: 128, Average loss: 1.8520893957465887\n",
            "Epoch: 533, Len of Training loss: 36, Average loss: 1.809496545129352\n",
            "Len of Validation loss: 128, Average loss: 1.7741079959087074\n",
            "Epoch: 534, Len of Training loss: 36, Average loss: 1.8693088326189253\n",
            "Len of Validation loss: 128, Average loss: 1.7689995758701116\n",
            "Epoch: 535, Len of Training loss: 36, Average loss: 1.7931876745488908\n",
            "Len of Validation loss: 128, Average loss: 1.7303543270099908\n",
            "Epoch: 536, Len of Training loss: 36, Average loss: 1.8473560876316495\n",
            "Len of Validation loss: 128, Average loss: 1.780836638994515\n",
            "Epoch: 537, Len of Training loss: 36, Average loss: 1.8411894506878324\n",
            "Len of Validation loss: 128, Average loss: 1.7404724366497248\n",
            "Epoch: 538, Len of Training loss: 36, Average loss: 1.7806955410374536\n",
            "Len of Validation loss: 128, Average loss: 1.7522523375228047\n",
            "Epoch: 539, Len of Training loss: 36, Average loss: 1.8205157385932074\n",
            "Len of Validation loss: 128, Average loss: 1.7745075784623623\n",
            "Epoch: 540, Len of Training loss: 36, Average loss: 1.8179747760295868\n",
            "Len of Validation loss: 128, Average loss: 1.7773226080462337\n",
            "Epoch: 541, Len of Training loss: 36, Average loss: 1.8180229034688737\n",
            "Len of Validation loss: 128, Average loss: 1.7424777382984757\n",
            "Epoch: 542, Len of Training loss: 36, Average loss: 1.775281439224879\n",
            "Len of Validation loss: 128, Average loss: 1.739401048514992\n",
            "Epoch: 543, Len of Training loss: 36, Average loss: 1.7826976246303983\n",
            "Len of Validation loss: 128, Average loss: 1.7183454800397158\n",
            "Epoch: 544, Len of Training loss: 36, Average loss: 1.798034088479148\n",
            "Len of Validation loss: 128, Average loss: 1.7727638720534742\n",
            "Epoch: 545, Len of Training loss: 36, Average loss: 1.829138179620107\n",
            "Len of Validation loss: 128, Average loss: 1.804666317999363\n",
            "Epoch: 546, Len of Training loss: 36, Average loss: 1.789374570051829\n",
            "Len of Validation loss: 128, Average loss: 1.7445614961907268\n",
            "Epoch: 547, Len of Training loss: 36, Average loss: 1.7934582365883722\n",
            "Len of Validation loss: 128, Average loss: 1.8306631590239704\n",
            "Epoch: 548, Len of Training loss: 36, Average loss: 1.7973145014709897\n",
            "Len of Validation loss: 128, Average loss: 1.7025273302569985\n",
            "Epoch: 549, Len of Training loss: 36, Average loss: 1.8093899521562788\n",
            "Len of Validation loss: 128, Average loss: 1.8307292154058814\n",
            "Epoch: 550, Len of Training loss: 36, Average loss: 1.8202363749345143\n",
            "Len of Validation loss: 128, Average loss: 1.7372520610224456\n",
            "Epoch: 551, Len of Training loss: 36, Average loss: 1.8230456312497456\n",
            "Len of Validation loss: 128, Average loss: 1.726947284070775\n",
            "Epoch: 552, Len of Training loss: 36, Average loss: 1.8018975655237834\n",
            "Len of Validation loss: 128, Average loss: 1.7194096334278584\n",
            "Epoch: 553, Len of Training loss: 36, Average loss: 1.8111894594298468\n",
            "Len of Validation loss: 128, Average loss: 1.7773348323535174\n",
            "Epoch: 554, Len of Training loss: 36, Average loss: 1.8048657013310327\n",
            "Len of Validation loss: 128, Average loss: 1.7014994812197983\n",
            "Epoch: 555, Len of Training loss: 36, Average loss: 1.7765826914045546\n",
            "Len of Validation loss: 128, Average loss: 1.7565956958569586\n",
            "Epoch: 556, Len of Training loss: 36, Average loss: 1.8283412059148152\n",
            "Len of Validation loss: 128, Average loss: 1.7615444562397897\n",
            "Epoch: 557, Len of Training loss: 36, Average loss: 1.8202987909317017\n",
            "Len of Validation loss: 128, Average loss: 1.784277678001672\n",
            "Epoch: 558, Len of Training loss: 36, Average loss: 1.7964683671792347\n",
            "Len of Validation loss: 128, Average loss: 1.864053525030613\n",
            "Epoch: 559, Len of Training loss: 36, Average loss: 1.8195767733785841\n",
            "Len of Validation loss: 128, Average loss: 1.734491570154205\n",
            "Epoch: 560, Len of Training loss: 36, Average loss: 1.8720872203509014\n",
            "Len of Validation loss: 128, Average loss: 1.714948202483356\n",
            "Epoch: 561, Len of Training loss: 36, Average loss: 1.7924029529094696\n",
            "Len of Validation loss: 128, Average loss: 1.7360435121227056\n",
            "Epoch: 562, Len of Training loss: 36, Average loss: 1.7972512940565746\n",
            "Len of Validation loss: 128, Average loss: 1.7327329234685749\n",
            "Epoch: 563, Len of Training loss: 36, Average loss: 1.8214583430025313\n",
            "Len of Validation loss: 128, Average loss: 1.7613530866801739\n",
            "Epoch: 564, Len of Training loss: 36, Average loss: 1.7443424827522702\n",
            "Len of Validation loss: 128, Average loss: 1.7240413015242666\n",
            "Epoch: 565, Len of Training loss: 36, Average loss: 1.774205184645123\n",
            "Len of Validation loss: 128, Average loss: 1.870441366918385\n",
            "Epoch: 566, Len of Training loss: 36, Average loss: 1.8601055178377364\n",
            "Len of Validation loss: 128, Average loss: 1.7983709550462663\n",
            "Epoch: 567, Len of Training loss: 36, Average loss: 1.8309667507807414\n",
            "Len of Validation loss: 128, Average loss: 1.787835936062038\n",
            "Epoch: 568, Len of Training loss: 36, Average loss: 1.7978966070546045\n",
            "Len of Validation loss: 128, Average loss: 1.757900707423687\n",
            "Epoch: 569, Len of Training loss: 36, Average loss: 1.8325253460142348\n",
            "Len of Validation loss: 128, Average loss: 1.7773744063451886\n",
            "Epoch: 570, Len of Training loss: 36, Average loss: 1.7678700851069555\n",
            "Len of Validation loss: 128, Average loss: 1.7618282258044928\n",
            "Epoch: 571, Len of Training loss: 36, Average loss: 1.7696586549282074\n",
            "Len of Validation loss: 128, Average loss: 1.716062743216753\n",
            "Epoch: 572, Len of Training loss: 36, Average loss: 1.7797660827636719\n",
            "Len of Validation loss: 128, Average loss: 1.7359462815802544\n",
            "Epoch: 573, Len of Training loss: 36, Average loss: 1.8082510034243267\n",
            "Len of Validation loss: 128, Average loss: 1.803546151611954\n",
            "Epoch: 574, Len of Training loss: 36, Average loss: 1.8387141558859084\n",
            "Len of Validation loss: 128, Average loss: 1.7330499703530222\n",
            "Epoch: 575, Len of Training loss: 36, Average loss: 1.7975164718098111\n",
            "Len of Validation loss: 128, Average loss: 1.7279589283280075\n",
            "Epoch: 576, Len of Training loss: 36, Average loss: 1.8036288652155135\n",
            "Len of Validation loss: 128, Average loss: 1.7285207861568779\n",
            "Epoch: 577, Len of Training loss: 36, Average loss: 1.7726118763287861\n",
            "Len of Validation loss: 128, Average loss: 1.7199967973865569\n",
            "Epoch: 578, Len of Training loss: 36, Average loss: 1.796626607577006\n",
            "Len of Validation loss: 128, Average loss: 1.728677146602422\n",
            "Epoch: 579, Len of Training loss: 36, Average loss: 1.788765960269504\n",
            "Len of Validation loss: 128, Average loss: 1.8108747182413936\n",
            "Epoch: 580, Len of Training loss: 36, Average loss: 1.7651409142547183\n",
            "Len of Validation loss: 128, Average loss: 1.691920518875122\n",
            "Epoch: 581, Len of Training loss: 36, Average loss: 1.7535802523295085\n",
            "Len of Validation loss: 128, Average loss: 1.752632207935676\n",
            "Epoch: 582, Len of Training loss: 36, Average loss: 1.7754862937662337\n",
            "Len of Validation loss: 128, Average loss: 1.833209082717076\n",
            "Epoch: 583, Len of Training loss: 36, Average loss: 1.767678393257989\n",
            "Len of Validation loss: 128, Average loss: 1.7532109275925905\n",
            "Epoch: 584, Len of Training loss: 36, Average loss: 1.8253940410084195\n",
            "Len of Validation loss: 128, Average loss: 1.8218706124462187\n",
            "Epoch: 585, Len of Training loss: 36, Average loss: 1.807031449344423\n",
            "Len of Validation loss: 128, Average loss: 1.7117933325935155\n",
            "Epoch: 586, Len of Training loss: 36, Average loss: 1.7497502664724986\n",
            "Len of Validation loss: 128, Average loss: 1.7178753414191306\n",
            "Epoch: 587, Len of Training loss: 36, Average loss: 1.7510822580920324\n",
            "Len of Validation loss: 128, Average loss: 1.8136935597285628\n",
            "Epoch: 588, Len of Training loss: 36, Average loss: 1.7979989250500996\n",
            "Len of Validation loss: 128, Average loss: 1.720539161702618\n",
            "Epoch: 589, Len of Training loss: 36, Average loss: 1.7682541410128276\n",
            "Len of Validation loss: 128, Average loss: 1.7190876733511686\n",
            "Epoch: 590, Len of Training loss: 36, Average loss: 1.7885798215866089\n",
            "Len of Validation loss: 128, Average loss: 1.785216897726059\n",
            "Epoch: 591, Len of Training loss: 36, Average loss: 1.8062283330493503\n",
            "Len of Validation loss: 128, Average loss: 1.7446325784549117\n",
            "Epoch: 592, Len of Training loss: 36, Average loss: 1.792865643898646\n",
            "Len of Validation loss: 128, Average loss: 1.6743528367951512\n",
            "Epoch: 593, Len of Training loss: 36, Average loss: 1.779233306646347\n",
            "Len of Validation loss: 128, Average loss: 1.7605651849880815\n",
            "Epoch: 594, Len of Training loss: 36, Average loss: 1.8228730625576444\n",
            "Len of Validation loss: 128, Average loss: 1.8017702498473227\n",
            "Epoch: 595, Len of Training loss: 36, Average loss: 1.8167913224962022\n",
            "Len of Validation loss: 128, Average loss: 1.7759123612195253\n",
            "Epoch: 596, Len of Training loss: 36, Average loss: 1.743633336491055\n",
            "Len of Validation loss: 128, Average loss: 1.7109469696879387\n",
            "Epoch: 597, Len of Training loss: 36, Average loss: 1.7677889830536313\n",
            "Len of Validation loss: 128, Average loss: 1.791261632926762\n",
            "Epoch: 598, Len of Training loss: 36, Average loss: 1.7761817905637953\n",
            "Len of Validation loss: 128, Average loss: 1.7695021955296397\n",
            "Epoch: 599, Len of Training loss: 36, Average loss: 1.812284419933955\n",
            "Len of Validation loss: 128, Average loss: 1.7153450506739318\n",
            "Epoch: 600, Len of Training loss: 36, Average loss: 1.7588385509120092\n",
            "Len of Validation loss: 128, Average loss: 1.7009222488850355\n",
            "Epoch: 601, Len of Training loss: 36, Average loss: 1.7383227149645488\n",
            "Len of Validation loss: 128, Average loss: 1.6913402450736612\n",
            "Epoch: 602, Len of Training loss: 36, Average loss: 1.7532464795642428\n",
            "Len of Validation loss: 128, Average loss: 1.6869847429916263\n",
            "Epoch: 603, Len of Training loss: 36, Average loss: 1.7696881459818945\n",
            "Len of Validation loss: 128, Average loss: 1.7487552887760103\n",
            "Epoch: 604, Len of Training loss: 36, Average loss: 1.761714220046997\n",
            "Len of Validation loss: 128, Average loss: 1.7188115688040853\n",
            "Epoch: 605, Len of Training loss: 36, Average loss: 1.7468984756204817\n",
            "Len of Validation loss: 128, Average loss: 1.774428813252598\n",
            "Epoch: 606, Len of Training loss: 36, Average loss: 1.780250095658832\n",
            "Len of Validation loss: 128, Average loss: 1.718034123070538\n",
            "Epoch: 607, Len of Training loss: 36, Average loss: 1.7384099430508084\n",
            "Len of Validation loss: 128, Average loss: 1.7013303108979017\n",
            "Epoch: 608, Len of Training loss: 36, Average loss: 1.7388006117608812\n",
            "Len of Validation loss: 128, Average loss: 1.723541859537363\n",
            "Epoch: 609, Len of Training loss: 36, Average loss: 1.7634984387291803\n",
            "Len of Validation loss: 128, Average loss: 1.7526771104894578\n",
            "Epoch: 610, Len of Training loss: 36, Average loss: 1.7759169439474742\n",
            "Len of Validation loss: 128, Average loss: 1.726948177907616\n",
            "Epoch: 611, Len of Training loss: 36, Average loss: 1.7667552795675066\n",
            "Len of Validation loss: 128, Average loss: 1.703692713752389\n",
            "Epoch: 612, Len of Training loss: 36, Average loss: 1.7881779538260565\n",
            "Len of Validation loss: 128, Average loss: 1.7045467523857951\n",
            "Epoch: 613, Len of Training loss: 36, Average loss: 1.7569350964493222\n",
            "Len of Validation loss: 128, Average loss: 1.7014569914899766\n",
            "Epoch: 614, Len of Training loss: 36, Average loss: 1.7493987215889826\n",
            "Len of Validation loss: 128, Average loss: 1.713738395832479\n",
            "Epoch: 615, Len of Training loss: 36, Average loss: 1.8014388316207461\n",
            "Len of Validation loss: 128, Average loss: 1.7713254978880286\n",
            "Epoch: 616, Len of Training loss: 36, Average loss: 1.7583355440033808\n",
            "Len of Validation loss: 128, Average loss: 1.7034727376885712\n",
            "Epoch: 617, Len of Training loss: 36, Average loss: 1.769740002022849\n",
            "Len of Validation loss: 128, Average loss: 1.7701004901900887\n",
            "Epoch: 618, Len of Training loss: 36, Average loss: 1.7840218113528357\n",
            "Len of Validation loss: 128, Average loss: 1.791461880900897\n",
            "Epoch: 619, Len of Training loss: 36, Average loss: 1.790331287516488\n",
            "Len of Validation loss: 128, Average loss: 1.702054874971509\n",
            "Epoch: 620, Len of Training loss: 36, Average loss: 1.768593387471305\n",
            "Len of Validation loss: 128, Average loss: 1.665015327744186\n",
            "Epoch: 621, Len of Training loss: 36, Average loss: 1.7537794941001468\n",
            "Len of Validation loss: 128, Average loss: 1.6913427314721048\n",
            "Epoch: 622, Len of Training loss: 36, Average loss: 1.7724718186590407\n",
            "Len of Validation loss: 128, Average loss: 1.7556554372422397\n",
            "Epoch: 623, Len of Training loss: 36, Average loss: 1.730589316950904\n",
            "Len of Validation loss: 128, Average loss: 1.7275125766173005\n",
            "Epoch: 624, Len of Training loss: 36, Average loss: 1.7368201447857752\n",
            "Len of Validation loss: 128, Average loss: 1.724441957194358\n",
            "Epoch: 625, Len of Training loss: 36, Average loss: 1.777879320912891\n",
            "Len of Validation loss: 128, Average loss: 1.7054091356694698\n",
            "Epoch: 626, Len of Training loss: 36, Average loss: 1.736850294801924\n",
            "Len of Validation loss: 128, Average loss: 1.7342449954012409\n",
            "Epoch: 627, Len of Training loss: 36, Average loss: 1.7410342693328857\n",
            "Len of Validation loss: 128, Average loss: 1.6829654476605356\n",
            "Epoch: 628, Len of Training loss: 36, Average loss: 1.7718373007244534\n",
            "Len of Validation loss: 128, Average loss: 1.797850995324552\n",
            "Epoch: 629, Len of Training loss: 36, Average loss: 1.7568899459309049\n",
            "Len of Validation loss: 128, Average loss: 1.6908169146627188\n",
            "Epoch: 630, Len of Training loss: 36, Average loss: 1.7499882578849792\n",
            "Len of Validation loss: 128, Average loss: 1.7023497885093093\n",
            "Epoch: 631, Len of Training loss: 36, Average loss: 1.7710198329554663\n",
            "Len of Validation loss: 128, Average loss: 1.761629207059741\n",
            "Epoch: 632, Len of Training loss: 36, Average loss: 1.779904067516327\n",
            "Len of Validation loss: 128, Average loss: 1.7505113794468343\n",
            "Epoch: 633, Len of Training loss: 36, Average loss: 1.766314533021715\n",
            "Len of Validation loss: 128, Average loss: 1.7175271483138204\n",
            "Epoch: 634, Len of Training loss: 36, Average loss: 1.7495431502660115\n",
            "Len of Validation loss: 128, Average loss: 1.6860810345970094\n",
            "Epoch: 635, Len of Training loss: 36, Average loss: 1.7188920577367146\n",
            "Len of Validation loss: 128, Average loss: 1.695076241856441\n",
            "Epoch: 636, Len of Training loss: 36, Average loss: 1.7798774904674954\n",
            "Len of Validation loss: 128, Average loss: 1.692286028759554\n",
            "Epoch: 637, Len of Training loss: 36, Average loss: 1.757770429054896\n",
            "Len of Validation loss: 128, Average loss: 1.7067029289901257\n",
            "Epoch: 638, Len of Training loss: 36, Average loss: 1.807980219523112\n",
            "Len of Validation loss: 128, Average loss: 1.7568552091252059\n",
            "Epoch: 639, Len of Training loss: 36, Average loss: 1.7924479345480602\n",
            "Len of Validation loss: 128, Average loss: 1.6768863371107727\n",
            "Epoch: 640, Len of Training loss: 36, Average loss: 1.7135826912191179\n",
            "Len of Validation loss: 128, Average loss: 1.6621036881115288\n",
            "Epoch: 641, Len of Training loss: 36, Average loss: 1.8095490435759227\n",
            "Len of Validation loss: 128, Average loss: 1.9117173962295055\n",
            "Epoch: 642, Len of Training loss: 36, Average loss: 1.8024061189757452\n",
            "Len of Validation loss: 128, Average loss: 1.7326401569880545\n",
            "Epoch: 643, Len of Training loss: 36, Average loss: 1.7338100009494357\n",
            "Len of Validation loss: 128, Average loss: 1.6729036846663803\n",
            "Epoch: 644, Len of Training loss: 36, Average loss: 1.750704242123498\n",
            "Len of Validation loss: 128, Average loss: 1.743513407651335\n",
            "Epoch: 645, Len of Training loss: 36, Average loss: 1.7532722651958466\n",
            "Len of Validation loss: 128, Average loss: 1.731044483371079\n",
            "Epoch: 646, Len of Training loss: 36, Average loss: 1.7109060850408342\n",
            "Len of Validation loss: 128, Average loss: 1.7126121933106333\n",
            "Epoch: 647, Len of Training loss: 36, Average loss: 1.7091511521074507\n",
            "Len of Validation loss: 128, Average loss: 1.7178724398836493\n",
            "Epoch: 648, Len of Training loss: 36, Average loss: 1.7558381723033056\n",
            "Len of Validation loss: 128, Average loss: 1.841355505399406\n",
            "Epoch: 649, Len of Training loss: 36, Average loss: 1.7437647415532007\n",
            "Len of Validation loss: 128, Average loss: 1.7190445601008832\n",
            "Epoch: 650, Len of Training loss: 36, Average loss: 1.726646237903171\n",
            "Len of Validation loss: 128, Average loss: 1.7032431759871542\n",
            "Epoch: 651, Len of Training loss: 36, Average loss: 1.7386502623558044\n",
            "Len of Validation loss: 128, Average loss: 1.7260202006436884\n",
            "Epoch: 652, Len of Training loss: 36, Average loss: 1.7075133323669434\n",
            "Len of Validation loss: 128, Average loss: 1.6938470867462456\n",
            "Epoch: 653, Len of Training loss: 36, Average loss: 1.7351361744933658\n",
            "Len of Validation loss: 128, Average loss: 1.6879160329699516\n",
            "Epoch: 654, Len of Training loss: 36, Average loss: 1.7465058863162994\n",
            "Len of Validation loss: 128, Average loss: 1.7149797107558697\n",
            "Epoch: 655, Len of Training loss: 36, Average loss: 1.733739048242569\n",
            "Len of Validation loss: 128, Average loss: 1.6838953737169504\n",
            "Epoch: 656, Len of Training loss: 36, Average loss: 1.757265826066335\n",
            "Len of Validation loss: 128, Average loss: 1.8149091587401927\n",
            "Epoch: 657, Len of Training loss: 36, Average loss: 1.7594068282180362\n",
            "Len of Validation loss: 128, Average loss: 1.7386855978984386\n",
            "Epoch: 658, Len of Training loss: 36, Average loss: 1.7237787279817793\n",
            "Len of Validation loss: 128, Average loss: 1.690506091574207\n",
            "Epoch: 659, Len of Training loss: 36, Average loss: 1.715261310338974\n",
            "Len of Validation loss: 128, Average loss: 1.728069324977696\n",
            "Epoch: 660, Len of Training loss: 36, Average loss: 1.7545293105973139\n",
            "Len of Validation loss: 128, Average loss: 1.69265869515948\n",
            "Epoch: 661, Len of Training loss: 36, Average loss: 1.7111812167697482\n",
            "Len of Validation loss: 128, Average loss: 1.7015789947472513\n",
            "Epoch: 662, Len of Training loss: 36, Average loss: 1.7249151402049594\n",
            "Len of Validation loss: 128, Average loss: 1.7043174500577152\n",
            "Epoch: 663, Len of Training loss: 36, Average loss: 1.7291642526785533\n",
            "Len of Validation loss: 128, Average loss: 1.7762663925532252\n",
            "Epoch: 664, Len of Training loss: 36, Average loss: 1.7371905479166243\n",
            "Len of Validation loss: 128, Average loss: 1.7126704356633127\n",
            "Epoch: 665, Len of Training loss: 36, Average loss: 1.7582891649670072\n",
            "Len of Validation loss: 128, Average loss: 1.7318403844255954\n",
            "Epoch: 666, Len of Training loss: 36, Average loss: 1.717714528242747\n",
            "Len of Validation loss: 128, Average loss: 1.7011012898292392\n",
            "Epoch: 667, Len of Training loss: 36, Average loss: 1.7263801395893097\n",
            "Len of Validation loss: 128, Average loss: 1.6948516834527254\n",
            "Epoch: 668, Len of Training loss: 36, Average loss: 1.7200806968741946\n",
            "Len of Validation loss: 128, Average loss: 1.7600605669431388\n",
            "Epoch: 669, Len of Training loss: 36, Average loss: 1.7587323586146038\n",
            "Len of Validation loss: 128, Average loss: 1.80720463139005\n",
            "Epoch: 670, Len of Training loss: 36, Average loss: 1.8317992554770575\n",
            "Len of Validation loss: 128, Average loss: 1.7707657220307738\n",
            "Epoch: 671, Len of Training loss: 36, Average loss: 1.7368657423390284\n",
            "Len of Validation loss: 128, Average loss: 1.7777130145113915\n",
            "Epoch: 672, Len of Training loss: 36, Average loss: 1.7610920667648315\n",
            "Len of Validation loss: 128, Average loss: 1.7695297438185662\n",
            "Epoch: 673, Len of Training loss: 36, Average loss: 1.7349309888150957\n",
            "Len of Validation loss: 128, Average loss: 1.6891746211331338\n",
            "Epoch: 674, Len of Training loss: 36, Average loss: 1.7006313966380224\n",
            "Len of Validation loss: 128, Average loss: 1.6649297699332237\n",
            "Epoch: 675, Len of Training loss: 36, Average loss: 1.7167709453238382\n",
            "Len of Validation loss: 128, Average loss: 1.6661798630375415\n",
            "Epoch: 676, Len of Training loss: 36, Average loss: 1.7679167952802446\n",
            "Len of Validation loss: 128, Average loss: 1.777746656909585\n",
            "Epoch: 677, Len of Training loss: 36, Average loss: 1.7571312387784321\n",
            "Len of Validation loss: 128, Average loss: 1.7596927736885846\n",
            "Epoch: 678, Len of Training loss: 36, Average loss: 1.7290250923898485\n",
            "Len of Validation loss: 128, Average loss: 1.6857047518715262\n",
            "Epoch: 679, Len of Training loss: 36, Average loss: 1.7278800441159143\n",
            "Len of Validation loss: 128, Average loss: 1.7341760017443448\n",
            "Epoch: 680, Len of Training loss: 36, Average loss: 1.7514691948890686\n",
            "Len of Validation loss: 128, Average loss: 1.7754995077848434\n",
            "Epoch: 681, Len of Training loss: 36, Average loss: 1.7169194949997797\n",
            "Len of Validation loss: 128, Average loss: 1.6732492798473686\n",
            "Epoch: 682, Len of Training loss: 36, Average loss: 1.7308043936888378\n",
            "Len of Validation loss: 128, Average loss: 1.6828873031772673\n",
            "Epoch: 683, Len of Training loss: 36, Average loss: 1.721738662984636\n",
            "Len of Validation loss: 128, Average loss: 1.7203163621015847\n",
            "Epoch: 684, Len of Training loss: 36, Average loss: 1.7146707342730627\n",
            "Len of Validation loss: 128, Average loss: 1.6758460393175483\n",
            "Epoch: 685, Len of Training loss: 36, Average loss: 1.7264325552516513\n",
            "Len of Validation loss: 128, Average loss: 1.6894632133189589\n",
            "Epoch: 686, Len of Training loss: 36, Average loss: 1.746880097521676\n",
            "Len of Validation loss: 128, Average loss: 1.7143178209662437\n",
            "Epoch: 687, Len of Training loss: 36, Average loss: 1.7579978538884058\n",
            "Len of Validation loss: 128, Average loss: 1.6768664245028049\n",
            "Epoch: 688, Len of Training loss: 36, Average loss: 1.7186329729027219\n",
            "Len of Validation loss: 128, Average loss: 1.6989402337931097\n",
            "Epoch: 689, Len of Training loss: 36, Average loss: 1.7681600782606337\n",
            "Len of Validation loss: 128, Average loss: 1.7990259933285415\n",
            "Epoch: 690, Len of Training loss: 36, Average loss: 1.7828539344999526\n",
            "Len of Validation loss: 128, Average loss: 1.6983757759444416\n",
            "Epoch: 691, Len of Training loss: 36, Average loss: 1.7279915942086115\n",
            "Len of Validation loss: 128, Average loss: 1.676217217464\n",
            "Epoch: 692, Len of Training loss: 36, Average loss: 1.7249472538630168\n",
            "Len of Validation loss: 128, Average loss: 1.804795244242996\n",
            "Epoch: 693, Len of Training loss: 36, Average loss: 1.7675894962416754\n",
            "Len of Validation loss: 128, Average loss: 1.7319100950844586\n",
            "Epoch: 694, Len of Training loss: 36, Average loss: 1.7074550489584606\n",
            "Len of Validation loss: 128, Average loss: 1.6992369145154953\n",
            "Epoch: 695, Len of Training loss: 36, Average loss: 1.6809725049469206\n",
            "Len of Validation loss: 128, Average loss: 1.7030169018544257\n",
            "Epoch: 696, Len of Training loss: 36, Average loss: 1.7367399566703372\n",
            "Len of Validation loss: 128, Average loss: 1.7179414911661297\n",
            "Epoch: 697, Len of Training loss: 36, Average loss: 1.7296922538015578\n",
            "Len of Validation loss: 128, Average loss: 1.6446978501044214\n",
            "Epoch: 698, Len of Training loss: 36, Average loss: 1.766270101070404\n",
            "Len of Validation loss: 128, Average loss: 1.6978929056786\n",
            "Epoch: 699, Len of Training loss: 36, Average loss: 1.7480743726094563\n",
            "Len of Validation loss: 128, Average loss: 1.7849664422683418\n",
            "Epoch: 700, Len of Training loss: 36, Average loss: 1.7044268747170765\n",
            "Len of Validation loss: 128, Average loss: 1.6693384782411158\n",
            "Epoch: 701, Len of Training loss: 36, Average loss: 1.6959220270315807\n",
            "Len of Validation loss: 128, Average loss: 1.7254362946841866\n",
            "Epoch: 702, Len of Training loss: 36, Average loss: 1.6846393545468648\n",
            "Len of Validation loss: 128, Average loss: 1.6779072815552354\n",
            "Epoch: 703, Len of Training loss: 36, Average loss: 1.7106222212314606\n",
            "Len of Validation loss: 128, Average loss: 1.708991423714906\n",
            "Epoch: 704, Len of Training loss: 36, Average loss: 1.7028976215256586\n",
            "Len of Validation loss: 128, Average loss: 1.6868736962787807\n",
            "Epoch: 705, Len of Training loss: 36, Average loss: 1.742569473054674\n",
            "Len of Validation loss: 128, Average loss: 1.7746944951359183\n",
            "Epoch: 706, Len of Training loss: 36, Average loss: 1.7427993483013577\n",
            "Len of Validation loss: 128, Average loss: 1.6838920288719237\n",
            "Epoch: 707, Len of Training loss: 36, Average loss: 1.723719424671597\n",
            "Len of Validation loss: 128, Average loss: 1.6460050870664418\n",
            "Epoch: 708, Len of Training loss: 36, Average loss: 1.7115308741728466\n",
            "Len of Validation loss: 128, Average loss: 1.6994540295563638\n",
            "Epoch: 709, Len of Training loss: 36, Average loss: 1.6959847907225292\n",
            "Len of Validation loss: 128, Average loss: 1.702056843554601\n",
            "Epoch: 710, Len of Training loss: 36, Average loss: 1.691121945778529\n",
            "Len of Validation loss: 128, Average loss: 1.6534818219952285\n",
            "Epoch: 711, Len of Training loss: 36, Average loss: 1.6994576785299513\n",
            "Len of Validation loss: 128, Average loss: 1.6897507386747748\n",
            "Epoch: 712, Len of Training loss: 36, Average loss: 1.717133492231369\n",
            "Len of Validation loss: 128, Average loss: 1.6866031154058874\n",
            "Epoch: 713, Len of Training loss: 36, Average loss: 1.7180151608255174\n",
            "Len of Validation loss: 128, Average loss: 1.6557957334443927\n",
            "Epoch: 714, Len of Training loss: 36, Average loss: 1.6789659559726715\n",
            "Len of Validation loss: 128, Average loss: 1.7462523547001183\n",
            "Epoch: 715, Len of Training loss: 36, Average loss: 1.7171438170803919\n",
            "Len of Validation loss: 128, Average loss: 1.7230558078736067\n",
            "Epoch: 716, Len of Training loss: 36, Average loss: 1.70127249095175\n",
            "Len of Validation loss: 128, Average loss: 1.7739061897154897\n",
            "Epoch: 717, Len of Training loss: 36, Average loss: 1.7064220971531339\n",
            "Len of Validation loss: 128, Average loss: 1.6732411128468812\n",
            "Epoch: 718, Len of Training loss: 36, Average loss: 1.6917044387923346\n",
            "Len of Validation loss: 128, Average loss: 1.68213626393117\n",
            "Epoch: 719, Len of Training loss: 36, Average loss: 1.7069016330771976\n",
            "Len of Validation loss: 128, Average loss: 1.6702904333360493\n",
            "Epoch: 720, Len of Training loss: 36, Average loss: 1.6960833768049877\n",
            "Len of Validation loss: 128, Average loss: 1.7054291870445013\n",
            "Epoch: 721, Len of Training loss: 36, Average loss: 1.743646161423789\n",
            "Len of Validation loss: 128, Average loss: 1.736277208197862\n",
            "Epoch: 722, Len of Training loss: 36, Average loss: 1.7204688787460327\n",
            "Len of Validation loss: 128, Average loss: 1.682023192755878\n",
            "Epoch: 723, Len of Training loss: 36, Average loss: 1.6947067115041945\n",
            "Len of Validation loss: 128, Average loss: 1.7124397251755\n",
            "Epoch: 724, Len of Training loss: 36, Average loss: 1.7052154938379924\n",
            "Len of Validation loss: 128, Average loss: 1.7151419906876981\n",
            "Epoch: 725, Len of Training loss: 36, Average loss: 1.7116915749178991\n",
            "Len of Validation loss: 128, Average loss: 1.8333413624204695\n",
            "Epoch: 726, Len of Training loss: 36, Average loss: 1.7324677507082622\n",
            "Len of Validation loss: 128, Average loss: 1.712306051980704\n",
            "Epoch: 727, Len of Training loss: 36, Average loss: 1.7211740612983704\n",
            "Len of Validation loss: 128, Average loss: 1.772156440652907\n",
            "Epoch: 728, Len of Training loss: 36, Average loss: 1.7181731594933405\n",
            "Len of Validation loss: 128, Average loss: 1.7013386436738074\n",
            "Epoch: 729, Len of Training loss: 36, Average loss: 1.7271139787303076\n",
            "Len of Validation loss: 128, Average loss: 1.6649806492496282\n",
            "Epoch: 730, Len of Training loss: 36, Average loss: 1.7180568642086453\n",
            "Len of Validation loss: 128, Average loss: 1.8170046049635857\n",
            "Epoch: 731, Len of Training loss: 36, Average loss: 1.7370705935690138\n",
            "Len of Validation loss: 128, Average loss: 1.6866005319170654\n",
            "Epoch: 732, Len of Training loss: 36, Average loss: 1.7102381984392803\n",
            "Len of Validation loss: 128, Average loss: 1.6916770599782467\n",
            "Epoch: 733, Len of Training loss: 36, Average loss: 1.7009666628307767\n",
            "Len of Validation loss: 128, Average loss: 1.720449848799035\n",
            "Epoch: 734, Len of Training loss: 36, Average loss: 1.6631130278110504\n",
            "Len of Validation loss: 128, Average loss: 1.7060169372707605\n",
            "Epoch: 735, Len of Training loss: 36, Average loss: 1.7001643843121\n",
            "Len of Validation loss: 128, Average loss: 1.6661759843118489\n",
            "Epoch: 736, Len of Training loss: 36, Average loss: 1.6887687411573198\n",
            "Len of Validation loss: 128, Average loss: 1.7842836696654558\n",
            "Epoch: 737, Len of Training loss: 36, Average loss: 1.68419153491656\n",
            "Len of Validation loss: 128, Average loss: 1.6741433143615723\n",
            "Epoch: 738, Len of Training loss: 36, Average loss: 1.6757467720243666\n",
            "Len of Validation loss: 128, Average loss: 1.6749898644629866\n",
            "Epoch: 739, Len of Training loss: 36, Average loss: 1.7581152485476599\n",
            "Len of Validation loss: 128, Average loss: 1.700809661531821\n",
            "Epoch: 740, Len of Training loss: 36, Average loss: 1.7087237934271495\n",
            "Len of Validation loss: 128, Average loss: 1.6916711546946317\n",
            "Epoch: 741, Len of Training loss: 36, Average loss: 1.6934707860151927\n",
            "Len of Validation loss: 128, Average loss: 1.6530870604328811\n",
            "Epoch: 742, Len of Training loss: 36, Average loss: 1.6693838437398274\n",
            "Len of Validation loss: 128, Average loss: 1.6843134094960988\n",
            "Epoch: 743, Len of Training loss: 36, Average loss: 1.6985592510965135\n",
            "Len of Validation loss: 128, Average loss: 1.7132166381925344\n",
            "Epoch: 744, Len of Training loss: 36, Average loss: 1.7298677563667297\n",
            "Len of Validation loss: 128, Average loss: 1.720892293844372\n",
            "Epoch: 745, Len of Training loss: 36, Average loss: 1.7376679215166304\n",
            "Len of Validation loss: 128, Average loss: 1.7268188544549048\n",
            "Epoch: 746, Len of Training loss: 36, Average loss: 1.766399297449324\n",
            "Len of Validation loss: 128, Average loss: 1.6928423147182912\n",
            "Epoch: 747, Len of Training loss: 36, Average loss: 1.795665681362152\n",
            "Len of Validation loss: 128, Average loss: 1.786159539129585\n",
            "Epoch: 748, Len of Training loss: 36, Average loss: 1.7862824764516618\n",
            "Len of Validation loss: 128, Average loss: 1.708232098724693\n",
            "Epoch: 749, Len of Training loss: 36, Average loss: 1.701292786333296\n",
            "Len of Validation loss: 128, Average loss: 1.7186159389093518\n",
            "Epoch: 750, Len of Training loss: 36, Average loss: 1.7199726965692308\n",
            "Len of Validation loss: 128, Average loss: 1.6858570664189756\n",
            "Epoch: 751, Len of Training loss: 36, Average loss: 1.7116700841320887\n",
            "Len of Validation loss: 128, Average loss: 1.6997117756400257\n",
            "Epoch: 752, Len of Training loss: 36, Average loss: 1.6814510822296143\n",
            "Len of Validation loss: 128, Average loss: 1.7069361011963338\n",
            "Epoch: 753, Len of Training loss: 36, Average loss: 1.7213224156035318\n",
            "Len of Validation loss: 128, Average loss: 1.664554828312248\n",
            "Epoch: 754, Len of Training loss: 36, Average loss: 1.7055459982819028\n",
            "Len of Validation loss: 128, Average loss: 1.671221601543948\n",
            "Epoch: 755, Len of Training loss: 36, Average loss: 1.7115688059065077\n",
            "Len of Validation loss: 128, Average loss: 1.6660383511334658\n",
            "Epoch: 756, Len of Training loss: 36, Average loss: 1.6877427697181702\n",
            "Len of Validation loss: 128, Average loss: 1.7190463738515973\n",
            "Epoch: 757, Len of Training loss: 36, Average loss: 1.7132756378915575\n",
            "Len of Validation loss: 128, Average loss: 1.6641238515730947\n",
            "Epoch: 758, Len of Training loss: 36, Average loss: 1.721999830669827\n",
            "Len of Validation loss: 128, Average loss: 1.7245749172288924\n",
            "Epoch: 759, Len of Training loss: 36, Average loss: 1.6773714522520702\n",
            "Len of Validation loss: 128, Average loss: 1.6539367702789605\n",
            "Epoch: 760, Len of Training loss: 36, Average loss: 1.6786612371603649\n",
            "Len of Validation loss: 128, Average loss: 1.6549746501259506\n",
            "Epoch: 761, Len of Training loss: 36, Average loss: 1.7024312913417816\n",
            "Len of Validation loss: 128, Average loss: 1.6952188709983602\n",
            "Epoch: 762, Len of Training loss: 36, Average loss: 1.680216713084115\n",
            "Len of Validation loss: 128, Average loss: 1.6804959382861853\n",
            "Epoch: 763, Len of Training loss: 36, Average loss: 1.6623407403628032\n",
            "Len of Validation loss: 128, Average loss: 1.6443537187296897\n",
            "Epoch: 764, Len of Training loss: 36, Average loss: 1.6618219051096175\n",
            "Len of Validation loss: 128, Average loss: 1.6643358417786658\n",
            "Epoch: 765, Len of Training loss: 36, Average loss: 1.64570994509591\n",
            "Len of Validation loss: 128, Average loss: 1.6560754289384931\n",
            "Epoch: 766, Len of Training loss: 36, Average loss: 1.6848988234996796\n",
            "Len of Validation loss: 128, Average loss: 1.6373173359315842\n",
            "Epoch: 767, Len of Training loss: 36, Average loss: 1.6820639504326715\n",
            "Len of Validation loss: 128, Average loss: 1.6868887662421912\n",
            "Epoch: 768, Len of Training loss: 36, Average loss: 1.6722080541981592\n",
            "Len of Validation loss: 128, Average loss: 1.745647597592324\n",
            "Epoch: 769, Len of Training loss: 36, Average loss: 1.7061375578244526\n",
            "Len of Validation loss: 128, Average loss: 1.6708878437057137\n",
            "Epoch: 770, Len of Training loss: 36, Average loss: 1.766307618882921\n",
            "Len of Validation loss: 128, Average loss: 1.7400302356109023\n",
            "Epoch: 771, Len of Training loss: 36, Average loss: 1.7715534369150798\n",
            "Len of Validation loss: 128, Average loss: 1.7811088589951396\n",
            "Epoch: 772, Len of Training loss: 36, Average loss: 1.7039013637436762\n",
            "Len of Validation loss: 128, Average loss: 1.664851265726611\n",
            "Epoch: 773, Len of Training loss: 36, Average loss: 1.6659256915251415\n",
            "Len of Validation loss: 128, Average loss: 1.646507830824703\n",
            "Epoch: 774, Len of Training loss: 36, Average loss: 1.6921584672398038\n",
            "Len of Validation loss: 128, Average loss: 1.6737830468919128\n",
            "Epoch: 775, Len of Training loss: 36, Average loss: 1.7123176919089422\n",
            "Len of Validation loss: 128, Average loss: 1.7649992676451802\n",
            "Epoch: 776, Len of Training loss: 36, Average loss: 1.6787975695398119\n",
            "Len of Validation loss: 128, Average loss: 1.6785094949882478\n",
            "Epoch: 777, Len of Training loss: 36, Average loss: 1.6848137742943234\n",
            "Len of Validation loss: 128, Average loss: 1.6701552639715374\n",
            "Epoch: 778, Len of Training loss: 36, Average loss: 1.7356198959880405\n",
            "Len of Validation loss: 128, Average loss: 1.710675428621471\n",
            "Epoch: 779, Len of Training loss: 36, Average loss: 1.7073423862457275\n",
            "Len of Validation loss: 128, Average loss: 1.7368329064920545\n",
            "Epoch: 780, Len of Training loss: 36, Average loss: 1.7152783936924405\n",
            "Len of Validation loss: 128, Average loss: 1.6871812553144991\n",
            "Epoch: 781, Len of Training loss: 36, Average loss: 1.6995396316051483\n",
            "Len of Validation loss: 128, Average loss: 1.666373607935384\n",
            "Epoch: 782, Len of Training loss: 36, Average loss: 1.6747695008913677\n",
            "Len of Validation loss: 128, Average loss: 1.6533798254095018\n",
            "Epoch: 783, Len of Training loss: 36, Average loss: 1.6873348355293274\n",
            "Len of Validation loss: 128, Average loss: 1.6494535733945668\n",
            "Epoch: 784, Len of Training loss: 36, Average loss: 1.6692550016774073\n",
            "Len of Validation loss: 128, Average loss: 1.66225092462264\n",
            "Epoch: 785, Len of Training loss: 36, Average loss: 1.6567277411619823\n",
            "Len of Validation loss: 128, Average loss: 1.6736919444520026\n",
            "Epoch: 786, Len of Training loss: 36, Average loss: 1.6569281982051\n",
            "Len of Validation loss: 128, Average loss: 1.655444160103798\n",
            "Epoch: 787, Len of Training loss: 36, Average loss: 1.6683212849828932\n",
            "Len of Validation loss: 128, Average loss: 1.6810643943026662\n",
            "Epoch: 788, Len of Training loss: 36, Average loss: 1.692044860786862\n",
            "Len of Validation loss: 128, Average loss: 1.691885138861835\n",
            "Epoch: 789, Len of Training loss: 36, Average loss: 1.65681564145618\n",
            "Len of Validation loss: 128, Average loss: 1.6280665495432913\n",
            "Epoch: 790, Len of Training loss: 36, Average loss: 1.664883057276408\n",
            "Len of Validation loss: 128, Average loss: 1.659009194234386\n",
            "Epoch: 791, Len of Training loss: 36, Average loss: 1.6784381435977087\n",
            "Len of Validation loss: 128, Average loss: 1.6326343892142177\n",
            "Epoch: 792, Len of Training loss: 36, Average loss: 1.693988412618637\n",
            "Len of Validation loss: 128, Average loss: 1.6828700178302824\n",
            "Epoch: 793, Len of Training loss: 36, Average loss: 1.735624565018548\n",
            "Len of Validation loss: 128, Average loss: 1.6439084692392498\n",
            "Epoch: 794, Len of Training loss: 36, Average loss: 1.662569436762068\n",
            "Len of Validation loss: 128, Average loss: 1.7131576370447874\n",
            "Epoch: 795, Len of Training loss: 36, Average loss: 1.6660424669583638\n",
            "Len of Validation loss: 128, Average loss: 1.7056620749644935\n",
            "Epoch: 796, Len of Training loss: 36, Average loss: 1.6834809448983934\n",
            "Len of Validation loss: 128, Average loss: 1.6455542154144496\n",
            "Epoch: 797, Len of Training loss: 36, Average loss: 1.7231639590528276\n",
            "Len of Validation loss: 128, Average loss: 1.714063101215288\n",
            "Epoch: 798, Len of Training loss: 36, Average loss: 1.7197386920452118\n",
            "Len of Validation loss: 128, Average loss: 1.6948834331706166\n",
            "Epoch: 799, Len of Training loss: 36, Average loss: 1.6715986000166998\n",
            "Len of Validation loss: 128, Average loss: 1.667049726471305\n",
            "Epoch: 800, Len of Training loss: 36, Average loss: 1.6582430203755696\n",
            "Len of Validation loss: 128, Average loss: 1.6958279646933079\n",
            "Epoch: 801, Len of Training loss: 36, Average loss: 1.7269480327765148\n",
            "Len of Validation loss: 128, Average loss: 1.90314065432176\n",
            "Epoch: 802, Len of Training loss: 36, Average loss: 1.6902644369337294\n",
            "Len of Validation loss: 128, Average loss: 1.703056296799332\n",
            "Epoch: 803, Len of Training loss: 36, Average loss: 1.7103389369116888\n",
            "Len of Validation loss: 128, Average loss: 1.7698321249336004\n",
            "Epoch: 804, Len of Training loss: 36, Average loss: 1.7258314655886755\n",
            "Len of Validation loss: 128, Average loss: 1.6603536577895284\n",
            "Epoch: 805, Len of Training loss: 36, Average loss: 1.6896654566129048\n",
            "Len of Validation loss: 128, Average loss: 1.6852074458729476\n",
            "Epoch: 806, Len of Training loss: 36, Average loss: 1.665443629026413\n",
            "Len of Validation loss: 128, Average loss: 1.7097513070330024\n",
            "Epoch: 807, Len of Training loss: 36, Average loss: 1.6769165926509433\n",
            "Len of Validation loss: 128, Average loss: 1.6577785992994905\n",
            "Epoch: 808, Len of Training loss: 36, Average loss: 1.6779530180825128\n",
            "Len of Validation loss: 128, Average loss: 1.6976393819786608\n",
            "Epoch: 809, Len of Training loss: 36, Average loss: 1.6853843265109592\n",
            "Len of Validation loss: 128, Average loss: 1.7289077728055418\n",
            "Epoch: 810, Len of Training loss: 36, Average loss: 1.6769275466601055\n",
            "Len of Validation loss: 128, Average loss: 1.6251811373513192\n",
            "Epoch: 811, Len of Training loss: 36, Average loss: 1.6882991426520877\n",
            "Len of Validation loss: 128, Average loss: 1.705016702413559\n",
            "Epoch: 812, Len of Training loss: 36, Average loss: 1.662728539771504\n",
            "Len of Validation loss: 128, Average loss: 1.6791936391964555\n",
            "Epoch: 813, Len of Training loss: 36, Average loss: 1.6833333339956071\n",
            "Len of Validation loss: 128, Average loss: 1.6507866424508393\n",
            "Epoch: 814, Len of Training loss: 36, Average loss: 1.6692258616288502\n",
            "Len of Validation loss: 128, Average loss: 1.6503666741773486\n",
            "Epoch: 815, Len of Training loss: 36, Average loss: 1.6525329682562087\n",
            "Len of Validation loss: 128, Average loss: 1.6451700762845576\n",
            "Epoch: 816, Len of Training loss: 36, Average loss: 1.6930544839964972\n",
            "Len of Validation loss: 128, Average loss: 1.6955199304502457\n",
            "Epoch: 817, Len of Training loss: 36, Average loss: 1.6657019257545471\n",
            "Len of Validation loss: 128, Average loss: 1.6553585547953844\n",
            "Epoch: 818, Len of Training loss: 36, Average loss: 1.6410606801509857\n",
            "Len of Validation loss: 128, Average loss: 1.6392620392143726\n",
            "Epoch: 819, Len of Training loss: 36, Average loss: 1.6693691114584606\n",
            "Len of Validation loss: 128, Average loss: 1.648740481119603\n",
            "Epoch: 820, Len of Training loss: 36, Average loss: 1.6767769091659122\n",
            "Len of Validation loss: 128, Average loss: 1.7909690798260272\n",
            "Epoch: 821, Len of Training loss: 36, Average loss: 1.6719765928056505\n",
            "Len of Validation loss: 128, Average loss: 1.6469111072365195\n",
            "Epoch: 822, Len of Training loss: 36, Average loss: 1.6760328809420268\n",
            "Len of Validation loss: 128, Average loss: 1.6814884652849287\n",
            "Epoch: 823, Len of Training loss: 36, Average loss: 1.6669744087590113\n",
            "Len of Validation loss: 128, Average loss: 1.6976272645406425\n",
            "Epoch: 824, Len of Training loss: 36, Average loss: 1.6485048433144887\n",
            "Len of Validation loss: 128, Average loss: 1.6649189502932131\n",
            "Epoch: 825, Len of Training loss: 36, Average loss: 1.6574241916338603\n",
            "Len of Validation loss: 128, Average loss: 1.6877800119109452\n",
            "Epoch: 826, Len of Training loss: 36, Average loss: 1.7128713230292003\n",
            "Len of Validation loss: 128, Average loss: 1.6743409952614456\n",
            "Epoch: 827, Len of Training loss: 36, Average loss: 1.6730472246805828\n",
            "Len of Validation loss: 128, Average loss: 1.63406223943457\n",
            "Epoch: 828, Len of Training loss: 36, Average loss: 1.6636857953336504\n",
            "Len of Validation loss: 128, Average loss: 1.6692082381341606\n",
            "Epoch: 829, Len of Training loss: 36, Average loss: 1.6497791475719876\n",
            "Len of Validation loss: 128, Average loss: 1.6360374786891043\n",
            "Epoch: 830, Len of Training loss: 36, Average loss: 1.6507293283939362\n",
            "Len of Validation loss: 128, Average loss: 1.6579060177318752\n",
            "Epoch: 831, Len of Training loss: 36, Average loss: 1.657160566912757\n",
            "Len of Validation loss: 128, Average loss: 1.6862701878417283\n",
            "Epoch: 832, Len of Training loss: 36, Average loss: 1.6516768402523465\n",
            "Len of Validation loss: 128, Average loss: 1.6225055383984\n",
            "Epoch: 833, Len of Training loss: 36, Average loss: 1.6470587915844388\n",
            "Len of Validation loss: 128, Average loss: 1.7306934767402709\n",
            "Epoch: 834, Len of Training loss: 36, Average loss: 1.6694385707378387\n",
            "Len of Validation loss: 128, Average loss: 1.6459011274855584\n",
            "Epoch: 835, Len of Training loss: 36, Average loss: 1.6496028569009569\n",
            "Len of Validation loss: 128, Average loss: 1.6293272152543068\n",
            "Epoch: 836, Len of Training loss: 36, Average loss: 1.655046158366733\n",
            "Len of Validation loss: 128, Average loss: 1.6526827104389668\n",
            "Epoch: 837, Len of Training loss: 36, Average loss: 1.674812258945571\n",
            "Len of Validation loss: 128, Average loss: 1.6714259423315525\n",
            "Epoch: 838, Len of Training loss: 36, Average loss: 1.6879811684290569\n",
            "Len of Validation loss: 128, Average loss: 1.6605936593841761\n",
            "Epoch: 839, Len of Training loss: 36, Average loss: 1.671757184796863\n",
            "Len of Validation loss: 128, Average loss: 1.6658994613680989\n",
            "Epoch: 840, Len of Training loss: 36, Average loss: 1.6315703723165724\n",
            "Len of Validation loss: 128, Average loss: 1.6556462852749974\n",
            "Epoch: 841, Len of Training loss: 36, Average loss: 1.6865199771192338\n",
            "Len of Validation loss: 128, Average loss: 1.6459139408543706\n",
            "Epoch: 842, Len of Training loss: 36, Average loss: 1.6381745768917932\n",
            "Len of Validation loss: 128, Average loss: 1.639463301282376\n",
            "Epoch: 843, Len of Training loss: 36, Average loss: 1.697773254579968\n",
            "Len of Validation loss: 128, Average loss: 1.6598534404765815\n",
            "Epoch: 844, Len of Training loss: 36, Average loss: 1.6299827032619052\n",
            "Len of Validation loss: 128, Average loss: 1.664596046321094\n",
            "Epoch: 845, Len of Training loss: 36, Average loss: 1.685835513803694\n",
            "Len of Validation loss: 128, Average loss: 1.6519488373305649\n",
            "Epoch: 846, Len of Training loss: 36, Average loss: 1.6306566794713337\n",
            "Len of Validation loss: 128, Average loss: 1.7141150198876858\n",
            "Epoch: 847, Len of Training loss: 36, Average loss: 1.7107977999581232\n",
            "Len of Validation loss: 128, Average loss: 1.6889363534282893\n",
            "Epoch: 848, Len of Training loss: 36, Average loss: 1.6718837983078427\n",
            "Len of Validation loss: 128, Average loss: 1.7532148198224604\n",
            "Epoch: 849, Len of Training loss: 36, Average loss: 1.7061549822489421\n",
            "Len of Validation loss: 128, Average loss: 1.676629255991429\n",
            "Epoch: 850, Len of Training loss: 36, Average loss: 1.6962893108526866\n",
            "Len of Validation loss: 128, Average loss: 1.6674191686324775\n",
            "Epoch: 851, Len of Training loss: 36, Average loss: 1.6489289038711124\n",
            "Len of Validation loss: 128, Average loss: 1.6367337959818542\n",
            "Epoch: 852, Len of Training loss: 36, Average loss: 1.6586447126335568\n",
            "Len of Validation loss: 128, Average loss: 1.6748972251079977\n",
            "Epoch: 853, Len of Training loss: 36, Average loss: 1.6648409763971965\n",
            "Len of Validation loss: 128, Average loss: 1.6180474709253758\n",
            "Epoch: 854, Len of Training loss: 36, Average loss: 1.6612835725148518\n",
            "Len of Validation loss: 128, Average loss: 1.6450092266313732\n",
            "Epoch: 855, Len of Training loss: 36, Average loss: 1.7160664565033383\n",
            "Len of Validation loss: 128, Average loss: 1.7371142450720072\n",
            "Epoch: 856, Len of Training loss: 36, Average loss: 1.7589889996581607\n",
            "Len of Validation loss: 128, Average loss: 1.6888943477533758\n",
            "Epoch: 857, Len of Training loss: 36, Average loss: 1.6370376182927027\n",
            "Len of Validation loss: 128, Average loss: 1.6364098154008389\n",
            "Epoch: 858, Len of Training loss: 36, Average loss: 1.6172591745853424\n",
            "Len of Validation loss: 128, Average loss: 1.6337842671200633\n",
            "Epoch: 859, Len of Training loss: 36, Average loss: 1.615407837761773\n",
            "Len of Validation loss: 128, Average loss: 1.6724995151162148\n",
            "Epoch: 860, Len of Training loss: 36, Average loss: 1.6181801623768277\n",
            "Len of Validation loss: 128, Average loss: 1.6414355982560664\n",
            "Epoch: 861, Len of Training loss: 36, Average loss: 1.65323187245263\n",
            "Len of Validation loss: 128, Average loss: 1.6581497029401362\n",
            "Epoch: 862, Len of Training loss: 36, Average loss: 1.6247348388036091\n",
            "Len of Validation loss: 128, Average loss: 1.649043851532042\n",
            "Epoch: 863, Len of Training loss: 36, Average loss: 1.6116700834698148\n",
            "Len of Validation loss: 128, Average loss: 1.677103858673945\n",
            "Epoch: 864, Len of Training loss: 36, Average loss: 1.6629525158140395\n",
            "Len of Validation loss: 128, Average loss: 1.6452382642310113\n",
            "Epoch: 865, Len of Training loss: 36, Average loss: 1.6795387301180098\n",
            "Len of Validation loss: 128, Average loss: 1.6985275312326849\n",
            "Epoch: 866, Len of Training loss: 36, Average loss: 2.22607613603274\n",
            "Len of Validation loss: 128, Average loss: 5.304983471520245\n",
            "Epoch: 867, Len of Training loss: 36, Average loss: 2.8613400194380016\n",
            "Len of Validation loss: 128, Average loss: 1.9657265706919134\n",
            "Epoch: 868, Len of Training loss: 36, Average loss: 2.0052085551950665\n",
            "Len of Validation loss: 128, Average loss: 1.8299823254346848\n",
            "Epoch: 869, Len of Training loss: 36, Average loss: 1.9126416345437367\n",
            "Len of Validation loss: 128, Average loss: 1.7778389886952937\n",
            "Epoch: 870, Len of Training loss: 36, Average loss: 1.788257936636607\n",
            "Len of Validation loss: 128, Average loss: 1.7203658558428288\n",
            "Epoch: 871, Len of Training loss: 36, Average loss: 1.7237680322594113\n",
            "Len of Validation loss: 128, Average loss: 1.7211091294884682\n",
            "Epoch: 872, Len of Training loss: 36, Average loss: 1.7410931024286482\n",
            "Len of Validation loss: 128, Average loss: 1.7369926555547863\n",
            "Epoch: 873, Len of Training loss: 36, Average loss: 1.6929427882035573\n",
            "Len of Validation loss: 128, Average loss: 1.676812604535371\n",
            "Epoch: 874, Len of Training loss: 36, Average loss: 1.6669393844074674\n",
            "Len of Validation loss: 128, Average loss: 1.6890121074393392\n",
            "Epoch: 875, Len of Training loss: 36, Average loss: 1.687846259938346\n",
            "Len of Validation loss: 128, Average loss: 1.6956639639101923\n",
            "Epoch: 876, Len of Training loss: 36, Average loss: 1.6855315996540918\n",
            "Len of Validation loss: 128, Average loss: 1.7059079837054014\n",
            "Epoch: 877, Len of Training loss: 36, Average loss: 1.7530722121397655\n",
            "Len of Validation loss: 128, Average loss: 1.7785284537822008\n",
            "Epoch: 878, Len of Training loss: 36, Average loss: 1.7390745282173157\n",
            "Len of Validation loss: 128, Average loss: 1.6955940150655806\n",
            "Epoch: 879, Len of Training loss: 36, Average loss: 1.7090980609258015\n",
            "Len of Validation loss: 128, Average loss: 1.7536001983098686\n",
            "Epoch: 880, Len of Training loss: 36, Average loss: 1.7739902569188013\n",
            "Len of Validation loss: 128, Average loss: 1.7798969908617437\n",
            "Epoch: 881, Len of Training loss: 36, Average loss: 1.6818652186128829\n",
            "Len of Validation loss: 128, Average loss: 1.6632056750822812\n",
            "Epoch: 882, Len of Training loss: 36, Average loss: 1.657613194651074\n",
            "Len of Validation loss: 128, Average loss: 1.6346315371338278\n",
            "Epoch: 883, Len of Training loss: 36, Average loss: 1.6548358268207974\n",
            "Len of Validation loss: 128, Average loss: 1.613417562795803\n",
            "Epoch: 884, Len of Training loss: 36, Average loss: 1.667857160170873\n",
            "Len of Validation loss: 128, Average loss: 1.6618865397758782\n",
            "Epoch: 885, Len of Training loss: 36, Average loss: 1.6494046515888638\n",
            "Len of Validation loss: 128, Average loss: 1.6717704650945961\n",
            "Epoch: 886, Len of Training loss: 36, Average loss: 1.6267026099893782\n",
            "Len of Validation loss: 128, Average loss: 1.6820110413245857\n",
            "Epoch: 887, Len of Training loss: 36, Average loss: 1.6639265616734822\n",
            "Len of Validation loss: 128, Average loss: 1.6596485432237387\n",
            "Epoch: 888, Len of Training loss: 36, Average loss: 1.6489542292224035\n",
            "Len of Validation loss: 128, Average loss: 1.7076490125618875\n",
            "Epoch: 889, Len of Training loss: 36, Average loss: 1.6862702601485782\n",
            "Len of Validation loss: 128, Average loss: 1.6890309946611524\n",
            "Epoch: 890, Len of Training loss: 36, Average loss: 1.6785336136817932\n",
            "Len of Validation loss: 128, Average loss: 1.6680012966971844\n",
            "Epoch: 891, Len of Training loss: 36, Average loss: 1.7008330192830827\n",
            "Len of Validation loss: 128, Average loss: 1.6553631420247257\n",
            "Epoch: 892, Len of Training loss: 36, Average loss: 1.7162323991457622\n",
            "Len of Validation loss: 128, Average loss: 1.6827385355718434\n",
            "Epoch: 893, Len of Training loss: 36, Average loss: 1.644507047202852\n",
            "Len of Validation loss: 128, Average loss: 1.6722112067509443\n",
            "Epoch: 894, Len of Training loss: 36, Average loss: 1.665311723947525\n",
            "Len of Validation loss: 128, Average loss: 1.6756967650726438\n",
            "Epoch: 895, Len of Training loss: 36, Average loss: 1.6911278896861606\n",
            "Len of Validation loss: 128, Average loss: 1.7650483008474112\n",
            "Epoch: 896, Len of Training loss: 36, Average loss: 1.682422677675883\n",
            "Len of Validation loss: 128, Average loss: 1.6445791739970446\n",
            "Epoch: 897, Len of Training loss: 36, Average loss: 1.6403635210461087\n",
            "Len of Validation loss: 128, Average loss: 1.629141095560044\n",
            "Epoch: 898, Len of Training loss: 36, Average loss: 1.656184537543191\n",
            "Len of Validation loss: 128, Average loss: 1.628817546647042\n",
            "Epoch: 899, Len of Training loss: 36, Average loss: 1.6735321482022603\n",
            "Len of Validation loss: 128, Average loss: 1.660304880933836\n",
            "Epoch: 900, Len of Training loss: 36, Average loss: 1.62725626428922\n",
            "Len of Validation loss: 128, Average loss: 1.6386649678461254\n",
            "Epoch: 901, Len of Training loss: 36, Average loss: 1.6488615340656705\n",
            "Len of Validation loss: 128, Average loss: 1.6870456617325544\n",
            "Epoch: 902, Len of Training loss: 36, Average loss: 1.6525685389836628\n",
            "Len of Validation loss: 128, Average loss: 1.6319123590365052\n",
            "Epoch: 903, Len of Training loss: 36, Average loss: 1.616759545273251\n",
            "Len of Validation loss: 128, Average loss: 1.6523274460341781\n",
            "Epoch: 904, Len of Training loss: 36, Average loss: 1.6186786989370983\n",
            "Len of Validation loss: 128, Average loss: 1.6762350043281913\n",
            "Epoch: 905, Len of Training loss: 36, Average loss: 1.664893044365777\n",
            "Len of Validation loss: 128, Average loss: 1.6531474604271352\n",
            "Epoch: 906, Len of Training loss: 36, Average loss: 1.628494444820616\n",
            "Len of Validation loss: 128, Average loss: 1.6397981781046838\n",
            "Epoch: 907, Len of Training loss: 36, Average loss: 1.6251956423123677\n",
            "Len of Validation loss: 128, Average loss: 1.6373441200703382\n",
            "Epoch: 908, Len of Training loss: 36, Average loss: 1.6305702957842085\n",
            "Len of Validation loss: 128, Average loss: 1.6736905183643103\n",
            "Epoch: 909, Len of Training loss: 36, Average loss: 1.7158237761921353\n",
            "Len of Validation loss: 128, Average loss: 1.8716802878770977\n",
            "Epoch: 910, Len of Training loss: 36, Average loss: 1.661814421415329\n",
            "Len of Validation loss: 128, Average loss: 1.6200645382050425\n",
            "Epoch: 911, Len of Training loss: 36, Average loss: 1.6067301796542273\n",
            "Len of Validation loss: 128, Average loss: 1.649386041564867\n",
            "Epoch: 912, Len of Training loss: 36, Average loss: 1.6700796551174588\n",
            "Len of Validation loss: 128, Average loss: 1.7793343514204025\n",
            "Epoch: 913, Len of Training loss: 36, Average loss: 1.6478206018606822\n",
            "Len of Validation loss: 128, Average loss: 1.6755655040033162\n",
            "Epoch: 914, Len of Training loss: 36, Average loss: 1.6383151710033417\n",
            "Len of Validation loss: 128, Average loss: 1.6397831046488136\n",
            "Epoch: 915, Len of Training loss: 36, Average loss: 1.6521271500322554\n",
            "Len of Validation loss: 128, Average loss: 1.721670423168689\n",
            "Epoch: 916, Len of Training loss: 36, Average loss: 1.6686386730935838\n",
            "Len of Validation loss: 128, Average loss: 1.6334367482922971\n",
            "Epoch: 917, Len of Training loss: 36, Average loss: 1.6847967074977026\n",
            "Len of Validation loss: 128, Average loss: 1.729043548926711\n",
            "Epoch: 918, Len of Training loss: 36, Average loss: 1.6540973385175068\n",
            "Len of Validation loss: 128, Average loss: 1.672392050968483\n",
            "Epoch: 919, Len of Training loss: 36, Average loss: 1.630855722559823\n",
            "Len of Validation loss: 128, Average loss: 1.6796762496232986\n",
            "Epoch: 920, Len of Training loss: 36, Average loss: 1.6427040729257796\n",
            "Len of Validation loss: 128, Average loss: 1.658154301578179\n",
            "Epoch: 921, Len of Training loss: 36, Average loss: 1.6253190901544359\n",
            "Len of Validation loss: 128, Average loss: 1.7148567480035126\n",
            "Epoch: 922, Len of Training loss: 36, Average loss: 1.6462600032488506\n",
            "Len of Validation loss: 128, Average loss: 1.6962007118854672\n",
            "Epoch: 923, Len of Training loss: 36, Average loss: 1.6722070078055065\n",
            "Len of Validation loss: 128, Average loss: 1.6181501860264689\n",
            "Epoch: 924, Len of Training loss: 36, Average loss: 1.6125440067715116\n",
            "Len of Validation loss: 128, Average loss: 1.6404689502669498\n",
            "Epoch: 925, Len of Training loss: 36, Average loss: 1.6230646934774187\n",
            "Len of Validation loss: 128, Average loss: 1.6192095763981342\n",
            "Epoch: 926, Len of Training loss: 36, Average loss: 1.64193613992797\n",
            "Len of Validation loss: 128, Average loss: 1.7267406058963388\n",
            "Epoch: 927, Len of Training loss: 36, Average loss: 1.6037485400835674\n",
            "Len of Validation loss: 128, Average loss: 1.6039465276990086\n",
            "Epoch: 928, Len of Training loss: 36, Average loss: 1.6668234566847484\n",
            "Len of Validation loss: 128, Average loss: 1.7153938408009708\n",
            "Epoch: 929, Len of Training loss: 36, Average loss: 1.627848916583591\n",
            "Len of Validation loss: 128, Average loss: 1.6385233697947115\n",
            "Epoch: 930, Len of Training loss: 36, Average loss: 1.6133296820852492\n",
            "Len of Validation loss: 128, Average loss: 1.6223049983382225\n",
            "Epoch: 931, Len of Training loss: 36, Average loss: 1.6091241836547852\n",
            "Len of Validation loss: 128, Average loss: 1.6443763328716159\n",
            "Epoch: 932, Len of Training loss: 36, Average loss: 1.6230140692657895\n",
            "Len of Validation loss: 128, Average loss: 1.659699332434684\n",
            "Epoch: 933, Len of Training loss: 36, Average loss: 1.6422732472419739\n",
            "Len of Validation loss: 128, Average loss: 1.7294978741556406\n",
            "Epoch: 934, Len of Training loss: 36, Average loss: 1.7057843108971913\n",
            "Len of Validation loss: 128, Average loss: 1.6595690087415278\n",
            "Epoch: 935, Len of Training loss: 36, Average loss: 1.6139120757579803\n",
            "Len of Validation loss: 128, Average loss: 1.6596187392715365\n",
            "Epoch: 936, Len of Training loss: 36, Average loss: 1.6420944333076477\n",
            "Len of Validation loss: 128, Average loss: 1.655167408986017\n",
            "Epoch: 937, Len of Training loss: 36, Average loss: 1.6723109781742096\n",
            "Len of Validation loss: 128, Average loss: 1.7947733108885586\n",
            "Epoch: 938, Len of Training loss: 36, Average loss: 1.7463879485925038\n",
            "Len of Validation loss: 128, Average loss: 1.6582559794187546\n",
            "Epoch: 939, Len of Training loss: 36, Average loss: 1.6703510748015509\n",
            "Len of Validation loss: 128, Average loss: 1.7277731937356293\n",
            "Epoch: 940, Len of Training loss: 36, Average loss: 1.6445192462868161\n",
            "Len of Validation loss: 128, Average loss: 1.6610114809591323\n",
            "Epoch: 941, Len of Training loss: 36, Average loss: 1.6165915197796292\n",
            "Len of Validation loss: 128, Average loss: 1.693075975868851\n",
            "Epoch: 942, Len of Training loss: 36, Average loss: 1.6252204709582858\n",
            "Len of Validation loss: 128, Average loss: 1.6331215088721365\n",
            "Epoch: 943, Len of Training loss: 36, Average loss: 1.632561104165183\n",
            "Len of Validation loss: 128, Average loss: 1.6262862486764789\n",
            "Epoch: 944, Len of Training loss: 36, Average loss: 1.600725147459242\n",
            "Len of Validation loss: 128, Average loss: 1.6416726072784513\n",
            "Epoch: 945, Len of Training loss: 36, Average loss: 1.6062796248330011\n",
            "Len of Validation loss: 128, Average loss: 1.6652555805630982\n",
            "Epoch: 946, Len of Training loss: 36, Average loss: 1.6222511728604634\n",
            "Len of Validation loss: 128, Average loss: 1.6295073351357132\n",
            "Epoch: 947, Len of Training loss: 36, Average loss: 1.6139714138375387\n",
            "Len of Validation loss: 128, Average loss: 1.6794654433615506\n",
            "Epoch: 948, Len of Training loss: 36, Average loss: 1.6367654667960272\n",
            "Len of Validation loss: 128, Average loss: 1.676195100415498\n",
            "Epoch: 949, Len of Training loss: 36, Average loss: 1.6089386079046462\n",
            "Len of Validation loss: 128, Average loss: 1.6296872943639755\n",
            "Epoch: 950, Len of Training loss: 36, Average loss: 1.6128620439105563\n",
            "Len of Validation loss: 128, Average loss: 1.6212847663555294\n",
            "Epoch: 951, Len of Training loss: 36, Average loss: 1.6506261428197224\n",
            "Len of Validation loss: 128, Average loss: 1.6316295061260462\n",
            "Epoch: 952, Len of Training loss: 36, Average loss: 1.6377287705739338\n",
            "Len of Validation loss: 128, Average loss: 1.6352797765284777\n",
            "Epoch: 953, Len of Training loss: 36, Average loss: 1.634021583530638\n",
            "Len of Validation loss: 128, Average loss: 1.617701386101544\n",
            "Epoch: 954, Len of Training loss: 36, Average loss: 1.6220711469650269\n",
            "Len of Validation loss: 128, Average loss: 1.6904551389161497\n",
            "Epoch: 955, Len of Training loss: 36, Average loss: 1.6541716357072194\n",
            "Len of Validation loss: 128, Average loss: 1.6578470072709024\n",
            "Epoch: 956, Len of Training loss: 36, Average loss: 1.6334365606307983\n",
            "Len of Validation loss: 128, Average loss: 1.7767810295335948\n",
            "Epoch: 957, Len of Training loss: 36, Average loss: 1.6566787726349301\n",
            "Len of Validation loss: 128, Average loss: 1.6837481344118714\n",
            "Epoch: 958, Len of Training loss: 36, Average loss: 1.62028954592016\n",
            "Len of Validation loss: 128, Average loss: 1.6329884105361998\n",
            "Epoch: 959, Len of Training loss: 36, Average loss: 1.600438416004181\n",
            "Len of Validation loss: 128, Average loss: 1.741696547018364\n",
            "Epoch: 960, Len of Training loss: 36, Average loss: 1.652921458085378\n",
            "Len of Validation loss: 128, Average loss: 1.6699050539173186\n",
            "Epoch: 961, Len of Training loss: 36, Average loss: 1.6209287113613553\n",
            "Len of Validation loss: 128, Average loss: 1.6182875770609826\n",
            "Epoch: 962, Len of Training loss: 36, Average loss: 1.6042827632692125\n",
            "Len of Validation loss: 128, Average loss: 1.614850840298459\n",
            "Epoch: 963, Len of Training loss: 36, Average loss: 1.6107116672727797\n",
            "Len of Validation loss: 128, Average loss: 1.6459350530058146\n",
            "Epoch: 964, Len of Training loss: 36, Average loss: 1.6354598667886522\n",
            "Len of Validation loss: 128, Average loss: 1.7373691638931632\n",
            "Epoch: 965, Len of Training loss: 36, Average loss: 1.616405361228519\n",
            "Len of Validation loss: 128, Average loss: 1.6690733882132918\n",
            "Epoch: 966, Len of Training loss: 36, Average loss: 1.6100968089368608\n",
            "Len of Validation loss: 128, Average loss: 1.6797741090413183\n",
            "Epoch: 967, Len of Training loss: 36, Average loss: 1.6483634908994038\n",
            "Len of Validation loss: 128, Average loss: 1.6204400374554098\n",
            "Epoch: 968, Len of Training loss: 36, Average loss: 1.5892726729313533\n",
            "Len of Validation loss: 128, Average loss: 1.7014595258515328\n",
            "Epoch: 969, Len of Training loss: 36, Average loss: 1.683364709218343\n",
            "Len of Validation loss: 128, Average loss: 1.7288898469414562\n",
            "Epoch: 970, Len of Training loss: 36, Average loss: 1.6867405341731176\n",
            "Len of Validation loss: 128, Average loss: 1.630014281021431\n",
            "Epoch: 971, Len of Training loss: 36, Average loss: 1.6300770541032155\n",
            "Len of Validation loss: 128, Average loss: 1.6456497027538717\n",
            "Epoch: 972, Len of Training loss: 36, Average loss: 1.62977886862225\n",
            "Len of Validation loss: 128, Average loss: 1.767129038926214\n",
            "Epoch: 973, Len of Training loss: 36, Average loss: 1.6643668545616999\n",
            "Len of Validation loss: 128, Average loss: 1.6802576938644052\n",
            "Epoch: 974, Len of Training loss: 36, Average loss: 1.617729190323088\n",
            "Len of Validation loss: 128, Average loss: 1.621917336480692\n",
            "Epoch: 975, Len of Training loss: 36, Average loss: 1.6059046685695648\n",
            "Len of Validation loss: 128, Average loss: 1.5974890331272036\n",
            "Epoch: 976, Len of Training loss: 36, Average loss: 1.610139442814721\n",
            "Len of Validation loss: 128, Average loss: 1.6797550208866596\n",
            "Epoch: 977, Len of Training loss: 36, Average loss: 1.6426074968443976\n",
            "Len of Validation loss: 128, Average loss: 1.659387566614896\n",
            "Epoch: 978, Len of Training loss: 36, Average loss: 1.6555025842454698\n",
            "Len of Validation loss: 128, Average loss: 1.6604619508143514\n",
            "Epoch: 979, Len of Training loss: 36, Average loss: 1.6010357803768582\n",
            "Len of Validation loss: 128, Average loss: 1.608585944864899\n",
            "Epoch: 980, Len of Training loss: 36, Average loss: 1.6084295974837408\n",
            "Len of Validation loss: 128, Average loss: 1.6150764962658286\n",
            "Epoch: 981, Len of Training loss: 36, Average loss: 1.5919249355793\n",
            "Len of Validation loss: 128, Average loss: 1.6349924434907734\n",
            "Epoch: 982, Len of Training loss: 36, Average loss: 1.6581645508607228\n",
            "Len of Validation loss: 128, Average loss: 1.738811606541276\n",
            "Epoch: 983, Len of Training loss: 36, Average loss: 1.63755754298634\n",
            "Len of Validation loss: 128, Average loss: 1.6723780033644289\n",
            "Epoch: 984, Len of Training loss: 36, Average loss: 1.5905977586905162\n",
            "Len of Validation loss: 128, Average loss: 1.6398247459437698\n",
            "Epoch: 985, Len of Training loss: 36, Average loss: 1.6706079542636871\n",
            "Len of Validation loss: 128, Average loss: 1.635311902500689\n",
            "Epoch: 986, Len of Training loss: 36, Average loss: 1.5963020788298712\n",
            "Len of Validation loss: 128, Average loss: 1.6344310832209885\n",
            "Epoch: 987, Len of Training loss: 36, Average loss: 1.610903153816859\n",
            "Len of Validation loss: 128, Average loss: 1.6529548983089626\n",
            "Epoch: 988, Len of Training loss: 36, Average loss: 1.5966577298111386\n",
            "Len of Validation loss: 128, Average loss: 1.6496248957701027\n",
            "Epoch: 989, Len of Training loss: 36, Average loss: 1.5871525175041623\n",
            "Len of Validation loss: 128, Average loss: 1.631498601520434\n",
            "Epoch: 990, Len of Training loss: 36, Average loss: 1.5850935412777796\n",
            "Len of Validation loss: 128, Average loss: 1.6399720003828406\n",
            "Epoch: 991, Len of Training loss: 36, Average loss: 1.6219013763798609\n",
            "Len of Validation loss: 128, Average loss: 1.7245066883042455\n",
            "Epoch: 992, Len of Training loss: 36, Average loss: 1.643963810470369\n",
            "Len of Validation loss: 128, Average loss: 1.6552479129750282\n",
            "Epoch: 993, Len of Training loss: 36, Average loss: 1.6526580254236858\n",
            "Len of Validation loss: 128, Average loss: 1.6330192571040243\n",
            "Epoch: 994, Len of Training loss: 36, Average loss: 1.6071096890502505\n",
            "Len of Validation loss: 128, Average loss: 1.680757000343874\n",
            "Epoch: 995, Len of Training loss: 36, Average loss: 1.5831896000438266\n",
            "Len of Validation loss: 128, Average loss: 1.6498782329726964\n",
            "Epoch: 996, Len of Training loss: 36, Average loss: 1.6061621606349945\n",
            "Len of Validation loss: 128, Average loss: 1.6573603791184723\n",
            "Epoch: 997, Len of Training loss: 36, Average loss: 1.5794897907310061\n",
            "Len of Validation loss: 128, Average loss: 1.6409673250745982\n",
            "Epoch: 998, Len of Training loss: 36, Average loss: 1.5731373694207933\n",
            "Len of Validation loss: 128, Average loss: 1.6239960980601609\n",
            "Epoch: 999, Len of Training loss: 36, Average loss: 1.6046299470795526\n",
            "Len of Validation loss: 128, Average loss: 1.6359613907989115\n",
            "Epoch: 1000, Len of Training loss: 36, Average loss: 1.648594041665395\n",
            "Len of Validation loss: 128, Average loss: 1.667726244078949\n",
            "Epoch: 1001, Len of Training loss: 36, Average loss: 1.6130090322759416\n",
            "Len of Validation loss: 128, Average loss: 1.6551935286261141\n",
            "Epoch: 1002, Len of Training loss: 36, Average loss: 1.6148778862423367\n",
            "Len of Validation loss: 128, Average loss: 1.6935720168985426\n",
            "Epoch: 1003, Len of Training loss: 36, Average loss: 1.625481880373425\n",
            "Len of Validation loss: 128, Average loss: 1.6634932497981936\n",
            "Epoch: 1004, Len of Training loss: 36, Average loss: 1.627946416536967\n",
            "Len of Validation loss: 128, Average loss: 1.6025722466874868\n",
            "Epoch: 1005, Len of Training loss: 36, Average loss: 1.607554617855284\n",
            "Len of Validation loss: 128, Average loss: 1.5895522439386696\n",
            "Epoch: 1006, Len of Training loss: 36, Average loss: 1.5883253183629777\n",
            "Len of Validation loss: 128, Average loss: 1.6249459688551724\n",
            "Epoch: 1007, Len of Training loss: 36, Average loss: 1.626014103492101\n",
            "Len of Validation loss: 128, Average loss: 1.635209056083113\n",
            "Epoch: 1008, Len of Training loss: 36, Average loss: 1.5969668759240045\n",
            "Len of Validation loss: 128, Average loss: 1.675188186694868\n",
            "Epoch: 1009, Len of Training loss: 36, Average loss: 1.5870853828059301\n",
            "Len of Validation loss: 128, Average loss: 1.5951416213065386\n",
            "Epoch: 1010, Len of Training loss: 36, Average loss: 1.6729375786251492\n",
            "Len of Validation loss: 128, Average loss: 1.7718916351441294\n",
            "Epoch: 1011, Len of Training loss: 36, Average loss: 1.7246571514341567\n",
            "Len of Validation loss: 128, Average loss: 1.8098451064433903\n",
            "Epoch: 1012, Len of Training loss: 36, Average loss: 1.643186953332689\n",
            "Len of Validation loss: 128, Average loss: 1.6292669402901083\n",
            "Epoch: 1013, Len of Training loss: 36, Average loss: 1.5948805775907304\n",
            "Len of Validation loss: 128, Average loss: 1.646148089086637\n",
            "Epoch: 1014, Len of Training loss: 36, Average loss: 1.621401806672414\n",
            "Len of Validation loss: 128, Average loss: 1.670318427728489\n",
            "Epoch: 1015, Len of Training loss: 36, Average loss: 1.598862412903044\n",
            "Len of Validation loss: 128, Average loss: 1.6018448371905833\n",
            "Epoch: 1016, Len of Training loss: 36, Average loss: 1.5955188506179385\n",
            "Len of Validation loss: 128, Average loss: 1.6824865750968456\n",
            "Epoch: 1017, Len of Training loss: 36, Average loss: 1.5934445526864793\n",
            "Len of Validation loss: 128, Average loss: 1.620107410242781\n",
            "Epoch: 1018, Len of Training loss: 36, Average loss: 1.567816588613722\n",
            "Len of Validation loss: 128, Average loss: 1.603193816030398\n",
            "Epoch: 1019, Len of Training loss: 36, Average loss: 1.5893348223633237\n",
            "Len of Validation loss: 128, Average loss: 1.734370468184352\n",
            "Epoch: 1020, Len of Training loss: 36, Average loss: 1.6134638289610546\n",
            "Len of Validation loss: 128, Average loss: 1.6486714901402593\n",
            "Epoch: 1021, Len of Training loss: 36, Average loss: 1.590680307812161\n",
            "Len of Validation loss: 128, Average loss: 1.6966649438254535\n",
            "Epoch: 1022, Len of Training loss: 36, Average loss: 1.59377141462432\n",
            "Len of Validation loss: 128, Average loss: 1.6417624361347407\n",
            "Epoch: 1023, Len of Training loss: 36, Average loss: 1.586661547422409\n",
            "Len of Validation loss: 128, Average loss: 1.6402801929507405\n",
            "Epoch: 1024, Len of Training loss: 36, Average loss: 1.57602083351877\n",
            "Len of Validation loss: 128, Average loss: 1.71280930750072\n",
            "Epoch: 1025, Len of Training loss: 36, Average loss: 1.5911803079975977\n",
            "Len of Validation loss: 128, Average loss: 1.5893300690222532\n",
            "Epoch: 1026, Len of Training loss: 36, Average loss: 1.5847261448701222\n",
            "Len of Validation loss: 128, Average loss: 1.7101697763428092\n",
            "Epoch: 1027, Len of Training loss: 36, Average loss: 1.596225627594524\n",
            "Len of Validation loss: 128, Average loss: 1.6649210378527641\n",
            "Epoch: 1028, Len of Training loss: 36, Average loss: 1.59888176785575\n",
            "Len of Validation loss: 128, Average loss: 1.586917392211035\n",
            "Epoch: 1029, Len of Training loss: 36, Average loss: 1.6005106270313263\n",
            "Len of Validation loss: 128, Average loss: 1.6425410509109497\n",
            "Epoch: 1030, Len of Training loss: 36, Average loss: 1.6298931936422985\n",
            "Len of Validation loss: 128, Average loss: 1.6357579361647367\n",
            "Epoch: 1031, Len of Training loss: 36, Average loss: 1.5764663020769756\n",
            "Len of Validation loss: 128, Average loss: 1.5787189321126789\n",
            "Epoch: 1032, Len of Training loss: 36, Average loss: 1.559504868255721\n",
            "Len of Validation loss: 128, Average loss: 1.6315317838452756\n",
            "Epoch: 1033, Len of Training loss: 36, Average loss: 1.6283360057406955\n",
            "Len of Validation loss: 128, Average loss: 1.680497511755675\n",
            "Epoch: 1034, Len of Training loss: 36, Average loss: 1.6176864869064755\n",
            "Len of Validation loss: 128, Average loss: 1.633795427158475\n",
            "Epoch: 1035, Len of Training loss: 36, Average loss: 1.581039246585634\n",
            "Len of Validation loss: 128, Average loss: 1.646508275764063\n",
            "Epoch: 1036, Len of Training loss: 36, Average loss: 1.5899528198772006\n",
            "Len of Validation loss: 128, Average loss: 1.6349846706725657\n",
            "Epoch: 1037, Len of Training loss: 36, Average loss: 1.5851987302303314\n",
            "Len of Validation loss: 128, Average loss: 1.6800277340225875\n",
            "Epoch: 1038, Len of Training loss: 36, Average loss: 1.5973376863532596\n",
            "Len of Validation loss: 128, Average loss: 1.675180331338197\n",
            "Epoch: 1039, Len of Training loss: 36, Average loss: 1.5950051446755726\n",
            "Len of Validation loss: 128, Average loss: 1.6083140296395868\n",
            "Epoch: 1040, Len of Training loss: 36, Average loss: 1.6143880022896662\n",
            "Len of Validation loss: 128, Average loss: 1.6334940157830715\n",
            "Epoch: 1041, Len of Training loss: 36, Average loss: 1.5915679136912029\n",
            "Len of Validation loss: 128, Average loss: 1.6985011361539364\n",
            "Epoch: 1042, Len of Training loss: 36, Average loss: 1.5935290488931868\n",
            "Len of Validation loss: 128, Average loss: 1.5915336553007364\n",
            "Epoch: 1043, Len of Training loss: 36, Average loss: 1.5846496456199222\n",
            "Len of Validation loss: 128, Average loss: 1.6193896394688636\n",
            "Epoch: 1044, Len of Training loss: 36, Average loss: 1.5808060699039035\n",
            "Len of Validation loss: 128, Average loss: 1.6398886935785413\n",
            "Epoch: 1045, Len of Training loss: 36, Average loss: 1.6197503639592066\n",
            "Len of Validation loss: 128, Average loss: 1.640271122334525\n",
            "Epoch: 1046, Len of Training loss: 36, Average loss: 1.5939346700906754\n",
            "Len of Validation loss: 128, Average loss: 1.676786266732961\n",
            "Epoch: 1047, Len of Training loss: 36, Average loss: 1.6342458460066054\n",
            "Len of Validation loss: 128, Average loss: 1.6807590178214014\n",
            "Epoch: 1048, Len of Training loss: 36, Average loss: 1.6004110508494906\n",
            "Len of Validation loss: 128, Average loss: 1.6547945626080036\n",
            "Epoch: 1049, Len of Training loss: 36, Average loss: 1.5982363720734913\n",
            "Len of Validation loss: 128, Average loss: 1.632341873832047\n",
            "Epoch: 1050, Len of Training loss: 36, Average loss: 1.6116574042373233\n",
            "Len of Validation loss: 128, Average loss: 1.6403478640131652\n",
            "Epoch: 1051, Len of Training loss: 36, Average loss: 1.58962478240331\n",
            "Len of Validation loss: 128, Average loss: 1.676660141441971\n",
            "Epoch: 1052, Len of Training loss: 36, Average loss: 1.5937546425395541\n",
            "Len of Validation loss: 128, Average loss: 1.589910629671067\n",
            "Epoch: 1053, Len of Training loss: 36, Average loss: 1.575916975736618\n",
            "Len of Validation loss: 128, Average loss: 1.6376060440670699\n",
            "Epoch: 1054, Len of Training loss: 36, Average loss: 1.5955701834625668\n",
            "Len of Validation loss: 128, Average loss: 1.6298683520872146\n",
            "Epoch: 1055, Len of Training loss: 36, Average loss: 1.6073800325393677\n",
            "Len of Validation loss: 128, Average loss: 1.660162310115993\n",
            "Epoch: 1056, Len of Training loss: 36, Average loss: 1.5929130680031247\n",
            "Len of Validation loss: 128, Average loss: 1.647200862178579\n",
            "Epoch: 1057, Len of Training loss: 36, Average loss: 1.5654937624931335\n",
            "Len of Validation loss: 128, Average loss: 1.6472822078503668\n",
            "Epoch: 1058, Len of Training loss: 36, Average loss: 1.5925042927265167\n",
            "Len of Validation loss: 128, Average loss: 1.6915644314140081\n",
            "Epoch: 1059, Len of Training loss: 36, Average loss: 1.6165889435344272\n",
            "Len of Validation loss: 128, Average loss: 1.628571174805984\n",
            "Epoch: 1060, Len of Training loss: 36, Average loss: 1.6007119450304244\n",
            "Len of Validation loss: 128, Average loss: 1.6406547566875815\n",
            "Epoch: 1061, Len of Training loss: 36, Average loss: 1.5868936710887485\n",
            "Len of Validation loss: 128, Average loss: 1.605251767206937\n",
            "Epoch: 1062, Len of Training loss: 36, Average loss: 1.5896767973899841\n",
            "Len of Validation loss: 128, Average loss: 1.6239357041195035\n",
            "Epoch: 1063, Len of Training loss: 36, Average loss: 1.5996221436394586\n",
            "Len of Validation loss: 128, Average loss: 1.6338339655194432\n",
            "Epoch: 1064, Len of Training loss: 36, Average loss: 1.5900789532396529\n",
            "Len of Validation loss: 128, Average loss: 1.6714518698863685\n",
            "Epoch: 1065, Len of Training loss: 36, Average loss: 1.5789392126931086\n",
            "Len of Validation loss: 128, Average loss: 1.6638933166395873\n",
            "Epoch: 1066, Len of Training loss: 36, Average loss: 1.585536625650194\n",
            "Len of Validation loss: 128, Average loss: 1.6560349632054567\n",
            "Epoch: 1067, Len of Training loss: 36, Average loss: 1.5930806034141116\n",
            "Len of Validation loss: 128, Average loss: 1.6249594546388835\n",
            "Epoch: 1068, Len of Training loss: 36, Average loss: 1.593816939327452\n",
            "Len of Validation loss: 128, Average loss: 1.605943891685456\n",
            "Epoch: 1069, Len of Training loss: 36, Average loss: 1.5831331544452243\n",
            "Len of Validation loss: 128, Average loss: 1.5988381577190012\n",
            "Epoch: 1070, Len of Training loss: 36, Average loss: 1.5539398259586759\n",
            "Len of Validation loss: 128, Average loss: 1.633454263675958\n",
            "Epoch: 1071, Len of Training loss: 36, Average loss: 1.5676827331384022\n",
            "Len of Validation loss: 128, Average loss: 1.6095176790840924\n",
            "Epoch: 1072, Len of Training loss: 36, Average loss: 1.5720992518795862\n",
            "Len of Validation loss: 128, Average loss: 1.6147416490130126\n",
            "Epoch: 1073, Len of Training loss: 36, Average loss: 1.7263577216201358\n",
            "Len of Validation loss: 128, Average loss: 1.7741289283148944\n",
            "Epoch: 1074, Len of Training loss: 36, Average loss: 1.5984472566180759\n",
            "Len of Validation loss: 128, Average loss: 1.6484424881637096\n",
            "Epoch: 1075, Len of Training loss: 36, Average loss: 1.578770597775777\n",
            "Len of Validation loss: 128, Average loss: 1.6457808283157647\n",
            "Epoch: 1076, Len of Training loss: 36, Average loss: 1.5688269270790949\n",
            "Len of Validation loss: 128, Average loss: 1.6272525016684085\n",
            "Epoch: 1077, Len of Training loss: 36, Average loss: 1.5681707345777087\n",
            "Len of Validation loss: 128, Average loss: 1.6572707737796009\n",
            "Epoch: 1078, Len of Training loss: 36, Average loss: 1.5948372019661798\n",
            "Len of Validation loss: 128, Average loss: 1.6455430020578206\n",
            "Epoch: 1079, Len of Training loss: 36, Average loss: 1.57216962840822\n",
            "Len of Validation loss: 128, Average loss: 1.6130958676803857\n",
            "Epoch: 1080, Len of Training loss: 36, Average loss: 1.6104409065511491\n",
            "Len of Validation loss: 128, Average loss: 1.664914348628372\n",
            "Epoch: 1081, Len of Training loss: 36, Average loss: 1.6248217456870608\n",
            "Len of Validation loss: 128, Average loss: 1.766326897079125\n",
            "Epoch: 1082, Len of Training loss: 36, Average loss: 1.6050146851274703\n",
            "Len of Validation loss: 128, Average loss: 1.625188944162801\n",
            "Epoch: 1083, Len of Training loss: 36, Average loss: 1.5989613797929552\n",
            "Len of Validation loss: 128, Average loss: 1.5871218715328723\n",
            "Epoch: 1084, Len of Training loss: 36, Average loss: 1.6138917538854811\n",
            "Len of Validation loss: 128, Average loss: 1.6339603622909635\n",
            "Epoch: 1085, Len of Training loss: 36, Average loss: 1.5880370603667364\n",
            "Len of Validation loss: 128, Average loss: 1.5965716717764735\n",
            "Epoch: 1086, Len of Training loss: 36, Average loss: 1.5751456783877478\n",
            "Len of Validation loss: 128, Average loss: 1.5758226120378822\n",
            "Epoch: 1087, Len of Training loss: 36, Average loss: 1.5678661796781752\n",
            "Len of Validation loss: 128, Average loss: 1.6091512890998274\n",
            "Epoch: 1088, Len of Training loss: 36, Average loss: 1.5710691312948863\n",
            "Len of Validation loss: 128, Average loss: 1.6230768878012896\n",
            "Epoch: 1089, Len of Training loss: 36, Average loss: 1.5779872569772933\n",
            "Len of Validation loss: 128, Average loss: 1.6975368191488087\n",
            "Epoch: 1090, Len of Training loss: 36, Average loss: 1.5911264452669356\n",
            "Len of Validation loss: 128, Average loss: 1.6247447463683784\n",
            "Epoch: 1091, Len of Training loss: 36, Average loss: 1.5724640554851956\n",
            "Len of Validation loss: 128, Average loss: 1.5990986295510083\n",
            "Epoch: 1092, Len of Training loss: 36, Average loss: 1.5651597181955974\n",
            "Len of Validation loss: 128, Average loss: 1.6195564894005656\n",
            "Epoch: 1093, Len of Training loss: 36, Average loss: 1.5904688504007127\n",
            "Len of Validation loss: 128, Average loss: 1.6462128558196127\n",
            "Epoch: 1094, Len of Training loss: 36, Average loss: 1.5915185709794362\n",
            "Len of Validation loss: 128, Average loss: 1.725269861984998\n",
            "Epoch: 1095, Len of Training loss: 36, Average loss: 1.602941976653205\n",
            "Len of Validation loss: 128, Average loss: 1.6040762902703136\n",
            "Epoch: 1096, Len of Training loss: 36, Average loss: 1.5744136836793687\n",
            "Len of Validation loss: 128, Average loss: 1.6279791134875268\n",
            "Epoch: 1097, Len of Training loss: 36, Average loss: 1.5500000152322981\n",
            "Len of Validation loss: 128, Average loss: 1.6123234806582332\n",
            "Epoch: 1098, Len of Training loss: 36, Average loss: 1.6518856883049011\n",
            "Len of Validation loss: 128, Average loss: 1.6472817037720233\n",
            "Epoch: 1099, Len of Training loss: 36, Average loss: 1.5777339604165819\n",
            "Len of Validation loss: 128, Average loss: 1.621845491696149\n",
            "Epoch: 1100, Len of Training loss: 36, Average loss: 1.601972586578793\n",
            "Len of Validation loss: 128, Average loss: 1.7196649829857051\n",
            "Epoch: 1101, Len of Training loss: 36, Average loss: 1.6793631845050387\n",
            "Len of Validation loss: 128, Average loss: 1.6377401547506452\n",
            "Epoch: 1102, Len of Training loss: 36, Average loss: 1.5787834094630346\n",
            "Len of Validation loss: 128, Average loss: 1.6102084913291037\n",
            "Epoch: 1103, Len of Training loss: 36, Average loss: 1.5658553044001262\n",
            "Len of Validation loss: 128, Average loss: 1.6084958747960627\n",
            "Epoch: 1104, Len of Training loss: 36, Average loss: 1.592716481950548\n",
            "Len of Validation loss: 128, Average loss: 1.617128761485219\n",
            "Epoch: 1105, Len of Training loss: 36, Average loss: 1.5633276005585988\n",
            "Len of Validation loss: 128, Average loss: 1.6045402120798826\n",
            "Epoch: 1106, Len of Training loss: 36, Average loss: 1.5530871484014723\n",
            "Len of Validation loss: 128, Average loss: 1.6205360111780465\n",
            "Epoch: 1107, Len of Training loss: 36, Average loss: 1.5720255143112607\n",
            "Len of Validation loss: 128, Average loss: 1.6168462315108627\n",
            "Epoch: 1108, Len of Training loss: 36, Average loss: 1.5648431645499334\n",
            "Len of Validation loss: 128, Average loss: 1.6241053915582597\n",
            "Epoch: 1109, Len of Training loss: 36, Average loss: 1.5621136095788744\n",
            "Len of Validation loss: 128, Average loss: 1.6032167847733945\n",
            "Epoch: 1110, Len of Training loss: 36, Average loss: 1.552294996049669\n",
            "Len of Validation loss: 128, Average loss: 1.6502779028378427\n",
            "Epoch: 1111, Len of Training loss: 36, Average loss: 1.5544506278302934\n",
            "Len of Validation loss: 128, Average loss: 1.6452573975548148\n",
            "Epoch: 1112, Len of Training loss: 36, Average loss: 1.595320463180542\n",
            "Len of Validation loss: 128, Average loss: 1.616813460830599\n",
            "Epoch: 1113, Len of Training loss: 36, Average loss: 1.5774285826418135\n",
            "Len of Validation loss: 128, Average loss: 1.620929621392861\n",
            "Epoch: 1114, Len of Training loss: 36, Average loss: 1.553121202521854\n",
            "Len of Validation loss: 128, Average loss: 1.6457378179766238\n",
            "Epoch: 1115, Len of Training loss: 36, Average loss: 1.5577029685179393\n",
            "Len of Validation loss: 128, Average loss: 1.652127974666655\n",
            "Epoch: 1116, Len of Training loss: 36, Average loss: 1.594235297706392\n",
            "Len of Validation loss: 128, Average loss: 1.645976149942726\n",
            "Epoch: 1117, Len of Training loss: 36, Average loss: 1.571250448624293\n",
            "Len of Validation loss: 128, Average loss: 1.6712362772086635\n",
            "Epoch: 1118, Len of Training loss: 36, Average loss: 1.5667943523989782\n",
            "Len of Validation loss: 128, Average loss: 1.6543563085142523\n",
            "Epoch: 1119, Len of Training loss: 36, Average loss: 1.582821958594852\n",
            "Len of Validation loss: 128, Average loss: 1.6592334536835551\n",
            "Epoch: 1120, Len of Training loss: 36, Average loss: 1.561106178495619\n",
            "Len of Validation loss: 128, Average loss: 1.6345846615731716\n",
            "Epoch: 1121, Len of Training loss: 36, Average loss: 1.5512339803907607\n",
            "Len of Validation loss: 128, Average loss: 1.6073313930537552\n",
            "Epoch: 1122, Len of Training loss: 36, Average loss: 1.5761968162324693\n",
            "Len of Validation loss: 128, Average loss: 1.669482660945505\n",
            "Epoch: 1123, Len of Training loss: 36, Average loss: 1.650032063325246\n",
            "Len of Validation loss: 128, Average loss: 1.6606665793806314\n",
            "Epoch: 1124, Len of Training loss: 36, Average loss: 1.5622345904509227\n",
            "Len of Validation loss: 128, Average loss: 1.6192233306355774\n",
            "Epoch: 1125, Len of Training loss: 36, Average loss: 1.5419478747579787\n",
            "Len of Validation loss: 128, Average loss: 1.604568694718182\n",
            "Epoch: 1126, Len of Training loss: 36, Average loss: 1.5975846317079332\n",
            "Len of Validation loss: 128, Average loss: 1.684897592291236\n",
            "Epoch: 1127, Len of Training loss: 36, Average loss: 1.5931084685855441\n",
            "Len of Validation loss: 128, Average loss: 1.7247297836001962\n",
            "Epoch: 1128, Len of Training loss: 36, Average loss: 1.5856420762009091\n",
            "Len of Validation loss: 128, Average loss: 1.5995286328252405\n",
            "Epoch: 1129, Len of Training loss: 36, Average loss: 1.5692655344804127\n",
            "Len of Validation loss: 128, Average loss: 1.6744643594138324\n",
            "Epoch: 1130, Len of Training loss: 36, Average loss: 1.5422731406158872\n",
            "Len of Validation loss: 128, Average loss: 1.6272183714900166\n",
            "Epoch: 1131, Len of Training loss: 36, Average loss: 1.6124152077568903\n",
            "Len of Validation loss: 128, Average loss: 1.647832022747025\n",
            "Epoch: 1132, Len of Training loss: 36, Average loss: 1.5448490513695612\n",
            "Len of Validation loss: 128, Average loss: 1.6341273155994713\n",
            "Epoch: 1133, Len of Training loss: 36, Average loss: 1.6491323709487915\n",
            "Len of Validation loss: 128, Average loss: 1.630519927246496\n",
            "Epoch: 1134, Len of Training loss: 36, Average loss: 1.575322836637497\n",
            "Len of Validation loss: 128, Average loss: 1.5931567654479295\n",
            "Epoch: 1135, Len of Training loss: 36, Average loss: 1.5430871380700006\n",
            "Len of Validation loss: 128, Average loss: 1.6281107128597796\n",
            "Epoch: 1136, Len of Training loss: 36, Average loss: 1.5358498460716672\n",
            "Len of Validation loss: 128, Average loss: 1.6485829027369618\n",
            "Epoch: 1137, Len of Training loss: 36, Average loss: 1.5476653079191844\n",
            "Len of Validation loss: 128, Average loss: 1.638240842614323\n",
            "Epoch: 1138, Len of Training loss: 36, Average loss: 1.5776832302411397\n",
            "Len of Validation loss: 128, Average loss: 1.6482513858936727\n",
            "Epoch: 1139, Len of Training loss: 36, Average loss: 1.5992055800226\n",
            "Len of Validation loss: 128, Average loss: 1.5808839097153395\n",
            "Epoch: 1140, Len of Training loss: 36, Average loss: 1.5346068855788972\n",
            "Len of Validation loss: 128, Average loss: 1.6304665189236403\n",
            "Epoch: 1141, Len of Training loss: 36, Average loss: 1.5381894475883908\n",
            "Len of Validation loss: 128, Average loss: 1.6008008969947696\n",
            "Epoch: 1142, Len of Training loss: 36, Average loss: 1.5767495897081163\n",
            "Len of Validation loss: 128, Average loss: 1.6332497303374112\n",
            "Epoch: 1143, Len of Training loss: 36, Average loss: 1.5554367668098874\n",
            "Len of Validation loss: 128, Average loss: 1.6290066759102046\n",
            "Epoch: 1144, Len of Training loss: 36, Average loss: 1.5410747792985704\n",
            "Len of Validation loss: 128, Average loss: 1.5958942959550768\n",
            "Epoch: 1145, Len of Training loss: 36, Average loss: 1.5692057808240254\n",
            "Len of Validation loss: 128, Average loss: 1.617966721765697\n",
            "Epoch: 1146, Len of Training loss: 36, Average loss: 1.5364245540565915\n",
            "Len of Validation loss: 128, Average loss: 1.5834626343566924\n",
            "Epoch: 1147, Len of Training loss: 36, Average loss: 1.5410804483625624\n",
            "Len of Validation loss: 128, Average loss: 1.666732077486813\n",
            "Epoch: 1148, Len of Training loss: 36, Average loss: 1.547952628797955\n",
            "Len of Validation loss: 128, Average loss: 1.6689160312525928\n",
            "Epoch: 1149, Len of Training loss: 36, Average loss: 1.5695393019252353\n",
            "Len of Validation loss: 128, Average loss: 1.6253623508382589\n",
            "Epoch: 1150, Len of Training loss: 36, Average loss: 1.545449584722519\n",
            "Len of Validation loss: 128, Average loss: 1.5716685205698013\n",
            "Epoch: 1151, Len of Training loss: 36, Average loss: 1.563892685704761\n",
            "Len of Validation loss: 128, Average loss: 1.6140339605044574\n",
            "Epoch: 1152, Len of Training loss: 36, Average loss: 1.5699880917867024\n",
            "Len of Validation loss: 128, Average loss: 1.5959946787916124\n",
            "Epoch: 1153, Len of Training loss: 36, Average loss: 1.558045380645328\n",
            "Len of Validation loss: 128, Average loss: 1.6239433237351477\n",
            "Epoch: 1154, Len of Training loss: 36, Average loss: 1.5512524710761175\n",
            "Len of Validation loss: 128, Average loss: 1.7398469559848309\n",
            "Epoch: 1155, Len of Training loss: 36, Average loss: 1.563980691962772\n",
            "Len of Validation loss: 128, Average loss: 1.635042688343674\n",
            "Epoch: 1156, Len of Training loss: 36, Average loss: 1.5718097587426503\n",
            "Len of Validation loss: 128, Average loss: 1.6099424210842699\n",
            "Epoch: 1157, Len of Training loss: 36, Average loss: 1.5464321209324732\n",
            "Len of Validation loss: 128, Average loss: 1.6433787755668163\n",
            "Epoch: 1158, Len of Training loss: 36, Average loss: 1.5494813753498926\n",
            "Len of Validation loss: 128, Average loss: 1.5983472913503647\n",
            "Epoch: 1159, Len of Training loss: 36, Average loss: 1.540894564655092\n",
            "Len of Validation loss: 128, Average loss: 1.6091592737939209\n",
            "Epoch: 1160, Len of Training loss: 36, Average loss: 1.5530288302236133\n",
            "Len of Validation loss: 128, Average loss: 1.5951256295666099\n",
            "Epoch: 1161, Len of Training loss: 36, Average loss: 1.5446012384361691\n",
            "Len of Validation loss: 128, Average loss: 1.602837136713788\n",
            "Epoch: 1162, Len of Training loss: 36, Average loss: 1.544591122203403\n",
            "Len of Validation loss: 128, Average loss: 1.643293762812391\n",
            "Epoch: 1163, Len of Training loss: 36, Average loss: 1.5573227769798703\n",
            "Len of Validation loss: 128, Average loss: 1.6022732420824468\n",
            "Epoch: 1164, Len of Training loss: 36, Average loss: 1.5385740001996357\n",
            "Len of Validation loss: 128, Average loss: 1.6532928487285972\n",
            "Epoch: 1165, Len of Training loss: 36, Average loss: 1.526884115404553\n",
            "Len of Validation loss: 128, Average loss: 1.5856013710144907\n",
            "Epoch: 1166, Len of Training loss: 36, Average loss: 1.5692764553758833\n",
            "Len of Validation loss: 128, Average loss: 1.620108479168266\n",
            "Epoch: 1167, Len of Training loss: 36, Average loss: 1.562866724199719\n",
            "Len of Validation loss: 128, Average loss: 1.6571684391237795\n",
            "Epoch: 1168, Len of Training loss: 36, Average loss: 1.5621051755216386\n",
            "Len of Validation loss: 128, Average loss: 1.6307772782165557\n",
            "Epoch: 1169, Len of Training loss: 36, Average loss: 1.5410971310403612\n",
            "Len of Validation loss: 128, Average loss: 1.6267335307784379\n",
            "Epoch: 1170, Len of Training loss: 36, Average loss: 1.5669881966378953\n",
            "Len of Validation loss: 128, Average loss: 1.6307381314691156\n",
            "Epoch: 1171, Len of Training loss: 36, Average loss: 1.5191485749350653\n",
            "Len of Validation loss: 128, Average loss: 1.605344231822528\n",
            "Epoch: 1172, Len of Training loss: 36, Average loss: 1.5685759286085765\n",
            "Len of Validation loss: 128, Average loss: 1.6751390784047544\n",
            "Epoch: 1173, Len of Training loss: 36, Average loss: 1.5719846222135756\n",
            "Len of Validation loss: 128, Average loss: 1.6352810349781066\n",
            "Epoch: 1174, Len of Training loss: 36, Average loss: 1.6122126148806677\n",
            "Len of Validation loss: 128, Average loss: 1.6821512118913233\n",
            "Epoch: 1175, Len of Training loss: 36, Average loss: 1.5882493522432115\n",
            "Len of Validation loss: 128, Average loss: 1.6356153907254338\n",
            "Epoch: 1176, Len of Training loss: 36, Average loss: 1.5570231642987993\n",
            "Len of Validation loss: 128, Average loss: 1.5901678623631597\n",
            "Epoch: 1177, Len of Training loss: 36, Average loss: 1.5347055527899\n",
            "Len of Validation loss: 128, Average loss: 1.6068964232690632\n",
            "Epoch: 1178, Len of Training loss: 36, Average loss: 1.5338861892620723\n",
            "Len of Validation loss: 128, Average loss: 1.5709244958125055\n",
            "Epoch: 1179, Len of Training loss: 36, Average loss: 1.564095030228297\n",
            "Len of Validation loss: 128, Average loss: 1.6381939689163119\n",
            "Epoch: 1180, Len of Training loss: 36, Average loss: 1.5315290358331468\n",
            "Len of Validation loss: 128, Average loss: 1.6161086387000978\n",
            "Epoch: 1181, Len of Training loss: 36, Average loss: 1.5468754238552518\n",
            "Len of Validation loss: 128, Average loss: 1.5943538169376552\n",
            "Epoch: 1182, Len of Training loss: 36, Average loss: 1.5511724452177684\n",
            "Len of Validation loss: 128, Average loss: 1.601641341112554\n",
            "Epoch: 1183, Len of Training loss: 36, Average loss: 1.5176763302750058\n",
            "Len of Validation loss: 128, Average loss: 1.5912976379040629\n",
            "Epoch: 1184, Len of Training loss: 36, Average loss: 1.54034310248163\n",
            "Len of Validation loss: 128, Average loss: 1.7165940834674984\n",
            "Epoch: 1185, Len of Training loss: 36, Average loss: 1.5462086995442708\n",
            "Len of Validation loss: 128, Average loss: 1.6099069803021848\n",
            "Epoch: 1186, Len of Training loss: 36, Average loss: 1.5189290708965726\n",
            "Len of Validation loss: 128, Average loss: 1.5736161274835467\n",
            "Epoch: 1187, Len of Training loss: 36, Average loss: 1.5460960666338603\n",
            "Len of Validation loss: 128, Average loss: 1.6491703977808356\n",
            "Epoch: 1188, Len of Training loss: 36, Average loss: 1.5293808380762737\n",
            "Len of Validation loss: 128, Average loss: 1.5811814237385988\n",
            "Epoch: 1189, Len of Training loss: 36, Average loss: 1.5205107231934865\n",
            "Len of Validation loss: 128, Average loss: 1.6058610104955733\n",
            "Epoch: 1190, Len of Training loss: 36, Average loss: 1.544560392697652\n",
            "Len of Validation loss: 128, Average loss: 1.6783379844855517\n",
            "Epoch: 1191, Len of Training loss: 36, Average loss: 1.569162153535419\n",
            "Len of Validation loss: 128, Average loss: 1.6680991428438574\n",
            "Epoch: 1192, Len of Training loss: 36, Average loss: 1.6385842363039653\n",
            "Len of Validation loss: 128, Average loss: 1.613863676553592\n",
            "Epoch: 1193, Len of Training loss: 36, Average loss: 1.5444442563586764\n",
            "Len of Validation loss: 128, Average loss: 1.6090169083327055\n",
            "Epoch: 1194, Len of Training loss: 36, Average loss: 1.550448348124822\n",
            "Len of Validation loss: 128, Average loss: 1.597181806107983\n",
            "Epoch: 1195, Len of Training loss: 36, Average loss: 1.5417533583111234\n",
            "Len of Validation loss: 128, Average loss: 1.646410659654066\n",
            "Epoch: 1196, Len of Training loss: 36, Average loss: 1.5505194233523474\n",
            "Len of Validation loss: 128, Average loss: 1.5918237189762294\n",
            "Epoch: 1197, Len of Training loss: 36, Average loss: 1.5085118181175656\n",
            "Len of Validation loss: 128, Average loss: 1.582331337267533\n",
            "Epoch: 1198, Len of Training loss: 36, Average loss: 1.5245290166801877\n",
            "Len of Validation loss: 128, Average loss: 1.586855286732316\n",
            "Epoch: 1199, Len of Training loss: 36, Average loss: 1.5588247544235654\n",
            "Len of Validation loss: 128, Average loss: 1.634667675709352\n",
            "Epoch: 1200, Len of Training loss: 36, Average loss: 1.5450234313805897\n",
            "Len of Validation loss: 128, Average loss: 1.6137370236683637\n",
            "Epoch: 1201, Len of Training loss: 36, Average loss: 1.5437694357501135\n",
            "Len of Validation loss: 128, Average loss: 1.6099764052778482\n",
            "Epoch: 1202, Len of Training loss: 36, Average loss: 1.536071565416124\n",
            "Len of Validation loss: 128, Average loss: 1.5715151610784233\n",
            "Epoch: 1203, Len of Training loss: 36, Average loss: 1.5170171426402197\n",
            "Len of Validation loss: 128, Average loss: 1.651341559132561\n",
            "Epoch: 1204, Len of Training loss: 36, Average loss: 1.53214630484581\n",
            "Len of Validation loss: 128, Average loss: 1.5968296574428678\n",
            "Epoch: 1205, Len of Training loss: 36, Average loss: 1.5491359366310968\n",
            "Len of Validation loss: 128, Average loss: 1.593848486430943\n",
            "Epoch: 1206, Len of Training loss: 36, Average loss: 1.5510629547966852\n",
            "Len of Validation loss: 128, Average loss: 1.5915893064811826\n",
            "Epoch: 1207, Len of Training loss: 36, Average loss: 1.5305815007951524\n",
            "Len of Validation loss: 128, Average loss: 1.608657855540514\n",
            "Epoch: 1208, Len of Training loss: 36, Average loss: 1.5347658263312445\n",
            "Len of Validation loss: 128, Average loss: 1.622999859508127\n",
            "Epoch: 1209, Len of Training loss: 36, Average loss: 1.681880599922604\n",
            "Len of Validation loss: 128, Average loss: 1.8942842923570424\n",
            "Epoch: 1210, Len of Training loss: 36, Average loss: 2.281588269604577\n",
            "Len of Validation loss: 128, Average loss: 1.8346237386576831\n",
            "Epoch: 1211, Len of Training loss: 36, Average loss: 1.9071746468544006\n",
            "Len of Validation loss: 128, Average loss: 1.793741294182837\n",
            "Epoch: 1212, Len of Training loss: 36, Average loss: 1.7503892944918737\n",
            "Len of Validation loss: 128, Average loss: 1.6573141408152878\n",
            "Epoch: 1213, Len of Training loss: 36, Average loss: 1.6483565502696567\n",
            "Len of Validation loss: 128, Average loss: 1.7470529759302735\n",
            "Epoch: 1214, Len of Training loss: 36, Average loss: 1.639276749557919\n",
            "Len of Validation loss: 128, Average loss: 1.6731818076223135\n",
            "Epoch: 1215, Len of Training loss: 36, Average loss: 1.5690474609533946\n",
            "Len of Validation loss: 128, Average loss: 1.5956350388005376\n",
            "Epoch: 1216, Len of Training loss: 36, Average loss: 1.572687652375963\n",
            "Len of Validation loss: 128, Average loss: 1.6723343094345182\n",
            "Epoch: 1217, Len of Training loss: 36, Average loss: 1.5753667520152197\n",
            "Len of Validation loss: 128, Average loss: 1.6270653898827732\n",
            "Epoch: 1218, Len of Training loss: 36, Average loss: 1.544509122769038\n",
            "Len of Validation loss: 128, Average loss: 1.6641276944428682\n",
            "Epoch: 1219, Len of Training loss: 36, Average loss: 1.5709379613399506\n",
            "Len of Validation loss: 128, Average loss: 1.638798791449517\n",
            "Epoch: 1220, Len of Training loss: 36, Average loss: 1.5500645968649123\n",
            "Len of Validation loss: 128, Average loss: 1.6670419776346534\n",
            "Epoch: 1221, Len of Training loss: 36, Average loss: 1.5431863996717665\n",
            "Len of Validation loss: 128, Average loss: 1.6385254201013595\n",
            "Epoch: 1222, Len of Training loss: 36, Average loss: 1.5349881880813174\n",
            "Len of Validation loss: 128, Average loss: 1.6184665008913726\n",
            "Epoch: 1223, Len of Training loss: 36, Average loss: 1.5544732477929857\n",
            "Len of Validation loss: 128, Average loss: 1.6576516737695783\n",
            "Epoch: 1224, Len of Training loss: 36, Average loss: 1.600399547153049\n",
            "Len of Validation loss: 128, Average loss: 1.631324717309326\n",
            "Epoch: 1225, Len of Training loss: 36, Average loss: 1.5444456769360437\n",
            "Len of Validation loss: 128, Average loss: 1.6170458777341992\n",
            "Epoch: 1226, Len of Training loss: 36, Average loss: 1.5288796325524647\n",
            "Len of Validation loss: 128, Average loss: 1.6007541448343545\n",
            "Epoch: 1227, Len of Training loss: 36, Average loss: 1.522635840707355\n",
            "Len of Validation loss: 128, Average loss: 1.6248718816787004\n",
            "Epoch: 1228, Len of Training loss: 36, Average loss: 1.5221633712450664\n",
            "Len of Validation loss: 128, Average loss: 1.5678886983077973\n",
            "Epoch: 1229, Len of Training loss: 36, Average loss: 1.5270635055171118\n",
            "Len of Validation loss: 128, Average loss: 1.6270052106119692\n",
            "Epoch: 1230, Len of Training loss: 36, Average loss: 1.5445072568125195\n",
            "Len of Validation loss: 128, Average loss: 1.6485615619458258\n",
            "Epoch: 1231, Len of Training loss: 36, Average loss: 1.5602445436848535\n",
            "Len of Validation loss: 128, Average loss: 1.596486067166552\n",
            "Epoch: 1232, Len of Training loss: 36, Average loss: 1.5176255769199796\n",
            "Len of Validation loss: 128, Average loss: 1.6023106896318495\n",
            "Epoch: 1233, Len of Training loss: 36, Average loss: 1.5414503150516086\n",
            "Len of Validation loss: 128, Average loss: 1.597067603142932\n",
            "Epoch: 1234, Len of Training loss: 36, Average loss: 1.5244478947586484\n",
            "Len of Validation loss: 128, Average loss: 1.570412770146504\n",
            "Epoch: 1235, Len of Training loss: 36, Average loss: 1.5253538224432204\n",
            "Len of Validation loss: 128, Average loss: 1.6098855142481625\n",
            "Epoch: 1236, Len of Training loss: 36, Average loss: 1.505649424261517\n",
            "Len of Validation loss: 128, Average loss: 1.6118055817205459\n",
            "Epoch: 1237, Len of Training loss: 36, Average loss: 1.515853914949629\n",
            "Len of Validation loss: 128, Average loss: 1.582372053526342\n",
            "Epoch: 1238, Len of Training loss: 36, Average loss: 1.547889037264718\n",
            "Len of Validation loss: 128, Average loss: 1.636855598539114\n",
            "Epoch: 1239, Len of Training loss: 36, Average loss: 1.554318173064126\n",
            "Len of Validation loss: 128, Average loss: 1.571840526536107\n",
            "Epoch: 1240, Len of Training loss: 36, Average loss: 1.518296738465627\n",
            "Len of Validation loss: 128, Average loss: 1.570462552132085\n",
            "Epoch: 1241, Len of Training loss: 36, Average loss: 1.548581149843004\n",
            "Len of Validation loss: 128, Average loss: 1.6149689839221537\n",
            "Epoch: 1242, Len of Training loss: 36, Average loss: 1.5328178074624803\n",
            "Len of Validation loss: 128, Average loss: 1.6147702678572387\n",
            "Epoch: 1243, Len of Training loss: 36, Average loss: 1.6111885971493192\n",
            "Len of Validation loss: 128, Average loss: 1.6318538128398359\n",
            "Epoch: 1244, Len of Training loss: 36, Average loss: 1.565588116645813\n",
            "Len of Validation loss: 128, Average loss: 1.5863439643289894\n",
            "Epoch: 1245, Len of Training loss: 36, Average loss: 1.5485440227720473\n",
            "Len of Validation loss: 128, Average loss: 1.59876953298226\n",
            "Epoch: 1246, Len of Training loss: 36, Average loss: 1.525885244210561\n",
            "Len of Validation loss: 128, Average loss: 1.6089948336593807\n",
            "Epoch: 1247, Len of Training loss: 36, Average loss: 1.5283195078372955\n",
            "Len of Validation loss: 128, Average loss: 1.6045087785460055\n",
            "Epoch: 1248, Len of Training loss: 36, Average loss: 1.513425138261583\n",
            "Len of Validation loss: 128, Average loss: 1.6231259119231254\n",
            "Epoch: 1249, Len of Training loss: 36, Average loss: 1.563934925529692\n",
            "Len of Validation loss: 128, Average loss: 1.6181610352359712\n",
            "Epoch: 1250, Len of Training loss: 36, Average loss: 1.5507179035080805\n",
            "Len of Validation loss: 128, Average loss: 1.5621156143024564\n",
            "Epoch: 1251, Len of Training loss: 36, Average loss: 1.5459250344170465\n",
            "Len of Validation loss: 128, Average loss: 1.6058233759831637\n",
            "Epoch: 1252, Len of Training loss: 36, Average loss: 1.5071573853492737\n",
            "Len of Validation loss: 128, Average loss: 1.6118338706437498\n",
            "Epoch: 1253, Len of Training loss: 36, Average loss: 1.51344010565016\n",
            "Len of Validation loss: 128, Average loss: 1.5898357999976724\n",
            "Epoch: 1254, Len of Training loss: 36, Average loss: 1.5356203847461276\n",
            "Len of Validation loss: 128, Average loss: 1.5911956103518605\n",
            "Epoch: 1255, Len of Training loss: 36, Average loss: 1.5770555469724867\n",
            "Len of Validation loss: 128, Average loss: 1.6807855875231326\n",
            "Epoch: 1256, Len of Training loss: 36, Average loss: 1.5205909344885085\n",
            "Len of Validation loss: 128, Average loss: 1.6146983618382365\n",
            "Epoch: 1257, Len of Training loss: 36, Average loss: 1.5371436675389607\n",
            "Len of Validation loss: 128, Average loss: 1.6329771932214499\n",
            "Epoch: 1258, Len of Training loss: 36, Average loss: 1.5396954516569774\n",
            "Len of Validation loss: 128, Average loss: 1.613049736013636\n",
            "Epoch: 1259, Len of Training loss: 36, Average loss: 1.5632308787769742\n",
            "Len of Validation loss: 128, Average loss: 1.610400040866807\n",
            "Epoch: 1260, Len of Training loss: 36, Average loss: 1.5320617920822568\n",
            "Len of Validation loss: 128, Average loss: 1.634553601499647\n",
            "Epoch: 1261, Len of Training loss: 36, Average loss: 1.526151802804735\n",
            "Len of Validation loss: 128, Average loss: 1.5870418772101402\n",
            "Epoch: 1262, Len of Training loss: 36, Average loss: 1.538048631615109\n",
            "Len of Validation loss: 128, Average loss: 1.6559265772812068\n",
            "Epoch: 1263, Len of Training loss: 36, Average loss: 1.525242633289761\n",
            "Len of Validation loss: 128, Average loss: 1.6445849935989827\n",
            "Epoch: 1264, Len of Training loss: 36, Average loss: 1.5448424087630377\n",
            "Len of Validation loss: 128, Average loss: 1.5618776276241988\n",
            "Epoch: 1265, Len of Training loss: 36, Average loss: 1.515603890021642\n",
            "Len of Validation loss: 128, Average loss: 1.618713199859485\n",
            "Epoch: 1266, Len of Training loss: 36, Average loss: 1.5266514619191487\n",
            "Len of Validation loss: 128, Average loss: 1.641484450083226\n",
            "Epoch: 1267, Len of Training loss: 36, Average loss: 1.5301355719566345\n",
            "Len of Validation loss: 128, Average loss: 1.5601891858968884\n",
            "Epoch: 1268, Len of Training loss: 36, Average loss: 1.4967335098319583\n",
            "Len of Validation loss: 128, Average loss: 1.6020476017147303\n",
            "Epoch: 1269, Len of Training loss: 36, Average loss: 1.4959012567996979\n",
            "Len of Validation loss: 128, Average loss: 1.5747631206177175\n",
            "Epoch: 1270, Len of Training loss: 36, Average loss: 1.5181441571977403\n",
            "Len of Validation loss: 128, Average loss: 1.577063363045454\n",
            "Epoch: 1271, Len of Training loss: 36, Average loss: 1.5275525748729706\n",
            "Len of Validation loss: 128, Average loss: 1.5525683390442282\n",
            "Epoch: 1272, Len of Training loss: 36, Average loss: 1.5207427342732747\n",
            "Len of Validation loss: 128, Average loss: 1.5669561941176653\n",
            "Epoch: 1273, Len of Training loss: 36, Average loss: 1.4948602881696489\n",
            "Len of Validation loss: 128, Average loss: 1.5845446069724858\n",
            "Epoch: 1274, Len of Training loss: 36, Average loss: 1.5045356220669217\n",
            "Len of Validation loss: 128, Average loss: 1.5810596626251936\n",
            "Epoch: 1275, Len of Training loss: 36, Average loss: 1.5079669654369354\n",
            "Len of Validation loss: 128, Average loss: 1.6219435904640704\n",
            "Epoch: 1276, Len of Training loss: 36, Average loss: 1.4849307139714558\n",
            "Len of Validation loss: 128, Average loss: 1.6180415078997612\n",
            "Epoch: 1277, Len of Training loss: 36, Average loss: 1.5753047068913777\n",
            "Len of Validation loss: 128, Average loss: 1.6422405284829438\n",
            "Epoch: 1278, Len of Training loss: 36, Average loss: 1.5195866127808888\n",
            "Len of Validation loss: 128, Average loss: 1.577669881284237\n",
            "Epoch: 1279, Len of Training loss: 36, Average loss: 1.500846071375741\n",
            "Len of Validation loss: 128, Average loss: 1.5972125311382115\n",
            "Epoch: 1280, Len of Training loss: 36, Average loss: 1.4982986847559612\n",
            "Len of Validation loss: 128, Average loss: 1.5999918384477496\n",
            "Epoch: 1281, Len of Training loss: 36, Average loss: 1.5041146477063496\n",
            "Len of Validation loss: 128, Average loss: 1.5794767660554498\n",
            "Epoch: 1282, Len of Training loss: 36, Average loss: 1.5852296385500166\n",
            "Len of Validation loss: 128, Average loss: 1.5716898303944618\n",
            "Epoch: 1283, Len of Training loss: 36, Average loss: 1.5139070964521832\n",
            "Len of Validation loss: 128, Average loss: 1.5982417652849108\n",
            "Epoch: 1284, Len of Training loss: 36, Average loss: 1.4823390146096547\n",
            "Len of Validation loss: 128, Average loss: 1.5504739753669128\n",
            "Epoch: 1285, Len of Training loss: 36, Average loss: 1.4922603733009763\n",
            "Len of Validation loss: 128, Average loss: 1.5941882610786706\n",
            "Epoch: 1286, Len of Training loss: 36, Average loss: 1.4958512286345165\n",
            "Len of Validation loss: 128, Average loss: 1.5907754553481936\n",
            "Epoch: 1287, Len of Training loss: 36, Average loss: 1.49314484000206\n",
            "Len of Validation loss: 128, Average loss: 1.5527677917852998\n",
            "Epoch: 1288, Len of Training loss: 36, Average loss: 1.51342464155621\n",
            "Len of Validation loss: 128, Average loss: 1.5994189628399909\n",
            "Epoch: 1289, Len of Training loss: 36, Average loss: 1.5034098012579813\n",
            "Len of Validation loss: 128, Average loss: 1.6018500598147511\n",
            "Epoch: 1290, Len of Training loss: 36, Average loss: 1.5285822451114655\n",
            "Len of Validation loss: 128, Average loss: 1.6414636892732233\n",
            "Epoch: 1291, Len of Training loss: 36, Average loss: 1.5369141631656222\n",
            "Len of Validation loss: 128, Average loss: 1.6407591486349702\n",
            "Epoch: 1292, Len of Training loss: 36, Average loss: 1.517736428313785\n",
            "Len of Validation loss: 128, Average loss: 1.5768238292075694\n",
            "Epoch: 1293, Len of Training loss: 36, Average loss: 1.5121555460823908\n",
            "Len of Validation loss: 128, Average loss: 1.6149699988309294\n",
            "Epoch: 1294, Len of Training loss: 36, Average loss: 1.491366250647439\n",
            "Len of Validation loss: 128, Average loss: 1.6354255615733564\n",
            "Epoch: 1295, Len of Training loss: 36, Average loss: 1.5052038331826527\n",
            "Len of Validation loss: 128, Average loss: 1.6687446935102344\n",
            "Epoch: 1296, Len of Training loss: 36, Average loss: 1.5017389721340604\n",
            "Len of Validation loss: 128, Average loss: 1.5495797244366258\n",
            "Epoch: 1297, Len of Training loss: 36, Average loss: 1.5117738594611485\n",
            "Len of Validation loss: 128, Average loss: 1.6697040968574584\n",
            "Epoch: 1298, Len of Training loss: 36, Average loss: 1.5571401516596477\n",
            "Len of Validation loss: 128, Average loss: 1.607838977361098\n",
            "Epoch: 1299, Len of Training loss: 36, Average loss: 1.49925477637185\n",
            "Len of Validation loss: 128, Average loss: 1.5926358533324674\n",
            "Epoch: 1300, Len of Training loss: 36, Average loss: 1.4823551873366039\n",
            "Len of Validation loss: 128, Average loss: 1.5836201435886323\n",
            "Epoch: 1301, Len of Training loss: 36, Average loss: 1.485093656513426\n",
            "Len of Validation loss: 128, Average loss: 1.580435262992978\n",
            "Epoch: 1302, Len of Training loss: 36, Average loss: 1.5078973041640387\n",
            "Len of Validation loss: 128, Average loss: 1.5965836194809526\n",
            "Epoch: 1303, Len of Training loss: 36, Average loss: 1.5317723618613348\n",
            "Len of Validation loss: 128, Average loss: 1.5683794780634344\n",
            "Epoch: 1304, Len of Training loss: 36, Average loss: 1.5347781115108066\n",
            "Len of Validation loss: 128, Average loss: 1.5940826481673867\n",
            "Epoch: 1305, Len of Training loss: 36, Average loss: 1.5206611620055304\n",
            "Len of Validation loss: 128, Average loss: 1.59215730545111\n",
            "Epoch: 1306, Len of Training loss: 36, Average loss: 1.510222527715895\n",
            "Len of Validation loss: 128, Average loss: 1.5837510565761477\n",
            "Epoch: 1307, Len of Training loss: 36, Average loss: 1.4904306266042922\n",
            "Len of Validation loss: 128, Average loss: 1.5787107225041837\n",
            "Epoch: 1308, Len of Training loss: 36, Average loss: 1.4870557950602636\n",
            "Len of Validation loss: 128, Average loss: 1.5648921227548271\n",
            "Epoch: 1309, Len of Training loss: 36, Average loss: 1.4866406685776181\n",
            "Len of Validation loss: 128, Average loss: 1.57149972114712\n",
            "Epoch: 1310, Len of Training loss: 36, Average loss: 1.529458953274621\n",
            "Len of Validation loss: 128, Average loss: 1.6338958363048732\n",
            "Epoch: 1311, Len of Training loss: 36, Average loss: 1.5296640164322324\n",
            "Len of Validation loss: 128, Average loss: 1.6276731870602816\n",
            "Epoch: 1312, Len of Training loss: 36, Average loss: 1.5010813573996227\n",
            "Len of Validation loss: 128, Average loss: 1.5799121335148811\n",
            "Epoch: 1313, Len of Training loss: 36, Average loss: 1.5366519192854564\n",
            "Len of Validation loss: 128, Average loss: 1.5687041790224612\n",
            "Epoch: 1314, Len of Training loss: 36, Average loss: 1.5023719171682994\n",
            "Len of Validation loss: 128, Average loss: 1.6138058206997812\n",
            "Epoch: 1315, Len of Training loss: 36, Average loss: 1.4936905602614086\n",
            "Len of Validation loss: 128, Average loss: 1.587108719162643\n",
            "Epoch: 1316, Len of Training loss: 36, Average loss: 1.517683506011963\n",
            "Len of Validation loss: 128, Average loss: 1.593969193752855\n",
            "Epoch: 1317, Len of Training loss: 36, Average loss: 1.492415491077635\n",
            "Len of Validation loss: 128, Average loss: 1.6086240666918457\n",
            "Epoch: 1318, Len of Training loss: 36, Average loss: 1.50400291217698\n",
            "Len of Validation loss: 128, Average loss: 1.5742809877265245\n",
            "Epoch: 1319, Len of Training loss: 36, Average loss: 1.4936147530873616\n",
            "Len of Validation loss: 128, Average loss: 1.586620868416503\n",
            "Epoch: 1320, Len of Training loss: 36, Average loss: 1.5084464881155226\n",
            "Len of Validation loss: 128, Average loss: 1.61186957010068\n",
            "Epoch: 1321, Len of Training loss: 36, Average loss: 1.5296372804376814\n",
            "Len of Validation loss: 128, Average loss: 1.6052326168864965\n",
            "Epoch: 1322, Len of Training loss: 36, Average loss: 1.5047383573320177\n",
            "Len of Validation loss: 128, Average loss: 1.6020799761172384\n",
            "Epoch: 1323, Len of Training loss: 36, Average loss: 1.4869806700282626\n",
            "Len of Validation loss: 128, Average loss: 1.6094884797930717\n",
            "Epoch: 1324, Len of Training loss: 36, Average loss: 1.5169830322265625\n",
            "Len of Validation loss: 128, Average loss: 1.6168047124519944\n",
            "Epoch: 1325, Len of Training loss: 36, Average loss: 1.5690326823128595\n",
            "Len of Validation loss: 128, Average loss: 1.6707038718741387\n",
            "Epoch: 1326, Len of Training loss: 36, Average loss: 1.5265847709443834\n",
            "Len of Validation loss: 128, Average loss: 1.616359057603404\n",
            "Epoch: 1327, Len of Training loss: 36, Average loss: 1.505785905652576\n",
            "Len of Validation loss: 128, Average loss: 1.5792472900357097\n",
            "Epoch: 1328, Len of Training loss: 36, Average loss: 1.5293036268817053\n",
            "Len of Validation loss: 128, Average loss: 1.5711419249419123\n",
            "Epoch: 1329, Len of Training loss: 36, Average loss: 1.4861614207426708\n",
            "Len of Validation loss: 128, Average loss: 1.5850299261510372\n",
            "Epoch: 1330, Len of Training loss: 36, Average loss: 1.4767658081319597\n",
            "Len of Validation loss: 128, Average loss: 1.55960659426637\n",
            "Epoch: 1331, Len of Training loss: 36, Average loss: 1.4935937755637698\n",
            "Len of Validation loss: 128, Average loss: 1.550581413321197\n",
            "Epoch: 1332, Len of Training loss: 36, Average loss: 1.494899524582757\n",
            "Len of Validation loss: 128, Average loss: 1.5722262160852551\n",
            "Epoch: 1333, Len of Training loss: 36, Average loss: 1.4913687474197812\n",
            "Len of Validation loss: 128, Average loss: 1.6395581054966897\n",
            "Epoch: 1334, Len of Training loss: 36, Average loss: 1.5175821218225691\n",
            "Len of Validation loss: 128, Average loss: 1.6074475809000432\n",
            "Epoch: 1335, Len of Training loss: 36, Average loss: 1.5100264582369063\n",
            "Len of Validation loss: 128, Average loss: 1.5737630715593696\n",
            "Epoch: 1336, Len of Training loss: 36, Average loss: 1.5309627519713507\n",
            "Len of Validation loss: 128, Average loss: 1.6900035582948476\n",
            "Epoch: 1337, Len of Training loss: 36, Average loss: 1.5898428161938984\n",
            "Len of Validation loss: 128, Average loss: 1.5946173595730215\n",
            "Epoch: 1338, Len of Training loss: 36, Average loss: 1.5208555195066664\n",
            "Len of Validation loss: 128, Average loss: 1.6226458442397416\n",
            "Epoch: 1339, Len of Training loss: 36, Average loss: 1.509227110279931\n",
            "Len of Validation loss: 128, Average loss: 1.6672357157804072\n",
            "Epoch: 1340, Len of Training loss: 36, Average loss: 1.5179871916770935\n",
            "Len of Validation loss: 128, Average loss: 1.5784526879433542\n",
            "Epoch: 1341, Len of Training loss: 36, Average loss: 1.5014421807395086\n",
            "Len of Validation loss: 128, Average loss: 1.5965584833174944\n",
            "Epoch: 1342, Len of Training loss: 36, Average loss: 1.4970134298006694\n",
            "Len of Validation loss: 128, Average loss: 1.576094954740256\n",
            "Epoch: 1343, Len of Training loss: 36, Average loss: 1.523071934779485\n",
            "Len of Validation loss: 128, Average loss: 1.5765161458402872\n",
            "Epoch: 1344, Len of Training loss: 36, Average loss: 1.4929245710372925\n",
            "Len of Validation loss: 128, Average loss: 1.6098327455110848\n",
            "Epoch: 1345, Len of Training loss: 36, Average loss: 1.519545058409373\n",
            "Len of Validation loss: 128, Average loss: 1.5952910187188536\n",
            "Epoch: 1346, Len of Training loss: 36, Average loss: 1.5198150939411588\n",
            "Len of Validation loss: 128, Average loss: 1.6061276425607502\n",
            "Epoch: 1347, Len of Training loss: 36, Average loss: 1.49333146876759\n",
            "Len of Validation loss: 128, Average loss: 1.625461557880044\n",
            "Epoch: 1348, Len of Training loss: 36, Average loss: 1.5204603175322216\n",
            "Len of Validation loss: 128, Average loss: 1.5663707251660526\n",
            "Epoch: 1349, Len of Training loss: 36, Average loss: 1.4822635021474626\n",
            "Len of Validation loss: 128, Average loss: 1.6048097903840244\n",
            "Epoch: 1350, Len of Training loss: 36, Average loss: 1.4929506613148584\n",
            "Len of Validation loss: 128, Average loss: 1.6383763168705627\n",
            "Epoch: 1351, Len of Training loss: 36, Average loss: 1.507582916153802\n",
            "Len of Validation loss: 128, Average loss: 1.5815929870586842\n",
            "Epoch: 1352, Len of Training loss: 36, Average loss: 1.5076790750026703\n",
            "Len of Validation loss: 128, Average loss: 1.5719030413310975\n",
            "Epoch: 1353, Len of Training loss: 36, Average loss: 1.4849310020605724\n",
            "Len of Validation loss: 128, Average loss: 1.5856117287185043\n",
            "Epoch: 1354, Len of Training loss: 36, Average loss: 1.4907075928317175\n",
            "Len of Validation loss: 128, Average loss: 1.5798846380785108\n",
            "Epoch: 1355, Len of Training loss: 36, Average loss: 1.4833078020148807\n",
            "Len of Validation loss: 128, Average loss: 1.5711934510618448\n",
            "Epoch: 1356, Len of Training loss: 36, Average loss: 1.4753095871872373\n",
            "Len of Validation loss: 128, Average loss: 1.647264985833317\n",
            "Epoch: 1357, Len of Training loss: 36, Average loss: 1.5077444281842973\n",
            "Len of Validation loss: 128, Average loss: 1.6282618681434542\n",
            "Epoch: 1358, Len of Training loss: 36, Average loss: 1.5061094628440008\n",
            "Len of Validation loss: 128, Average loss: 1.5890019552316517\n",
            "Epoch: 1359, Len of Training loss: 36, Average loss: 1.511736856566535\n",
            "Len of Validation loss: 128, Average loss: 1.6162311097141355\n",
            "Epoch: 1360, Len of Training loss: 36, Average loss: 1.4808038539356656\n",
            "Len of Validation loss: 128, Average loss: 1.5997329775709659\n",
            "Epoch: 1361, Len of Training loss: 36, Average loss: 1.5249118937386408\n",
            "Len of Validation loss: 128, Average loss: 1.5696725219022483\n",
            "Epoch: 1362, Len of Training loss: 36, Average loss: 1.5197840399212308\n",
            "Len of Validation loss: 128, Average loss: 1.583076183218509\n",
            "Epoch: 1363, Len of Training loss: 36, Average loss: 1.482271628247367\n",
            "Len of Validation loss: 128, Average loss: 1.5644635669887066\n",
            "Epoch: 1364, Len of Training loss: 36, Average loss: 1.48215032948388\n",
            "Len of Validation loss: 128, Average loss: 1.6184416234027594\n",
            "Epoch: 1365, Len of Training loss: 36, Average loss: 1.5113756855328877\n",
            "Len of Validation loss: 128, Average loss: 1.591998401330784\n",
            "Epoch: 1366, Len of Training loss: 36, Average loss: 1.4885291225380368\n",
            "Len of Validation loss: 128, Average loss: 1.5873474760446697\n",
            "Epoch: 1367, Len of Training loss: 36, Average loss: 1.4773683945337932\n",
            "Len of Validation loss: 128, Average loss: 1.5739100107457489\n",
            "Epoch: 1368, Len of Training loss: 36, Average loss: 1.485215499997139\n",
            "Len of Validation loss: 128, Average loss: 1.5701998572330922\n",
            "Epoch: 1369, Len of Training loss: 36, Average loss: 1.4957590864764319\n",
            "Len of Validation loss: 128, Average loss: 1.6832872754894197\n",
            "Epoch: 1370, Len of Training loss: 36, Average loss: 1.6046578387419383\n",
            "Len of Validation loss: 128, Average loss: 1.5688804758246988\n",
            "Epoch: 1371, Len of Training loss: 36, Average loss: 1.4856611788272858\n",
            "Len of Validation loss: 128, Average loss: 1.5580931513104588\n",
            "Epoch: 1372, Len of Training loss: 36, Average loss: 1.4833649297555287\n",
            "Len of Validation loss: 128, Average loss: 1.6283767137210816\n",
            "Epoch: 1373, Len of Training loss: 36, Average loss: 1.5744373169210222\n",
            "Len of Validation loss: 128, Average loss: 1.6701506804674864\n",
            "Epoch: 1374, Len of Training loss: 36, Average loss: 1.5385336544778612\n",
            "Len of Validation loss: 128, Average loss: 1.6722139744088054\n",
            "Epoch: 1375, Len of Training loss: 36, Average loss: 1.505903081761466\n",
            "Len of Validation loss: 128, Average loss: 1.5603953413665295\n",
            "Epoch: 1376, Len of Training loss: 36, Average loss: 1.4905045628547668\n",
            "Len of Validation loss: 128, Average loss: 1.5714269638992846\n",
            "Epoch: 1377, Len of Training loss: 36, Average loss: 1.5193742050064936\n",
            "Len of Validation loss: 128, Average loss: 1.624110272154212\n",
            "Epoch: 1378, Len of Training loss: 36, Average loss: 1.508271402782864\n",
            "Len of Validation loss: 128, Average loss: 1.5644082424696535\n",
            "Epoch: 1379, Len of Training loss: 36, Average loss: 1.4937658409277599\n",
            "Len of Validation loss: 128, Average loss: 1.589905644301325\n",
            "Epoch: 1380, Len of Training loss: 36, Average loss: 1.532843182484309\n",
            "Len of Validation loss: 128, Average loss: 1.5706732249818742\n",
            "Epoch: 1381, Len of Training loss: 36, Average loss: 1.4730476670795016\n",
            "Len of Validation loss: 128, Average loss: 1.5998258143663406\n",
            "Epoch: 1382, Len of Training loss: 36, Average loss: 1.5036388039588928\n",
            "Len of Validation loss: 128, Average loss: 1.6040012799203396\n",
            "Epoch: 1383, Len of Training loss: 36, Average loss: 1.492980311314265\n",
            "Len of Validation loss: 128, Average loss: 1.6803183043375611\n",
            "Epoch: 1384, Len of Training loss: 36, Average loss: 1.5091284943951502\n",
            "Len of Validation loss: 128, Average loss: 1.5869394149631262\n",
            "Epoch: 1385, Len of Training loss: 36, Average loss: 1.4786860048770905\n",
            "Len of Validation loss: 128, Average loss: 1.6193107061553746\n",
            "Epoch: 1386, Len of Training loss: 36, Average loss: 1.4685859779516857\n",
            "Len of Validation loss: 128, Average loss: 1.5372951121535152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "n7mjcXV3coC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 500\n",
        "\n",
        "plt.plot(np.linspace(1, num_epochs, num_epochs).astype(int), train_losses[:500], label=\"Training loss\")\n",
        "plt.plot(np.linspace(1, num_epochs, num_epochs).astype(int), valid_losses[:500], label=\"Testing loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fmZa5ypccoC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 2000\n",
        "\n",
        "plt.plot(np.linspace(1, num_epochs, num_epochs).astype(int), train_losses[:3000], label=\"Training loss\")\n",
        "plt.plot(np.linspace(1, num_epochs, num_epochs).astype(int), valid_losses[:3000], label=\"Testing loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MZ4PODuucoC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model,\"GNN_RSSI_DIST.pth\")"
      ],
      "metadata": {
        "id": "iYfRwBEScoC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelv2 = torch.load(\"/content/GNN_RSSI_DIST.pth\")\n",
        "modelv2.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uy1xuS5ZcoC8",
        "outputId": "68e5b5e3-81e3-4d20-dfbc-c0dc4e7a61a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MetaNet(\n",
              "  (input): MetaLayer(\n",
              "    edge_model=EdgeModel(\n",
              "    (edge_mlp): Sequential(\n",
              "      (0): Linear(in_features=28, out_features=128, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "  ),\n",
              "    node_model=NodeModel(\n",
              "    (node_mlp_1): Sequential(\n",
              "      (0): Linear(in_features=141, out_features=128, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "    (node_mlp_2): Sequential(\n",
              "      (0): Linear(in_features=141, out_features=128, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "  ),\n",
              "    global_model=None\n",
              "  )\n",
              "  (output): MetaLayer(\n",
              "    edge_model=EdgeModel(\n",
              "    (edge_mlp): Sequential(\n",
              "      (0): Linear(in_features=384, out_features=128, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "  ),\n",
              "    node_model=NodeModel(\n",
              "    (node_mlp_1): Sequential(\n",
              "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "    (node_mlp_2): Sequential(\n",
              "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
              "    )\n",
              "  ),\n",
              "    global_model=None\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate the model on the test set\n",
        "test_epoch_losses= evaluate(test_loader)\n",
        "print(f\"Len of Validation loss: {len(test_epoch_losses)}, Average loss: {float(np.sum(test_epoch_losses))/len(test_epoch_losses)}\")\n",
        "valid_losses.append(np.mean(test_epoch_losses))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3JS5k_jcoC8",
        "outputId": "93a84d45-d59d-438e-a201-45998e7395c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Len of Validation loss: 384, Average loss: 1.321975545026362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(np.array(test_epoch_losses)).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20xjqvi0coC9",
        "outputId": "0b6d04db-ea48-489f-ddfa-69e210475080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(384,)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate the mean squared error for the test set\n",
        "print(np.mean(test_epoch_losses))\n",
        "#calculate the mean absolute error for the test set\n",
        "print(np.mean(np.sqrt(test_epoch_losses)))\n",
        "#calculate the root mean squared error for the test set\n",
        "print(np.sqrt(np.mean(test_epoch_losses)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYnsCHSpcoC-",
        "outputId": "013f470c-913a-4bc8-bcdb-6b764b489f9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.321975545026362\n",
            "1.064567638246661\n",
            "1.1497719534874566\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "squared_loss = [i**2 for i in test_epoch_losses]\n",
        "print(np.mean(squared_loss))\n",
        "#calculate the mean absolute error for the test set\n",
        "print(np.mean(np.sqrt(squared_loss)))\n",
        "#calculate the root mean squared error for the test set\n",
        "print(np.sqrt(np.mean(squared_loss)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2aUkX4uMe7x",
        "outputId": "b022529c-6f3d-4339-85d5-287d9b5cdf2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.2179725564693857\n",
            "1.321975545026362\n",
            "1.793870830486238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate model and predict on the test set\n",
        "np.set_printoptions(suppress=True)\n",
        "#dont print tensor in scientific notation\n",
        "torch.set_printoptions(sci_mode=False)\n",
        "modelv2.eval()\n",
        "for data in test_loader:\n",
        "    # print(data.shape)\n",
        "    out = modelv2(data.to(device))\n",
        "    # #print the predicted values and the actual values side by side for comparison\n",
        "\n",
        "    print(out)\n",
        "    print(data.y)\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3CHUFo5coC_",
        "outputId": "5b0a09ba-6282-47bb-f7de-82f4259e1f39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    28.3341],\n",
            "        [     1.2571],\n",
            "        [     2.7333],\n",
            "        [     3.2697],\n",
            "        [     2.7787],\n",
            "        [     0.3983],\n",
            "        [     0.8931],\n",
            "        [     2.3163],\n",
            "        [     2.2052],\n",
            "        [     3.3243],\n",
            "        [     3.4888],\n",
            "        [     2.2671],\n",
            "        [     5.7321],\n",
            "        [    -0.0133],\n",
            "        [     0.0150],\n",
            "        [     0.0125],\n",
            "        [     0.6104],\n",
            "        [     0.0096],\n",
            "        [     1.3151],\n",
            "        [    -0.0080],\n",
            "        [    -0.0029],\n",
            "        [     0.3748],\n",
            "        [     0.0084],\n",
            "        [     0.1397],\n",
            "        [     0.0008],\n",
            "        [    -0.0012],\n",
            "        [    -0.0066],\n",
            "        [    -0.0054],\n",
            "        [     0.6199],\n",
            "        [    -0.0129],\n",
            "        [     0.7121],\n",
            "        [     0.5525],\n",
            "        [    -0.0042],\n",
            "        [     0.3807],\n",
            "        [    -0.0589],\n",
            "        [     0.0092],\n",
            "        [    -0.0605],\n",
            "        [    -0.0466],\n",
            "        [    -0.0357],\n",
            "        [    -0.0129],\n",
            "        [     0.0058],\n",
            "        [    -0.0277],\n",
            "        [    -0.0238],\n",
            "        [    -0.0026],\n",
            "        [    -0.0313],\n",
            "        [    -0.0261],\n",
            "        [    10.6720],\n",
            "        [    -0.0319],\n",
            "        [     1.3684],\n",
            "        [     0.0104],\n",
            "        [     0.9869],\n",
            "        [    -0.0070],\n",
            "        [     0.8292],\n",
            "        [     0.0008],\n",
            "        [    -0.0159],\n",
            "        [     0.4067],\n",
            "        [     0.0105],\n",
            "        [     1.9854],\n",
            "        [     0.4216],\n",
            "        [     0.9268],\n",
            "        [     0.5594],\n",
            "        [    -0.0104],\n",
            "        [     0.0367],\n",
            "        [     0.0091],\n",
            "        [     1.0457],\n",
            "        [    12.5778],\n",
            "        [     0.9541],\n",
            "        [     0.2068],\n",
            "        [     0.9357],\n",
            "        [     0.2855],\n",
            "        [     0.0016],\n",
            "        [     1.1646],\n",
            "        [     1.8558],\n",
            "        [     1.0807],\n",
            "        [     2.1878],\n",
            "        [     0.8289],\n",
            "        [     1.4800],\n",
            "        [    -0.0034],\n",
            "        [     2.2152],\n",
            "        [     1.9614],\n",
            "        [    -0.0310],\n",
            "        [     1.3261],\n",
            "        [     2.1264],\n",
            "        [    -0.0186],\n",
            "        [    -0.0272],\n",
            "        [    -0.0209],\n",
            "        [    -0.0232],\n",
            "        [    -0.0230],\n",
            "        [    -0.0305],\n",
            "        [     0.7258],\n",
            "        [    -0.0406],\n",
            "        [    -0.0324],\n",
            "        [    -0.0363],\n",
            "        [     1.0503],\n",
            "        [     3.7812],\n",
            "        [    -0.0182],\n",
            "        [     0.1729],\n",
            "        [     0.0036],\n",
            "        [     0.3925],\n",
            "        [     0.3575],\n",
            "        [    -0.0134],\n",
            "        [     0.9578],\n",
            "        [     0.5937],\n",
            "        [    -0.0143],\n",
            "        [     0.2267],\n",
            "        [    -0.0163],\n",
            "        [     0.9871],\n",
            "        [    -0.0228],\n",
            "        [     0.6099],\n",
            "        [    -0.0185],\n",
            "        [    20.2498],\n",
            "        [     2.5897],\n",
            "        [     1.6759],\n",
            "        [     2.1521],\n",
            "        [    -0.0395],\n",
            "        [     0.3389],\n",
            "        [     2.7909],\n",
            "        [     1.7627],\n",
            "        [     1.2774],\n",
            "        [     1.2165],\n",
            "        [     1.4943],\n",
            "        [    18.3919],\n",
            "        [     2.2726],\n",
            "        [     2.1391],\n",
            "        [     1.3719],\n",
            "        [     0.3630],\n",
            "        [     0.6867],\n",
            "        [     2.5247],\n",
            "        [     1.0199],\n",
            "        [     1.9643],\n",
            "        [     1.0011],\n",
            "        [     0.5097],\n",
            "        [     0.4812],\n",
            "        [    -0.1794],\n",
            "        [     0.0094],\n",
            "        [    -0.0135],\n",
            "        [    -0.0190],\n",
            "        [    -0.0185],\n",
            "        [    -0.0187],\n",
            "        [    -0.0177],\n",
            "        [     0.0027],\n",
            "        [     0.0034],\n",
            "        [    -0.0276],\n",
            "        [    -0.0171]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor([31.8500,  0.7500,  3.9200,  2.1200,  7.6000,  0.0000,  0.7500,  5.0700,\n",
            "         1.8700,  4.2400,  2.5100,  3.0400,  3.7900,  0.0000,  0.0000,  0.0000,\n",
            "         0.7500,  0.0000,  1.1200,  0.0000,  0.0000,  0.6700,  0.0000,  0.0000,\n",
            "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.7500,  0.5100,\n",
            "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.8300,  0.0000,\n",
            "         0.5600,  0.0000,  0.5100,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000,  2.8300,  0.0000,  0.5600,  0.3700,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000, 12.6000,  0.7500,  0.0000,  0.0000,  0.0000,  0.0000,  1.4900,\n",
            "         1.1200,  0.3700,  3.5300,  1.1200,  0.5600,  0.0000,  1.4100,  1.1200,\n",
            "         0.0000,  1.1200,  1.9600,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000,  1.2500,  0.0000,  0.0000,  0.0000,  0.7100,  2.2400,  0.0000,\n",
            "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.5600,  0.0000,  0.0000,\n",
            "         0.0000,  0.0000,  1.6800,  0.0000,  0.0000,  0.0000, 14.9300,  2.1200,\n",
            "         2.8000,  0.6300,  0.0000,  0.0000,  4.9500,  0.6300,  1.0100,  1.6800,\n",
            "         1.1200, 17.0700,  2.8300,  3.5300,  2.0300,  0.0000,  0.0000,  2.1200,\n",
            "         1.8700,  2.5100,  1.5200,  0.6700,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate model and predict on the test set\n",
        "np.set_printoptions(suppress=True)\n",
        "#dont print tensor in scientific notation\n",
        "torch.set_printoptions(sci_mode=False)\n",
        "modelv2.eval()\n",
        "for data in test_loader:\n",
        "    out = modelv2(data.to(device))\n",
        "    #print the predicted values and the actual values side by side for comparison\n",
        "    print(torch.cat((out.view(-1,1),data.y.view(-1,1)),1))\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku68BCZicoDA",
        "outputId": "7aa88723-7a0d-4382-9bd5-1bd277985a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "        [    -0.0370,      0.0000],\n",
            "        [    -0.0359,      0.0000],\n",
            "        [    -0.0489,      0.0000],\n",
            "        [    -0.0265,      0.0000],\n",
            "        [    -0.0186,      0.0000],\n",
            "        [    -0.0329,      0.0000],\n",
            "        [    -0.0271,      0.0000],\n",
            "        [     0.2930,      0.0000],\n",
            "        [    -0.0462,      0.0000],\n",
            "        [    -0.0430,      0.0000],\n",
            "        [    -0.0172,      0.0000],\n",
            "        [    -0.0421,      0.0000],\n",
            "        [    -0.0693,      0.0000],\n",
            "        [     0.1593,      0.3700],\n",
            "        [    -0.0500,      0.0000],\n",
            "        [    -0.0277,      0.0000],\n",
            "        [    -0.0351,      0.0000],\n",
            "        [    -0.0169,      0.0000],\n",
            "        [    -0.0460,      0.0000],\n",
            "        [    -0.0202,      0.0000],\n",
            "        [    -0.0038,      0.0000],\n",
            "        [    -0.0138,      0.0000],\n",
            "        [     0.0096,      0.0000],\n",
            "        [    -0.0451,      0.0000],\n",
            "        [    -0.0098,      0.0000],\n",
            "        [    -0.0498,      0.0000]], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([[     6.6019,      1.0800],\n",
            "        [     0.0015,      0.0000],\n",
            "        [    -0.0112,      0.0000],\n",
            "        [     0.0101,      0.0000],\n",
            "        [    -0.0202,      0.0000],\n",
            "        [     1.8219,      0.7100],\n",
            "        [    -0.0297,      0.0000],\n",
            "        [    -0.0029,      0.0000],\n",
            "        [     0.0073,      0.0000],\n",
            "        [     0.2852,      0.3700],\n",
            "        [    -0.0379,      0.0000],\n",
            "        [     0.0160,      0.0000],\n",
            "        [    -0.0258,      0.0000],\n",
            "        [     0.0175,      0.0000],\n",
            "        [     0.0044,      0.0000],\n",
            "        [     0.4412,      0.0000],\n",
            "        [    -0.0158,      0.0000],\n",
            "        [    -0.0047,      0.0000],\n",
            "        [    -0.0102,      0.0000],\n",
            "        [    -0.0118,      0.0000],\n",
            "        [    -0.0105,      0.0000],\n",
            "        [     0.0025,      0.0000],\n",
            "        [     0.0079,      0.0000],\n",
            "        [    -0.0159,      0.0000],\n",
            "        [    -0.0235,      0.0000],\n",
            "        [     0.0061,      0.0000],\n",
            "        [    -0.0223,      0.0000],\n",
            "        [     0.0112,      0.0000],\n",
            "        [     0.0006,      0.0000],\n",
            "        [    -0.0042,      0.0000],\n",
            "        [     1.0340,      0.0000],\n",
            "        [    -0.0179,      0.0000],\n",
            "        [    -0.0128,      0.0000],\n",
            "        [    -0.0262,      0.0000],\n",
            "        [    -0.0141,      0.0000],\n",
            "        [    -0.0273,      0.0000],\n",
            "        [    -0.0284,      0.0000],\n",
            "        [    -0.0172,      0.0000],\n",
            "        [    -0.0224,      0.0000],\n",
            "        [    -0.0213,      0.0000],\n",
            "        [    -0.0173,      0.0000],\n",
            "        [    -0.0337,      0.0000],\n",
            "        [    -0.0178,      0.0000],\n",
            "        [    -0.0168,      0.0000],\n",
            "        [    -0.0212,      0.0000],\n",
            "        [     6.7756,      6.3100],\n",
            "        [    -0.0215,      0.0000],\n",
            "        [    -0.0130,      0.0000],\n",
            "        [     1.8230,      0.7100],\n",
            "        [     1.2518,      1.2500],\n",
            "        [    -0.0074,      0.0000],\n",
            "        [     0.8696,      0.5100],\n",
            "        [     0.6332,      0.3700],\n",
            "        [     1.5959,      0.7100],\n",
            "        [     0.6947,      0.3700],\n",
            "        [     1.2167,      0.5600],\n",
            "        [     1.3027,      0.7100],\n",
            "        [     0.9048,      0.0000],\n",
            "        [     0.4138,      0.3700],\n",
            "        [     0.1343,      0.0000],\n",
            "        [     0.0487,      0.0000],\n",
            "        [     0.0120,      0.0000],\n",
            "        [     0.5957,      0.3700],\n",
            "        [     0.5296,      0.3700],\n",
            "        [     0.0178,      0.0000],\n",
            "        [     0.2092,      0.0000],\n",
            "        [     2.6531,      1.0800],\n",
            "        [     0.2782,      0.0000],\n",
            "        [     0.0098,      0.0000],\n",
            "        [    -0.0259,      0.0000],\n",
            "        [     0.7667,      0.7100],\n",
            "        [     0.0156,      0.0000],\n",
            "        [    -0.0311,      0.0000],\n",
            "        [     0.0688,      0.0000],\n",
            "        [    -0.0124,      0.0000],\n",
            "        [    -0.0156,      0.0000],\n",
            "        [    -0.0173,      0.0000],\n",
            "        [     0.2316,      0.3700],\n",
            "        [     0.0016,      0.0000],\n",
            "        [     0.1187,      0.0000],\n",
            "        [     0.0116,      0.0000],\n",
            "        [    -0.0099,      0.0000],\n",
            "        [    -0.0107,      0.0000],\n",
            "        [    -0.0010,      0.0000],\n",
            "        [    -0.0118,      0.0000],\n",
            "        [     0.0022,      0.0000],\n",
            "        [     0.0038,      0.0000],\n",
            "        [     0.0027,      0.0000],\n",
            "        [     0.0089,      0.0000],\n",
            "        [     0.0074,      0.0000],\n",
            "        [    -0.0130,      0.0000],\n",
            "        [     0.0064,      0.0000],\n",
            "        [    -0.0006,      0.0000],\n",
            "        [    -0.0021,      0.0000],\n",
            "        [    -0.0047,      0.0000],\n",
            "        [    -0.0027,      0.0000],\n",
            "        [    -0.0145,      0.0000],\n",
            "        [     0.0125,      0.0000],\n",
            "        [     1.9689,      3.2300],\n",
            "        [     0.9099,      1.4100],\n",
            "        [    -0.0103,      0.0000],\n",
            "        [    -0.0328,      0.0000],\n",
            "        [     0.0021,      0.0000],\n",
            "        [     0.6925,      1.2500],\n",
            "        [    -0.0446,      0.0000],\n",
            "        [    -0.0843,      0.0000],\n",
            "        [    -0.0052,      0.0000],\n",
            "        [    -0.0379,      0.0000],\n",
            "        [    -0.0039,      0.0000],\n",
            "        [     0.6386,      0.5600],\n",
            "        [    -0.0092,      0.0000],\n",
            "        [    -0.0265,      0.0000],\n",
            "        [    -0.0081,      0.0000],\n",
            "        [    -0.0026,      0.0000],\n",
            "        [     0.2567,      0.0000],\n",
            "        [     5.4254,      6.2800],\n",
            "        [     1.2806,      2.1200],\n",
            "        [     0.3555,      0.7500],\n",
            "        [     0.0309,      0.0000],\n",
            "        [     0.6531,      0.7500],\n",
            "        [     0.9892,      1.2500],\n",
            "        [    -0.0097,      0.0000],\n",
            "        [    -0.0185,      0.0000],\n",
            "        [     0.9453,      0.7100],\n",
            "        [     1.0919,      0.7100],\n",
            "        [    -0.0121,      0.0000],\n",
            "        [     2.6889,      2.4300],\n",
            "        [    -0.0170,      0.0000],\n",
            "        [    -0.0698,      0.0000],\n",
            "        [    -0.0598,      0.0000],\n",
            "        [     0.4162,      0.0000],\n",
            "        [     0.4630,      0.7500],\n",
            "        [     0.6735,      1.6800],\n",
            "        [    -0.0651,      0.0000],\n",
            "        [    -0.0110,      0.0000],\n",
            "        [    -0.0390,      0.0000],\n",
            "        [    -0.0449,      0.0000],\n",
            "        [    -0.0586,      0.0000],\n",
            "        [    -0.0450,      0.0000],\n",
            "        [     0.1306,      0.0000],\n",
            "        [    -0.0150,      0.0000],\n",
            "        [    -0.0253,      0.0000],\n",
            "        [    -0.0500,      0.0000],\n",
            "        [    -0.0308,      0.0000],\n",
            "        [    -0.0302,      0.0000],\n",
            "        [    -0.0267,      0.0000],\n",
            "        [    -0.0583,      0.0000],\n",
            "        [    -0.0068,      0.0000],\n",
            "        [    -0.0160,      0.0000],\n",
            "        [    -0.0104,      0.0000],\n",
            "        [    -0.0333,      0.0000],\n",
            "        [    -0.0261,      0.0000],\n",
            "        [    -0.0389,      0.0000],\n",
            "        [    -0.0066,      0.0000],\n",
            "        [    -0.0319,      0.0000],\n",
            "        [    -0.0430,      0.0000],\n",
            "        [    -0.0452,      0.0000]], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([[     0.1500,      0.0000],\n",
            "        [    -0.0111,      0.0000],\n",
            "        [    -0.0526,      0.0000],\n",
            "        [    -0.0675,      0.0000],\n",
            "        [    -0.0719,      0.0000],\n",
            "        [    -0.0040,      0.0000],\n",
            "        [    -0.1034,      0.0000],\n",
            "        [    -0.1601,      0.0000],\n",
            "        [    -0.1005,      0.0000],\n",
            "        [    -0.0569,      0.0000],\n",
            "        [    -0.1227,      0.0000],\n",
            "        [    -0.0355,      0.0000],\n",
            "        [    -0.0230,      0.0000],\n",
            "        [    -0.1132,      0.0000],\n",
            "        [    -0.0381,      0.0000],\n",
            "        [    -0.1085,      0.0000],\n",
            "        [     0.0020,      0.0000],\n",
            "        [    -0.0776,      0.0000],\n",
            "        [     0.0335,      0.0000],\n",
            "        [    -0.1033,      0.0000],\n",
            "        [    -0.0495,      0.0000],\n",
            "        [     1.9646,      0.0000],\n",
            "        [    -0.0095,      0.0000],\n",
            "        [     0.0260,      0.0000],\n",
            "        [    -0.0025,      0.0000],\n",
            "        [    -0.0598,      0.0000],\n",
            "        [    -0.0478,      0.0000],\n",
            "        [    -0.0523,      0.0000],\n",
            "        [    -0.0641,      0.0000],\n",
            "        [    -0.0341,      0.0000],\n",
            "        [    -0.0456,      0.0000],\n",
            "        [     0.3034,      0.0000],\n",
            "        [    -0.0623,      0.0000],\n",
            "        [    -0.0160,      0.0000],\n",
            "        [    -0.0149,      0.0000],\n",
            "        [    -0.0095,      0.0000],\n",
            "        [    -0.0610,      0.0000],\n",
            "        [     0.0573,      0.0000],\n",
            "        [    -0.0046,      0.0000],\n",
            "        [     0.0532,      0.0000],\n",
            "        [    -0.0551,      0.0000],\n",
            "        [    -0.0268,      0.0000],\n",
            "        [    -0.0275,      0.0000],\n",
            "        [    -0.0364,      0.0000],\n",
            "        [    -0.0422,      0.0000],\n",
            "        [    -0.0372,      0.0000],\n",
            "        [    -0.0366,      0.0000],\n",
            "        [    -0.0179,      0.0000],\n",
            "        [    -0.0203,      0.0000],\n",
            "        [    -0.0359,      0.0000],\n",
            "        [    -0.0431,      0.0000],\n",
            "        [     0.0106,      0.0000],\n",
            "        [    -0.0197,      0.0000],\n",
            "        [     0.0058,      0.0000],\n",
            "        [    -0.0171,      0.0000],\n",
            "        [    -0.0239,      0.0000],\n",
            "        [    -0.0526,      0.0000],\n",
            "        [    -0.0502,      0.0000],\n",
            "        [    -0.0318,      0.0000],\n",
            "        [    16.2837,     13.8500],\n",
            "        [     1.4475,      1.0100],\n",
            "        [     1.0723,      0.0000],\n",
            "        [     1.0901,      0.7500],\n",
            "        [     0.6398,      0.3700],\n",
            "        [     0.7144,      0.0000],\n",
            "        [     1.6827,      1.6800],\n",
            "        [     1.7720,      1.2500],\n",
            "        [     2.2721,      2.1200],\n",
            "        [     0.5557,      0.0000],\n",
            "        [     2.1727,      2.8300],\n",
            "        [     0.9016,      0.0000],\n",
            "        [     0.1533,      0.0000],\n",
            "        [     1.3889,      1.0100],\n",
            "        [     2.1272,      0.7100],\n",
            "        [    -0.0069,      0.0000],\n",
            "        [     0.1448,      0.0000],\n",
            "        [     0.3479,      0.0000],\n",
            "        [    -0.0162,      0.0000],\n",
            "        [     2.0258,      2.1200],\n",
            "        [     0.0327,      0.0000],\n",
            "        [    -0.0695,      0.0000],\n",
            "        [     0.0050,      0.0000],\n",
            "        [     0.1376,      0.0000],\n",
            "        [    -0.0333,      0.0000],\n",
            "        [    -0.0045,      0.0000],\n",
            "        [     0.0029,      0.0000],\n",
            "        [    -0.0194,      0.0000],\n",
            "        [     0.0046,      0.0000],\n",
            "        [     0.0051,      0.0000],\n",
            "        [     0.0019,      0.0000],\n",
            "        [     0.0274,      0.0000],\n",
            "        [     0.0027,      0.0000],\n",
            "        [     0.0010,      0.0000],\n",
            "        [    -0.0044,      0.0000],\n",
            "        [     0.0046,      0.0000],\n",
            "        [    -0.0079,      0.0000],\n",
            "        [    -0.0351,      0.0000],\n",
            "        [     0.0080,      0.0000],\n",
            "        [    -0.0371,      0.0000],\n",
            "        [     0.0073,      0.0000],\n",
            "        [     0.0175,      0.0000],\n",
            "        [    -0.0528,      0.0000],\n",
            "        [    -0.0259,      0.0000],\n",
            "        [    -0.2067,      0.0000],\n",
            "        [    -0.1504,      0.0000],\n",
            "        [    -0.0096,      0.0000],\n",
            "        [    -0.0127,      0.0000],\n",
            "        [    -0.0210,      0.0000],\n",
            "        [    -0.0216,      0.0000],\n",
            "        [    -0.0613,      0.0000],\n",
            "        [    -0.0165,      0.0000],\n",
            "        [    -0.1615,      0.0000],\n",
            "        [    -0.1385,      0.0000],\n",
            "        [    -0.0211,      0.0000],\n",
            "        [    -0.0120,      0.0000],\n",
            "        [    -0.0074,      0.0000],\n",
            "        [    -0.0273,      0.0000],\n",
            "        [    -0.0014,      0.0000],\n",
            "        [    -0.0247,      0.0000],\n",
            "        [    -0.0163,      0.0000],\n",
            "        [    -0.0091,      0.0000],\n",
            "        [     1.1944,      1.2500],\n",
            "        [    -0.0755,      0.0000],\n",
            "        [    -0.0649,      0.0000],\n",
            "        [     0.1012,      0.0000],\n",
            "        [    -0.0440,      0.0000],\n",
            "        [    -0.0473,      0.0000],\n",
            "        [    -0.0365,      0.0000],\n",
            "        [    -0.0382,      0.0000],\n",
            "        [     0.6512,      0.6300],\n",
            "        [     0.0073,      0.0000],\n",
            "        [    -0.0175,      0.0000],\n",
            "        [    -0.0438,      0.0000],\n",
            "        [     0.5925,      0.6300],\n",
            "        [    -0.0670,      0.0000],\n",
            "        [    -0.0418,      0.0000],\n",
            "        [     8.4838,      9.3900],\n",
            "        [     0.4755,      0.5100],\n",
            "        [     0.9060,      2.8000],\n",
            "        [     0.4998,      0.3700],\n",
            "        [    -0.0664,      0.0000],\n",
            "        [    -0.0763,      0.0000],\n",
            "        [    -0.0737,      0.0000],\n",
            "        [    -0.0807,      0.0000],\n",
            "        [     1.1125,      1.8800],\n",
            "        [    -0.0645,      0.0000],\n",
            "        [    -0.0797,      0.0000],\n",
            "        [     1.0575,      1.1200],\n",
            "        [     1.6436,      0.7100],\n",
            "        [    -0.0704,      0.0000],\n",
            "        [     1.3270,      1.2500],\n",
            "        [     0.1494,      0.0000],\n",
            "        [     0.7484,      0.7500],\n",
            "        [     0.0977,      0.0000],\n",
            "        [     0.1169,      0.0000],\n",
            "        [     0.0254,      0.0000],\n",
            "        [     0.0179,      0.0000],\n",
            "        [     0.0185,      0.0000],\n",
            "        [     0.0540,      0.0000],\n",
            "        [     0.0007,      0.0000],\n",
            "        [     0.0554,      0.0000],\n",
            "        [     0.0573,      0.0000],\n",
            "        [     0.0315,      0.0000],\n",
            "        [     0.0184,      0.0000],\n",
            "        [     0.0364,      0.0000],\n",
            "        [     0.0187,      0.0000],\n",
            "        [     0.0297,      0.0000],\n",
            "        [     0.0205,      0.0000],\n",
            "        [     0.0230,      0.0000],\n",
            "        [     0.0247,      0.0000],\n",
            "        [     0.0296,      0.0000],\n",
            "        [     0.0473,      0.0000],\n",
            "        [    -0.1667,      0.0000],\n",
            "        [     0.0205,      0.0000],\n",
            "        [    -0.0084,      0.0000],\n",
            "        [     0.0173,      0.0000],\n",
            "        [     0.0117,      0.0000],\n",
            "        [     0.0067,      0.0000],\n",
            "        [     0.0112,      0.0000],\n",
            "        [     0.0135,      0.0000],\n",
            "        [    -0.0088,      0.0000],\n",
            "        [     0.0176,      0.0000],\n",
            "        [     0.0073,      0.0000],\n",
            "        [     0.0148,      0.0000],\n",
            "        [    -0.0090,      0.0000],\n",
            "        [     0.0134,      0.0000],\n",
            "        [     0.0119,      0.0000],\n",
            "        [    -0.0022,      0.0000],\n",
            "        [     0.0125,      0.0000],\n",
            "        [     4.1917,      2.7500],\n",
            "        [     0.1153,      0.0000],\n",
            "        [     0.7591,      0.6300],\n",
            "        [     0.1247,      0.0000],\n",
            "        [    -0.0326,      0.0000],\n",
            "        [     1.2339,      1.4100],\n",
            "        [    -0.0326,      0.0000],\n",
            "        [    -0.0052,      0.0000],\n",
            "        [     0.2707,      0.0000],\n",
            "        [     0.0049,      0.0000],\n",
            "        [     0.1934,      0.0000],\n",
            "        [    -0.0095,      0.0000],\n",
            "        [     1.2522,      0.7100],\n",
            "        [    -0.0026,      0.0000],\n",
            "        [    -0.2811,      0.0000],\n",
            "        [    -0.0311,      0.0000],\n",
            "        [    -0.0338,      0.0000],\n",
            "        [    -0.0158,      0.0000],\n",
            "        [    -0.0348,      0.0000],\n",
            "        [    -0.0193,      0.0000],\n",
            "        [     0.0221,      0.0000],\n",
            "        [    -0.0298,      0.0000],\n",
            "        [    -0.0312,      0.0000],\n",
            "        [    -0.0104,      0.0000],\n",
            "        [    -0.0118,      0.0000],\n",
            "        [    -0.0289,      0.0000],\n",
            "        [     0.0259,      0.0000]], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([[    13.4006,      7.7500],\n",
            "        [     0.8153,      0.0000],\n",
            "        [     0.6482,      1.1200],\n",
            "        [     0.8078,      0.7500],\n",
            "        [     0.0045,      0.0000],\n",
            "        [     0.7892,      0.0000],\n",
            "        [     1.2016,      0.0000],\n",
            "        [     0.5727,      0.0000],\n",
            "        [     0.5672,      0.0000],\n",
            "        [     2.1376,      0.7100],\n",
            "        [     0.6199,      0.0000],\n",
            "        [     0.8648,      0.3700],\n",
            "        [     0.7927,      1.1200],\n",
            "        [     0.8710,      1.8700],\n",
            "        [     0.6136,      0.0000],\n",
            "        [     1.0236,      0.0000],\n",
            "        [     1.3619,      0.5600],\n",
            "        [     0.7559,      0.0000],\n",
            "        [     1.8410,      1.2500],\n",
            "        [    17.4569,     21.7300],\n",
            "        [     1.5267,      3.3600],\n",
            "        [     1.9059,      1.2500],\n",
            "        [     1.4784,      1.2500],\n",
            "        [     1.7065,      0.6300],\n",
            "        [     1.5723,      1.8800],\n",
            "        [     1.4737,      0.5600],\n",
            "        [     0.9605,      1.0100],\n",
            "        [     1.1683,      0.5100],\n",
            "        [     0.7966,      1.8700],\n",
            "        [     1.1545,      1.1200],\n",
            "        [     1.2166,      2.0300],\n",
            "        [     2.0540,      1.4100],\n",
            "        [     1.0315,      2.0300],\n",
            "        [     2.0450,      2.8300],\n",
            "        [    12.2933,     12.7600],\n",
            "        [     0.9417,      2.0300],\n",
            "        [     0.8318,      1.1200],\n",
            "        [     1.1522,      3.0400],\n",
            "        [     0.6186,      0.3700],\n",
            "        [    -0.1928,      0.0000],\n",
            "        [     1.5905,      0.6300],\n",
            "        [     0.6760,      1.1200],\n",
            "        [     0.6121,      0.0000],\n",
            "        [    -0.0254,      0.0000],\n",
            "        [     0.8531,      0.5100],\n",
            "        [     1.7094,      2.1200],\n",
            "        [     0.3184,      1.1200],\n",
            "        [    -0.0332,      0.0000],\n",
            "        [     1.7271,      0.7100],\n",
            "        [    14.8881,     11.6100],\n",
            "        [     0.7384,      1.1200],\n",
            "        [     1.7218,      0.5600],\n",
            "        [     0.9072,      0.7500],\n",
            "        [     0.4443,      0.0000],\n",
            "        [     1.5159,      0.5600],\n",
            "        [    -0.0422,      0.0000],\n",
            "        [     0.5970,      0.7500],\n",
            "        [     0.9472,      0.7500],\n",
            "        [     1.2454,      1.4900],\n",
            "        [     0.7063,      0.0000],\n",
            "        [     2.1427,      0.7100],\n",
            "        [     1.3219,      1.5200],\n",
            "        [     0.7262,      0.3700],\n",
            "        [     1.4626,      0.5100],\n",
            "        [     2.0022,      1.4100],\n",
            "        [    -0.0701,      0.0000],\n",
            "        [     1.1874,      1.1200],\n",
            "        [    -0.0215,      0.0000],\n",
            "        [     0.2014,      0.0000],\n",
            "        [    -0.0641,      0.0000],\n",
            "        [    -0.0248,      0.0000],\n",
            "        [    -0.0260,      0.0000],\n",
            "        [    -0.0433,      0.0000],\n",
            "        [    -0.0933,      0.0000],\n",
            "        [    -0.0287,      0.0000],\n",
            "        [    -0.0908,      0.0000],\n",
            "        [    -0.0228,      0.0000],\n",
            "        [    -0.0425,      0.0000],\n",
            "        [    -0.0477,      0.0000],\n",
            "        [    -0.0248,      0.0000],\n",
            "        [    -0.0265,      0.0000],\n",
            "        [    -0.0448,      0.0000],\n",
            "        [    -0.0512,      0.0000],\n",
            "        [    -0.0445,      0.0000],\n",
            "        [    -0.0302,      0.0000],\n",
            "        [    -0.0481,      0.0000],\n",
            "        [     2.6053,      0.5100],\n",
            "        [     0.0286,      0.0000],\n",
            "        [     0.1170,      0.0000],\n",
            "        [    -0.0220,      0.0000],\n",
            "        [     0.4400,      0.5100],\n",
            "        [     0.4278,      0.0000],\n",
            "        [    -0.0243,      0.0000],\n",
            "        [     0.7513,      0.0000],\n",
            "        [    -0.0407,      0.0000],\n",
            "        [    -0.0262,      0.0000],\n",
            "        [     0.3217,      0.0000],\n",
            "        [     0.1814,      0.0000],\n",
            "        [     5.2894,      3.6500],\n",
            "        [    -0.0222,      0.0000],\n",
            "        [    -0.0282,      0.0000],\n",
            "        [    -0.0353,      0.0000],\n",
            "        [    -0.0292,      0.0000],\n",
            "        [     0.6330,      0.5600],\n",
            "        [    -0.0336,      0.0000],\n",
            "        [     1.5782,      0.7100],\n",
            "        [     0.5741,      0.5600],\n",
            "        [    -0.0268,      0.0000],\n",
            "        [    -0.0263,      0.0000],\n",
            "        [     0.7061,      0.5600],\n",
            "        [     1.5115,      0.7100],\n",
            "        [    -0.0256,      0.0000],\n",
            "        [     0.7053,      0.5600],\n",
            "        [    -0.0330,      0.0000],\n",
            "        [     0.0374,      0.0000],\n",
            "        [     0.0029,      0.0000],\n",
            "        [    -0.0276,      0.0000],\n",
            "        [    -0.0307,      0.0000],\n",
            "        [    -0.0392,      0.0000],\n",
            "        [    -0.0309,      0.0000],\n",
            "        [    -0.0136,      0.0000],\n",
            "        [    -0.0203,      0.0000],\n",
            "        [    -0.0103,      0.0000],\n",
            "        [    -0.0178,      0.0000],\n",
            "        [    -0.0123,      0.0000],\n",
            "        [     4.0239,      0.7100],\n",
            "        [    -0.0260,      0.0000],\n",
            "        [     0.0005,      0.0000],\n",
            "        [     0.0250,      0.0000],\n",
            "        [    -0.0424,      0.0000],\n",
            "        [    -0.0166,      0.0000],\n",
            "        [    -0.0020,      0.0000],\n",
            "        [    -0.0345,      0.0000],\n",
            "        [     1.1542,      0.7100],\n",
            "        [    -0.0291,      0.0000],\n",
            "        [    -0.0072,      0.0000],\n",
            "        [    -0.0233,      0.0000],\n",
            "        [     0.4497,      0.0000],\n",
            "        [    -0.2147,      0.0000],\n",
            "        [    -0.0268,      0.0000],\n",
            "        [    -0.0192,      0.0000],\n",
            "        [    -0.0206,      0.0000],\n",
            "        [    -0.0071,      0.0000],\n",
            "        [    -0.0100,      0.0000],\n",
            "        [    -0.0192,      0.0000],\n",
            "        [    -0.0067,      0.0000],\n",
            "        [    -0.0110,      0.0000],\n",
            "        [    -0.0310,      0.0000],\n",
            "        [    -0.0445,      0.0000],\n",
            "        [    -0.0356,      0.0000],\n",
            "        [    -0.0191,      0.0000],\n",
            "        [    -0.0231,      0.0000],\n",
            "        [    -0.0147,      0.0000],\n",
            "        [    -0.0371,      0.0000],\n",
            "        [    -0.0512,      0.0000],\n",
            "        [    -0.0106,      0.0000],\n",
            "        [    -0.0336,      0.0000],\n",
            "        [    11.0062,     12.7500],\n",
            "        [     0.4953,      1.5200],\n",
            "        [     0.5192,      0.7500],\n",
            "        [     0.5747,      0.5100],\n",
            "        [     1.6006,      2.1200],\n",
            "        [    -0.0363,      0.0000],\n",
            "        [     0.7583,      0.7500],\n",
            "        [     1.7009,      1.4100],\n",
            "        [    -0.0250,      0.0000],\n",
            "        [     0.4744,      0.0000],\n",
            "        [     1.8009,      2.1200],\n",
            "        [     0.6601,      0.7500],\n",
            "        [     1.6843,      2.8300],\n",
            "        [    18.7383,     15.8400],\n",
            "        [     0.9563,      1.4900],\n",
            "        [     2.5465,      1.8800],\n",
            "        [     1.3859,      1.1200],\n",
            "        [     2.6255,      2.1200],\n",
            "        [     0.9519,      0.0000],\n",
            "        [     1.1320,      0.0000],\n",
            "        [     1.5295,      3.0400],\n",
            "        [     1.0839,      1.0100],\n",
            "        [     1.2067,      0.7500],\n",
            "        [     1.4173,      0.5100],\n",
            "        [     2.2295,      1.6800],\n",
            "        [     1.8263,      1.1200],\n",
            "        [    -0.0046,      0.0000],\n",
            "        [     0.8181,      0.0000],\n",
            "        [     1.1815,      1.1200]], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([[     0.3198,      0.0000],\n",
            "        [    -0.0439,      0.0000],\n",
            "        [    -0.0341,      0.0000],\n",
            "        [    -0.0257,      0.0000],\n",
            "        [    -0.0234,      0.0000],\n",
            "        [    -0.0237,      0.0000],\n",
            "        [    -0.0239,      0.0000],\n",
            "        [    -0.0269,      0.0000],\n",
            "        [    -0.0178,      0.0000],\n",
            "        [    -0.0192,      0.0000],\n",
            "        [    -0.0247,      0.0000],\n",
            "        [    -0.0587,      0.0000],\n",
            "        [    -0.0335,      0.0000],\n",
            "        [    -0.0598,      0.0000],\n",
            "        [    -0.0351,      0.0000],\n",
            "        [    -0.0363,      0.0000],\n",
            "        [    -0.0166,      0.0000],\n",
            "        [    -0.0290,      0.0000],\n",
            "        [     4.0767,      0.6300],\n",
            "        [    -0.1037,      0.0000],\n",
            "        [    -0.0828,      0.0000],\n",
            "        [    -0.0712,      0.0000],\n",
            "        [    -0.0723,      0.0000],\n",
            "        [    -0.0716,      0.0000],\n",
            "        [     0.7993,      0.6300],\n",
            "        [    -0.0988,      0.0000],\n",
            "        [    -0.0952,      0.0000],\n",
            "        [     0.2644,      0.0000],\n",
            "        [    -0.0892,      0.0000],\n",
            "        [     1.2142,      0.0000],\n",
            "        [    -0.0689,      0.0000],\n",
            "        [     0.8935,      0.0000],\n",
            "        [     7.4395,      2.1200],\n",
            "        [    -0.1294,      0.0000],\n",
            "        [     0.0798,      0.0000],\n",
            "        [     1.9786,      0.7100],\n",
            "        [     0.3339,      0.0000],\n",
            "        [     0.0830,      0.0000],\n",
            "        [    -0.1231,      0.0000],\n",
            "        [     1.2811,      0.0000],\n",
            "        [    -0.0862,      0.0000],\n",
            "        [     0.3616,      0.0000],\n",
            "        [     0.6270,      0.0000],\n",
            "        [     0.0449,      0.0000],\n",
            "        [    -0.0022,      0.0000],\n",
            "        [     0.9225,      0.0000],\n",
            "        [    -0.0052,      0.0000],\n",
            "        [     1.5939,      1.4100],\n",
            "        [     4.1073,      2.4900],\n",
            "        [     0.7542,      0.0000],\n",
            "        [    -0.0478,      0.0000],\n",
            "        [     1.1039,      0.0000],\n",
            "        [    -0.0387,      0.0000],\n",
            "        [     1.0125,      0.0000],\n",
            "        [     0.9292,      0.3700],\n",
            "        [     1.2500,      2.1200],\n",
            "        [     1.2947,      0.0000],\n",
            "        [     0.1917,      0.0000],\n",
            "        [    -0.0382,      0.0000],\n",
            "        [     3.9629,      3.7900],\n",
            "        [    -0.0066,      0.0000],\n",
            "        [    -0.0051,      0.0000],\n",
            "        [     0.6203,      0.7500],\n",
            "        [    -0.0119,      0.0000],\n",
            "        [    -0.0022,      0.0000],\n",
            "        [     0.4631,      0.0000],\n",
            "        [     0.9965,      0.5600],\n",
            "        [     0.7247,      0.5100],\n",
            "        [     1.1053,      0.5600],\n",
            "        [     1.2328,      1.4100],\n",
            "        [     0.9115,      0.0000],\n",
            "        [    -0.0152,      0.0000],\n",
            "        [     4.6349,      2.8700],\n",
            "        [     0.4140,      0.0000],\n",
            "        [    -0.0858,      0.0000],\n",
            "        [     0.4215,      0.0000],\n",
            "        [    -0.0458,      0.0000],\n",
            "        [    -0.0347,      0.0000],\n",
            "        [     0.3727,      0.0000],\n",
            "        [     0.5931,      0.3700],\n",
            "        [    -0.0382,      0.0000],\n",
            "        [    -0.0451,      0.0000],\n",
            "        [     1.5967,      2.1200],\n",
            "        [     0.1546,      0.0000],\n",
            "        [    -0.0769,      0.0000],\n",
            "        [    -0.0662,      0.0000],\n",
            "        [     0.4493,      0.3700],\n",
            "        [     3.0545,      2.1200],\n",
            "        [     0.0539,      0.0000],\n",
            "        [    -0.0232,      0.0000],\n",
            "        [     0.4073,      0.0000],\n",
            "        [    -0.0310,      0.0000],\n",
            "        [    -0.0419,      0.0000],\n",
            "        [    -0.0209,      0.0000],\n",
            "        [     1.1236,      0.7100],\n",
            "        [    -0.0215,      0.0000],\n",
            "        [    -0.0305,      0.0000],\n",
            "        [    -0.0316,      0.0000],\n",
            "        [     0.0264,      0.0000],\n",
            "        [    -0.0214,      0.0000],\n",
            "        [     1.7335,      0.7100],\n",
            "        [    -0.0307,      0.0000],\n",
            "        [    -0.0351,      0.0000],\n",
            "        [    -0.0414,      0.0000],\n",
            "        [    -0.0325,      0.0000],\n",
            "        [    -0.0255,      0.0000],\n",
            "        [     1.4291,      0.7100],\n",
            "        [    -0.0298,      0.0000],\n",
            "        [    -0.4243,      0.0000],\n",
            "        [     0.0397,      0.0000],\n",
            "        [     0.0249,      0.0000],\n",
            "        [     0.0127,      0.0000],\n",
            "        [    -0.0020,      0.0000],\n",
            "        [     0.0002,      0.0000],\n",
            "        [     0.0135,      0.0000],\n",
            "        [     0.0070,      0.0000],\n",
            "        [     0.0153,      0.0000],\n",
            "        [     0.0205,      0.0000],\n",
            "        [     0.0095,      0.0000],\n",
            "        [     0.0095,      0.0000],\n",
            "        [     0.0046,      0.0000],\n",
            "        [     0.0252,      0.0000],\n",
            "        [    -0.0011,      0.0000],\n",
            "        [    -0.0052,      0.0000],\n",
            "        [     0.0025,      0.0000],\n",
            "        [     0.0115,      0.0000],\n",
            "        [     0.0101,      0.0000],\n",
            "        [    -0.0061,      0.0000]], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([[    98.1182,    105.8100],\n",
            "        [     7.0197,     11.0900],\n",
            "        [     9.9957,      5.1200],\n",
            "        [     8.2564,      7.6800],\n",
            "        [     9.3277,      5.9700],\n",
            "        [     2.0406,      1.7100],\n",
            "        [     6.5546,      5.9700],\n",
            "        [     8.6554,      5.9700],\n",
            "        [     7.5908,      6.8300],\n",
            "        [     9.1012,     17.0700],\n",
            "        [     9.0775,     11.0900],\n",
            "        [    11.0051,     11.9500],\n",
            "        [     4.6193,      3.4100],\n",
            "        [     7.2382,      6.8300],\n",
            "        [     7.8966,      5.1200],\n",
            "        [    76.2382,     78.0300],\n",
            "        [     3.0225,      0.7600],\n",
            "        [     5.6510,      5.1200],\n",
            "        [     3.5856,      3.0400],\n",
            "        [     2.5735,      3.3300],\n",
            "        [     0.7629,      0.5100],\n",
            "        [     1.2334,      1.0100],\n",
            "        [     5.9037,      7.6800],\n",
            "        [     7.7303,      6.8300],\n",
            "        [     8.2526,      6.8300],\n",
            "        [     6.7613,      7.6800],\n",
            "        [     4.8168,      4.5600],\n",
            "        [     3.0976,      4.0000],\n",
            "        [     2.3377,      3.8000],\n",
            "        [     8.3986,      2.5600],\n",
            "        [     6.2239,      5.9700],\n",
            "        [     3.6858,      3.8000],\n",
            "        [     4.8976,      3.8000],\n",
            "        [     3.9169,      6.0800],\n",
            "        [     2.7099,      0.6700],\n",
            "        [    70.5260,     48.6400],\n",
            "        [     4.0831,      5.1200],\n",
            "        [     4.8580,     10.2400],\n",
            "        [     2.4303,      2.5600],\n",
            "        [     5.1072,      5.1200],\n",
            "        [     3.2073,      4.2700],\n",
            "        [     4.3415,      5.1200],\n",
            "        [     3.1498,      9.3900],\n",
            "        [     1.9172,      0.8500],\n",
            "        [     2.1125,      1.7100],\n",
            "        [     6.2357,      2.5600],\n",
            "        [     3.1880,      1.7100],\n",
            "        [    76.2223,     61.5100],\n",
            "        [     6.3945,     12.8000],\n",
            "        [     4.8019,      6.0800],\n",
            "        [     1.9585,      1.5200],\n",
            "        [     2.6507,      4.0000],\n",
            "        [     4.5648,      7.6000],\n",
            "        [     3.0606,      4.5600],\n",
            "        [     1.0666,      1.5200],\n",
            "        [     4.0789,      8.3600],\n",
            "        [     1.1710,      1.0100],\n",
            "        [     6.1390,      6.8300],\n",
            "        [     2.3068,      2.6700],\n",
            "        [     3.2252,      4.5600],\n",
            "        [    41.6145,     42.6700],\n",
            "        [     2.8681,      5.1200],\n",
            "        [     3.0660,      2.5600],\n",
            "        [     1.9187,      4.2700],\n",
            "        [     2.8452,      1.7100],\n",
            "        [     2.7296,      1.7100],\n",
            "        [     2.1904,      3.4100],\n",
            "        [     2.9153,      1.7100],\n",
            "        [     1.4529,      2.5600],\n",
            "        [     3.0029,      1.7100],\n",
            "        [     3.5292,      3.4100],\n",
            "        [     2.7693,      3.4100],\n",
            "        [     2.4574,      0.8500],\n",
            "        [     4.7040,      4.2700],\n",
            "        [     2.3546,      2.5600],\n",
            "        [     2.4732,      3.4100],\n",
            "        [     1.7178,      0.0000],\n",
            "        [    -0.2441,      0.0000],\n",
            "        [    -0.0901,      0.0000],\n",
            "        [    -0.2800,      0.0000],\n",
            "        [    -0.1228,      0.0000],\n",
            "        [    -0.1661,      0.0000],\n",
            "        [    -0.1530,      0.0000],\n",
            "        [    -0.0748,      0.0000],\n",
            "        [    -0.1236,      0.0000],\n",
            "        [    -0.1658,      0.0000],\n",
            "        [    -0.2530,      0.0000],\n",
            "        [    -0.1395,      0.0000],\n",
            "        [    -0.1108,      0.0000],\n",
            "        [    -0.0553,      0.0000],\n",
            "        [     0.3331,      0.0000],\n",
            "        [    -0.2079,      0.0000],\n",
            "        [    -0.1282,      0.0000],\n",
            "        [    -0.4405,      0.0000],\n",
            "        [     0.0465,      0.0000],\n",
            "        [    -0.0492,      0.0000],\n",
            "        [    91.9617,     95.0100],\n",
            "        [     7.4689,     16.2100],\n",
            "        [     6.0255,      9.1200],\n",
            "        [     2.8384,      8.1100],\n",
            "        [     5.4045,     13.6800],\n",
            "        [     5.3905,     10.6400],\n",
            "        [     6.3351,      2.2800],\n",
            "        [     6.7477,     14.4400],\n",
            "        [     5.5805,      6.0800],\n",
            "        [     7.3071,      5.1200],\n",
            "        [     4.5054,      9.3300],\n",
            "        [    70.8159,     44.5700],\n",
            "        [     3.1820,      3.3300],\n",
            "        [     5.4432,      5.1200],\n",
            "        [     4.5832,      5.3300],\n",
            "        [     2.1392,      4.5600],\n",
            "        [     4.5291,      4.0500],\n",
            "        [     4.3347,     10.6400],\n",
            "        [     4.1151,      3.8000],\n",
            "        [     3.8901,      0.5100],\n",
            "        [     4.1059,      4.6700],\n",
            "        [     4.5642,      2.5600],\n",
            "        [    70.5080,     59.7300],\n",
            "        [     2.9823,      1.7100],\n",
            "        [     4.9811,      5.9700],\n",
            "        [     4.4995,      7.6800],\n",
            "        [     7.5015,      3.4100],\n",
            "        [     6.2804,      3.4100],\n",
            "        [     4.4351,      0.8500],\n",
            "        [     1.3782,      2.5600],\n",
            "        [     4.9741,      4.2700],\n",
            "        [     5.3042,      3.4100],\n",
            "        [     4.4325,      3.4100],\n",
            "        [     2.8262,      2.5600],\n",
            "        [     5.3690,      1.7100],\n",
            "        [     2.4318,      4.2700],\n",
            "        [     0.4472,      0.8500],\n",
            "        [     2.4109,      0.0000],\n",
            "        [     3.5476,      4.2700],\n",
            "        [     3.4710,      2.5600],\n",
            "        [     4.3459,      1.7100],\n",
            "        [     4.9899,      5.1200],\n",
            "        [    41.3438,     50.3500],\n",
            "        [     3.8908,      4.2700],\n",
            "        [    -0.1486,      0.0000],\n",
            "        [     2.6887,      0.0000],\n",
            "        [     3.2860,      4.2700],\n",
            "        [     4.8765,      0.8500],\n",
            "        [     2.1801,      3.4100],\n",
            "        [     4.9375,      8.5300],\n",
            "        [     0.1196,      0.8500],\n",
            "        [     3.4589,      4.2700],\n",
            "        [     5.6160,      4.2700],\n",
            "        [     2.9564,      3.4100],\n",
            "        [     2.8235,      6.8300],\n",
            "        [     3.2485,      2.5600],\n",
            "        [     2.6775,      5.1200],\n",
            "        [     3.1998,      1.7100]], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([[    79.6561,     66.1600],\n",
            "        [     8.3321,      8.5300],\n",
            "        [     7.2233,      4.5600],\n",
            "        [     5.6333,      6.0800],\n",
            "        [     4.2060,      0.6700],\n",
            "        [     6.3970,     14.5100],\n",
            "        [     6.0417,      3.8000],\n",
            "        [     3.7925,      1.3300],\n",
            "        [     6.5216,      6.8400],\n",
            "        [     7.7964,      9.3900],\n",
            "        [     4.3029,      0.7600],\n",
            "        [     3.2215,      2.0000],\n",
            "        [     9.2676,      4.2700],\n",
            "        [     6.6540,      0.7600],\n",
            "        [     4.2713,      2.6700],\n",
            "        [   124.9931,    126.1900],\n",
            "        [    11.1080,      5.3300],\n",
            "        [    13.1173,     11.0900],\n",
            "        [    10.7213,     14.4400],\n",
            "        [     8.3825,      6.5900],\n",
            "        [    11.8054,     13.6500],\n",
            "        [    10.9457,     13.6800],\n",
            "        [    11.6490,     11.3300],\n",
            "        [    10.2718,     12.9200],\n",
            "        [     7.9275,      5.5700],\n",
            "        [    14.0179,     14.5100],\n",
            "        [    12.5384,     17.0700],\n",
            "        [   112.9448,     67.0400],\n",
            "        [     8.1596,      7.6000],\n",
            "        [     7.3425,      3.8000],\n",
            "        [     6.9069,      2.6700],\n",
            "        [     7.3385,      1.5200],\n",
            "        [     8.1637,      7.6000],\n",
            "        [     8.4911,      6.0800],\n",
            "        [     6.1393,      3.3300],\n",
            "        [     1.5918,      4.0500],\n",
            "        [     6.7512,      2.2800],\n",
            "        [     6.4553,      6.0800],\n",
            "        [     6.0837,      1.5200],\n",
            "        [     8.0831,     13.6800],\n",
            "        [    10.3145,      6.8300],\n",
            "        [    57.6810,     63.1500],\n",
            "        [     5.1091,      2.5600],\n",
            "        [     5.9488,      9.3900],\n",
            "        [     0.4766,      0.0000],\n",
            "        [     7.2825,      1.7100],\n",
            "        [     4.6702,      5.1200],\n",
            "        [     2.1169,      0.0000],\n",
            "        [     6.2441,      5.1200],\n",
            "        [     7.1879,      3.4100],\n",
            "        [     6.2732,      3.4100],\n",
            "        [     6.3136,      6.8300],\n",
            "        [     5.8907,      0.8500],\n",
            "        [     4.3650,      5.1200],\n",
            "        [     7.6325,      6.8300],\n",
            "        [     2.6711,      3.4100],\n",
            "        [     6.4753,      2.5600],\n",
            "        [     6.0497,      2.5600],\n",
            "        [     6.0584,      4.2700],\n",
            "        [    50.7690,     88.0300],\n",
            "        [     7.3930,      7.6800],\n",
            "        [     7.3559,      3.4100],\n",
            "        [     3.8581,     12.8000],\n",
            "        [     7.6650,     11.0900],\n",
            "        [     4.2488,      6.8300],\n",
            "        [     0.2242,      3.0400],\n",
            "        [     1.1489,      7.3300],\n",
            "        [     8.2819,     13.6500],\n",
            "        [    10.8859,      5.9700],\n",
            "        [    10.4598,     10.2400],\n",
            "        [    -0.1156,      0.0000],\n",
            "        [     4.4596,      5.9700],\n",
            "        [     9.8652,      8.4100],\n",
            "        [    -0.2597,      0.0000],\n",
            "        [    -0.2828,      0.0000],\n",
            "        [     0.9327,      0.5100],\n",
            "        [    -0.2088,      0.0000],\n",
            "        [    -0.2185,      0.0000],\n",
            "        [     1.0393,      0.7600],\n",
            "        [     0.8523,      0.7600],\n",
            "        [     1.7857,      0.8500],\n",
            "        [    -0.2948,      0.0000],\n",
            "        [    -0.2232,      0.0000],\n",
            "        [     1.6276,      1.7100],\n",
            "        [    -0.2197,      0.0000],\n",
            "        [     0.8073,      0.7600],\n",
            "        [     1.4644,      1.7100],\n",
            "        [     0.4378,      0.5100],\n",
            "        [    -0.2286,      0.0000],\n",
            "        [    -0.2864,      0.0000],\n",
            "        [     1.6027,      0.8500],\n",
            "        [    -0.2503,      0.0000],\n",
            "        [    -0.2533,      0.0000],\n",
            "        [    36.1595,     38.1700],\n",
            "        [     3.8236,     11.0900],\n",
            "        [     1.1049,      0.5100],\n",
            "        [     3.0955,      4.2700],\n",
            "        [     2.7255,      3.8000],\n",
            "        [    -0.0928,      0.0000],\n",
            "        [     1.3797,      1.3300],\n",
            "        [     1.0083,      0.0000],\n",
            "        [     2.2956,      3.0400],\n",
            "        [     2.1582,      1.3300],\n",
            "        [     4.3975,      3.4100],\n",
            "        [     3.1914,      7.6800],\n",
            "        [     3.2207,      1.7100],\n",
            "        [    70.8818,     86.2100],\n",
            "        [     4.6065,      3.9900],\n",
            "        [     5.5754,      7.6800],\n",
            "        [     5.3724,      5.1200],\n",
            "        [     5.3824,      6.8300],\n",
            "        [     3.7761,      4.8400],\n",
            "        [     4.4273,      4.9300],\n",
            "        [     5.0837,      4.2700],\n",
            "        [     2.6986,      5.2300],\n",
            "        [     2.1493,      5.2800],\n",
            "        [     4.1542,      3.1300],\n",
            "        [     4.6510,      4.0800],\n",
            "        [     6.5740,      5.9700],\n",
            "        [     6.4131,     11.9500],\n",
            "        [     1.6856,      3.5300],\n",
            "        [     2.7881,      2.5600],\n",
            "        [     5.3338,      6.8300],\n",
            "        [   109.0497,    118.8300],\n",
            "        [    11.4453,     14.5100],\n",
            "        [     9.8116,      5.5700],\n",
            "        [    11.3599,     11.0900],\n",
            "        [     8.4530,      6.5900],\n",
            "        [     9.8102,      6.0800],\n",
            "        [    11.2319,      6.0800],\n",
            "        [     8.8141,      5.5700],\n",
            "        [     6.4620,      6.0000],\n",
            "        [    11.8172,      8.3600],\n",
            "        [    10.0609,     12.1600],\n",
            "        [     8.4160,     12.9200],\n",
            "        [    10.8184,     14.5100],\n",
            "        [    10.5853,      9.3900],\n",
            "        [    88.3466,     81.9200],\n",
            "        [     0.6703,      0.0000],\n",
            "        [     3.0979,      2.5600],\n",
            "        [    10.0781,     11.0900],\n",
            "        [    12.7039,     10.2400],\n",
            "        [     0.9867,      0.0000],\n",
            "        [     7.9265,      7.6800],\n",
            "        [     8.7890,      4.2700],\n",
            "        [     4.1513,      0.0000],\n",
            "        [     6.5732,      6.8300],\n",
            "        [     6.8187,      6.8300],\n",
            "        [     6.4517,      6.8300],\n",
            "        [    11.7052,      5.1200],\n",
            "        [     7.0443,      7.6800],\n",
            "        [     4.8236,      2.5600],\n",
            "        [     4.1995,      0.0000],\n",
            "        [     7.3758,     10.2400],\n",
            "        [   129.9893,     96.6100],\n",
            "        [     7.8607,      5.3200],\n",
            "        [     6.1888,      8.3600],\n",
            "        [     5.9711,     10.0000],\n",
            "        [     7.3017,      7.6000],\n",
            "        [    11.2056,      5.9700],\n",
            "        [     3.6141,      2.5300],\n",
            "        [    10.8086,      8.5300],\n",
            "        [     5.9977,      5.5700],\n",
            "        [     8.6435,      6.0800],\n",
            "        [    10.3508,      5.9700],\n",
            "        [     7.0823,     13.6800],\n",
            "        [    10.4440,      9.3900],\n",
            "        [     6.6233,      7.6000],\n",
            "        [    68.7412,     61.8700],\n",
            "        [     3.8419,      1.5200],\n",
            "        [     4.3992,      7.3300],\n",
            "        [     4.1604,      8.0000],\n",
            "        [     7.0727,     11.0900],\n",
            "        [     4.5456,      9.1200],\n",
            "        [     4.0127,      6.0000],\n",
            "        [     5.6310,      5.1200],\n",
            "        [     3.0636,      1.5200],\n",
            "        [     4.5598,      9.8800],\n",
            "        [     2.9839,      2.2800]], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([[   151.6629,     85.3300],\n",
            "        [     3.0055,      0.8500],\n",
            "        [     7.4569,      8.5300],\n",
            "        [    -0.1486,      0.0000],\n",
            "        [    10.6402,      9.3900],\n",
            "        [     9.0190,      9.3900],\n",
            "        [     3.3252,     10.2400],\n",
            "        [    -0.2390,      5.1200],\n",
            "        [     3.0784,      0.8500],\n",
            "        [     5.4222,     11.9500],\n",
            "        [    12.2094,     11.9500],\n",
            "        [    11.3297,     17.0700],\n",
            "        [    80.3381,     61.1300],\n",
            "        [     2.3395,      2.0000],\n",
            "        [     0.9836,      2.0000],\n",
            "        [     4.5155,      2.2800],\n",
            "        [     0.9624,      0.5100],\n",
            "        [     2.3710,      6.6700],\n",
            "        [     0.5042,      1.0100],\n",
            "        [     3.1653,      3.3300],\n",
            "        [     3.0969,      2.2800],\n",
            "        [     7.3714,      4.2700],\n",
            "        [     5.1892,      4.2700],\n",
            "        [     6.7979,      5.9700],\n",
            "        [     4.4901,      7.6000],\n",
            "        [     4.9285,      2.2800],\n",
            "        [     4.2303,      1.5200],\n",
            "        [     5.8070,      8.5300],\n",
            "        [     7.4384,      2.5600],\n",
            "        [     1.2611,      4.0500],\n",
            "        [    74.1038,     73.0400],\n",
            "        [     2.7192,      2.5600],\n",
            "        [     2.6095,      1.7100],\n",
            "        [     6.7814,      2.5600],\n",
            "        [     1.8241,      5.1200],\n",
            "        [     2.4810,      1.7100],\n",
            "        [     2.3632,      5.6300],\n",
            "        [     4.5701,      4.2700],\n",
            "        [     4.3809,      3.4100],\n",
            "        [     5.4070,      7.6800],\n",
            "        [     1.0898,      0.0000],\n",
            "        [     4.2333,      4.2700],\n",
            "        [     5.8588,      7.6800],\n",
            "        [     1.3121,      2.5600],\n",
            "        [     0.5997,      0.0000],\n",
            "        [     1.1088,      1.7100],\n",
            "        [     0.8103,      0.0000],\n",
            "        [     5.6096,      9.3900],\n",
            "        [     6.5161,      2.5600],\n",
            "        [     3.4544,      3.4100],\n",
            "        [     5.8812,      6.8300],\n",
            "        [    55.6201,     50.8500],\n",
            "        [     4.8319,      3.0400],\n",
            "        [     2.0216,      1.3300],\n",
            "        [     5.0563,      3.4100],\n",
            "        [     2.2457,      1.0100],\n",
            "        [     3.7112,      3.0400],\n",
            "        [     3.4021,      1.5200],\n",
            "        [     3.7291,      0.0000],\n",
            "        [     4.6884,      4.5600],\n",
            "        [     4.8683,      4.2700],\n",
            "        [     5.4259,      3.4100],\n",
            "        [     4.1973,      3.8000],\n",
            "        [     2.4583,      0.0000],\n",
            "        [     5.2268,      5.9700],\n",
            "        [     3.1659,      2.2800],\n",
            "        [     3.0201,      0.7600],\n",
            "        [     3.2265,      0.7600],\n",
            "        [     3.1130,      0.0000],\n",
            "        [     5.1140,      4.2700],\n",
            "        [     3.4067,      4.0000],\n",
            "        [     4.6818,      3.4100],\n",
            "        [    13.2818,     15.4500],\n",
            "        [     1.8509,      1.5200],\n",
            "        [     2.6242,      1.7100],\n",
            "        [     2.0102,      1.5200],\n",
            "        [     1.2704,      0.7600],\n",
            "        [     2.6122,      0.8500],\n",
            "        [     1.7947,      2.2800],\n",
            "        [     2.3056,      0.8500],\n",
            "        [    -0.0659,      0.0000],\n",
            "        [     0.3413,      1.0100],\n",
            "        [     1.0279,      1.3300],\n",
            "        [     1.7605,      0.0000],\n",
            "        [     0.5270,      0.0000],\n",
            "        [     1.8303,      0.7600],\n",
            "        [     1.9834,      1.5200],\n",
            "        [    -0.0821,      0.0000],\n",
            "        [     1.0085,      1.3300],\n",
            "        [    56.2589,     37.9600],\n",
            "        [     0.0430,      0.8500],\n",
            "        [     4.1454,      2.4700],\n",
            "        [     4.0261,      4.2700],\n",
            "        [    -0.0689,      0.5100],\n",
            "        [    -0.0078,      0.0000],\n",
            "        [     6.8596,     10.2400],\n",
            "        [     6.0834,      6.8300],\n",
            "        [     0.7105,      0.8500],\n",
            "        [    -0.3794,      0.0000],\n",
            "        [     4.2482,      4.2700],\n",
            "        [     5.2256,      6.8300],\n",
            "        [     0.5184,      0.8500],\n",
            "        [     1.9216,      0.0000],\n",
            "        [    -0.1582,      0.0000],\n",
            "        [    -0.1429,      0.0000],\n",
            "        [    -0.0749,      0.0000],\n",
            "        [    -0.1708,      0.0000],\n",
            "        [    -0.0432,      0.0000],\n",
            "        [     0.0486,      0.0000],\n",
            "        [    -0.0817,      0.0000],\n",
            "        [    -0.1259,      0.0000],\n",
            "        [    -0.1786,      0.0000],\n",
            "        [     0.0421,      0.0000],\n",
            "        [     0.0163,      0.0000],\n",
            "        [    70.1338,     56.8300],\n",
            "        [     3.8614,      0.8500],\n",
            "        [     4.0977,      1.7100],\n",
            "        [     6.2660,      2.5600],\n",
            "        [     1.3896,      0.0000],\n",
            "        [     6.1815,      4.2700],\n",
            "        [     4.2610,      2.5600],\n",
            "        [     0.9536,      1.7100],\n",
            "        [     3.2050,      1.7100],\n",
            "        [     6.2914,      5.1200],\n",
            "        [     0.8565,      0.0000],\n",
            "        [     5.9519,      7.6800],\n",
            "        [     1.5452,      0.0000],\n",
            "        [     5.9394,      4.2700],\n",
            "        [     2.7262,      0.5100],\n",
            "        [     6.8769,      7.6800],\n",
            "        [     5.5589,     16.2100],\n",
            "        [     0.5282,      0.0000],\n",
            "        [    66.1154,     65.6500],\n",
            "        [     5.2965,      8.0000],\n",
            "        [     2.2467,      1.5200],\n",
            "        [     4.6983,     10.2400],\n",
            "        [     2.6258,      4.5600],\n",
            "        [     5.7741,      6.6700],\n",
            "        [     7.2979,      2.5600],\n",
            "        [     4.7650,      6.8400],\n",
            "        [     6.0777,     11.9500],\n",
            "        [     3.8275,      2.0300],\n",
            "        [     2.7844,      5.3200],\n",
            "        [     5.4879,      5.9700],\n",
            "        [   116.2843,    116.6300],\n",
            "        [     7.8577,     10.2400],\n",
            "        [     6.6976,      5.5700],\n",
            "        [     8.6057,      5.3300],\n",
            "        [     8.3952,     11.4000],\n",
            "        [     7.6587,      9.3900],\n",
            "        [     6.9835,      6.6700],\n",
            "        [     8.9774,      4.2700],\n",
            "        [     7.4494,      6.8300],\n",
            "        [     6.4805,      6.8400],\n",
            "        [     8.2806,      7.6000],\n",
            "        [     5.0815,      4.5600],\n",
            "        [     5.6038,      9.1200],\n",
            "        [     5.7601,      5.0700],\n",
            "        [     7.6534,      5.3200],\n",
            "        [     3.5830,      7.0900],\n",
            "        [     4.9113,     11.3300],\n",
            "        [    73.2582,     58.8800],\n",
            "        [     7.3610,     13.6500],\n",
            "        [     4.5177,     10.2400],\n",
            "        [     7.1436,      5.9700],\n",
            "        [     2.9038,      4.2700],\n",
            "        [     1.0988,      0.0000],\n",
            "        [     0.6416,      2.5600],\n",
            "        [     5.4332,     11.0900],\n",
            "        [     4.4464,      4.2700],\n",
            "        [     1.7300,      0.8500],\n",
            "        [     3.3115,      5.9700],\n",
            "        [    20.4414,     20.4800],\n",
            "        [    -0.1129,      0.0000],\n",
            "        [    -0.0471,      0.0000],\n",
            "        [     1.6067,      0.8500],\n",
            "        [     1.6696,      1.7100],\n",
            "        [     1.8962,      0.8500],\n",
            "        [     1.6201,      0.8500],\n",
            "        [     2.2153,      2.5600],\n",
            "        [     0.0317,      0.0000],\n",
            "        [    -0.0919,      0.0000],\n",
            "        [    -0.1298,      0.0000],\n",
            "        [     2.5856,      4.2700],\n",
            "        [     2.6822,      1.7100],\n",
            "        [     2.6643,      2.5600],\n",
            "        [    -0.2247,      0.0000],\n",
            "        [     1.2383,      2.5600],\n",
            "        [    -0.0170,      0.0000],\n",
            "        [     0.4783,      0.0000],\n",
            "        [     2.6309,      1.7100],\n",
            "        [     1.6829,      0.8500]], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([[    -0.4136,      0.0000],\n",
            "        [    -0.0156,      0.0000],\n",
            "        [     0.0157,      0.0000],\n",
            "        [    -0.0113,      0.0000],\n",
            "        [    -0.0147,      0.0000],\n",
            "        [    -0.0001,      0.0000],\n",
            "        [     0.0127,      0.0000],\n",
            "        [    -0.0491,      0.0000],\n",
            "        [     0.0013,      0.0000],\n",
            "        [    -0.0044,      0.0000],\n",
            "        [     0.0111,      0.0000],\n",
            "        [    -0.0223,      0.0000],\n",
            "        [     0.0177,      0.0000],\n",
            "        [    -0.0101,      0.0000],\n",
            "        [    -0.0083,      0.0000],\n",
            "        [    -0.0134,      0.0000],\n",
            "        [    -0.0098,      0.0000],\n",
            "        [     0.0287,      0.0000],\n",
            "        [    -0.0420,      0.0000],\n",
            "        [     7.1798,      7.0500],\n",
            "        [     0.0135,      0.0000],\n",
            "        [     1.5445,      0.6300],\n",
            "        [     0.8064,      2.0300],\n",
            "        [     0.5594,      0.0000],\n",
            "        [     1.4297,      1.1200],\n",
            "        [     1.5755,      1.4100],\n",
            "        [     0.0078,      0.0000],\n",
            "        [     0.9103,      0.0000],\n",
            "        [     1.0108,      1.1200],\n",
            "        [     0.5054,      0.7500],\n",
            "        [     0.4104,      0.0000],\n",
            "        [     5.7267,      4.1200],\n",
            "        [     1.4762,      0.6300],\n",
            "        [    -0.0006,      0.0000],\n",
            "        [     1.0982,      0.0000],\n",
            "        [    -0.0124,      0.0000],\n",
            "        [     0.4910,      0.3700],\n",
            "        [     0.6748,      0.3700],\n",
            "        [     1.0744,      0.5600],\n",
            "        [     0.2416,      0.0000],\n",
            "        [     1.3280,      1.6800],\n",
            "        [    -0.0441,      0.0000],\n",
            "        [     0.9858,      0.5100],\n",
            "        [     0.4614,      0.0000],\n",
            "        [    -0.7152,      0.0000],\n",
            "        [     0.0127,      0.0000],\n",
            "        [     0.0124,      0.0000],\n",
            "        [     0.0143,      0.0000],\n",
            "        [    -0.0041,      0.0000],\n",
            "        [     0.0038,      0.0000],\n",
            "        [     0.0085,      0.0000],\n",
            "        [     0.0067,      0.0000],\n",
            "        [     0.0145,      0.0000],\n",
            "        [     0.0031,      0.0000],\n",
            "        [    -0.0029,      0.0000],\n",
            "        [     0.0121,      0.0000],\n",
            "        [     0.0057,      0.0000],\n",
            "        [     0.0141,      0.0000],\n",
            "        [     0.0120,      0.0000],\n",
            "        [     0.0143,      0.0000],\n",
            "        [     0.0078,      0.0000],\n",
            "        [    -0.1686,      0.0000],\n",
            "        [     0.0219,      0.0000],\n",
            "        [     6.8945,      8.9900],\n",
            "        [     1.2036,      1.1200],\n",
            "        [    -0.0558,      0.0000],\n",
            "        [     1.6057,      0.7100],\n",
            "        [    -0.0421,      0.0000],\n",
            "        [     0.3938,      0.0000],\n",
            "        [     0.8332,      0.5100],\n",
            "        [    -0.0366,      0.0000],\n",
            "        [    -0.0254,      0.0000],\n",
            "        [     1.5839,      3.5300],\n",
            "        [    -0.0282,      0.0000],\n",
            "        [     1.0025,      2.2400],\n",
            "        [    -0.0213,      0.0000],\n",
            "        [     0.5624,      0.3700],\n",
            "        [     0.7300,      0.5100],\n",
            "        [    -0.0327,      0.0000],\n",
            "        [    -0.0620,      0.0000],\n",
            "        [     2.8149,      1.2700],\n",
            "        [     0.8372,      0.5600],\n",
            "        [    -0.0256,      0.0000],\n",
            "        [    -0.0042,      0.0000],\n",
            "        [    -0.0101,      0.0000],\n",
            "        [     0.9908,      0.7100],\n",
            "        [    -0.0030,      0.0000],\n",
            "        [    -0.0133,      0.0000],\n",
            "        [    -0.0073,      0.0000],\n",
            "        [     0.0079,      0.0000],\n",
            "        [    -0.0229,      0.0000],\n",
            "        [    -0.0228,      0.0000],\n",
            "        [     0.1295,      0.0000],\n",
            "        [    -0.0333,      0.0000],\n",
            "        [    -0.0211,      0.0000],\n",
            "        [    -0.0088,      0.0000],\n",
            "        [     0.1337,      0.0000],\n",
            "        [    -0.0004,      0.0000],\n",
            "        [     0.0105,      0.0000],\n",
            "        [     0.0082,      0.0000],\n",
            "        [    -0.0106,      0.0000],\n",
            "        [     0.0150,      0.0000],\n",
            "        [    -0.0156,      0.0000],\n",
            "        [    -0.0120,      0.0000],\n",
            "        [     0.0003,      0.0000],\n",
            "        [    -0.0053,      0.0000],\n",
            "        [     0.0051,      0.0000],\n",
            "        [     0.0038,      0.0000],\n",
            "        [    -0.0094,      0.0000],\n",
            "        [     0.0097,      0.0000],\n",
            "        [    -0.0147,      0.0000],\n",
            "        [     3.6278,      3.4300],\n",
            "        [     0.7990,      0.0000],\n",
            "        [     0.0056,      0.0000],\n",
            "        [     1.0773,      1.1200],\n",
            "        [     0.7104,      1.1200],\n",
            "        [     1.2553,      0.6300],\n",
            "        [     0.0072,      0.0000],\n",
            "        [     0.6462,      0.0000],\n",
            "        [    -0.0028,      0.0000],\n",
            "        [     0.0210,      0.0000],\n",
            "        [     0.7527,      0.0000],\n",
            "        [     1.0035,      0.5600],\n",
            "        [    -0.0220,      0.0000],\n",
            "        [     1.4721,      0.0000],\n",
            "        [     0.1824,      0.0000],\n",
            "        [    -0.0048,      0.0000],\n",
            "        [     0.0014,      0.0000],\n",
            "        [    -0.0171,      0.0000],\n",
            "        [     0.0179,      0.0000],\n",
            "        [    -0.0038,      0.0000],\n",
            "        [    11.0322,     12.7300],\n",
            "        [    -0.0467,      0.0000],\n",
            "        [     1.0715,      0.0000],\n",
            "        [     1.0092,      2.0300],\n",
            "        [     1.1227,      1.0100],\n",
            "        [    -0.0599,      0.0000],\n",
            "        [     1.1440,      1.0100],\n",
            "        [    -0.0446,      0.0000],\n",
            "        [     1.5637,      5.6400],\n",
            "        [     1.0598,      3.0400],\n",
            "        [     0.6809,      0.0000],\n",
            "        [    -0.0610,      0.0000],\n",
            "        [    -0.0192,      0.0000],\n",
            "        [    -0.0439,      0.0000],\n",
            "        [    -0.0542,      0.0000],\n",
            "        [    -0.0312,      0.0000],\n",
            "        [    -0.0142,      0.0000],\n",
            "        [    -0.0173,      0.0000],\n",
            "        [    -0.0492,      0.0000],\n",
            "        [    -0.0266,      0.0000],\n",
            "        [    -0.0276,      0.0000],\n",
            "        [    -0.0346,      0.0000],\n",
            "        [    -0.0542,      0.0000],\n",
            "        [    -0.0389,      0.0000],\n",
            "        [     6.2820,      2.7200],\n",
            "        [    -0.0431,      0.0000],\n",
            "        [    -0.0486,      0.0000],\n",
            "        [    -0.0409,      0.0000],\n",
            "        [     0.3813,      0.0000],\n",
            "        [     1.2121,      0.5600],\n",
            "        [    -0.0481,      0.0000],\n",
            "        [    -0.0515,      0.0000],\n",
            "        [     0.4789,      0.0000],\n",
            "        [     0.8348,      0.0000],\n",
            "        [    -0.0441,      0.0000],\n",
            "        [     1.2458,      1.4100],\n",
            "        [     0.4050,      0.7500],\n",
            "        [     0.9222,      0.0000],\n",
            "        [     0.3611,      0.0000],\n",
            "        [     1.1080,      0.0000],\n",
            "        [    30.4898,     28.7100],\n",
            "        [     3.1028,      1.6800],\n",
            "        [    -0.0186,      0.0000],\n",
            "        [     3.8153,      6.3600],\n",
            "        [     1.9010,      1.8700],\n",
            "        [     1.7697,      0.7500],\n",
            "        [     1.3380,      0.0000],\n",
            "        [     2.5661,      4.0500],\n",
            "        [     0.8070,      0.0000],\n",
            "        [     1.7474,      2.9900],\n",
            "        [     3.1947,      8.4000],\n",
            "        [     1.9553,      2.6100]], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([[     8.4063,      2.2300],\n",
            "        [     0.0067,      0.0000],\n",
            "        [     0.0935,      0.0000],\n",
            "        [    -0.0070,      0.0000],\n",
            "        [     0.0221,      0.0000],\n",
            "        [     0.3138,      0.0000],\n",
            "        [    -0.0258,      0.0000],\n",
            "        [     0.0046,      0.0000],\n",
            "        [     0.0023,      0.0000],\n",
            "        [     0.5729,      0.5100],\n",
            "        [     1.6007,      0.7100],\n",
            "        [    -0.0023,      0.0000],\n",
            "        [     0.0438,      0.0000],\n",
            "        [     0.0674,      0.0000],\n",
            "        [     0.4736,      0.0000],\n",
            "        [    -0.0181,      0.0000],\n",
            "        [     0.0072,      0.0000],\n",
            "        [     0.8320,      1.0100],\n",
            "        [    -0.0044,      0.0000],\n",
            "        [    -0.1726,      0.0000],\n",
            "        [    -0.0112,      0.0000],\n",
            "        [    -0.0370,      0.0000],\n",
            "        [    -0.0009,      0.0000],\n",
            "        [     0.0022,      0.0000],\n",
            "        [    -0.0260,      0.0000],\n",
            "        [    -0.0192,      0.0000],\n",
            "        [    -0.0150,      0.0000],\n",
            "        [     0.0080,      0.0000],\n",
            "        [    -0.0040,      0.0000],\n",
            "        [    -0.0218,      0.0000],\n",
            "        [    -0.0253,      0.0000],\n",
            "        [    -0.0240,      0.0000],\n",
            "        [    -0.0005,      0.0000],\n",
            "        [     7.3426,      6.4800],\n",
            "        [     0.0246,      0.0000],\n",
            "        [     0.3056,      0.3700],\n",
            "        [     0.5063,      0.0000],\n",
            "        [     0.4933,      0.3700],\n",
            "        [     1.3438,      1.8800],\n",
            "        [     0.5518,      0.7500],\n",
            "        [    -0.0025,      0.0000],\n",
            "        [     1.7870,      0.7100],\n",
            "        [     0.7519,      1.0100],\n",
            "        [     0.5883,      0.3700],\n",
            "        [     0.6551,      1.0100],\n",
            "        [    17.6314,     23.8700],\n",
            "        [     1.0428,      0.7500],\n",
            "        [     1.3969,      2.2400],\n",
            "        [     1.3460,      1.5200],\n",
            "        [     1.1495,      1.0100],\n",
            "        [     1.3309,      0.7500],\n",
            "        [     0.7781,      1.1200],\n",
            "        [     1.5522,      0.5600],\n",
            "        [     1.5330,      0.5600],\n",
            "        [     2.3789,      5.6500],\n",
            "        [     2.3048,      4.2400],\n",
            "        [     1.5407,      1.5200],\n",
            "        [     2.0717,      2.8300],\n",
            "        [     0.8884,      1.1200],\n",
            "        [     8.6919,      6.6700],\n",
            "        [     1.6469,      0.7100],\n",
            "        [     1.4359,      0.5600],\n",
            "        [     0.0386,      0.0000],\n",
            "        [    -0.0226,      0.0000],\n",
            "        [     0.0326,      0.0000],\n",
            "        [     1.4588,      1.8800],\n",
            "        [     1.4304,      1.8800],\n",
            "        [     0.6425,      0.3700],\n",
            "        [     0.9978,      0.5600],\n",
            "        [     1.4638,      0.7100],\n",
            "        [     0.0348,      0.0000],\n",
            "        [     3.8323,      5.4400],\n",
            "        [     0.0047,      0.0000],\n",
            "        [     0.0161,      0.0000],\n",
            "        [     1.1227,      1.4100],\n",
            "        [     0.6735,      0.5600],\n",
            "        [     0.0102,      0.0000],\n",
            "        [     0.0114,      0.0000],\n",
            "        [     0.5334,      1.1200],\n",
            "        [     0.6745,      0.5600],\n",
            "        [    -0.0120,      0.0000],\n",
            "        [     0.1631,      0.3700],\n",
            "        [     0.0156,      0.0000],\n",
            "        [     0.0198,      0.0000],\n",
            "        [     0.0187,      0.0000],\n",
            "        [     0.0141,      0.0000],\n",
            "        [     0.0026,      0.0000],\n",
            "        [     0.0217,      0.0000],\n",
            "        [     1.2139,      1.4100],\n",
            "        [     0.0152,      0.0000],\n",
            "        [     0.0175,      0.0000],\n",
            "        [     8.8040,      6.4400],\n",
            "        [     1.7975,      1.4100],\n",
            "        [     0.3712,      0.7500],\n",
            "        [    -0.0164,      0.0000],\n",
            "        [     0.0121,      0.0000],\n",
            "        [     0.5671,      0.3700],\n",
            "        [     1.9806,      0.7100],\n",
            "        [     0.6420,      0.3700],\n",
            "        [     0.0023,      0.0000],\n",
            "        [     0.5881,      0.0000],\n",
            "        [     0.0525,      0.0000],\n",
            "        [     0.0109,      0.0000],\n",
            "        [     1.6607,      0.7100],\n",
            "        [     1.7456,      1.4100],\n",
            "        [     1.9613,      0.7100],\n",
            "        [     0.0127,      0.0000],\n",
            "        [    -0.0094,      0.0000],\n",
            "        [    20.7178,     25.8100],\n",
            "        [     1.0422,      1.1200],\n",
            "        [     2.3989,      1.4100],\n",
            "        [     1.2342,      1.0100],\n",
            "        [     1.2393,      2.6100],\n",
            "        [     2.4929,      2.8300],\n",
            "        [     1.9843,      3.9200],\n",
            "        [     1.6456,      1.5200],\n",
            "        [     1.5090,      2.0300],\n",
            "        [     1.5093,      3.3600],\n",
            "        [     0.9052,      0.7500],\n",
            "        [     1.4468,      1.1200],\n",
            "        [     1.0995,      0.7500],\n",
            "        [     1.3513,      1.8700],\n",
            "        [     0.0102,      0.0000],\n",
            "        [     1.4599,      1.5200],\n",
            "        [    22.8826,     27.0400],\n",
            "        [     1.9911,      3.9200],\n",
            "        [     1.5263,      3.0400],\n",
            "        [     1.3720,      1.4900],\n",
            "        [     1.2103,      0.7500],\n",
            "        [     1.3012,      1.8700],\n",
            "        [     2.6659,      3.5300],\n",
            "        [     2.6961,      0.6300],\n",
            "        [     0.7881,      1.3300],\n",
            "        [     1.3674,      1.8700],\n",
            "        [     1.1980,      2.2400],\n",
            "        [     2.2220,      2.2400],\n",
            "        [     2.6619,      1.8800],\n",
            "        [     0.6194,      0.3700],\n",
            "        [     2.6285,      1.8800],\n",
            "        [    18.8349,     17.4800],\n",
            "        [     0.8434,      0.0000],\n",
            "        [     1.2577,      1.1200],\n",
            "        [     2.5451,      3.5300],\n",
            "        [     1.0238,      0.5100],\n",
            "        [     0.9665,      2.2400],\n",
            "        [     1.3138,      1.0100],\n",
            "        [     1.1332,      1.0100],\n",
            "        [     1.9362,      3.3600],\n",
            "        [     0.4212,      0.7500],\n",
            "        [     2.3866,      2.8300],\n",
            "        [     1.4283,      1.1200]], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([[    69.4928,     78.2300],\n",
            "        [     6.0979,     10.3500],\n",
            "        [     7.2979,      5.4100],\n",
            "        [     7.8012,     10.4400],\n",
            "        [     3.7920,      5.6000],\n",
            "        [     7.3819,     12.0500],\n",
            "        [     4.3885,      3.7300],\n",
            "        [     7.4448,      7.6800],\n",
            "        [     7.8706,      8.5300],\n",
            "        [     7.0823,     11.2000],\n",
            "        [     4.0203,      3.2300],\n",
            "        [    37.9171,     29.0100],\n",
            "        [     1.3890,      0.0000],\n",
            "        [     2.3903,      2.5600],\n",
            "        [     2.8824,      1.7100],\n",
            "        [     2.3784,      3.4100],\n",
            "        [     2.2006,      2.5600],\n",
            "        [     2.6691,      3.4100],\n",
            "        [     0.2117,      0.0000],\n",
            "        [     1.4306,      0.0000],\n",
            "        [     0.1540,      0.0000],\n",
            "        [     3.6409,      0.8500],\n",
            "        [     2.5611,      2.5600],\n",
            "        [     2.9089,      4.2700],\n",
            "        [     0.1785,      0.0000],\n",
            "        [     3.2766,      5.1200],\n",
            "        [     3.8480,      0.8500],\n",
            "        [     4.3689,      1.7100],\n",
            "        [    14.3422,     14.5100],\n",
            "        [     0.0295,      0.0000],\n",
            "        [     0.1275,      0.0000],\n",
            "        [     0.0705,      0.0000],\n",
            "        [     0.0557,      0.0000],\n",
            "        [     1.3871,      0.8500],\n",
            "        [     0.1425,      0.0000],\n",
            "        [     1.8558,      3.4100],\n",
            "        [     1.6556,      3.4100],\n",
            "        [     0.1513,      0.0000],\n",
            "        [     1.1469,      3.4100],\n",
            "        [     0.1284,      0.0000],\n",
            "        [     1.6383,      3.4100],\n",
            "        [     0.0099,      0.0000],\n",
            "        [    49.6730,     40.9600],\n",
            "        [     4.0580,      2.5600],\n",
            "        [     4.0203,      2.5600],\n",
            "        [     3.4371,      2.5600],\n",
            "        [     3.5285,      2.5600],\n",
            "        [     4.2097,      4.2700],\n",
            "        [     3.1922,      4.2700],\n",
            "        [     2.3618,      1.7100],\n",
            "        [     4.2701,      2.5600],\n",
            "        [     3.3992,      3.4100],\n",
            "        [     2.9019,      1.7100],\n",
            "        [     4.2991,      1.7100],\n",
            "        [     3.5187,      0.8500],\n",
            "        [     2.4915,      1.7100],\n",
            "        [     0.3259,      0.0000],\n",
            "        [     3.7264,      1.7100],\n",
            "        [     4.1276,      2.5600],\n",
            "        [     4.0576,      1.7100],\n",
            "        [     3.4862,      1.7100],\n",
            "        [     1.8752,      0.0000],\n",
            "        [     2.7939,      0.8500],\n",
            "        [    62.4263,     63.6000],\n",
            "        [     4.9113,      6.0800],\n",
            "        [     4.3207,      3.0400],\n",
            "        [     6.5646,      3.4100],\n",
            "        [     3.3906,      3.3300],\n",
            "        [     3.8557,      2.6700],\n",
            "        [     5.5930,      4.2700],\n",
            "        [     2.9619,      2.2800],\n",
            "        [     4.1851,      1.5200],\n",
            "        [     2.9189,      0.7600],\n",
            "        [     3.0436,      0.7600],\n",
            "        [     5.0257,      2.2800],\n",
            "        [     4.0319,      1.3300],\n",
            "        [     3.1861,      0.7600],\n",
            "        [     5.9577,      3.4100],\n",
            "        [     4.1616,      2.5300],\n",
            "        [     4.6801,      7.6000],\n",
            "        [     3.8268,      6.0800],\n",
            "        [     6.8938,      3.4100],\n",
            "        [     5.7792,      4.2700],\n",
            "        [     4.6266,      3.8000],\n",
            "        [     7.8676,      5.9700],\n",
            "        [     1.0474,      0.8500],\n",
            "        [     0.0088,      0.0000],\n",
            "        [     0.0013,      0.0000],\n",
            "        [    -0.0188,      0.0000],\n",
            "        [     0.9980,      0.8500],\n",
            "        [     0.0139,      0.0000],\n",
            "        [     1.4262,      1.7100],\n",
            "        [     0.0388,      0.0000],\n",
            "        [     1.1043,      0.8500],\n",
            "        [     0.5794,      0.8500],\n",
            "        [    -0.0096,      0.0000],\n",
            "        [    -0.1031,      0.0000],\n",
            "        [     0.9884,      0.8500],\n",
            "        [     0.0146,      0.0000],\n",
            "        [    15.1349,      8.5300],\n",
            "        [     0.1523,      0.0000],\n",
            "        [     1.2079,      0.0000],\n",
            "        [     0.7785,      0.8500],\n",
            "        [     0.1103,      0.0000],\n",
            "        [     0.0454,      0.0000],\n",
            "        [     0.8742,      1.7100],\n",
            "        [     0.4533,      0.0000],\n",
            "        [     0.0111,      0.0000],\n",
            "        [     0.0948,      0.0000],\n",
            "        [     1.2836,      0.8500],\n",
            "        [     0.3935,      0.0000],\n",
            "        [     1.5060,      1.7100],\n",
            "        [     2.0381,      0.8500],\n",
            "        [     0.1992,      0.0000],\n",
            "        [     1.4187,      0.0000],\n",
            "        [     1.9993,      1.7100],\n",
            "        [     1.5326,      0.8500],\n",
            "        [     0.1175,      0.0000],\n",
            "        [    44.8344,     49.6700],\n",
            "        [     2.7986,      1.5200],\n",
            "        [     3.5407,      4.2700],\n",
            "        [     2.7714,      1.5200],\n",
            "        [     2.8768,      3.8000],\n",
            "        [     3.0497,      1.5200],\n",
            "        [     2.6691,      1.5200],\n",
            "        [     2.7831,      2.2800],\n",
            "        [     2.9942,      3.8000],\n",
            "        [     2.9351,      4.5600],\n",
            "        [     3.3011,      2.5600],\n",
            "        [     3.0724,      0.7600],\n",
            "        [     2.5611,      1.3300],\n",
            "        [     3.2003,      0.7600],\n",
            "        [     3.1007,      3.4100],\n",
            "        [     0.1471,      0.0000],\n",
            "        [     3.6517,      5.1200],\n",
            "        [     3.5375,      2.5600],\n",
            "        [     2.7522,      4.6700],\n",
            "        [     3.3555,      1.7100],\n",
            "        [     2.7463,      2.0000],\n",
            "        [    73.5286,     81.5700],\n",
            "        [     6.6946,     12.0500],\n",
            "        [     6.3305,      5.6900],\n",
            "        [     6.2099,      3.2300],\n",
            "        [     5.0488,      7.1200],\n",
            "        [     8.1627,      7.6800],\n",
            "        [     6.9854,      9.2000],\n",
            "        [     6.9818,      7.8800],\n",
            "        [     6.6965,      9.1700],\n",
            "        [     3.8546,      3.0400],\n",
            "        [     6.0052,      7.9700],\n",
            "        [     7.0323,      8.5300],\n",
            "        [    60.0035,     64.6700],\n",
            "        [     3.3056,      1.7100],\n",
            "        [     3.6083,      3.4100],\n",
            "        [     3.8723,      7.6800],\n",
            "        [     2.4785,      3.4100],\n",
            "        [     3.6909,     10.9100],\n",
            "        [     3.2754,     11.9500],\n",
            "        [     3.7122,      3.4100],\n",
            "        [     3.5856,      2.5600],\n",
            "        [     3.8875,      8.5300],\n",
            "        [     3.9589,      5.1200],\n",
            "        [     5.4566,      5.9700]], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([[    40.8461,     35.8500],\n",
            "        [     3.9490,      2.5600],\n",
            "        [     3.4282,      2.5600],\n",
            "        [     0.1546,      0.0000],\n",
            "        [     3.1278,      3.0400],\n",
            "        [     2.5464,      2.2800],\n",
            "        [     1.5811,      0.0000],\n",
            "        [     4.5115,      5.1200],\n",
            "        [     1.9588,      1.0100],\n",
            "        [     1.7918,      0.0000],\n",
            "        [     2.7540,      2.0000],\n",
            "        [     3.4279,      0.7600],\n",
            "        [     5.2022,      1.7100],\n",
            "        [     2.7866,      0.7600],\n",
            "        [     2.3985,      2.2800],\n",
            "        [     3.3040,      3.0400],\n",
            "        [     2.8774,      0.7600],\n",
            "        [     3.7553,      3.4100],\n",
            "        [     2.2063,      0.5100],\n",
            "        [     2.4326,      1.5200],\n",
            "        [     2.4348,      2.5300],\n",
            "        [    21.6793,     18.5500],\n",
            "        [     1.4392,      0.5100],\n",
            "        [     1.8595,      3.8000],\n",
            "        [    -0.1744,      0.0000],\n",
            "        [     2.4281,      2.5600],\n",
            "        [    -0.1987,      0.0000],\n",
            "        [     1.2765,      1.5200],\n",
            "        [     3.1707,      0.8500],\n",
            "        [     1.1102,      0.0000],\n",
            "        [     1.4143,      0.7600],\n",
            "        [     1.7928,      3.0400],\n",
            "        [     1.3416,      0.7600],\n",
            "        [     1.0709,      0.0000],\n",
            "        [     2.4124,      1.7100],\n",
            "        [     1.4463,      0.7600],\n",
            "        [     1.8432,      2.2800],\n",
            "        [    48.1006,     50.3500],\n",
            "        [    -0.0265,      0.0000],\n",
            "        [     2.2018,      0.8500],\n",
            "        [     4.3726,      0.0000],\n",
            "        [     0.9671,      0.0000],\n",
            "        [    -0.4171,      0.0000],\n",
            "        [    -0.6521,      0.0000],\n",
            "        [    -0.4597,      0.0000],\n",
            "        [     7.5046,      6.8300],\n",
            "        [     8.3053,     11.0900],\n",
            "        [     9.8184,     14.5100],\n",
            "        [    -0.3277,      0.0000],\n",
            "        [    -0.3005,      0.0000],\n",
            "        [     3.5654,      0.8500],\n",
            "        [     2.5261,      4.2700],\n",
            "        [     9.2789,     11.9500],\n",
            "        [    48.1453,     58.8800],\n",
            "        [     4.2264,      3.4100],\n",
            "        [     4.3177,     11.0900],\n",
            "        [     4.4353,      4.2700],\n",
            "        [     4.4954,      7.6800],\n",
            "        [     4.1608,      6.8300],\n",
            "        [     3.2447,      5.1200],\n",
            "        [     3.1157,      5.1200],\n",
            "        [     4.1503,      4.2700],\n",
            "        [     4.1647,      5.1200],\n",
            "        [     4.1520,      5.9700],\n",
            "        [    82.3005,     92.1600],\n",
            "        [     6.1541,      5.9700],\n",
            "        [     6.6398,      5.1200],\n",
            "        [     3.0040,      7.6800],\n",
            "        [     5.9376,      4.2700],\n",
            "        [     7.5396,      4.2700],\n",
            "        [     6.1585,      2.5600],\n",
            "        [     7.1963,      3.4100],\n",
            "        [     8.0192,      4.2700],\n",
            "        [     8.0823,      5.9700],\n",
            "        [     8.0181,      5.1200],\n",
            "        [     3.3902,      5.1200],\n",
            "        [     6.4076,      2.5600],\n",
            "        [     7.6950,      6.8300],\n",
            "        [     6.3879,      2.5600],\n",
            "        [     4.5009,      5.1200],\n",
            "        [     7.9723,      4.2700],\n",
            "        [     6.2375,      5.1200],\n",
            "        [     7.5973,      6.8300],\n",
            "        [     9.0238,      5.1200],\n",
            "        [    32.4230,     34.0000],\n",
            "        [     3.1308,      0.7600],\n",
            "        [     1.9595,      0.0000],\n",
            "        [     3.7042,      2.5600],\n",
            "        [     4.1349,      7.6800],\n",
            "        [     4.0764,      1.7100],\n",
            "        [     2.6000,      1.5200],\n",
            "        [     4.1227,      1.7100],\n",
            "        [     4.9055,      1.7100],\n",
            "        [     3.8353,      4.2700],\n",
            "        [     2.8130,      3.0400],\n",
            "        [     2.0210,      0.7600],\n",
            "        [     2.8986,      4.5600],\n",
            "        [     1.7260,      0.5100],\n",
            "        [     3.7736,      1.7100],\n",
            "        [     2.6743,      1.5200],\n",
            "        [    13.7637,     15.3600],\n",
            "        [    -0.0746,      0.0000],\n",
            "        [     2.0284,      0.8500],\n",
            "        [     0.9950,      1.7100],\n",
            "        [    -0.1291,      0.0000],\n",
            "        [    -0.0583,      0.0000],\n",
            "        [     0.0738,      0.8500],\n",
            "        [     0.8269,      0.8500],\n",
            "        [     0.9425,      0.8500],\n",
            "        [     0.7932,      0.8500],\n",
            "        [     2.6876,      0.8500],\n",
            "        [     0.9324,      2.5600],\n",
            "        [    -0.1227,      0.0000],\n",
            "        [    -0.1659,      0.0000],\n",
            "        [     2.9662,      3.4100],\n",
            "        [    -0.0866,      0.0000],\n",
            "        [    -0.0774,      0.0000],\n",
            "        [     1.0080,      0.0000],\n",
            "        [     2.5841,      2.5600],\n",
            "        [    -0.1559,      0.0000],\n",
            "        [    58.4240,    134.8300],\n",
            "        [    12.1069,     13.6500],\n",
            "        [    10.4475,     16.2100],\n",
            "        [    13.1669,     14.5100],\n",
            "        [     9.5011,     18.7700],\n",
            "        [    12.8582,     10.2400],\n",
            "        [     3.4617,      0.0000],\n",
            "        [     7.9504,     12.8000],\n",
            "        [     7.0428,     11.0900],\n",
            "        [     4.8770,      6.8300],\n",
            "        [    10.4428,     17.0700],\n",
            "        [    10.9772,     13.6500],\n",
            "        [   120.0897,    140.9200],\n",
            "        [     6.2767,      6.6700],\n",
            "        [     7.1170,      6.0800],\n",
            "        [     6.1224,      6.6700],\n",
            "        [     9.8370,     11.9500],\n",
            "        [     6.2429,      8.3600],\n",
            "        [     8.3762,     11.9500],\n",
            "        [     8.4058,      8.5300],\n",
            "        [     8.4353,     11.9500],\n",
            "        [     6.7768,      9.1200],\n",
            "        [     8.2442,      4.2700],\n",
            "        [     7.5878,      7.6000],\n",
            "        [     6.1288,      4.5600],\n",
            "        [     9.0762,      9.3900],\n",
            "        [     5.0356,      5.5700],\n",
            "        [     8.3032,      9.3900],\n",
            "        [     8.2715,     12.8000],\n",
            "        [     7.0176,      6.0800],\n",
            "        [    95.6910,     85.3300],\n",
            "        [     6.3495,      5.9700],\n",
            "        [     6.5069,     10.2400],\n",
            "        [     8.0131,     11.9500],\n",
            "        [     6.3483,     10.2400],\n",
            "        [     7.5548,     11.0900],\n",
            "        [     6.9300,      7.6800],\n",
            "        [     3.1892,      5.1200],\n",
            "        [     2.8534,      1.7100],\n",
            "        [     8.2362,      9.3900],\n",
            "        [     6.7951,     11.9500]], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([[    15.0945,     17.4100],\n",
            "        [     1.0912,      1.0100],\n",
            "        [     0.7251,      1.1200],\n",
            "        [     0.3013,      0.0000],\n",
            "        [     0.9671,      1.4900],\n",
            "        [     1.6905,      1.1200],\n",
            "        [     1.3940,      3.3600],\n",
            "        [     1.3915,      2.2400],\n",
            "        [     1.4217,      1.6800],\n",
            "        [     1.3177,      2.0300],\n",
            "        [     0.9395,      1.1200],\n",
            "        [     1.6010,      2.2400],\n",
            "        [     0.7104,      0.0000],\n",
            "        [     4.5237,      1.2500],\n",
            "        [     0.0184,      0.0000],\n",
            "        [     0.0101,      0.0000],\n",
            "        [     0.0176,      0.0000],\n",
            "        [     0.0217,      0.0000],\n",
            "        [     0.0038,      0.0000],\n",
            "        [     0.8681,      1.2500],\n",
            "        [     0.0258,      0.0000],\n",
            "        [     0.0218,      0.0000],\n",
            "        [     0.0039,      0.0000],\n",
            "        [     1.3003,      0.0000],\n",
            "        [    -0.0012,      0.0000],\n",
            "        [     0.0203,      0.0000],\n",
            "        [     0.1975,      0.0000],\n",
            "        [     0.0071,      0.0000],\n",
            "        [     0.0040,      0.0000],\n",
            "        [     0.0069,      0.0000],\n",
            "        [    -0.0035,      0.0000],\n",
            "        [     0.0062,      0.0000],\n",
            "        [     0.0005,      0.0000],\n",
            "        [    -0.0130,      0.0000],\n",
            "        [     0.0104,      0.0000],\n",
            "        [     0.0113,      0.0000],\n",
            "        [     0.0011,      0.0000],\n",
            "        [    -0.0038,      0.0000],\n",
            "        [     0.0092,      0.0000],\n",
            "        [    -0.0108,      0.0000],\n",
            "        [    -0.0157,      0.0000],\n",
            "        [     0.3248,      0.0000],\n",
            "        [     0.0356,      0.0000],\n",
            "        [     0.0028,      0.0000],\n",
            "        [     0.0308,      0.0000],\n",
            "        [     0.0180,      0.0000],\n",
            "        [     0.0020,      0.0000],\n",
            "        [     0.0137,      0.0000],\n",
            "        [     0.0127,      0.0000],\n",
            "        [     0.0247,      0.0000],\n",
            "        [     0.0410,      0.0000],\n",
            "        [     0.0379,      0.0000],\n",
            "        [    22.7811,     27.0300],\n",
            "        [     1.9085,      2.8000],\n",
            "        [     1.6391,      1.0100],\n",
            "        [     1.6901,      3.3600],\n",
            "        [     1.4087,      1.1200],\n",
            "        [     0.0319,      0.0000],\n",
            "        [     1.0904,      1.4900],\n",
            "        [     0.0191,      0.0000],\n",
            "        [     2.0707,      3.1300],\n",
            "        [     1.4703,      1.4900],\n",
            "        [     2.4264,      2.8300],\n",
            "        [     1.4249,      0.7500],\n",
            "        [     2.1808,      1.4100],\n",
            "        [     1.6962,      1.1200],\n",
            "        [     1.2292,      1.8700],\n",
            "        [     1.4755,      1.4900],\n",
            "        [     1.1550,      1.1200],\n",
            "        [     1.5625,      1.5200],\n",
            "        [     1.5442,      0.5100],\n",
            "        [     2.8422,      1.3300],\n",
            "        [    -0.0102,      0.0000],\n",
            "        [     0.1898,      0.0000],\n",
            "        [     0.0197,      0.0000],\n",
            "        [    -0.0009,      0.0000],\n",
            "        [     0.7860,      0.6300],\n",
            "        [     0.1381,      0.0000],\n",
            "        [     0.3951,      0.0000],\n",
            "        [     0.0500,      0.0000],\n",
            "        [     0.8297,      0.7100],\n",
            "        [     0.0370,      0.0000],\n",
            "        [     0.0333,      0.0000],\n",
            "        [     1.7335,      0.0000],\n",
            "        [     0.0196,      0.0000],\n",
            "        [     0.0275,      0.0000],\n",
            "        [     0.0534,      0.0000],\n",
            "        [     0.0059,      0.0000],\n",
            "        [     0.0150,      0.0000],\n",
            "        [     0.0459,      0.0000],\n",
            "        [     0.0016,      0.0000],\n",
            "        [     0.0054,      0.0000],\n",
            "        [     0.0292,      0.0000],\n",
            "        [     0.0127,      0.0000],\n",
            "        [     0.0005,      0.0000],\n",
            "        [     0.0016,      0.0000],\n",
            "        [     0.0333,      0.0000],\n",
            "        [     0.0097,      0.0000],\n",
            "        [     0.1397,      0.0000],\n",
            "        [    -0.0044,      0.0000],\n",
            "        [     0.0124,      0.0000],\n",
            "        [     0.2966,      0.0000],\n",
            "        [     0.9065,      0.0000],\n",
            "        [     0.0190,      0.0000],\n",
            "        [     0.0632,      0.0000],\n",
            "        [     0.0120,      0.0000],\n",
            "        [     0.0576,      0.0000],\n",
            "        [     0.0140,      0.0000],\n",
            "        [     0.0489,      0.0000],\n",
            "        [     0.0403,      0.0000],\n",
            "        [     0.0598,      0.0000],\n",
            "        [     0.0348,      0.0000],\n",
            "        [     0.0629,      0.0000],\n",
            "        [     0.0334,      0.0000],\n",
            "        [     0.0359,      0.0000],\n",
            "        [     0.0388,      0.0000]], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([[    10.8380,     12.6700],\n",
            "        [    -0.0174,      0.0000],\n",
            "        [     0.9326,      1.1200],\n",
            "        [     1.0959,      2.2400],\n",
            "        [     0.0787,      0.0000],\n",
            "        [     0.8295,      0.3700],\n",
            "        [     0.9549,      0.3700],\n",
            "        [     1.4976,      1.4100],\n",
            "        [     1.3792,      0.5600],\n",
            "        [     1.0678,      1.5200],\n",
            "        [     1.6302,      2.8300],\n",
            "        [     1.1855,      2.2400],\n",
            "        [     3.4040,      1.7700],\n",
            "        [     0.6861,      0.5600],\n",
            "        [     0.0727,      0.0000],\n",
            "        [    -0.0211,      0.0000],\n",
            "        [    -0.0607,      0.0000],\n",
            "        [     0.0057,      0.0000],\n",
            "        [    -0.0303,      0.0000],\n",
            "        [    -0.0238,      0.0000],\n",
            "        [    -0.0111,      0.0000],\n",
            "        [     1.2992,      0.7100],\n",
            "        [     0.6792,      0.0000],\n",
            "        [    -0.0228,      0.0000],\n",
            "        [     0.0100,      0.0000],\n",
            "        [    -0.0574,      0.0000],\n",
            "        [    -0.0199,      0.0000],\n",
            "        [     0.5767,      0.5100],\n",
            "        [     0.3180,      0.0000],\n",
            "        [    -0.0111,      0.0000],\n",
            "        [    -0.0083,      0.0000],\n",
            "        [    -0.0116,      0.0000],\n",
            "        [    -0.0065,      0.0000],\n",
            "        [    -0.0116,      0.0000],\n",
            "        [    -0.0102,      0.0000],\n",
            "        [    -0.0012,      0.0000],\n",
            "        [    -0.0107,      0.0000],\n",
            "        [    -0.0035,      0.0000],\n",
            "        [    -0.0017,      0.0000],\n",
            "        [    -0.0039,      0.0000],\n",
            "        [    -0.0064,      0.0000],\n",
            "        [    -0.0108,      0.0000],\n",
            "        [    -0.0062,      0.0000],\n",
            "        [    -0.0074,      0.0000],\n",
            "        [    -0.0108,      0.0000],\n",
            "        [    -0.0072,      0.0000],\n",
            "        [    -0.0116,      0.0000],\n",
            "        [    -0.0097,      0.0000],\n",
            "        [    -0.1198,      0.0000],\n",
            "        [    -0.0383,      0.0000],\n",
            "        [    -0.0115,      0.0000],\n",
            "        [     0.0003,      0.0000],\n",
            "        [    -0.0305,      0.0000],\n",
            "        [    -0.0268,      0.0000],\n",
            "        [    -0.0275,      0.0000],\n",
            "        [    -0.0392,      0.0000],\n",
            "        [    -0.0243,      0.0000],\n",
            "        [    -0.0050,      0.0000],\n",
            "        [    -0.0115,      0.0000],\n",
            "        [    -0.0145,      0.0000],\n",
            "        [    -0.0125,      0.0000],\n",
            "        [    -0.0197,      0.0000],\n",
            "        [    -0.0269,      0.0000],\n",
            "        [    -0.0070,      0.0000],\n",
            "        [    -0.0219,      0.0000],\n",
            "        [    -0.0263,      0.0000],\n",
            "        [    14.5673,     18.2700],\n",
            "        [     1.2997,      1.4900],\n",
            "        [     1.3523,      2.5100],\n",
            "        [     1.1505,      1.4900],\n",
            "        [     1.6249,      1.8800],\n",
            "        [     1.4142,      1.6800],\n",
            "        [     0.9929,      0.5100],\n",
            "        [     1.3649,      1.1200],\n",
            "        [     1.0456,      0.3300],\n",
            "        [     1.1504,      0.7500],\n",
            "        [     1.1331,      0.7500],\n",
            "        [     0.3232,      0.0000],\n",
            "        [     1.7752,      4.2400],\n",
            "        [     0.8809,      1.5200],\n",
            "        [     4.9037,      3.3500],\n",
            "        [     0.0304,      0.0000],\n",
            "        [     0.5410,      0.0000],\n",
            "        [    -0.0005,      0.0000],\n",
            "        [     0.2178,      0.0000],\n",
            "        [     0.0089,      0.0000],\n",
            "        [     0.5584,      0.0000],\n",
            "        [    -0.0065,      0.0000],\n",
            "        [    -0.0197,      0.0000],\n",
            "        [     0.4822,      0.0000],\n",
            "        [    -0.0230,      0.0000],\n",
            "        [     0.0167,      0.0000],\n",
            "        [     0.5990,      0.0000],\n",
            "        [     0.5729,      0.0000],\n",
            "        [    -0.0238,      0.0000],\n",
            "        [    -0.0023,      0.0000],\n",
            "        [     0.8520,      1.1200],\n",
            "        [     1.4974,      0.7100],\n",
            "        [    -0.0143,      0.0000],\n",
            "        [     1.0358,      1.5200],\n",
            "        [     0.7083,      0.0000],\n",
            "        [     6.4980,      1.3300],\n",
            "        [    -0.0455,      0.0000],\n",
            "        [    -0.0107,      0.0000],\n",
            "        [    -0.0642,      0.0000],\n",
            "        [    -0.0424,      0.0000],\n",
            "        [     0.4435,      0.0000],\n",
            "        [     0.2922,      0.0000],\n",
            "        [    -0.0570,      0.0000],\n",
            "        [     0.0379,      0.0000],\n",
            "        [    -0.0652,      0.0000],\n",
            "        [    -0.0400,      0.0000],\n",
            "        [    -0.0370,      0.0000],\n",
            "        [    -0.0637,      0.0000],\n",
            "        [    -0.0694,      0.0000],\n",
            "        [    -0.0388,      0.0000],\n",
            "        [    -0.0188,      0.0000],\n",
            "        [    -0.0365,      0.0000],\n",
            "        [    -0.0371,      0.0000],\n",
            "        [     1.0998,      0.6300],\n",
            "        [     1.5146,      0.7100],\n",
            "        [    -0.0225,      0.0000],\n",
            "        [     1.5615,      0.3700],\n",
            "        [    -0.0106,      0.0000],\n",
            "        [     0.2604,      0.3700],\n",
            "        [     0.0118,      0.0000],\n",
            "        [     0.0015,      0.0000],\n",
            "        [    -0.0082,      0.0000],\n",
            "        [     0.0084,      0.0000],\n",
            "        [    -0.0034,      0.0000],\n",
            "        [     0.0053,      0.0000],\n",
            "        [     0.0068,      0.0000],\n",
            "        [     0.0059,      0.0000],\n",
            "        [    -0.0081,      0.0000]], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([[    27.0619,     50.3500],\n",
            "        [     3.0677,      6.8300],\n",
            "        [     3.2091,      5.1200],\n",
            "        [     3.3250,      4.2700],\n",
            "        [     3.2110,      3.4100],\n",
            "        [     1.0611,      0.0000],\n",
            "        [     3.3275,      4.2700],\n",
            "        [     3.3047,      4.2700],\n",
            "        [     3.7461,      4.2700],\n",
            "        [     3.0148,      0.8500],\n",
            "        [     2.2811,      0.8500],\n",
            "        [     2.2313,      2.5600],\n",
            "        [     3.4700,      5.9700],\n",
            "        [     2.3039,      1.7100],\n",
            "        [     2.3880,      3.4100],\n",
            "        [     1.0818,      0.0000],\n",
            "        [    -0.2786,      0.0000],\n",
            "        [     2.7402,      2.5600],\n",
            "        [     0.2105,      0.0000],\n",
            "        [    51.6748,     59.7300],\n",
            "        [     0.8126,      2.5600],\n",
            "        [     1.6348,      2.5600],\n",
            "        [     1.6559,      2.5600],\n",
            "        [     2.9386,      1.7100],\n",
            "        [     1.7342,      1.7100],\n",
            "        [     3.8835,      5.1200],\n",
            "        [     5.2940,      7.6800],\n",
            "        [     5.0928,      7.6800],\n",
            "        [     2.7811,      0.8500],\n",
            "        [     4.8148,      9.3900],\n",
            "        [     5.9493,      5.1200],\n",
            "        [     0.8903,      0.0000],\n",
            "        [     5.1152,      5.9700],\n",
            "        [     1.5100,      1.7100],\n",
            "        [     5.5178,      0.8500],\n",
            "        [     3.7881,      4.2700],\n",
            "        [    55.2374,     49.5500],\n",
            "        [     4.3121,      3.8000],\n",
            "        [     5.3202,      1.7100],\n",
            "        [     4.9630,      5.1200],\n",
            "        [     5.3915,      6.8300],\n",
            "        [     4.3543,      2.2800],\n",
            "        [     2.3057,      1.5200],\n",
            "        [     5.1214,      3.4100],\n",
            "        [     0.7387,      0.0000],\n",
            "        [     4.7975,      0.7600],\n",
            "        [     4.8232,      4.5600],\n",
            "        [     3.8929,      3.3300],\n",
            "        [     3.7608,      0.7600],\n",
            "        [     4.6511,      2.2800],\n",
            "        [     4.8433,      4.5600],\n",
            "        [     2.6225,      1.0100],\n",
            "        [     2.3669,      5.3300],\n",
            "        [     4.2970,      2.2800],\n",
            "        [    93.2379,     72.5300],\n",
            "        [     7.0741,      4.2700],\n",
            "        [     4.5507,      2.5600],\n",
            "        [     5.0276,      1.7100],\n",
            "        [     6.4530,      3.4100],\n",
            "        [     6.0402,      2.5600],\n",
            "        [     3.6208,      5.9700],\n",
            "        [     6.0052,      5.1200],\n",
            "        [     6.0397,      5.1200],\n",
            "        [     2.7105,      3.4100],\n",
            "        [     7.3198,      8.5300],\n",
            "        [     6.4101,      7.6800],\n",
            "        [     6.1500,      0.8500],\n",
            "        [     2.9792,      3.4100],\n",
            "        [     6.7091,      1.7100],\n",
            "        [     5.9665,      3.4100],\n",
            "        [     1.3753,      0.0000],\n",
            "        [     6.4959,      3.4100],\n",
            "        [     7.2105,      4.2700],\n",
            "        [     6.7808,      3.4100],\n",
            "        [     5.2459,      1.7100],\n",
            "        [    10.5036,     10.2400],\n",
            "        [    -0.0465,      0.0000],\n",
            "        [     0.9801,      0.8500],\n",
            "        [     1.0766,      0.0000],\n",
            "        [     0.0316,      0.0000],\n",
            "        [     0.9133,      1.7100],\n",
            "        [     2.1717,      0.8500],\n",
            "        [    -0.0505,      0.0000],\n",
            "        [    -0.0203,      0.0000],\n",
            "        [     2.5462,      4.2700],\n",
            "        [     1.0006,      0.0000],\n",
            "        [     0.4622,      0.8500],\n",
            "        [    -0.2632,      0.0000],\n",
            "        [     0.7340,      1.7100],\n",
            "        [    47.6597,     62.2900],\n",
            "        [     3.7587,      2.5600],\n",
            "        [     3.2931,      3.4100],\n",
            "        [     2.7261,      1.7100],\n",
            "        [     2.6086,      4.2700],\n",
            "        [     2.1240,      7.6800],\n",
            "        [     4.4643,      2.5600],\n",
            "        [     1.8345,      4.2700],\n",
            "        [     2.3048,      1.7100],\n",
            "        [     4.3275,      8.5300],\n",
            "        [     1.9775,      5.9700],\n",
            "        [     4.7446,      3.4100],\n",
            "        [     5.8471,      6.8300],\n",
            "        [     5.7153,      9.3900],\n",
            "        [    49.0580,     48.0000],\n",
            "        [     4.0051,      3.0400],\n",
            "        [     2.9946,      3.3300],\n",
            "        [     2.0243,      2.5300],\n",
            "        [     4.3825,      4.2700],\n",
            "        [     1.1426,      4.0500],\n",
            "        [     4.6137,      5.1200],\n",
            "        [     3.1307,      3.0400],\n",
            "        [     4.5253,      4.2700],\n",
            "        [     3.1119,      2.5300],\n",
            "        [     4.8722,     10.2400],\n",
            "        [     2.3046,      5.5700],\n",
            "        [    90.7211,     72.5300],\n",
            "        [     8.0679,      6.8300],\n",
            "        [     4.5247,      1.7100],\n",
            "        [     5.7351,      3.4100],\n",
            "        [     7.0888,      3.4100],\n",
            "        [     7.1824,      2.5600],\n",
            "        [     5.1174,      2.5600],\n",
            "        [     7.4069,      6.8300],\n",
            "        [     8.2663,      7.6800],\n",
            "        [     6.4957,      1.7100],\n",
            "        [     3.9696,      5.9700],\n",
            "        [     6.0091,      5.1200],\n",
            "        [     5.5253,      2.5600],\n",
            "        [     7.0146,      2.5600],\n",
            "        [     6.6265,      4.2700],\n",
            "        [     6.9399,      1.7100],\n",
            "        [     7.7972,      2.5600],\n",
            "        [     2.9774,      0.8500],\n",
            "        [     5.1849,      0.8500],\n",
            "        [     6.7531,      5.9700],\n",
            "        [     6.4243,      3.4100],\n",
            "        [    87.9370,     55.4700],\n",
            "        [     4.0553,      0.8500],\n",
            "        [     7.8233,      7.6800],\n",
            "        [     4.2102,      3.4100],\n",
            "        [     8.1635,      5.1200],\n",
            "        [     1.0347,      0.0000],\n",
            "        [     8.1077,      1.7100],\n",
            "        [     3.4734,      1.7100],\n",
            "        [     4.3230,      3.4100],\n",
            "        [     7.5587,      5.9700],\n",
            "        [     7.0720,      7.6800],\n",
            "        [     8.8082,      2.5600],\n",
            "        [    10.0794,      2.5600],\n",
            "        [     4.0321,      4.2700],\n",
            "        [     3.2193,      0.0000],\n",
            "        [     5.9113,      4.2700],\n",
            "        [     8.9549,      4.2700],\n",
            "        [    94.0947,     94.2300],\n",
            "        [     7.1913,      7.6000],\n",
            "        [     8.9609,      8.5300],\n",
            "        [     7.5749,      1.7100],\n",
            "        [     4.6209,      5.5700],\n",
            "        [     5.5948,      3.8000],\n",
            "        [     3.9896,      0.6700],\n",
            "        [     5.2841,      4.0000],\n",
            "        [     8.7253,     11.9500],\n",
            "        [     4.0337,      4.5600],\n",
            "        [     5.5665,      3.0400],\n",
            "        [     6.0491,      2.2800],\n",
            "        [     9.6817,      9.3900],\n",
            "        [     9.7943,      6.8300],\n",
            "        [     7.4601,      5.1200],\n",
            "        [     3.8915,      4.6700],\n",
            "        [     4.6157,      6.8400],\n",
            "        [     9.4530,      7.6800]], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([[    38.5651,     34.0300],\n",
            "        [     2.5538,      1.4900],\n",
            "        [     1.7741,      1.4900],\n",
            "        [     1.7669,      2.0000],\n",
            "        [     1.9680,      1.4900],\n",
            "        [     1.9904,      1.1200],\n",
            "        [     2.5569,      3.0400],\n",
            "        [     2.0672,      1.4900],\n",
            "        [     1.8132,      2.6100],\n",
            "        [     1.8649,      1.6700],\n",
            "        [     1.9285,      0.7500],\n",
            "        [     2.6981,      2.0300],\n",
            "        [     4.4179,      0.6300],\n",
            "        [     2.8579,      3.3600],\n",
            "        [     2.0072,      1.8700],\n",
            "        [     3.0144,      1.5200],\n",
            "        [     2.1115,      1.4900],\n",
            "        [     3.2736,      2.2400],\n",
            "        [     2.3294,      1.4900],\n",
            "        [     1.5396,      2.2400],\n",
            "        [    11.1491,     12.5600],\n",
            "        [     1.6267,      1.1200],\n",
            "        [     0.0100,      0.0000],\n",
            "        [     0.9836,      1.4900],\n",
            "        [     0.7399,      0.0000],\n",
            "        [     0.0209,      0.0000],\n",
            "        [     0.7961,      0.0000],\n",
            "        [     1.6457,      1.6800],\n",
            "        [     1.6737,      1.6800],\n",
            "        [     1.8619,      1.8800],\n",
            "        [     1.7801,      1.8800],\n",
            "        [     1.2167,      1.5200],\n",
            "        [     0.0362,      0.0000],\n",
            "        [     1.0763,      0.7500],\n",
            "        [     1.1748,      0.5600],\n",
            "        [    15.7126,     14.5200],\n",
            "        [     1.2069,      1.1200],\n",
            "        [     1.8664,      1.8800],\n",
            "        [     1.4195,      0.5600],\n",
            "        [     2.2797,      0.7100],\n",
            "        [     2.0449,      1.4100],\n",
            "        [     1.6672,      1.1200],\n",
            "        [     0.5804,      0.0000],\n",
            "        [     2.1405,      1.4100],\n",
            "        [     0.7671,      0.3700],\n",
            "        [     0.1261,      0.0000],\n",
            "        [     0.8122,      2.9900],\n",
            "        [     0.8913,      0.3700],\n",
            "        [     0.9460,      0.0000],\n",
            "        [     0.0411,      0.0000],\n",
            "        [     0.7751,      0.0000],\n",
            "        [     1.0561,      0.0000],\n",
            "        [     0.0387,      0.0000],\n",
            "        [     2.2872,      0.7100],\n",
            "        [     0.8351,      1.8700],\n",
            "        [    19.2297,     24.6400],\n",
            "        [     1.7604,      2.2400],\n",
            "        [     2.0279,      5.5700],\n",
            "        [     1.3978,      2.2400],\n",
            "        [     2.1672,      5.0700],\n",
            "        [     2.1484,      2.0300],\n",
            "        [     0.9027,      1.1200],\n",
            "        [     2.0488,      1.1200],\n",
            "        [     1.3467,      1.5200],\n",
            "        [     1.9154,      2.9900],\n",
            "        [     1.2284,      0.7500],\n",
            "        [     6.9496,      9.2000],\n",
            "        [     1.8334,      1.4100],\n",
            "        [     0.3591,      0.0000],\n",
            "        [     1.4176,      0.5600],\n",
            "        [     0.5643,      0.3300],\n",
            "        [     1.6215,      0.7100],\n",
            "        [     0.9649,      1.0100],\n",
            "        [     0.9331,      3.0400],\n",
            "        [     0.7423,      0.0000],\n",
            "        [     0.8706,      0.5100],\n",
            "        [    -0.0106,      0.0000],\n",
            "        [     0.2045,      0.0000],\n",
            "        [    -0.0223,      0.0000],\n",
            "        [     0.9041,      1.1200],\n",
            "        [     0.7341,      0.5100],\n",
            "        [     3.3973,      2.6000],\n",
            "        [    -0.0029,      0.0000],\n",
            "        [    -0.0026,      0.0000],\n",
            "        [    -0.0138,      0.0000],\n",
            "        [    -0.0025,      0.0000],\n",
            "        [    -0.0101,      0.0000],\n",
            "        [     0.0029,      0.0000],\n",
            "        [     0.0168,      0.0000],\n",
            "        [     0.0028,      0.0000],\n",
            "        [     1.0526,      0.6300],\n",
            "        [    -0.0052,      0.0000],\n",
            "        [     0.0271,      0.0000],\n",
            "        [     1.7300,      0.7100],\n",
            "        [    -0.0237,      0.0000],\n",
            "        [     0.8718,      0.5600],\n",
            "        [     0.2043,      0.0000],\n",
            "        [    -0.0082,      0.0000],\n",
            "        [     1.3541,      0.7100],\n",
            "        [    -0.0125,      0.0000],\n",
            "        [     0.0046,      0.0000],\n",
            "        [    -0.0740,      0.0000],\n",
            "        [     0.0254,      0.0000],\n",
            "        [     0.0241,      0.0000],\n",
            "        [     0.0288,      0.0000],\n",
            "        [     0.0223,      0.0000],\n",
            "        [     0.0313,      0.0000],\n",
            "        [     0.0625,      0.0000],\n",
            "        [     0.0306,      0.0000],\n",
            "        [     0.0184,      0.0000],\n",
            "        [     0.0308,      0.0000],\n",
            "        [     0.0185,      0.0000],\n",
            "        [     0.0245,      0.0000],\n",
            "        [     0.0191,      0.0000],\n",
            "        [     0.0276,      0.0000],\n",
            "        [     0.0187,      0.0000],\n",
            "        [     0.0219,      0.0000],\n",
            "        [     0.0215,      0.0000],\n",
            "        [     5.6818,      8.1500],\n",
            "        [     0.4238,      0.7500],\n",
            "        [     0.5501,      0.0000],\n",
            "        [     1.4700,      1.4100],\n",
            "        [     0.0152,      0.0000],\n",
            "        [     1.6076,      1.4100],\n",
            "        [     1.1424,      1.1200],\n",
            "        [     0.6200,      0.0000],\n",
            "        [     1.4149,      2.1200],\n",
            "        [     0.4364,      0.6700],\n",
            "        [     0.3766,      0.6700],\n",
            "        [    13.5085,     16.8100],\n",
            "        [     0.8670,      0.0000],\n",
            "        [     1.7869,      0.5600],\n",
            "        [     2.0581,      1.8800],\n",
            "        [     0.8978,      0.3700],\n",
            "        [     0.4416,      0.0000],\n",
            "        [     0.9047,      0.0000],\n",
            "        [     2.0279,      2.5100],\n",
            "        [     0.8824,      0.0000],\n",
            "        [     1.1377,      0.7500],\n",
            "        [     1.3048,      2.5300],\n",
            "        [    -0.1368,      0.0000],\n",
            "        [     1.6869,      2.5100],\n",
            "        [     1.9779,      2.5100],\n",
            "        [     1.2746,      0.0000],\n",
            "        [     1.2574,      1.5200],\n",
            "        [    -0.1619,      0.0000],\n",
            "        [     1.6168,      0.5600],\n",
            "        [     0.5993,      1.1200],\n",
            "        [     0.7068,      0.0000],\n",
            "        [    -0.0801,      0.0000],\n",
            "        [    -0.0226,      0.0000],\n",
            "        [    -0.0441,      0.0000],\n",
            "        [    -0.0010,      0.0000],\n",
            "        [     0.0005,      0.0000],\n",
            "        [    -0.0265,      0.0000],\n",
            "        [     0.0022,      0.0000],\n",
            "        [    -0.0092,      0.0000],\n",
            "        [     0.0128,      0.0000],\n",
            "        [    -0.0158,      0.0000],\n",
            "        [    -0.0443,      0.0000],\n",
            "        [     5.3458,      4.4300],\n",
            "        [     1.4716,      1.2500],\n",
            "        [    -0.0088,      0.0000],\n",
            "        [     0.4274,      0.0000],\n",
            "        [     0.8817,      0.5100],\n",
            "        [     0.3226,      0.0000],\n",
            "        [     1.4441,      1.4100],\n",
            "        [    -0.0813,      0.0000],\n",
            "        [     1.4471,      1.2500],\n",
            "        [     0.7504,      0.0000],\n",
            "        [    -0.0179,      0.0000],\n",
            "        [    22.5081,     27.6000],\n",
            "        [     1.8100,      2.2400],\n",
            "        [     1.6280,      0.7500],\n",
            "        [     1.4487,      2.2400],\n",
            "        [     0.3790,      0.0000],\n",
            "        [     1.8374,      1.4900],\n",
            "        [     2.0442,      2.0300],\n",
            "        [     1.5621,      1.0100],\n",
            "        [     3.1866,      7.0700],\n",
            "        [     1.3325,      1.4900],\n",
            "        [     1.4714,      2.9900],\n",
            "        [     2.3089,      2.2400],\n",
            "        [     2.0607,      4.0500]], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([[     3.6194,      3.2700],\n",
            "        [     0.0005,      0.0000],\n",
            "        [    -0.0221,      0.0000],\n",
            "        [    -0.0187,      0.0000],\n",
            "        [     0.4558,      0.0000],\n",
            "        [     0.3274,      0.3700],\n",
            "        [     1.3470,      0.7100],\n",
            "        [    -0.0135,      0.0000],\n",
            "        [     0.2288,      0.0000],\n",
            "        [     0.2655,      0.0000],\n",
            "        [    -0.0139,      0.0000],\n",
            "        [     0.2333,      0.3700],\n",
            "        [     0.6013,      0.5600],\n",
            "        [     0.4971,      0.5100],\n",
            "        [    -0.0244,      0.0000],\n",
            "        [     0.3919,      0.3700],\n",
            "        [    -0.0161,      0.0000],\n",
            "        [    -0.0257,      0.0000],\n",
            "        [     0.0046,      0.0000],\n",
            "        [     0.3034,      0.3700],\n",
            "        [    -0.0248,      0.0000],\n",
            "        [    -0.0092,      0.0000],\n",
            "        [    -0.0275,      0.0000],\n",
            "        [    -0.0235,      0.0000],\n",
            "        [    -0.0319,      0.0000],\n",
            "        [    -0.0267,      0.0000],\n",
            "        [    -0.0121,      0.0000],\n",
            "        [    -0.0173,      0.0000],\n",
            "        [    -0.0291,      0.0000],\n",
            "        [    -0.0206,      0.0000],\n",
            "        [    -0.0119,      0.0000],\n",
            "        [    -0.0050,      0.0000],\n",
            "        [    -0.0209,      0.0000],\n",
            "        [    -0.0233,      0.0000],\n",
            "        [    -0.0188,      0.0000],\n",
            "        [    -0.0063,      0.0000],\n",
            "        [    -0.0187,      0.0000],\n",
            "        [    -0.0249,      0.0000],\n",
            "        [    -0.0139,      0.0000],\n",
            "        [    -0.0280,      0.0000],\n",
            "        [    -0.1078,      0.0000],\n",
            "        [    -0.0411,      0.0000],\n",
            "        [    -0.0275,      0.0000],\n",
            "        [    -0.0397,      0.0000],\n",
            "        [    -0.0256,      0.0000],\n",
            "        [    -0.0454,      0.0000],\n",
            "        [    -0.0351,      0.0000],\n",
            "        [    -0.0292,      0.0000],\n",
            "        [    -0.0232,      0.0000],\n",
            "        [    -0.0422,      0.0000],\n",
            "        [    -0.0280,      0.0000],\n",
            "        [    -0.0349,      0.0000],\n",
            "        [    -0.0117,      0.0000],\n",
            "        [    -0.0479,      0.0000],\n",
            "        [    -0.4116,      0.0000],\n",
            "        [    -0.0152,      0.0000],\n",
            "        [    -0.0320,      0.0000],\n",
            "        [    -0.0282,      0.0000],\n",
            "        [    -0.0234,      0.0000],\n",
            "        [    -0.0367,      0.0000],\n",
            "        [    -0.0485,      0.0000],\n",
            "        [    -0.0152,      0.0000],\n",
            "        [    -0.0150,      0.0000],\n",
            "        [    -0.0198,      0.0000],\n",
            "        [    -0.0240,      0.0000],\n",
            "        [    -0.0097,      0.0000],\n",
            "        [    -0.0370,      0.0000],\n",
            "        [    -0.0394,      0.0000],\n",
            "        [    -0.0230,      0.0000],\n",
            "        [     4.5771,      4.3500],\n",
            "        [     0.0160,      0.0000],\n",
            "        [     0.9183,      0.0000],\n",
            "        [     0.5657,      0.3700],\n",
            "        [     0.0920,      0.0000],\n",
            "        [    -0.0009,      0.0000],\n",
            "        [    -0.0184,      0.0000],\n",
            "        [    -0.0190,      0.0000],\n",
            "        [     0.5604,      0.7500],\n",
            "        [     0.7756,      0.5100],\n",
            "        [     0.2384,      0.0000],\n",
            "        [    -0.0336,      0.0000],\n",
            "        [     0.4346,      0.0000],\n",
            "        [     0.3221,      0.3700],\n",
            "        [     1.3733,      0.7100],\n",
            "        [    -0.0302,      0.0000],\n",
            "        [     0.8379,      1.0100],\n",
            "        [     1.1813,      0.6300],\n",
            "        [     0.0732,      0.0000],\n",
            "        [    -0.0044,      0.0000],\n",
            "        [    -0.0023,      0.0000],\n",
            "        [    -0.0152,      0.0000],\n",
            "        [    -0.0031,      0.0000],\n",
            "        [    -0.0053,      0.0000],\n",
            "        [    -0.0054,      0.0000],\n",
            "        [    -0.0099,      0.0000],\n",
            "        [    -0.0002,      0.0000],\n",
            "        [     0.0019,      0.0000],\n",
            "        [     0.0024,      0.0000],\n",
            "        [     0.0042,      0.0000],\n",
            "        [     0.0101,      0.0000],\n",
            "        [    -0.0004,      0.0000],\n",
            "        [     0.0042,      0.0000],\n",
            "        [    -0.0039,      0.0000],\n",
            "        [     0.0147,      0.0000],\n",
            "        [     0.0009,      0.0000],\n",
            "        [    -0.0522,      0.0000],\n",
            "        [    -0.0088,      0.0000],\n",
            "        [     0.0070,      0.0000],\n",
            "        [    -0.0050,      0.0000],\n",
            "        [    -0.0049,      0.0000],\n",
            "        [     0.0063,      0.0000],\n",
            "        [    -0.0085,      0.0000],\n",
            "        [     0.0102,      0.0000],\n",
            "        [    -0.0047,      0.0000],\n",
            "        [    -0.0056,      0.0000],\n",
            "        [    -0.0115,      0.0000],\n",
            "        [     1.5086,      0.0000],\n",
            "        [     0.0043,      0.0000],\n",
            "        [    -0.0116,      0.0000],\n",
            "        [    -0.0381,      0.0000],\n",
            "        [    -0.0221,      0.0000],\n",
            "        [    -0.0179,      0.0000],\n",
            "        [    -0.0217,      0.0000],\n",
            "        [    -0.0249,      0.0000],\n",
            "        [    -0.0134,      0.0000],\n",
            "        [    -0.0128,      0.0000],\n",
            "        [    -0.0601,      0.0000],\n",
            "        [    -0.0270,      0.0000],\n",
            "        [     0.0060,      0.0000],\n",
            "        [     0.0101,      0.0000],\n",
            "        [    -0.0094,      0.0000],\n",
            "        [     0.2172,      0.0000],\n",
            "        [    -0.0047,      0.0000],\n",
            "        [    -0.0097,      0.0000],\n",
            "        [    -0.0030,      0.0000],\n",
            "        [    -0.0011,      0.0000],\n",
            "        [    -0.0343,      0.0000],\n",
            "        [    20.6681,     20.7300],\n",
            "        [    -0.0076,      0.0000],\n",
            "        [     1.2741,      0.7500],\n",
            "        [     1.8470,      1.8800],\n",
            "        [     1.1445,      1.0100],\n",
            "        [     1.5501,      1.6800],\n",
            "        [    -0.0176,      0.0000],\n",
            "        [     1.2993,      2.0300],\n",
            "        [     0.8150,      0.7500],\n",
            "        [     1.7665,      1.1200],\n",
            "        [     1.2148,      0.5100],\n",
            "        [     1.2616,      1.1200],\n",
            "        [    -0.0239,      0.0000],\n",
            "        [     1.4854,      2.2400],\n",
            "        [     0.9644,      1.1200],\n",
            "        [     1.6912,      1.1200],\n",
            "        [     1.7025,      1.1200],\n",
            "        [     1.0582,      1.1200],\n",
            "        [     1.8883,      1.1200],\n",
            "        [     1.9354,      1.6800],\n",
            "        [     0.9863,      0.3700],\n",
            "        [     5.0078,      5.0300],\n",
            "        [    -0.0836,      0.0000],\n",
            "        [     0.0019,      0.0000],\n",
            "        [     0.8285,      0.0000],\n",
            "        [     1.6482,      1.4100],\n",
            "        [     1.2628,      0.7100],\n",
            "        [     0.8097,      0.0000],\n",
            "        [     1.0832,      0.0000],\n",
            "        [    -0.0750,      0.0000],\n",
            "        [    -0.0786,      0.0000],\n",
            "        [     1.7566,      0.7100],\n",
            "        [     0.0114,      0.0000],\n",
            "        [    -0.0045,      0.0000],\n",
            "        [    -0.0023,      0.0000],\n",
            "        [     0.9211,      1.1200],\n",
            "        [     0.8054,      0.3700],\n",
            "        [     1.1775,      0.7100]], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([[    10.7273,      3.9600],\n",
            "        [     1.2072,      1.0100],\n",
            "        [     1.0245,      0.0000],\n",
            "        [     0.0137,      0.0000],\n",
            "        [     0.0648,      0.0000],\n",
            "        [     0.6676,      0.0000],\n",
            "        [     0.0048,      0.0000],\n",
            "        [     0.0230,      0.0000],\n",
            "        [    -0.0077,      0.0000],\n",
            "        [     0.2908,      0.0000],\n",
            "        [     0.9629,      1.1200],\n",
            "        [     0.8117,      1.1200],\n",
            "        [     0.0035,      0.0000],\n",
            "        [     1.9509,      0.7100],\n",
            "        [     0.8971,      0.0000],\n",
            "        [     0.4997,      0.0000],\n",
            "        [     0.0733,      0.0000],\n",
            "        [     0.7810,      0.0000],\n",
            "        [    -0.0054,      0.0000],\n",
            "        [    -0.0129,      0.0000],\n",
            "        [     0.0021,      0.0000],\n",
            "        [    -0.0215,      0.0000],\n",
            "        [    -0.0165,      0.0000],\n",
            "        [    -0.0146,      0.0000],\n",
            "        [    -0.0301,      0.0000],\n",
            "        [    -0.0212,      0.0000],\n",
            "        [    -0.0273,      0.0000],\n",
            "        [    -0.0217,      0.0000],\n",
            "        [    -0.0184,      0.0000],\n",
            "        [    -0.0136,      0.0000],\n",
            "        [    -0.0180,      0.0000],\n",
            "        [    -0.0194,      0.0000],\n",
            "        [    -0.0290,      0.0000],\n",
            "        [    -0.0263,      0.0000],\n",
            "        [    -0.0122,      0.0000],\n",
            "        [    -0.0156,      0.0000],\n",
            "        [    -0.0111,      0.0000],\n",
            "        [    -0.0240,      0.0000],\n",
            "        [     4.2404,      3.4300],\n",
            "        [    -0.0207,      0.0000],\n",
            "        [    -0.0007,      0.0000],\n",
            "        [    -0.0135,      0.0000],\n",
            "        [    -0.0280,      0.0000],\n",
            "        [    -0.0272,      0.0000],\n",
            "        [     0.3213,      0.7500],\n",
            "        [     0.8289,      0.5600],\n",
            "        [    -0.0319,      0.0000],\n",
            "        [    -0.0135,      0.0000],\n",
            "        [    -0.0326,      0.0000],\n",
            "        [    -0.0333,      0.0000],\n",
            "        [     0.2736,      0.0000],\n",
            "        [    -0.0157,      0.0000],\n",
            "        [     0.0079,      0.0000],\n",
            "        [    -0.0257,      0.0000],\n",
            "        [     0.4194,      0.0000],\n",
            "        [     1.2097,      1.4100],\n",
            "        [     1.2191,      0.7100],\n",
            "        [    10.6203,     12.1300],\n",
            "        [     0.7265,      0.3700],\n",
            "        [    -0.0308,      0.0000],\n",
            "        [     1.5736,      2.1200],\n",
            "        [    -0.0259,      0.0000],\n",
            "        [     1.0046,      1.4900],\n",
            "        [     1.4476,      0.7100],\n",
            "        [     0.0030,      0.0000],\n",
            "        [     1.6405,      1.4100],\n",
            "        [     0.9924,      1.4900],\n",
            "        [    -0.0134,      0.0000],\n",
            "        [     0.7866,      0.3700],\n",
            "        [     1.0471,      0.5100],\n",
            "        [     1.7765,      0.7100],\n",
            "        [     1.2285,      1.1200],\n",
            "        [     0.8320,      1.1200],\n",
            "        [     1.7082,      0.7100],\n",
            "        [     9.7458,      9.6400],\n",
            "        [     0.7005,      0.7500],\n",
            "        [    -0.0251,      0.0000],\n",
            "        [    -0.0426,      0.0000],\n",
            "        [     1.5132,      2.5100],\n",
            "        [    -0.0116,      0.0000],\n",
            "        [     1.0359,      0.5600],\n",
            "        [    -0.0133,      0.0000],\n",
            "        [     0.9761,      1.1200],\n",
            "        [    -0.0132,      0.0000],\n",
            "        [     0.6956,      1.0100],\n",
            "        [    -0.0129,      0.0000],\n",
            "        [     0.1925,      0.0000],\n",
            "        [    -0.0482,      0.0000],\n",
            "        [     0.3661,      0.3700],\n",
            "        [     1.4392,      0.7100],\n",
            "        [     0.5423,      0.3700],\n",
            "        [     0.4703,      0.7500],\n",
            "        [     0.3901,      1.4900],\n",
            "        [     1.4697,      0.5600],\n",
            "        [    -0.0244,      0.0000],\n",
            "        [     0.3896,      0.5600],\n",
            "        [    -0.0158,      0.0000],\n",
            "        [     0.0034,      0.0000],\n",
            "        [     0.0406,      0.0000],\n",
            "        [     0.0435,      0.0000],\n",
            "        [    -0.0314,      0.0000],\n",
            "        [    -0.0152,      0.0000],\n",
            "        [     0.5506,      0.0000],\n",
            "        [    -0.0165,      0.0000],\n",
            "        [     0.0852,      0.0000],\n",
            "        [    -0.0153,      0.0000],\n",
            "        [    -0.0348,      0.0000],\n",
            "        [    -0.0208,      0.0000],\n",
            "        [    -0.0326,      0.0000],\n",
            "        [    -0.0096,      0.0000],\n",
            "        [    -0.0249,      0.0000],\n",
            "        [    -0.0304,      0.0000],\n",
            "        [    -0.0214,      0.0000],\n",
            "        [    -0.0361,      0.0000],\n",
            "        [    -0.0064,      0.0000],\n",
            "        [    -0.0282,      0.0000],\n",
            "        [    -0.0298,      0.0000],\n",
            "        [    -0.0316,      0.0000],\n",
            "        [    -0.0219,      0.0000],\n",
            "        [    -0.0208,      0.0000],\n",
            "        [    -0.0120,      0.0000],\n",
            "        [    -0.0182,      0.0000],\n",
            "        [    11.7267,     10.1900],\n",
            "        [     1.5825,      0.0000],\n",
            "        [     0.5183,      0.0000],\n",
            "        [     0.7216,      0.3700],\n",
            "        [     1.8049,      0.6300],\n",
            "        [     2.1046,      4.2400],\n",
            "        [    -0.0587,      0.0000],\n",
            "        [     1.5394,      0.7100],\n",
            "        [     0.8079,      0.7500],\n",
            "        [     1.3896,      0.5600],\n",
            "        [    -0.0073,      0.0000],\n",
            "        [     1.0899,      0.5100],\n",
            "        [    -0.0170,      0.0000],\n",
            "        [     0.9660,      0.7500],\n",
            "        [    -0.0736,      0.0000],\n",
            "        [    -0.0156,      0.0000],\n",
            "        [     0.7770,      1.1200],\n",
            "        [     1.6782,      0.5600],\n",
            "        [    34.4047,     38.7100],\n",
            "        [     1.8563,      3.0400],\n",
            "        [     4.1358,      6.3600],\n",
            "        [     1.3796,      0.7500],\n",
            "        [     4.3576,      7.0700],\n",
            "        [     3.2861,      6.1600],\n",
            "        [     2.7364,      2.2400],\n",
            "        [     2.6174,      1.0100],\n",
            "        [     2.7250,      5.0400],\n",
            "        [     2.7654,      2.0300],\n",
            "        [     3.4593,      5.0100],\n",
            "        [     7.6100,      7.3100],\n",
            "        [     1.1828,      0.5600],\n",
            "        [    -0.0357,      0.0000],\n",
            "        [    -0.0103,      0.0000],\n",
            "        [     0.1040,      0.0000],\n",
            "        [    -0.0426,      0.0000],\n",
            "        [     0.2768,      0.0000],\n",
            "        [     0.6615,      0.3700],\n",
            "        [    -0.0230,      0.0000],\n",
            "        [    -0.0282,      0.0000],\n",
            "        [    -0.0396,      0.0000],\n",
            "        [    -0.0335,      0.0000],\n",
            "        [    -0.0206,      0.0000],\n",
            "        [     0.8718,      2.0300],\n",
            "        [     0.9584,      0.5600],\n",
            "        [    -0.0256,      0.0000],\n",
            "        [     0.4440,      1.0100],\n",
            "        [     0.9996,      1.0100],\n",
            "        [     0.6757,      0.7500],\n",
            "        [     0.7316,      1.0100]], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([[    71.5245,     76.0400],\n",
            "        [     5.9126,      5.1200],\n",
            "        [     4.6959,      6.0000],\n",
            "        [     5.8495,      7.6000],\n",
            "        [     3.1003,      0.6700],\n",
            "        [     3.7315,      0.7600],\n",
            "        [     3.2752,      2.2800],\n",
            "        [     2.2450,      0.0000],\n",
            "        [     5.9931,      4.5600],\n",
            "        [     6.0319,      7.6800],\n",
            "        [     6.1566,     12.8000],\n",
            "        [     3.6033,      4.5600],\n",
            "        [     5.2437,      0.7600],\n",
            "        [     5.2970,      8.3600],\n",
            "        [     6.4545,     11.0900],\n",
            "        [     4.9960,      3.8000],\n",
            "        [    44.8923,     44.2000],\n",
            "        [     0.5942,      0.5100],\n",
            "        [     3.2830,      0.7600],\n",
            "        [     2.3802,      0.7600],\n",
            "        [     3.9369,      6.0800],\n",
            "        [     3.7318,      5.3200],\n",
            "        [     4.8800,      3.4100],\n",
            "        [     4.1921,      3.4100],\n",
            "        [     3.3643,      3.8000],\n",
            "        [     3.4759,      1.5200],\n",
            "        [     3.6976,      3.0400],\n",
            "        [     3.3439,      3.0400],\n",
            "        [     2.5565,      2.0300],\n",
            "        [     3.6087,      5.3200],\n",
            "        [     3.0975,      2.0300],\n",
            "        [     1.8821,      2.6700],\n",
            "        [     2.7952,      0.5100],\n",
            "        [    14.7366,     10.2400],\n",
            "        [    -0.0349,      0.0000],\n",
            "        [     1.9867,      1.7100],\n",
            "        [     1.9468,      0.8500],\n",
            "        [    -0.1658,      0.0000],\n",
            "        [     1.9668,      0.8500],\n",
            "        [    -0.0906,      0.0000],\n",
            "        [     0.0478,      0.0000],\n",
            "        [    -0.0903,      0.0000],\n",
            "        [    -0.0729,      0.0000],\n",
            "        [     2.0373,      0.8500],\n",
            "        [    -0.1110,      0.0000],\n",
            "        [     2.3561,      0.8500],\n",
            "        [     1.3378,      0.8500],\n",
            "        [     1.8345,      0.8500],\n",
            "        [    -0.0749,      0.0000],\n",
            "        [     1.6599,      0.8500],\n",
            "        [    -0.0500,      0.0000],\n",
            "        [     1.9124,      1.7100],\n",
            "        [     0.8862,      0.8500],\n",
            "        [     0.5424,      0.0000],\n",
            "        [   147.8705,    151.8900],\n",
            "        [     8.5129,     11.9500],\n",
            "        [     9.6037,     11.9500],\n",
            "        [    10.8188,      6.8300],\n",
            "        [    11.0364,     13.6500],\n",
            "        [    10.5930,     11.9500],\n",
            "        [     6.8576,      9.3900],\n",
            "        [     8.9905,     11.9500],\n",
            "        [     9.1330,      7.6800],\n",
            "        [     8.4200,      6.8300],\n",
            "        [     8.3035,     11.0900],\n",
            "        [     5.7821,     11.0900],\n",
            "        [    10.9963,     18.7700],\n",
            "        [     8.1541,      6.8300],\n",
            "        [    10.2358,     11.9500],\n",
            "        [   110.6935,    113.9700],\n",
            "        [     7.7879,     18.6800],\n",
            "        [     6.8360,      4.2700],\n",
            "        [     8.4407,     12.8000],\n",
            "        [     4.4349,      9.2900],\n",
            "        [     7.1378,      9.3900],\n",
            "        [     3.7345,     10.2400],\n",
            "        [     6.9122,      3.4100],\n",
            "        [     7.6615,     14.3200],\n",
            "        [     8.1101,      5.9700],\n",
            "        [     7.8842,      5.1200],\n",
            "        [     8.4217,     13.6500],\n",
            "        [     8.4570,      6.8300],\n",
            "        [    10.6168,     10.2400],\n",
            "        [     1.7215,      0.8500],\n",
            "        [     2.0293,      3.4100],\n",
            "        [     0.2973,      0.0000],\n",
            "        [     0.1481,      0.0000],\n",
            "        [     0.1619,      0.8500],\n",
            "        [     1.3670,      0.8500],\n",
            "        [     0.2443,      0.0000],\n",
            "        [     2.3072,      0.8500],\n",
            "        [    -0.0217,      0.0000],\n",
            "        [     2.0552,      2.5600],\n",
            "        [    -0.0641,      0.0000],\n",
            "        [    -0.0181,      0.0000],\n",
            "        [     0.3253,      0.8500],\n",
            "        [     0.3869,      0.0000],\n",
            "        [    -0.0673,      0.0000],\n",
            "        [    13.4804,      8.5300],\n",
            "        [     0.5521,      1.7100],\n",
            "        [     0.0019,      0.0000],\n",
            "        [     2.9988,      2.5600],\n",
            "        [     1.3309,      0.8500],\n",
            "        [     0.2618,      0.0000],\n",
            "        [     2.7215,      0.8500],\n",
            "        [     0.0311,      0.0000],\n",
            "        [     0.1053,      0.0000],\n",
            "        [    -0.1772,      0.0000],\n",
            "        [     0.1550,      0.0000],\n",
            "        [    -0.1490,      0.0000],\n",
            "        [    -0.0923,      0.0000],\n",
            "        [    -0.1124,      0.0000],\n",
            "        [     0.5203,      0.0000],\n",
            "        [     0.8336,      1.7100],\n",
            "        [     0.0236,      0.0000],\n",
            "        [     0.5338,      0.0000],\n",
            "        [    -0.1529,      0.0000],\n",
            "        [     0.1175,      0.0000],\n",
            "        [     0.4181,      0.8500],\n",
            "        [    39.2595,     48.6400],\n",
            "        [     0.0860,      0.0000],\n",
            "        [    -0.2339,      0.0000],\n",
            "        [     0.8974,      0.8500],\n",
            "        [     4.8289,      8.5300],\n",
            "        [     5.7537,     10.2400],\n",
            "        [     4.3694,      7.6800],\n",
            "        [     4.2470,      8.5300],\n",
            "        [    -0.0252,      0.0000],\n",
            "        [     2.7340,      7.6800],\n",
            "        [     1.4864,      5.1200],\n",
            "        [    54.3608,     51.6800],\n",
            "        [     4.0719,      3.8000],\n",
            "        [     1.7634,      6.6700],\n",
            "        [     3.0734,      4.5600],\n",
            "        [     3.6257,      2.2800],\n",
            "        [     1.5212,      2.6700],\n",
            "        [     5.2078,      0.8500],\n",
            "        [     5.5184,      5.9700],\n",
            "        [     5.7758,      6.8300],\n",
            "        [     5.1953,      4.2700],\n",
            "        [     3.5783,      4.5600],\n",
            "        [     2.7170,      4.6700],\n",
            "        [     4.1105,      4.5600],\n",
            "        [    44.3138,     48.6500],\n",
            "        [     3.2491,      2.5600],\n",
            "        [     1.9886,      1.5200],\n",
            "        [     3.0030,      1.5200],\n",
            "        [     4.8138,      2.5600],\n",
            "        [     3.8796,      3.8000],\n",
            "        [     0.4642,      0.5100],\n",
            "        [     0.7131,      2.6700],\n",
            "        [     2.6981,      3.8000],\n",
            "        [     1.7739,      5.3200],\n",
            "        [     4.7475,      1.7100],\n",
            "        [     2.8249,      4.5600],\n",
            "        [     1.6644,      3.8000],\n",
            "        [     4.0340,      2.2800],\n",
            "        [     1.8994,      3.0400],\n",
            "        [     3.9945,      3.4100],\n",
            "        [     3.7751,      3.0400],\n",
            "        [     4.3155,      2.5600],\n",
            "        [   117.7819,    116.0500],\n",
            "        [     8.8072,      8.5300],\n",
            "        [     9.7214,      8.5300],\n",
            "        [     7.8778,      8.5300],\n",
            "        [     6.8400,      9.3900],\n",
            "        [     7.0398,      8.5300],\n",
            "        [     9.8524,      5.1200],\n",
            "        [     7.4133,      5.1200],\n",
            "        [     8.7399,      5.9700],\n",
            "        [     7.1285,      8.5300],\n",
            "        [     6.8903,      8.5300],\n",
            "        [     9.2563,      6.8300],\n",
            "        [     8.1704,     11.9500],\n",
            "        [     8.6713,      5.9700],\n",
            "        [     7.6070,     10.2400],\n",
            "        [     2.5393,      4.2700],\n",
            "        [   100.1472,    102.4000],\n",
            "        [     5.1835,      5.9700],\n",
            "        [     9.9335,     13.6500],\n",
            "        [    11.0913,     11.0900],\n",
            "        [     4.8936,      5.9700],\n",
            "        [     8.0924,     11.0900],\n",
            "        [    10.7205,     14.5100],\n",
            "        [     8.7233,     11.0900],\n",
            "        [    10.7235,      7.6800],\n",
            "        [     9.6993,      6.8300],\n",
            "        [     8.7824,     14.5100]], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([[     3.0110,      1.0800],\n",
            "        [     0.0103,      0.0000],\n",
            "        [     1.4870,      0.7100],\n",
            "        [    -0.0197,      0.0000],\n",
            "        [     0.3314,      0.3700],\n",
            "        [     0.0051,      0.0000],\n",
            "        [     0.0012,      0.0000],\n",
            "        [     0.0047,      0.0000],\n",
            "        [    -0.0029,      0.0000],\n",
            "        [     0.7226,      0.0000],\n",
            "        [    -0.0068,      0.0000],\n",
            "        [     0.0480,      0.0000],\n",
            "        [    -0.0210,      0.0000],\n",
            "        [    -0.0177,      0.0000],\n",
            "        [    -0.0050,      0.0000],\n",
            "        [    -0.0223,      0.0000],\n",
            "        [    -0.0185,      0.0000],\n",
            "        [    -0.0134,      0.0000],\n",
            "        [    -0.0175,      0.0000],\n",
            "        [    -0.0083,      0.0000],\n",
            "        [    -0.0200,      0.0000],\n",
            "        [    -0.0252,      0.0000],\n",
            "        [    -0.0162,      0.0000],\n",
            "        [     3.7319,      2.1200],\n",
            "        [     0.0140,      0.0000],\n",
            "        [     0.4733,      0.0000],\n",
            "        [     0.0235,      0.0000],\n",
            "        [    -0.0311,      0.0000],\n",
            "        [     0.2658,      0.0000],\n",
            "        [    -0.0269,      0.0000],\n",
            "        [     1.0338,      1.4100],\n",
            "        [    -0.0442,      0.0000],\n",
            "        [     0.9574,      0.7100],\n",
            "        [    -0.0191,      0.0000],\n",
            "        [    -0.0113,      0.0000],\n",
            "        [    -0.0034,      0.0000],\n",
            "        [    -0.0220,      0.0000],\n",
            "        [     0.2969,      0.0000],\n",
            "        [     0.2670,      0.0000],\n",
            "        [     0.3134,      0.0000],\n",
            "        [    -0.0098,      0.0000],\n",
            "        [    -0.0108,      0.0000],\n",
            "        [     6.9630,      5.8400],\n",
            "        [     0.4663,      0.3700],\n",
            "        [     0.0174,      0.0000],\n",
            "        [     1.1592,      0.6300],\n",
            "        [     0.8873,      0.5100],\n",
            "        [     0.7258,      1.1200],\n",
            "        [     0.5497,      0.0000],\n",
            "        [    -0.0814,      0.0000],\n",
            "        [    -0.0124,      0.0000],\n",
            "        [     1.0145,      0.5600],\n",
            "        [     1.2948,      0.7100],\n",
            "        [     0.5027,      0.0000],\n",
            "        [     0.8052,      0.3700],\n",
            "        [     0.9819,      0.5600],\n",
            "        [     0.9594,      1.0100],\n",
            "        [     0.0079,      0.0000],\n",
            "        [     0.0675,      0.0000],\n",
            "        [    -0.0288,      0.0000],\n",
            "        [    -0.0103,      0.0000],\n",
            "        [     0.0021,      0.0000],\n",
            "        [    -0.0136,      0.0000],\n",
            "        [    -0.0025,      0.0000],\n",
            "        [    -0.0116,      0.0000],\n",
            "        [    -0.0196,      0.0000],\n",
            "        [    -0.0389,      0.0000],\n",
            "        [    -0.0345,      0.0000],\n",
            "        [    -0.0288,      0.0000],\n",
            "        [    -0.0119,      0.0000],\n",
            "        [     0.7320,      0.0000],\n",
            "        [    -0.0157,      0.0000],\n",
            "        [    -0.0180,      0.0000],\n",
            "        [     0.0000,      0.0000],\n",
            "        [     0.2054,      0.0000],\n",
            "        [     0.0017,      0.0000],\n",
            "        [    -0.0071,      0.0000],\n",
            "        [     0.0889,      0.0000],\n",
            "        [     0.0028,      0.0000],\n",
            "        [    -0.0238,      0.0000],\n",
            "        [     0.0000,      0.0000],\n",
            "        [    -0.0105,      0.0000],\n",
            "        [     0.0497,      0.0000],\n",
            "        [     0.0057,      0.0000],\n",
            "        [     0.0082,      0.0000],\n",
            "        [    -0.0338,      0.0000],\n",
            "        [    -0.0066,      0.0000],\n",
            "        [    -0.0341,      0.0000],\n",
            "        [    -0.0243,      0.0000],\n",
            "        [    -0.0193,      0.0000],\n",
            "        [    -0.0405,      0.0000],\n",
            "        [    -0.0140,      0.0000],\n",
            "        [    -0.0029,      0.0000],\n",
            "        [    -0.0227,      0.0000],\n",
            "        [    -0.0403,      0.0000],\n",
            "        [    -0.0225,      0.0000],\n",
            "        [    -0.0383,      0.0000],\n",
            "        [    -0.0329,      0.0000],\n",
            "        [    12.9274,      7.4100],\n",
            "        [    -0.0138,      0.0000],\n",
            "        [    -0.0201,      0.0000],\n",
            "        [     1.3909,      1.1200],\n",
            "        [     1.9627,      1.4100],\n",
            "        [    -0.0280,      0.0000],\n",
            "        [     0.6797,      0.0000],\n",
            "        [     0.8563,      0.0000],\n",
            "        [     1.1479,      0.3700],\n",
            "        [     1.3638,      0.0000],\n",
            "        [    -0.0082,      0.0000],\n",
            "        [    -0.0992,      0.0000],\n",
            "        [     1.8620,      1.2500],\n",
            "        [     1.7012,      2.2400],\n",
            "        [     0.2922,      0.0000],\n",
            "        [     1.1354,      0.0000],\n",
            "        [     1.4460,      1.0100],\n",
            "        [     9.1542,      6.9600],\n",
            "        [     0.7209,      0.3700],\n",
            "        [     0.2425,      0.0000],\n",
            "        [     1.1274,      0.5600],\n",
            "        [     1.1344,      1.1200],\n",
            "        [     1.5872,      0.7100],\n",
            "        [     1.4067,      2.8300],\n",
            "        [     0.0744,      0.0000],\n",
            "        [     1.2320,      0.6300],\n",
            "        [     0.5804,      0.7500],\n",
            "        [    -0.0287,      0.0000],\n",
            "        [     4.0321,      2.1300],\n",
            "        [    -0.0145,      0.0000],\n",
            "        [     0.6298,      1.0100],\n",
            "        [    -0.0469,      0.0000],\n",
            "        [    -0.0223,      0.0000],\n",
            "        [    -0.0429,      0.0000],\n",
            "        [     0.6178,      0.3700],\n",
            "        [     0.5826,      0.3700],\n",
            "        [     0.4746,      0.0000],\n",
            "        [    -0.0195,      0.0000],\n",
            "        [    -0.0402,      0.0000],\n",
            "        [    -0.0462,      0.0000],\n",
            "        [     0.2700,      0.3700],\n",
            "        [    -0.0011,      0.0000],\n",
            "        [     0.0045,      0.0000],\n",
            "        [     0.0013,      0.0000],\n",
            "        [    -0.0056,      0.0000],\n",
            "        [    -0.0148,      0.0000],\n",
            "        [    -0.0543,      0.0000],\n",
            "        [     0.0031,      0.0000],\n",
            "        [    -0.0116,      0.0000]], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([[     7.2872,      6.8800],\n",
            "        [     0.4356,      0.0000],\n",
            "        [     0.3883,      1.0100],\n",
            "        [     0.7863,      1.5200],\n",
            "        [     1.0389,      1.5200],\n",
            "        [     0.1121,      0.0000],\n",
            "        [     0.0026,      0.0000],\n",
            "        [     1.3240,      0.6300],\n",
            "        [    -0.0303,      0.0000],\n",
            "        [     1.4966,      0.7100],\n",
            "        [    -0.0405,      0.0000],\n",
            "        [     0.6475,      1.4900],\n",
            "        [     0.0260,      0.0000],\n",
            "        [     0.0313,      0.0000],\n",
            "        [     0.0617,      0.0000],\n",
            "        [     0.0394,      0.0000],\n",
            "        [     0.0650,      0.0000],\n",
            "        [     0.0625,      0.0000],\n",
            "        [     0.0114,      0.0000],\n",
            "        [     0.0362,      0.0000],\n",
            "        [     0.0277,      0.0000],\n",
            "        [     0.0535,      0.0000],\n",
            "        [     0.0690,      0.0000],\n",
            "        [     0.0322,      0.0000],\n",
            "        [     0.0462,      0.0000],\n",
            "        [     0.1425,      0.0000],\n",
            "        [     0.0232,      0.0000],\n",
            "        [     0.0312,      0.0000],\n",
            "        [     8.1780,      8.6900],\n",
            "        [     0.0309,      0.0000],\n",
            "        [     0.9069,      1.0100],\n",
            "        [    -0.0144,      0.0000],\n",
            "        [     0.9018,      1.5200],\n",
            "        [     1.0231,      0.0000],\n",
            "        [     0.3665,      0.0000],\n",
            "        [    -0.0051,      0.0000],\n",
            "        [    -0.0099,      0.0000],\n",
            "        [     0.8455,      0.3700],\n",
            "        [     0.4702,      1.4900],\n",
            "        [     1.0695,      0.0000],\n",
            "        [     0.9263,      2.5300],\n",
            "        [     0.6691,      0.7500],\n",
            "        [    -0.0037,      0.0000],\n",
            "        [     0.9006,      1.0100],\n",
            "        [     0.0589,      0.0000],\n",
            "        [    40.0481,     48.5900],\n",
            "        [     3.4893,      3.1300],\n",
            "        [     2.6543,      4.0500],\n",
            "        [     2.7756,      2.5300],\n",
            "        [     2.7757,      1.5200],\n",
            "        [     3.8857,      5.6000],\n",
            "        [     2.6766,      1.1200],\n",
            "        [     3.5647,      3.7600],\n",
            "        [     3.2460,      1.8700],\n",
            "        [     2.9692,      3.3600],\n",
            "        [     2.7756,      1.8700],\n",
            "        [     4.2104,      4.2400],\n",
            "        [     2.2435,      1.4900],\n",
            "        [     4.8816,      6.3600],\n",
            "        [     4.2625,      5.6500],\n",
            "        [     2.9060,      2.0300],\n",
            "        [     4.3590,      4.6500],\n",
            "        [     0.7911,      0.0000],\n",
            "        [    -0.0092,      0.0000],\n",
            "        [     0.5040,      0.0000],\n",
            "        [     0.5341,      1.5200],\n",
            "        [     0.8811,      1.0100],\n",
            "        [     1.1022,      0.6300],\n",
            "        [    -0.0051,      0.0000],\n",
            "        [     0.5218,      1.4900],\n",
            "        [     0.3006,      0.0000],\n",
            "        [    -0.0191,      0.0000],\n",
            "        [     0.0789,      0.0000],\n",
            "        [     0.0235,      0.0000],\n",
            "        [     0.0198,      0.0000],\n",
            "        [     0.0168,      0.0000],\n",
            "        [     0.0081,      0.0000],\n",
            "        [     0.0239,      0.0000],\n",
            "        [     0.0236,      0.0000],\n",
            "        [     0.0195,      0.0000],\n",
            "        [     0.0219,      0.0000],\n",
            "        [     0.0487,      0.0000],\n",
            "        [     0.0104,      0.0000],\n",
            "        [     0.0200,      0.0000],\n",
            "        [     0.0154,      0.0000],\n",
            "        [     0.0226,      0.0000],\n",
            "        [     0.0280,      0.0000],\n",
            "        [     0.0163,      0.0000],\n",
            "        [     0.0267,      0.0000],\n",
            "        [     0.0208,      0.0000],\n",
            "        [     0.0175,      0.0000],\n",
            "        [     0.0264,      0.0000],\n",
            "        [     1.1773,      0.3700],\n",
            "        [     0.0322,      0.0000],\n",
            "        [     0.4355,      0.3700],\n",
            "        [     0.0015,      0.0000],\n",
            "        [     0.0062,      0.0000],\n",
            "        [     0.0181,      0.0000],\n",
            "        [     0.0152,      0.0000],\n",
            "        [     0.0268,      0.0000],\n",
            "        [     0.3072,      0.0000],\n",
            "        [     0.0093,      0.0000],\n",
            "        [     0.0144,      0.0000],\n",
            "        [     0.0004,      0.0000],\n",
            "        [     0.0130,      0.0000],\n",
            "        [     0.0105,      0.0000],\n",
            "        [    -0.0027,      0.0000],\n",
            "        [     0.0295,      0.0000],\n",
            "        [     0.0027,      0.0000],\n",
            "        [    11.4696,      8.7100],\n",
            "        [     0.0380,      0.0000],\n",
            "        [     1.6448,      0.7100],\n",
            "        [     1.4221,      1.5200],\n",
            "        [     1.3009,      1.0100],\n",
            "        [     0.0030,      0.0000],\n",
            "        [     1.7904,      0.7100],\n",
            "        [     0.7344,      0.7500],\n",
            "        [     0.0048,      0.0000],\n",
            "        [     1.2418,      0.5600],\n",
            "        [     1.5466,      1.1200],\n",
            "        [     0.8665,      0.5100],\n",
            "        [     0.8505,      1.1200],\n",
            "        [    -0.0241,      0.0000],\n",
            "        [    -0.0340,      0.0000],\n",
            "        [     1.7410,      0.7100],\n",
            "        [    21.6586,     18.1100],\n",
            "        [     1.7684,      2.0300],\n",
            "        [     2.0255,      1.5200],\n",
            "        [     1.6438,      1.8700],\n",
            "        [     0.9888,      0.0000],\n",
            "        [     2.0733,      1.2500],\n",
            "        [     1.8087,      1.5200],\n",
            "        [     1.7483,      3.0400],\n",
            "        [     1.3379,      1.8700],\n",
            "        [     1.3055,      0.5100],\n",
            "        [     1.5957,      0.7500],\n",
            "        [     2.4007,      3.7600],\n",
            "        [     8.6322,     11.1200],\n",
            "        [     0.6881,      0.7500],\n",
            "        [     0.9679,      1.5200],\n",
            "        [     1.4956,      2.2400],\n",
            "        [     0.6163,      0.0000],\n",
            "        [     1.0387,      1.0100],\n",
            "        [     0.7740,      0.0000],\n",
            "        [     0.4773,      1.1200],\n",
            "        [     1.1640,      2.2400],\n",
            "        [     0.4087,      0.7500],\n",
            "        [     0.7949,      1.4900],\n",
            "        [    11.4923,     11.3700],\n",
            "        [     0.4591,      0.3300],\n",
            "        [     1.5116,      2.1200],\n",
            "        [     0.7718,      0.3700],\n",
            "        [     0.0025,      0.0000],\n",
            "        [     0.8620,      0.3700],\n",
            "        [     0.6875,      0.0000],\n",
            "        [     1.4132,      1.8800],\n",
            "        [     0.5842,      0.3700],\n",
            "        [     1.3835,      1.1200],\n",
            "        [    -0.0602,      0.0000],\n",
            "        [     1.7026,      1.4100],\n",
            "        [     1.4808,      1.1200],\n",
            "        [     0.3675,      0.3700],\n",
            "        [    -0.0035,      0.0000],\n",
            "        [     1.1008,      1.0100],\n",
            "        [     0.7618,      0.5100],\n",
            "        [     0.9732,      0.3700],\n",
            "        [    -0.0079,      0.0000],\n",
            "        [    40.8616,     40.9300],\n",
            "        [     2.9973,      2.0300],\n",
            "        [     2.5208,      1.1200],\n",
            "        [     2.2412,      3.0400],\n",
            "        [     2.8013,      1.5200],\n",
            "        [     2.0507,      1.1200],\n",
            "        [     2.2116,      1.1200],\n",
            "        [     2.2100,      2.6100],\n",
            "        [     3.1075,      0.5600],\n",
            "        [     2.0535,      1.8700],\n",
            "        [     2.4403,      3.7300],\n",
            "        [     2.1037,      1.8700],\n",
            "        [     2.7656,      7.0900],\n",
            "        [     2.5710,      3.5500],\n",
            "        [     3.9235,      2.8300],\n",
            "        [     3.8854,      2.8300],\n",
            "        [     2.3850,      4.0500]], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([[    65.1377,     63.1500],\n",
            "        [     2.8106,      0.0000],\n",
            "        [     0.9192,      0.0000],\n",
            "        [     5.6505,      4.2700],\n",
            "        [     5.6188,      5.1200],\n",
            "        [     5.3592,      4.2700],\n",
            "        [     5.9368,      5.9700],\n",
            "        [     5.2185,      9.3900],\n",
            "        [     5.3813,      7.6800],\n",
            "        [     6.1728,      5.9700],\n",
            "        [     3.1465,      2.5600],\n",
            "        [     3.1944,      3.4100],\n",
            "        [     5.1454,     14.5100],\n",
            "        [    38.4580,     41.8100],\n",
            "        [     3.0619,      0.8500],\n",
            "        [     3.3694,      0.8500],\n",
            "        [     4.1597,      4.2700],\n",
            "        [     0.0856,      0.0000],\n",
            "        [     4.4155,      3.4100],\n",
            "        [     2.6295,      0.8500],\n",
            "        [     3.9541,      2.5600],\n",
            "        [     2.3959,      3.4100],\n",
            "        [     3.6055,      1.7100],\n",
            "        [     2.1994,      2.5600],\n",
            "        [     4.3284,      3.4100],\n",
            "        [     1.7223,      1.7100],\n",
            "        [    -0.0405,      0.0000],\n",
            "        [     2.9975,      4.2700],\n",
            "        [     4.2586,      4.2700],\n",
            "        [     0.0986,      0.0000],\n",
            "        [     2.2942,      1.7100],\n",
            "        [     2.7236,      0.0000],\n",
            "        [     2.7058,      3.4100],\n",
            "        [     3.4231,      2.5600],\n",
            "        [   128.5374,     92.1600],\n",
            "        [    11.8523,      5.9700],\n",
            "        [    14.8982,      9.3900],\n",
            "        [     3.2102,      5.9700],\n",
            "        [     4.1003,      5.1200],\n",
            "        [     8.3259,     10.2400],\n",
            "        [     2.4003,      9.3900],\n",
            "        [     5.7671,      7.6800],\n",
            "        [     1.3983,      1.7100],\n",
            "        [     1.8785,      4.2700],\n",
            "        [    -0.5735,      0.0000],\n",
            "        [     4.0605,      5.1200],\n",
            "        [     8.8693,      5.9700],\n",
            "        [    -0.6031,      0.0000],\n",
            "        [    -0.3264,      0.0000],\n",
            "        [     8.7273,      5.1200],\n",
            "        [     4.7549,      5.9700],\n",
            "        [    -0.5755,      0.0000],\n",
            "        [     9.5304,      6.8300],\n",
            "        [    10.1749,      3.4100],\n",
            "        [    46.0580,     56.3200],\n",
            "        [     4.9491,      3.4100],\n",
            "        [     3.0191,      4.2700],\n",
            "        [     4.9611,      7.6800],\n",
            "        [     1.6292,      1.7100],\n",
            "        [     1.6572,      2.5600],\n",
            "        [     4.4424,      3.4100],\n",
            "        [     4.8008,      0.8500],\n",
            "        [     3.6664,      0.8500],\n",
            "        [     4.3513,      3.4100],\n",
            "        [     2.8296,      7.6800],\n",
            "        [     3.8581,      4.2700],\n",
            "        [     3.2208,      3.4100],\n",
            "        [     4.7329,      5.1200],\n",
            "        [     4.4114,      7.6800],\n",
            "        [    14.5122,     21.1600],\n",
            "        [     1.6720,      4.0000],\n",
            "        [     1.9458,      0.7600],\n",
            "        [     1.4142,      0.5100],\n",
            "        [     1.7553,      3.0400],\n",
            "        [     1.9736,      3.0400],\n",
            "        [     1.6295,      0.6700],\n",
            "        [     1.9198,      1.5200],\n",
            "        [     1.5751,      1.5200],\n",
            "        [    -0.3028,      0.0000],\n",
            "        [     0.8525,      0.5100],\n",
            "        [     1.7850,      0.0000],\n",
            "        [     2.0121,      1.5200],\n",
            "        [     2.3828,      2.5600],\n",
            "        [     1.7467,      1.5200],\n",
            "        [     0.2569,      0.0000],\n",
            "        [    15.6097,     14.5100],\n",
            "        [     2.4431,      1.7100],\n",
            "        [     2.8460,      3.4100],\n",
            "        [     2.0412,      4.2700],\n",
            "        [     2.8370,      0.8500],\n",
            "        [     2.0898,      0.8500],\n",
            "        [     0.1764,      0.0000],\n",
            "        [     0.2385,      0.0000],\n",
            "        [     0.3590,      0.8500],\n",
            "        [    -0.0052,      0.0000],\n",
            "        [    -0.0254,      0.0000],\n",
            "        [    -0.2217,      0.0000],\n",
            "        [     2.0683,      1.7100],\n",
            "        [    -0.2044,      0.0000],\n",
            "        [     1.3054,      0.8500],\n",
            "        [    -0.2074,      0.0000],\n",
            "        [    24.2317,     12.8000],\n",
            "        [     0.0018,      0.0000],\n",
            "        [     0.4155,      0.0000],\n",
            "        [     2.2529,      5.1200],\n",
            "        [    -0.0399,      0.0000],\n",
            "        [     0.0193,      1.7100],\n",
            "        [     0.0741,      0.0000],\n",
            "        [     0.8719,      0.8500],\n",
            "        [    -0.0890,      0.0000],\n",
            "        [     0.7478,      2.5600],\n",
            "        [     0.0882,      0.8500],\n",
            "        [     4.9944,      1.7100],\n",
            "        [    72.2794,     71.9100],\n",
            "        [     7.3815,      6.8300],\n",
            "        [     8.0517,      3.4100],\n",
            "        [     5.9022,      4.2700],\n",
            "        [     4.6833,      5.9700],\n",
            "        [     4.6419,      2.5600],\n",
            "        [     6.5343,      7.5900],\n",
            "        [     7.8450,      5.9700],\n",
            "        [     4.9496,      0.8500],\n",
            "        [     5.1799,      5.1200],\n",
            "        [     6.7071,      4.2700],\n",
            "        [     6.7027,      5.1200],\n",
            "        [     4.1900,      1.3600],\n",
            "        [     4.7538,      1.7100],\n",
            "        [     6.4749,      5.9700],\n",
            "        [     7.7676,      6.8300],\n",
            "        [     3.4601,      0.8500],\n",
            "        [     2.2460,      0.8500],\n",
            "        [     6.4191,      0.8500],\n",
            "        [     4.9595,      1.5200],\n",
            "        [    30.2137,     31.3100],\n",
            "        [     3.1743,      3.8000],\n",
            "        [     1.6380,      2.0300],\n",
            "        [     2.7201,      3.0400],\n",
            "        [     1.9242,      0.6700],\n",
            "        [     2.5279,      1.5200],\n",
            "        [     2.8229,      3.5500],\n",
            "        [     3.9558,      6.8300],\n",
            "        [     2.2549,      5.3200],\n",
            "        [     3.6888,      0.8500],\n",
            "        [     2.2626,      0.6700],\n",
            "        [    -0.0782,      0.0000],\n",
            "        [     1.9401,      3.0400],\n",
            "        [    67.9987,     58.8800],\n",
            "        [     5.3133,      2.5600],\n",
            "        [     5.5935,      5.1200],\n",
            "        [     3.4975,      2.5600],\n",
            "        [     2.6797,      2.5600],\n",
            "        [     5.8404,      0.8500],\n",
            "        [     6.1762,      5.9700],\n",
            "        [     4.7472,      5.9700],\n",
            "        [     7.2204,      5.1200],\n",
            "        [     4.5170,      5.1200],\n",
            "        [     2.9712,      0.8500],\n",
            "        [     4.6640,      3.4100],\n",
            "        [     0.5710,      5.9700],\n",
            "        [     4.0920,      0.0000],\n",
            "        [     6.6049,      8.5300],\n",
            "        [     6.4666,      4.2700],\n",
            "        [   115.6258,    111.3500],\n",
            "        [     5.2470,      6.6700],\n",
            "        [     6.0163,      5.3200],\n",
            "        [     6.2671,      5.3200],\n",
            "        [     5.3746,      3.8000],\n",
            "        [     9.3055,      5.9700],\n",
            "        [     7.8782,     10.2400],\n",
            "        [     6.9694,     10.6400],\n",
            "        [     5.7623,      7.6000],\n",
            "        [     6.5174,      9.1200],\n",
            "        [     6.3080,      7.6000],\n",
            "        [     8.5744,      5.1200],\n",
            "        [     5.5063,      6.6700],\n",
            "        [     7.5720,      6.0800],\n",
            "        [     5.1532,      6.0000],\n",
            "        [     7.3933,      7.6000],\n",
            "        [     6.7089,      7.6000],\n",
            "        [    77.8822,     64.4300],\n",
            "        [     2.6910,      3.5500],\n",
            "        [     5.0587,      6.0800],\n",
            "        [     4.9851,      0.0000],\n",
            "        [     4.9710,      2.0000],\n",
            "        [     6.1365,      5.9700],\n",
            "        [     7.3957,     11.0900],\n",
            "        [     2.4868,      3.0400],\n",
            "        [     4.8082,      7.6000],\n",
            "        [     4.9121,      3.8000],\n",
            "        [     7.5886,      9.3900],\n",
            "        [     5.1313,      8.3600],\n",
            "        [     4.7367,      3.5500]], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([[ 0.1089,  0.0000],\n",
            "        [-0.0470,  0.0000],\n",
            "        [-0.0513,  0.0000],\n",
            "        [-0.0680,  0.0000],\n",
            "        [-0.0745,  0.0000],\n",
            "        [-0.0831,  0.0000],\n",
            "        [-0.0769,  0.0000],\n",
            "        [-0.0604,  0.0000],\n",
            "        [-0.0596,  0.0000],\n",
            "        [-0.0906,  0.0000],\n",
            "        [-0.0717,  0.0000],\n",
            "        [-0.0635,  0.0000],\n",
            "        [-0.0524,  0.0000],\n",
            "        [-0.0514,  0.0000],\n",
            "        [-0.0709,  0.0000],\n",
            "        [ 1.4688,  1.4100],\n",
            "        [-0.0439,  0.0000],\n",
            "        [-0.0334,  0.0000],\n",
            "        [-0.0292,  0.0000],\n",
            "        [-0.0365,  0.0000],\n",
            "        [-0.0266,  0.0000],\n",
            "        [ 0.9572,  1.4100],\n",
            "        [-0.0533,  0.0000],\n",
            "        [-0.0479,  0.0000],\n",
            "        [-0.0541,  0.0000],\n",
            "        [-0.0461,  0.0000],\n",
            "        [-0.0374,  0.0000],\n",
            "        [-0.0525,  0.0000],\n",
            "        [-0.0497,  0.0000],\n",
            "        [-0.0582,  0.0000],\n",
            "        [ 0.2025,  0.0000],\n",
            "        [-0.0459,  0.0000],\n",
            "        [-0.0426,  0.0000],\n",
            "        [-0.0449,  0.0000],\n",
            "        [ 0.1796,  0.0000],\n",
            "        [-0.0440,  0.0000],\n",
            "        [-0.0563,  0.0000],\n",
            "        [-0.0602,  0.0000],\n",
            "        [-0.0551,  0.0000],\n",
            "        [-0.0465,  0.0000],\n",
            "        [-0.0542,  0.0000],\n",
            "        [-0.0550,  0.0000],\n",
            "        [-0.0478,  0.0000],\n",
            "        [-0.0583,  0.0000],\n",
            "        [-0.0405,  0.0000],\n",
            "        [-0.0630,  0.0000],\n",
            "        [-0.0538,  0.0000],\n",
            "        [-0.1123,  0.0000],\n",
            "        [-0.0449,  0.0000],\n",
            "        [-0.0548,  0.0000],\n",
            "        [-0.0537,  0.0000],\n",
            "        [-0.0496,  0.0000],\n",
            "        [-0.0573,  0.0000],\n",
            "        [-0.0627,  0.0000],\n",
            "        [-0.0430,  0.0000],\n",
            "        [-0.0597,  0.0000],\n",
            "        [-0.0479,  0.0000],\n",
            "        [-0.0472,  0.0000],\n",
            "        [ 0.0170,  0.0000],\n",
            "        [-0.0550,  0.0000],\n",
            "        [-0.0489,  0.0000],\n",
            "        [-0.0468,  0.0000],\n",
            "        [-0.0477,  0.0000],\n",
            "        [-0.0524,  0.0000],\n",
            "        [-0.0544,  0.0000],\n",
            "        [-0.0460,  0.0000],\n",
            "        [-0.0423,  0.0000],\n",
            "        [-0.0389,  0.0000],\n",
            "        [-0.0576,  0.0000],\n",
            "        [-0.0534,  0.0000],\n",
            "        [-0.0456,  0.0000],\n",
            "        [-0.0570,  0.0000],\n",
            "        [-0.0620,  0.0000],\n",
            "        [-0.0530,  0.0000],\n",
            "        [-0.0453,  0.0000],\n",
            "        [-0.0559,  0.0000],\n",
            "        [ 0.4440,  0.0000],\n",
            "        [-0.0437,  0.0000],\n",
            "        [-0.0367,  0.0000],\n",
            "        [-0.0128,  0.0000],\n",
            "        [-0.0310,  0.0000],\n",
            "        [-0.0401,  0.0000],\n",
            "        [-0.0342,  0.0000],\n",
            "        [-0.0427,  0.0000],\n",
            "        [-0.0467,  0.0000],\n",
            "        [-0.0397,  0.0000],\n",
            "        [-0.0352,  0.0000],\n",
            "        [-0.0474,  0.0000],\n",
            "        [-0.0397,  0.0000],\n",
            "        [-0.0129,  0.0000],\n",
            "        [-0.0438,  0.0000],\n",
            "        [-0.0310,  0.0000],\n",
            "        [-0.0320,  0.0000],\n",
            "        [-0.0418,  0.0000],\n",
            "        [-0.0907,  0.0000],\n",
            "        [-0.0676,  0.0000],\n",
            "        [-0.0550,  0.0000],\n",
            "        [-0.0366,  0.0000],\n",
            "        [-0.0368,  0.0000],\n",
            "        [-0.0399,  0.0000],\n",
            "        [-0.0368,  0.0000],\n",
            "        [-0.0408,  0.0000],\n",
            "        [-0.0407,  0.0000],\n",
            "        [-0.0526,  0.0000],\n",
            "        [-0.0552,  0.0000],\n",
            "        [-0.0426,  0.0000],\n",
            "        [-0.0325,  0.0000],\n",
            "        [-0.0435,  0.0000],\n",
            "        [-0.0406,  0.0000],\n",
            "        [-0.0609,  0.0000],\n",
            "        [-0.0414,  0.0000],\n",
            "        [-0.0433,  0.0000],\n",
            "        [-0.0391,  0.0000],\n",
            "        [ 1.3141,  0.0000],\n",
            "        [-0.0178,  0.0000],\n",
            "        [-0.0058,  0.0000],\n",
            "        [-0.0098,  0.0000],\n",
            "        [-0.0245,  0.0000],\n",
            "        [-0.0240,  0.0000],\n",
            "        [-0.0324,  0.0000],\n",
            "        [-0.0193,  0.0000],\n",
            "        [-0.1121,  0.0000],\n",
            "        [-0.0319,  0.0000],\n",
            "        [-0.0205,  0.0000],\n",
            "        [-0.0251,  0.0000],\n",
            "        [-0.1171,  0.0000]], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([[    33.0381,     38.8900],\n",
            "        [     1.7593,      2.2400],\n",
            "        [     1.9397,      3.0400],\n",
            "        [     2.0014,      1.1200],\n",
            "        [     3.3690,      1.8800],\n",
            "        [     2.2279,      4.0500],\n",
            "        [     1.8610,      1.1200],\n",
            "        [     2.8970,      5.6000],\n",
            "        [     1.9650,      1.8700],\n",
            "        [     3.0359,      2.8000],\n",
            "        [     2.3335,      3.5500],\n",
            "        [     1.8709,      0.7500],\n",
            "        [     1.6428,      1.1200],\n",
            "        [     2.6818,      2.8000],\n",
            "        [     1.9225,      1.1200],\n",
            "        [     2.7770,      2.8000],\n",
            "        [     2.3655,      3.0400],\n",
            "        [    18.8025,     20.3100],\n",
            "        [     1.4867,      1.0100],\n",
            "        [     1.7838,      1.6800],\n",
            "        [     1.2357,      1.5200],\n",
            "        [     1.0233,      1.8700],\n",
            "        [     1.7431,      2.2400],\n",
            "        [     2.1914,      0.7100],\n",
            "        [     1.6603,      2.2400],\n",
            "        [     0.9857,      0.3700],\n",
            "        [    -0.1059,      0.0000],\n",
            "        [     0.6583,      0.3700],\n",
            "        [     2.1667,      2.1200],\n",
            "        [     1.1922,      1.5200],\n",
            "        [     0.9482,      0.7500],\n",
            "        [     1.2213,      2.0300],\n",
            "        [     2.0298,      1.8800],\n",
            "        [    21.7925,     20.6800],\n",
            "        [     2.2208,      0.7100],\n",
            "        [     1.2136,      0.7500],\n",
            "        [     1.3255,      1.4900],\n",
            "        [     1.6120,      0.5600],\n",
            "        [     1.5780,      3.0400],\n",
            "        [     0.0288,      0.0000],\n",
            "        [     1.7570,      1.0100],\n",
            "        [     0.8488,      1.1200],\n",
            "        [     1.0733,      0.7500],\n",
            "        [     1.6709,      3.3600],\n",
            "        [    -0.0190,      0.0000],\n",
            "        [     0.9292,      0.7500],\n",
            "        [     1.5227,      3.0400],\n",
            "        [     1.6698,      2.2400],\n",
            "        [     0.9393,      1.8700],\n",
            "        [    39.8393,     40.6300],\n",
            "        [     2.2897,      2.6100],\n",
            "        [     2.0567,      0.7500],\n",
            "        [     2.5255,      2.2400],\n",
            "        [     3.1406,      3.3600],\n",
            "        [     3.3155,      5.6000],\n",
            "        [     2.7254,      5.5700],\n",
            "        [     3.0751,      2.5300],\n",
            "        [     2.7851,      3.3600],\n",
            "        [     3.7218,      5.6400],\n",
            "        [     3.1044,      1.1200],\n",
            "        [     2.6717,      1.1200],\n",
            "        [     1.7898,      1.8700],\n",
            "        [     3.4625,      2.2400],\n",
            "        [     2.0186,      2.6100],\n",
            "        [     8.1451,      9.3900],\n",
            "        [    -0.0728,      0.0000],\n",
            "        [    -0.0455,      0.0000],\n",
            "        [     0.6777,      0.3700],\n",
            "        [     2.0255,      2.1200],\n",
            "        [     0.0046,      0.0000],\n",
            "        [     0.7971,      0.7500],\n",
            "        [     0.9667,      1.0100],\n",
            "        [     0.8914,      0.5100],\n",
            "        [     0.7274,      1.1200],\n",
            "        [     0.6629,      0.3700],\n",
            "        [     1.0418,      0.5600],\n",
            "        [    -0.0365,      0.0000],\n",
            "        [     0.7481,      1.1200],\n",
            "        [     0.2589,      0.0000],\n",
            "        [     0.6788,      0.7500],\n",
            "        [    -0.0259,      0.0000],\n",
            "        [    -0.0978,      0.0000],\n",
            "        [     0.5725,      0.0000],\n",
            "        [     0.0111,      0.0000],\n",
            "        [     1.4844,      0.7100],\n",
            "        [     3.2667,      4.4300],\n",
            "        [     0.6389,      0.3700],\n",
            "        [    -0.0201,      0.0000],\n",
            "        [     0.5618,      1.1200],\n",
            "        [     0.2451,      0.0000],\n",
            "        [    -0.0027,      0.0000],\n",
            "        [     0.7417,      0.5100],\n",
            "        [     0.9874,      1.6800],\n",
            "        [    -0.0688,      0.0000],\n",
            "        [     0.7142,      0.3700],\n",
            "        [     0.6014,      0.3700],\n",
            "        [     3.5502,      3.2000],\n",
            "        [     0.1623,      0.0000],\n",
            "        [    -0.0286,      0.0000],\n",
            "        [     0.6168,      0.3700],\n",
            "        [    -0.0292,      0.0000],\n",
            "        [    -0.0364,      0.0000],\n",
            "        [    -0.0257,      0.0000],\n",
            "        [    -0.0270,      0.0000],\n",
            "        [     0.8572,      0.7100],\n",
            "        [    -0.0260,      0.0000],\n",
            "        [    -0.0470,      0.0000],\n",
            "        [    -0.0382,      0.0000],\n",
            "        [    -0.0264,      0.0000],\n",
            "        [    -0.0347,      0.0000],\n",
            "        [    -0.0235,      0.0000],\n",
            "        [     1.0417,      2.1200],\n",
            "        [    -0.0261,      0.0000],\n",
            "        [    -0.0285,      0.0000],\n",
            "        [     9.3560,      4.8300],\n",
            "        [    -0.0568,      0.0000],\n",
            "        [    -0.0269,      0.0000],\n",
            "        [    -0.0106,      0.0000],\n",
            "        [    -0.0579,      0.0000],\n",
            "        [    -0.0441,      0.0000],\n",
            "        [    -0.0624,      0.0000],\n",
            "        [     0.1565,      0.3700],\n",
            "        [     0.5700,      0.3700],\n",
            "        [    -0.0174,      0.0000],\n",
            "        [     0.0406,      0.0000],\n",
            "        [     1.3045,      1.2500],\n",
            "        [    -0.0069,      0.0000],\n",
            "        [    -0.0552,      0.0000],\n",
            "        [     1.1798,      1.4100],\n",
            "        [    -0.0442,      0.0000],\n",
            "        [    -0.0455,      0.0000],\n",
            "        [    -0.0445,      0.0000],\n",
            "        [    -0.0240,      0.0000],\n",
            "        [     1.2773,      1.4100],\n",
            "        [    19.8752,     18.4800],\n",
            "        [    -0.1326,      0.0000],\n",
            "        [     0.1658,      0.0000],\n",
            "        [     2.4274,      2.1200],\n",
            "        [     2.7946,      3.5300],\n",
            "        [     1.6321,      0.0000],\n",
            "        [     1.1055,      0.0000],\n",
            "        [     2.2584,      1.4100],\n",
            "        [     0.2342,      0.0000],\n",
            "        [     1.5216,      1.5200],\n",
            "        [     1.8973,      2.5100],\n",
            "        [     1.3228,      2.9900],\n",
            "        [     1.2714,      0.3700],\n",
            "        [     1.3212,      2.5300],\n",
            "        [     0.9358,      1.4900],\n",
            "        [    -0.1188,      0.0000],\n",
            "        [    -0.0680,      0.0000],\n",
            "        [    -0.0131,      0.0000],\n",
            "        [    -0.0488,      0.0000],\n",
            "        [    -0.1072,      0.0000],\n",
            "        [    -0.0624,      0.0000],\n",
            "        [    -0.0868,      0.0000],\n",
            "        [    -0.0790,      0.0000],\n",
            "        [    -0.0747,      0.0000],\n",
            "        [    -0.0422,      0.0000],\n",
            "        [    -0.0616,      0.0000],\n",
            "        [    -0.0459,      0.0000],\n",
            "        [    -0.0872,      0.0000],\n",
            "        [    -0.0529,      0.0000],\n",
            "        [    -0.0110,      0.0000],\n",
            "        [    -0.0312,      0.0000],\n",
            "        [    -0.0568,      0.0000],\n",
            "        [    -0.0677,      0.0000],\n",
            "        [    -0.0030,      0.0000],\n",
            "        [    -0.0806,      0.0000],\n",
            "        [    -0.0043,      0.0000],\n",
            "        [     8.8435,     11.0100],\n",
            "        [    -0.0772,      0.0000],\n",
            "        [    -0.0336,      0.0000],\n",
            "        [     1.1022,      4.4800],\n",
            "        [     0.4157,      0.0000],\n",
            "        [     0.5119,      0.3700],\n",
            "        [     0.6563,      0.3700],\n",
            "        [     0.7364,      0.7500],\n",
            "        [     1.0492,      2.5300],\n",
            "        [     1.5547,      2.5100],\n",
            "        [     0.8044,      0.0000],\n",
            "        [    46.3994,     46.3100],\n",
            "        [     2.0679,      1.1200],\n",
            "        [     3.0867,      3.1300],\n",
            "        [     3.3145,      4.4800],\n",
            "        [     3.8726,      3.1300],\n",
            "        [     2.1792,      1.8700],\n",
            "        [     3.0433,      2.2400],\n",
            "        [     1.9945,      1.4900],\n",
            "        [     2.4539,      1.5200],\n",
            "        [     2.4494,      1.5200],\n",
            "        [     2.6246,      2.5300],\n",
            "        [     4.3005,      8.4800],\n",
            "        [     2.1112,      1.5200],\n",
            "        [     2.3927,      2.6100],\n",
            "        [     0.0094,      0.0000],\n",
            "        [     2.7206,      1.0100],\n",
            "        [     2.2633,      1.4900],\n",
            "        [     2.5341,      0.3700],\n",
            "        [     4.5199,      4.9500],\n",
            "        [     4.8721,      2.8300]], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([[    27.9624,     33.3500],\n",
            "        [     2.8759,      4.3900],\n",
            "        [     3.0256,      6.8900],\n",
            "        [     1.2626,      0.7500],\n",
            "        [     1.8617,      2.2400],\n",
            "        [     2.0759,      2.0300],\n",
            "        [     2.4838,      2.8000],\n",
            "        [     2.3348,      2.5300],\n",
            "        [     1.9492,      4.0500],\n",
            "        [     2.3157,      2.0300],\n",
            "        [     2.9677,      5.6400],\n",
            "        [    20.1462,     18.2100],\n",
            "        [     1.2815,      1.0100],\n",
            "        [     1.2091,      0.3700],\n",
            "        [     1.8759,      1.1200],\n",
            "        [     2.5789,      1.4100],\n",
            "        [     1.2001,      1.8700],\n",
            "        [     0.6584,      0.7500],\n",
            "        [     2.5209,      0.7100],\n",
            "        [     0.0775,      0.0000],\n",
            "        [     0.5742,      0.7500],\n",
            "        [     2.0382,      1.2500],\n",
            "        [     0.1407,      0.0000],\n",
            "        [     2.4502,      3.5300],\n",
            "        [     1.4049,      1.0100],\n",
            "        [     1.8058,      1.6800],\n",
            "        [     0.9976,      2.0000],\n",
            "        [     1.6526,      0.7500],\n",
            "        [    17.9073,     22.6300],\n",
            "        [     1.9190,      1.8800],\n",
            "        [     1.0848,      0.7500],\n",
            "        [     1.0520,      0.7500],\n",
            "        [     2.1364,      1.4100],\n",
            "        [     1.0146,      2.5300],\n",
            "        [     0.8593,      1.1200],\n",
            "        [     0.9180,      1.4900],\n",
            "        [     0.9498,      0.0000],\n",
            "        [     2.3759,      5.6500],\n",
            "        [     0.9237,      0.3700],\n",
            "        [     2.2165,      3.5300],\n",
            "        [     2.0376,      3.1300],\n",
            "        [    17.4025,     20.8800],\n",
            "        [     2.5745,      3.5300],\n",
            "        [    -0.0587,      0.0000],\n",
            "        [     1.3164,      1.5200],\n",
            "        [    -0.0264,      0.0000],\n",
            "        [     1.1926,      1.1200],\n",
            "        [     1.4575,      0.0000],\n",
            "        [     1.4486,      0.0000],\n",
            "        [     1.4968,      2.2400],\n",
            "        [     1.2848,      1.8700],\n",
            "        [     2.2363,      2.8300],\n",
            "        [     2.2548,      4.2400],\n",
            "        [     2.4907,      3.5300],\n",
            "        [    15.3675,      8.1600],\n",
            "        [     1.1213,      0.5100],\n",
            "        [     1.6301,      0.5600],\n",
            "        [     1.6331,      0.5100],\n",
            "        [     2.2718,      0.7100],\n",
            "        [     2.2373,      1.4100],\n",
            "        [     0.0053,      0.0000],\n",
            "        [     1.7991,      0.5600],\n",
            "        [    -0.0715,      0.0000],\n",
            "        [     1.5409,      1.1200],\n",
            "        [     2.3534,      0.7100],\n",
            "        [     1.7319,      0.5600],\n",
            "        [    -0.0110,      0.0000],\n",
            "        [    -0.0697,      0.0000],\n",
            "        [     1.2926,      0.5100],\n",
            "        [     1.4949,      1.0100],\n",
            "        [    -0.1055,      0.0000],\n",
            "        [     1.6445,      2.3700],\n",
            "        [    -0.0303,      0.0000],\n",
            "        [    -0.0645,      0.0000],\n",
            "        [    -0.0706,      0.0000],\n",
            "        [    -0.0261,      0.0000],\n",
            "        [    -0.0279,      0.0000],\n",
            "        [    -0.0305,      0.0000],\n",
            "        [     0.6848,      0.5600],\n",
            "        [     0.7243,      0.5600],\n",
            "        [     0.5611,      0.5100],\n",
            "        [     0.3432,      0.3700],\n",
            "        [     0.3421,      0.3700],\n",
            "        [     2.8491,      4.7600],\n",
            "        [     0.2553,      0.7500],\n",
            "        [    -0.0170,      0.0000],\n",
            "        [    -0.0093,      0.0000],\n",
            "        [     1.1021,      0.6300],\n",
            "        [    -0.0163,      0.0000],\n",
            "        [     0.8042,      1.1200],\n",
            "        [    -0.0116,      0.0000],\n",
            "        [     0.2011,      0.0000],\n",
            "        [    -0.0158,      0.0000],\n",
            "        [    -0.0117,      0.0000],\n",
            "        [     0.0703,      0.0000],\n",
            "        [     0.6445,      0.6300],\n",
            "        [     0.4583,      0.5100],\n",
            "        [     0.4576,      0.5100],\n",
            "        [    -0.0081,      0.0000],\n",
            "        [     0.8734,      0.6300],\n",
            "        [    -0.0134,      0.0000],\n",
            "        [    -0.2240,      0.0000],\n",
            "        [     0.0189,      0.0000],\n",
            "        [    -0.0130,      0.0000],\n",
            "        [     0.0221,      0.0000],\n",
            "        [    -0.0250,      0.0000],\n",
            "        [     0.0266,      0.0000],\n",
            "        [     0.0279,      0.0000],\n",
            "        [    -0.0504,      0.0000],\n",
            "        [     0.0182,      0.0000],\n",
            "        [     0.0162,      0.0000],\n",
            "        [    -0.0044,      0.0000],\n",
            "        [    -0.0255,      0.0000],\n",
            "        [    31.3545,     37.3300],\n",
            "        [     1.7838,      2.0300],\n",
            "        [     1.6622,      1.1200],\n",
            "        [     1.3560,      1.8700],\n",
            "        [     2.4170,      1.6800],\n",
            "        [     1.1096,      1.4900],\n",
            "        [     1.4788,      4.1100],\n",
            "        [     3.0845,      4.2400],\n",
            "        [     1.8806,      4.4800],\n",
            "        [     1.9941,      2.2400],\n",
            "        [     3.2991,      2.8300],\n",
            "        [     2.5339,      5.6400],\n",
            "        [     1.2718,      2.2400],\n",
            "        [     2.6883,      1.8800],\n",
            "        [     1.1053,      1.4900],\n",
            "        [    12.4236,     12.3700],\n",
            "        [     0.6930,      0.0000],\n",
            "        [    -0.0120,      0.0000],\n",
            "        [    -0.0253,      0.0000],\n",
            "        [     0.9347,      0.0000],\n",
            "        [     0.7891,      1.1200],\n",
            "        [     1.0097,      0.0000],\n",
            "        [     2.1600,      0.7100],\n",
            "        [     0.2587,      0.0000],\n",
            "        [     1.2116,      3.0400],\n",
            "        [     0.4507,      0.3700],\n",
            "        [     0.8032,      1.4900],\n",
            "        [     0.8083,      0.0000],\n",
            "        [     1.7016,      1.2500],\n",
            "        [    -0.0388,      0.0000],\n",
            "        [     1.5388,      1.8800],\n",
            "        [     1.7854,      2.5100],\n",
            "        [    -0.1786,      0.0000],\n",
            "        [    -0.0177,      0.0000],\n",
            "        [    -0.0457,      0.0000],\n",
            "        [    -0.0284,      0.0000],\n",
            "        [    -0.0648,      0.0000],\n",
            "        [    -0.0772,      0.0000],\n",
            "        [    -0.0555,      0.0000],\n",
            "        [    -0.0446,      0.0000],\n",
            "        [    -0.0687,      0.0000],\n",
            "        [    -0.0135,      0.0000],\n",
            "        [    -0.0394,      0.0000],\n",
            "        [    -0.0288,      0.0000],\n",
            "        [    -0.0300,      0.0000],\n",
            "        [    -0.0480,      0.0000],\n",
            "        [     5.0617,      5.0000],\n",
            "        [     0.2176,      0.0000],\n",
            "        [     0.3216,      0.0000],\n",
            "        [     0.4918,      0.0000],\n",
            "        [     0.3695,      0.3700],\n",
            "        [     0.1231,      1.1200],\n",
            "        [    -0.0338,      0.0000],\n",
            "        [     1.0309,      1.8800],\n",
            "        [    -0.0588,      0.0000],\n",
            "        [     0.2496,      0.3700],\n",
            "        [     1.0280,      1.2500]], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([[    12.3899,     17.7700],\n",
            "        [     1.2472,      0.6300],\n",
            "        [     1.9059,      0.7100],\n",
            "        [     1.4462,      1.4100],\n",
            "        [     1.1926,      2.0300],\n",
            "        [     0.8690,      0.3700],\n",
            "        [     1.0330,      0.7500],\n",
            "        [     1.0056,      0.7500],\n",
            "        [    -0.0799,      0.0000],\n",
            "        [     0.9263,      1.1200],\n",
            "        [    -0.0796,      0.0000],\n",
            "        [    -0.0871,      0.0000],\n",
            "        [     1.5481,      1.1200],\n",
            "        [     1.0095,      0.5600],\n",
            "        [     1.2339,      2.2400],\n",
            "        [     1.5643,      1.4100],\n",
            "        [    -0.0669,      0.0000],\n",
            "        [    -0.0750,      0.0000],\n",
            "        [     0.9882,      2.8000],\n",
            "        [     1.6616,      1.8800],\n",
            "        [     2.1221,      4.5200],\n",
            "        [    -0.0084,      0.0000],\n",
            "        [     0.2385,      0.0000],\n",
            "        [    -0.0015,      0.0000],\n",
            "        [    -0.0193,      0.0000],\n",
            "        [    -0.0137,      0.0000],\n",
            "        [     0.8550,      1.8800],\n",
            "        [     0.0048,      0.0000],\n",
            "        [     0.8458,      1.1200],\n",
            "        [    -0.0071,      0.0000],\n",
            "        [     0.3983,      0.0000],\n",
            "        [    -0.0078,      0.0000],\n",
            "        [     0.6408,      0.5100],\n",
            "        [     0.7184,      1.0100],\n",
            "        [     0.1364,      0.0000],\n",
            "        [     3.0652,      2.1200],\n",
            "        [    -0.0155,      0.0000],\n",
            "        [     0.8793,      0.0000],\n",
            "        [    -0.0196,      0.0000],\n",
            "        [     1.3340,      2.1200],\n",
            "        [    -0.0081,      0.0000],\n",
            "        [    -0.0122,      0.0000],\n",
            "        [     0.6942,      0.0000],\n",
            "        [     0.3733,      0.0000],\n",
            "        [     0.4731,      0.0000],\n",
            "        [     0.0949,      0.0000],\n",
            "        [    -0.2597,      0.0000],\n",
            "        [    -0.0299,      0.0000],\n",
            "        [    -0.0158,      0.0000],\n",
            "        [    -0.0453,      0.0000],\n",
            "        [    -0.0400,      0.0000],\n",
            "        [    -0.0478,      0.0000],\n",
            "        [    -0.0137,      0.0000],\n",
            "        [    -0.0259,      0.0000],\n",
            "        [    -0.0213,      0.0000],\n",
            "        [    -0.0391,      0.0000],\n",
            "        [    -0.0150,      0.0000],\n",
            "        [    -0.0347,      0.0000],\n",
            "        [    -0.0304,      0.0000],\n",
            "        [    -0.0303,      0.0000],\n",
            "        [    -0.0245,      0.0000],\n",
            "        [    -0.0115,      0.0000],\n",
            "        [    18.4267,     10.3300],\n",
            "        [     1.2758,      0.3700],\n",
            "        [     0.4750,      0.0000],\n",
            "        [     1.6162,      1.1200],\n",
            "        [     0.2692,      0.0000],\n",
            "        [     0.4585,      0.3700],\n",
            "        [     1.7968,      0.7100],\n",
            "        [     1.3037,      0.7500],\n",
            "        [     1.1070,      1.4900],\n",
            "        [     1.3139,      0.3700],\n",
            "        [    -0.0242,      0.0000],\n",
            "        [     0.9520,      0.0000],\n",
            "        [     1.4074,      1.1200],\n",
            "        [     0.0047,      0.0000],\n",
            "        [     1.4558,      1.1200],\n",
            "        [     0.5807,      1.4900],\n",
            "        [     2.1350,      0.0000],\n",
            "        [     1.9546,      1.4100],\n",
            "        [     2.0332,      0.5100],\n",
            "        [     0.0115,      0.0000],\n",
            "        [     0.0091,      0.0000],\n",
            "        [     0.1801,      0.0000],\n",
            "        [    -0.0285,      0.0000],\n",
            "        [     0.6653,      0.5100],\n",
            "        [    -0.0191,      0.0000],\n",
            "        [    -0.0080,      0.0000],\n",
            "        [     0.0041,      0.0000],\n",
            "        [     0.0057,      0.0000],\n",
            "        [    -0.0124,      0.0000],\n",
            "        [     0.0153,      0.0000],\n",
            "        [     0.0063,      0.0000],\n",
            "        [    -0.0028,      0.0000],\n",
            "        [     0.4321,      0.0000],\n",
            "        [     0.0270,      0.0000],\n",
            "        [    -0.0094,      0.0000],\n",
            "        [    -0.0013,      0.0000],\n",
            "        [    -0.0110,      0.0000],\n",
            "        [    -0.0024,      0.0000],\n",
            "        [    -0.0118,      0.0000],\n",
            "        [     0.0098,      0.0000],\n",
            "        [     0.0215,      0.0000],\n",
            "        [    -0.0038,      0.0000],\n",
            "        [    -0.0048,      0.0000],\n",
            "        [    -0.0102,      0.0000],\n",
            "        [     0.0253,      0.0000],\n",
            "        [     0.0026,      0.0000],\n",
            "        [     0.0161,      0.0000],\n",
            "        [    -0.0143,      0.0000],\n",
            "        [     0.0021,      0.0000],\n",
            "        [    -0.0026,      0.0000],\n",
            "        [     1.8888,      0.0000],\n",
            "        [     0.0087,      0.0000],\n",
            "        [    -0.0061,      0.0000],\n",
            "        [     0.4348,      0.0000],\n",
            "        [     0.0106,      0.0000],\n",
            "        [     0.0070,      0.0000],\n",
            "        [    -0.0177,      0.0000],\n",
            "        [     0.0196,      0.0000],\n",
            "        [     0.6087,      0.0000],\n",
            "        [    -0.0062,      0.0000],\n",
            "        [    -0.0336,      0.0000],\n",
            "        [    -0.0114,      0.0000],\n",
            "        [     0.0078,      0.0000],\n",
            "        [    -0.0064,      0.0000],\n",
            "        [     0.0010,      0.0000],\n",
            "        [    -0.0112,      0.0000],\n",
            "        [     0.0146,      0.0000],\n",
            "        [    -0.0027,      0.0000],\n",
            "        [     0.0071,      0.0000],\n",
            "        [     0.3906,      0.0000],\n",
            "        [     0.0043,      0.0000]], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([[    21.5312,     19.9600],\n",
            "        [     1.7765,      1.1200],\n",
            "        [     1.7216,      2.6100],\n",
            "        [     0.8773,      0.0000],\n",
            "        [     1.8681,      1.0100],\n",
            "        [     0.9984,      0.0000],\n",
            "        [     1.8610,      1.5200],\n",
            "        [     2.0844,      4.0500],\n",
            "        [     2.1400,      4.2400],\n",
            "        [     1.2729,      0.0000],\n",
            "        [     2.5508,      3.5300],\n",
            "        [     1.4700,      1.1200],\n",
            "        [     1.1760,      0.7500],\n",
            "        [    23.0797,     18.6400],\n",
            "        [     0.4603,      1.1200],\n",
            "        [     0.8237,      0.3700],\n",
            "        [     1.6256,      3.3600],\n",
            "        [     0.0106,      0.0000],\n",
            "        [     0.0171,      0.0000],\n",
            "        [     0.6105,      0.7500],\n",
            "        [     1.1107,      1.0100],\n",
            "        [     1.9216,      1.2500],\n",
            "        [     1.4345,      0.5600],\n",
            "        [     1.7342,      1.1200],\n",
            "        [     0.7341,      1.8700],\n",
            "        [     1.4745,      2.5300],\n",
            "        [     1.8847,      0.5600],\n",
            "        [     1.9427,      3.7600],\n",
            "        [     1.2310,      0.3700],\n",
            "        [    -0.0053,      0.0000],\n",
            "        [    26.0925,     26.5600],\n",
            "        [     1.9820,      2.8000],\n",
            "        [     0.8200,      2.9900],\n",
            "        [     0.9591,      1.8700],\n",
            "        [     0.9667,      1.8700],\n",
            "        [     2.4078,      5.6500],\n",
            "        [     2.5223,      4.2400],\n",
            "        [     1.5467,      2.5300],\n",
            "        [     1.9040,      0.5600],\n",
            "        [     1.3094,      1.5200],\n",
            "        [     1.5744,      2.5300],\n",
            "        [    34.8230,     35.4000],\n",
            "        [     1.9820,      3.0400],\n",
            "        [     1.8869,      2.5300],\n",
            "        [     1.9168,      2.5300],\n",
            "        [     1.5869,      2.2400],\n",
            "        [     2.4767,      2.8000],\n",
            "        [     1.3410,      3.0000],\n",
            "        [     1.6698,      2.2400],\n",
            "        [     3.3672,      5.6400],\n",
            "        [     2.5929,      3.3600],\n",
            "        [     1.7098,      1.1200],\n",
            "        [     1.5333,      2.2400],\n",
            "        [     1.1125,      2.9900],\n",
            "        [     1.1283,      1.6700],\n",
            "        [     0.0116,      0.0000],\n",
            "        [    -0.0405,      0.0000],\n",
            "        [    -0.0186,      0.0000],\n",
            "        [    -0.0207,      0.0000],\n",
            "        [    -0.0322,      0.0000],\n",
            "        [    -0.0531,      0.0000],\n",
            "        [    -0.0295,      0.0000],\n",
            "        [    -0.0214,      0.0000],\n",
            "        [    -0.0512,      0.0000],\n",
            "        [    -0.0457,      0.0000],\n",
            "        [    -0.0066,      0.0000],\n",
            "        [    -0.0298,      0.0000],\n",
            "        [    -0.0121,      0.0000],\n",
            "        [     0.1692,      2.1300],\n",
            "        [    -0.0618,      0.0000],\n",
            "        [     0.2386,      0.3700],\n",
            "        [     0.0815,      0.3700],\n",
            "        [    -0.0516,      0.0000],\n",
            "        [    -0.0600,      0.0000],\n",
            "        [    -0.0555,      0.0000],\n",
            "        [    -0.0554,      0.0000],\n",
            "        [    -0.0581,      0.0000],\n",
            "        [     0.0115,      0.0000],\n",
            "        [    -0.0623,      0.0000],\n",
            "        [     0.0916,      0.3700],\n",
            "        [    -0.0504,      0.0000],\n",
            "        [     0.4186,      1.0100],\n",
            "        [     5.1667,      3.2800],\n",
            "        [    -0.0842,      0.0000],\n",
            "        [     1.0109,      1.4100],\n",
            "        [    -0.0385,      0.0000],\n",
            "        [    -0.0467,      0.0000],\n",
            "        [     0.1372,      0.3700],\n",
            "        [    -0.0575,      0.0000],\n",
            "        [    -0.0751,      0.0000],\n",
            "        [    -0.0517,      0.0000],\n",
            "        [     0.2209,      0.3700],\n",
            "        [     0.4444,      0.3700],\n",
            "        [     0.2133,      0.7500],\n",
            "        [     6.1889,      8.0800],\n",
            "        [     1.4276,      2.1200],\n",
            "        [     0.1076,      0.0000],\n",
            "        [     1.1281,      0.5600],\n",
            "        [     0.2109,      0.7500],\n",
            "        [    -0.1012,      0.0000],\n",
            "        [    -0.1171,      0.0000],\n",
            "        [     1.1614,      1.1200],\n",
            "        [     1.1988,      0.6300],\n",
            "        [     0.6043,      0.3700],\n",
            "        [     0.2840,      0.0000],\n",
            "        [    -0.0952,      0.0000],\n",
            "        [     0.2967,      0.0000],\n",
            "        [     1.6648,      1.4100],\n",
            "        [     0.4653,      0.7500],\n",
            "        [    -0.0875,      0.0000],\n",
            "        [    -0.0711,      0.0000],\n",
            "        [     0.2305,      0.3700],\n",
            "        [    10.7142,     15.1200],\n",
            "        [    -0.0090,      0.0000],\n",
            "        [     1.5423,      2.8000],\n",
            "        [     0.0660,      0.0000],\n",
            "        [     1.2171,      0.7500],\n",
            "        [     1.2827,      1.4900],\n",
            "        [    -0.0266,      0.0000],\n",
            "        [     1.7681,      3.3600],\n",
            "        [    -0.0311,      0.0000],\n",
            "        [     0.0952,      0.0000],\n",
            "        [    -0.0201,      0.0000],\n",
            "        [     1.5755,      1.6800],\n",
            "        [     1.6140,      1.6800],\n",
            "        [     0.9771,      0.0000],\n",
            "        [     1.1822,      1.6800],\n",
            "        [     1.4971,      1.6800],\n",
            "        [    26.9391,     18.4300],\n",
            "        [     1.8482,      2.8000],\n",
            "        [    -0.0038,      0.0000],\n",
            "        [     2.5538,      2.8300],\n",
            "        [     1.0308,      0.7500],\n",
            "        [     2.1365,      0.5600],\n",
            "        [    -0.0145,      0.0000],\n",
            "        [     1.1001,      1.4900],\n",
            "        [     1.2467,      1.4900],\n",
            "        [     1.0185,      0.0000],\n",
            "        [     0.7528,      0.7500],\n",
            "        [     1.1181,      1.5200],\n",
            "        [     2.0904,      2.2400],\n",
            "        [     1.1517,      1.1200],\n",
            "        [     1.1670,      0.5100],\n",
            "        [     0.0139,      0.0000],\n",
            "        [     2.0170,      1.2500],\n",
            "        [     1.0648,      1.1200],\n",
            "        [    13.5469,     15.5100],\n",
            "        [    -0.0382,      0.0000],\n",
            "        [     1.0835,      0.7500],\n",
            "        [     1.2100,      2.8000],\n",
            "        [     0.4510,      1.4900],\n",
            "        [     0.7001,      1.5200],\n",
            "        [     1.9631,      2.1200],\n",
            "        [     0.2272,      0.0000],\n",
            "        [     1.6126,      1.8800],\n",
            "        [    -0.1157,      0.0000],\n",
            "        [     2.0097,      1.4100],\n",
            "        [     1.1870,      0.0000],\n",
            "        [     1.7383,      1.4100],\n",
            "        [     1.7108,      0.7100],\n",
            "        [     1.7595,      1.4100],\n",
            "        [     0.5328,      0.0000],\n",
            "        [     0.0127,      0.0000],\n",
            "        [    -0.0483,      0.0000],\n",
            "        [    -0.0847,      0.0000],\n",
            "        [    -0.0614,      0.0000],\n",
            "        [    -0.0734,      0.0000],\n",
            "        [    -0.1002,      0.0000],\n",
            "        [    -0.0511,      0.0000],\n",
            "        [    -0.0527,      0.0000],\n",
            "        [    -0.0812,      0.0000],\n",
            "        [    -0.0474,      0.0000],\n",
            "        [    -0.0559,      0.0000],\n",
            "        [    -0.0859,      0.0000],\n",
            "        [    -0.0564,      0.0000],\n",
            "        [    -0.0744,      0.0000],\n",
            "        [    -0.0960,      0.0000],\n",
            "        [    -0.0922,      0.0000],\n",
            "        [    -0.0810,      0.0000]], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([[     6.5126,      3.5100],\n",
            "        [     0.0653,      0.0000],\n",
            "        [     0.0149,      0.0000],\n",
            "        [     0.6745,      1.0100],\n",
            "        [     0.6439,      0.3700],\n",
            "        [     0.1538,      0.0000],\n",
            "        [     0.0051,      0.0000],\n",
            "        [     0.0062,      0.0000],\n",
            "        [     1.5717,      0.7100],\n",
            "        [     1.7864,      1.4100],\n",
            "        [     0.8142,      0.0000],\n",
            "        [     0.0024,      0.0000],\n",
            "        [     0.5807,      0.0000],\n",
            "        [     0.0104,      0.0000],\n",
            "        [     0.0007,      0.0000],\n",
            "        [     0.0199,      0.0000],\n",
            "        [     0.0115,      0.0000],\n",
            "        [    -0.0007,      0.0000],\n",
            "        [     0.0152,      0.0000],\n",
            "        [     0.0193,      0.0000],\n",
            "        [     0.0009,      0.0000],\n",
            "        [     0.0201,      0.0000],\n",
            "        [    -0.0018,      0.0000],\n",
            "        [     0.0154,      0.0000],\n",
            "        [     0.0166,      0.0000],\n",
            "        [    -0.0018,      0.0000],\n",
            "        [     3.7823,      0.0000],\n",
            "        [     0.1562,      0.0000],\n",
            "        [     0.0885,      0.0000],\n",
            "        [     0.0236,      0.0000],\n",
            "        [     0.2049,      0.0000],\n",
            "        [     0.0511,      0.0000],\n",
            "        [     0.0271,      0.0000],\n",
            "        [     0.0276,      0.0000],\n",
            "        [     0.0279,      0.0000],\n",
            "        [     0.0226,      0.0000],\n",
            "        [     0.0228,      0.0000],\n",
            "        [     0.0153,      0.0000],\n",
            "        [     0.0169,      0.0000],\n",
            "        [     0.0276,      0.0000],\n",
            "        [     0.2885,      0.0000],\n",
            "        [     0.0060,      0.0000],\n",
            "        [     0.0194,      0.0000],\n",
            "        [     0.0233,      0.0000],\n",
            "        [     0.0174,      0.0000],\n",
            "        [     2.6246,      1.6400],\n",
            "        [     0.0178,      0.0000],\n",
            "        [     0.0300,      0.0000],\n",
            "        [     0.0223,      0.0000],\n",
            "        [     0.3173,      0.3700],\n",
            "        [     0.5899,      0.0000],\n",
            "        [     0.0022,      0.0000],\n",
            "        [     0.6378,      0.5600],\n",
            "        [     0.0166,      0.0000],\n",
            "        [     0.0371,      0.0000],\n",
            "        [     0.9825,      0.7100],\n",
            "        [    -0.0139,      0.0000],\n",
            "        [     0.0281,      0.0000],\n",
            "        [     0.0234,      0.0000],\n",
            "        [     0.0215,      0.0000],\n",
            "        [     4.3396,      4.3300],\n",
            "        [     0.0054,      0.0000],\n",
            "        [     0.0082,      0.0000],\n",
            "        [     0.0112,      0.0000],\n",
            "        [     0.0082,      0.0000],\n",
            "        [     1.2065,      1.4100],\n",
            "        [     0.0105,      0.0000],\n",
            "        [     0.0132,      0.0000],\n",
            "        [     0.0097,      0.0000],\n",
            "        [     0.0165,      0.0000],\n",
            "        [     0.2881,      0.0000],\n",
            "        [     0.0047,      0.0000],\n",
            "        [     0.8109,      0.6300],\n",
            "        [     0.5376,      0.3700],\n",
            "        [    -0.0037,      0.0000],\n",
            "        [     0.5988,      0.5100],\n",
            "        [     0.1217,      0.0000],\n",
            "        [     0.0161,      0.0000],\n",
            "        [     1.0858,      1.4100],\n",
            "        [     0.0029,      0.0000],\n",
            "        [     0.0055,      0.0000],\n",
            "        [     1.6544,      0.0000],\n",
            "        [     0.0190,      0.0000],\n",
            "        [     0.3740,      0.0000],\n",
            "        [     0.2270,      0.0000],\n",
            "        [     0.0208,      0.0000],\n",
            "        [     0.2829,      0.0000],\n",
            "        [     0.0433,      0.0000],\n",
            "        [     0.0264,      0.0000],\n",
            "        [     0.1163,      0.0000],\n",
            "        [     0.0038,      0.0000],\n",
            "        [     0.0319,      0.0000],\n",
            "        [     0.0654,      0.0000],\n",
            "        [     0.0289,      0.0000],\n",
            "        [     0.0278,      0.0000],\n",
            "        [     0.0215,      0.0000],\n",
            "        [     0.0487,      0.0000],\n",
            "        [     0.0476,      0.0000],\n",
            "        [     0.0191,      0.0000],\n",
            "        [     0.0321,      0.0000],\n",
            "        [     0.0489,      0.0000],\n",
            "        [     0.0252,      0.0000],\n",
            "        [     0.0231,      0.0000],\n",
            "        [     0.0364,      0.0000],\n",
            "        [     0.0367,      0.0000],\n",
            "        [     0.0354,      0.0000],\n",
            "        [     0.0564,      0.0000],\n",
            "        [     0.0404,      0.0000],\n",
            "        [     3.7262,      2.7500],\n",
            "        [     0.6076,      0.0000],\n",
            "        [     0.0185,      0.0000],\n",
            "        [     0.0408,      0.0000],\n",
            "        [     0.0436,      0.0000],\n",
            "        [     0.2739,      0.0000],\n",
            "        [     0.0419,      0.0000],\n",
            "        [     1.6019,      1.4100],\n",
            "        [     0.0396,      0.0000],\n",
            "        [     0.1665,      0.0000],\n",
            "        [     0.0399,      0.0000],\n",
            "        [     0.8492,      0.6300],\n",
            "        [     0.0404,      0.0000],\n",
            "        [     0.9426,      0.7100]], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([[    15.6035,     15.1100],\n",
            "        [     0.1602,      0.0000],\n",
            "        [     1.1568,      0.3700],\n",
            "        [     1.0898,      1.4900],\n",
            "        [     1.9470,      1.5200],\n",
            "        [     1.1142,      0.3700],\n",
            "        [     2.2854,      3.3600],\n",
            "        [     1.2076,      0.5100],\n",
            "        [     2.1961,      1.1200],\n",
            "        [     1.6160,      1.4900],\n",
            "        [     2.2962,      1.1200],\n",
            "        [     1.9704,      1.5200],\n",
            "        [     2.1967,      0.7100],\n",
            "        [     1.5725,      1.5200],\n",
            "        [     2.6049,      4.1300],\n",
            "        [     0.1243,      0.0000],\n",
            "        [    -0.0198,      0.0000],\n",
            "        [     0.0080,      0.0000],\n",
            "        [     0.3148,      1.1200],\n",
            "        [     0.5728,      1.1200],\n",
            "        [     0.0072,      0.0000],\n",
            "        [     0.5154,      1.0100],\n",
            "        [     0.9187,      0.5100],\n",
            "        [     0.7444,      0.3700],\n",
            "        [     0.0110,      0.0000],\n",
            "        [     2.0531,      1.4100],\n",
            "        [     0.3928,      0.0000],\n",
            "        [     0.0183,      0.0000],\n",
            "        [     0.0199,      0.0000],\n",
            "        [     0.4681,      0.0000],\n",
            "        [     0.0582,      0.0000],\n",
            "        [     0.0154,      0.0000],\n",
            "        [     0.0165,      0.0000],\n",
            "        [     0.0284,      0.0000],\n",
            "        [     0.8369,      0.7100],\n",
            "        [     0.0169,      0.0000],\n",
            "        [     0.0220,      0.0000],\n",
            "        [     0.0141,      0.0000],\n",
            "        [     0.8759,      0.7100],\n",
            "        [    -0.5403,      0.0000],\n",
            "        [    -0.0076,      0.0000],\n",
            "        [     0.0140,      0.0000],\n",
            "        [     0.0057,      0.0000],\n",
            "        [    -0.0066,      0.0000],\n",
            "        [     0.0024,      0.0000],\n",
            "        [     0.0010,      0.0000],\n",
            "        [    -0.0115,      0.0000],\n",
            "        [     0.0088,      0.0000],\n",
            "        [     0.0092,      0.0000],\n",
            "        [     0.0061,      0.0000],\n",
            "        [    -0.0123,      0.0000],\n",
            "        [    -0.0211,      0.0000],\n",
            "        [    -0.0133,      0.0000],\n",
            "        [    20.0025,     20.0100],\n",
            "        [     1.8838,      2.2400],\n",
            "        [     1.9432,      1.0100],\n",
            "        [     1.9191,      2.2400],\n",
            "        [     1.8016,      1.1200],\n",
            "        [     0.1089,      0.0000],\n",
            "        [     1.9538,      1.8800],\n",
            "        [     2.2802,      1.8800],\n",
            "        [     2.2774,      0.6300],\n",
            "        [     2.6205,      2.1200],\n",
            "        [     1.4137,      0.7500],\n",
            "        [     2.4597,      0.7100],\n",
            "        [     1.5166,      1.4900],\n",
            "        [     1.9517,      1.6800],\n",
            "        [     1.5878,      1.5200],\n",
            "        [     1.2630,      0.7500],\n",
            "        [     6.0019,      6.1900],\n",
            "        [     1.2514,      0.5600],\n",
            "        [    -0.0083,      0.0000],\n",
            "        [     0.0297,      0.0000],\n",
            "        [     1.7315,      2.1200],\n",
            "        [     1.1999,      0.3700],\n",
            "        [     0.8193,      0.3700],\n",
            "        [     1.2402,      0.3700],\n",
            "        [     0.0395,      0.0000],\n",
            "        [     0.0289,      0.0000],\n",
            "        [     1.3636,      0.5600],\n",
            "        [     0.0304,      0.0000],\n",
            "        [     1.5641,      0.7100],\n",
            "        [     0.0258,      0.0000],\n",
            "        [    -0.0107,      0.0000],\n",
            "        [     0.9157,      0.3700],\n",
            "        [     0.6786,      0.3700],\n",
            "        [     1.1716,      0.3700],\n",
            "        [     3.8526,      1.6400],\n",
            "        [     0.3244,      0.0000],\n",
            "        [     0.0477,      0.0000],\n",
            "        [     0.0372,      0.0000],\n",
            "        [     0.7867,      0.5600],\n",
            "        [     1.5226,      0.7100],\n",
            "        [     0.0245,      0.0000],\n",
            "        [     0.6414,      0.3700],\n",
            "        [     0.0672,      0.0000],\n",
            "        [     0.2779,      0.0000],\n",
            "        [     0.0345,      0.0000],\n",
            "        [     0.0121,      0.0000],\n",
            "        [     0.0526,      0.0000],\n",
            "        [     0.0241,      0.0000],\n",
            "        [     0.0124,      0.0000],\n",
            "        [     0.0474,      0.0000],\n",
            "        [     0.0116,      0.0000],\n",
            "        [     3.5467,      1.5900],\n",
            "        [     0.4320,      0.3700],\n",
            "        [     0.0286,      0.0000],\n",
            "        [     0.0393,      0.0000],\n",
            "        [     0.0365,      0.0000],\n",
            "        [     0.7995,      0.5100],\n",
            "        [     1.1638,      0.7100],\n",
            "        [     0.3278,      0.0000],\n",
            "        [     0.0184,      0.0000],\n",
            "        [     0.0274,      0.0000],\n",
            "        [     0.0259,      0.0000]], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([[    -0.0601,      0.0000],\n",
            "        [     0.0214,      0.0000],\n",
            "        [    -0.0003,      0.0000],\n",
            "        [     0.0059,      0.0000],\n",
            "        [     0.0345,      0.0000],\n",
            "        [    -0.0097,      0.0000],\n",
            "        [     0.0163,      0.0000],\n",
            "        [     0.0014,      0.0000],\n",
            "        [     0.0400,      0.0000],\n",
            "        [     0.0052,      0.0000],\n",
            "        [     0.0172,      0.0000],\n",
            "        [     0.0217,      0.0000],\n",
            "        [     0.0010,      0.0000],\n",
            "        [     0.0493,      0.0000],\n",
            "        [    12.1559,      5.3900],\n",
            "        [     0.9036,      0.0000],\n",
            "        [     0.9252,      1.1200],\n",
            "        [     0.5382,      0.0000],\n",
            "        [     1.5895,      2.5100],\n",
            "        [     0.6908,      0.0000],\n",
            "        [    -0.0049,      0.0000],\n",
            "        [     0.9479,      0.0000],\n",
            "        [     0.8055,      0.7500],\n",
            "        [    -0.0107,      0.0000],\n",
            "        [     0.6659,      0.0000],\n",
            "        [     0.7837,      1.0100],\n",
            "        [     0.7660,      0.0000],\n",
            "        [     0.6802,      0.0000],\n",
            "        [    20.3158,     17.4800],\n",
            "        [     0.9193,      0.3700],\n",
            "        [     0.9090,      0.7500],\n",
            "        [     1.0876,      0.7500],\n",
            "        [     0.9413,      0.7500],\n",
            "        [     0.7645,      1.4900],\n",
            "        [     2.3974,      1.4100],\n",
            "        [     1.5359,      2.0300],\n",
            "        [     1.0372,      0.7500],\n",
            "        [     0.9660,      1.4900],\n",
            "        [     2.1898,      1.8800],\n",
            "        [     1.7367,      0.5600],\n",
            "        [     1.5253,      1.5200],\n",
            "        [     1.1762,      1.1200],\n",
            "        [     0.9487,      0.7500],\n",
            "        [    -0.0251,      0.0000],\n",
            "        [     1.9958,      1.1200],\n",
            "        [     1.0519,      0.7500],\n",
            "        [    46.5736,     54.1200],\n",
            "        [     4.2206,      9.4000],\n",
            "        [     1.9840,      2.6100],\n",
            "        [     3.0399,      2.5300],\n",
            "        [     3.5943,      6.1600],\n",
            "        [     4.7127,      4.2400],\n",
            "        [     2.9478,      3.3600],\n",
            "        [     3.9781,      3.1300],\n",
            "        [     4.1820,      4.9500],\n",
            "        [     4.5289,      7.0700],\n",
            "        [     3.2453,      5.0700],\n",
            "        [     3.6129,      5.6000],\n",
            "        [     6.9298,      6.7900],\n",
            "        [     0.5442,      0.5100],\n",
            "        [    -0.0431,      0.0000],\n",
            "        [    -0.0211,      0.0000],\n",
            "        [     1.6098,      0.7100],\n",
            "        [     0.0028,      0.0000],\n",
            "        [    -0.0145,      0.0000],\n",
            "        [     0.4863,      0.7500],\n",
            "        [     0.2722,      0.5100],\n",
            "        [     1.3656,      0.7100],\n",
            "        [     1.7290,      2.1200],\n",
            "        [    -0.0252,      0.0000],\n",
            "        [    -0.0108,      0.0000],\n",
            "        [    -0.0313,      0.0000],\n",
            "        [     0.5564,      0.3700],\n",
            "        [     0.5307,      0.7500],\n",
            "        [     0.0001,      0.0000],\n",
            "        [    -0.0411,      0.0000],\n",
            "        [    -0.0342,      0.0000],\n",
            "        [     0.5154,      0.3700],\n",
            "        [    -0.0249,      0.0000],\n",
            "        [     5.6986,      4.2700],\n",
            "        [    -0.0198,      0.0000],\n",
            "        [     1.2141,      1.8800],\n",
            "        [     1.3922,      0.7100],\n",
            "        [     0.8276,      1.1200],\n",
            "        [     0.7438,      0.5600],\n",
            "        [     0.4108,      0.0000],\n",
            "        [     0.4067,      0.0000],\n",
            "        [     0.1814,      0.0000],\n",
            "        [     0.4613,      0.0000],\n",
            "        [    -0.0267,      0.0000],\n",
            "        [    -0.0280,      0.0000],\n",
            "        [     0.0259,      0.0000],\n",
            "        [     0.0986,      0.0000],\n",
            "        [     0.6281,      0.0000],\n",
            "        [     0.1915,      0.0000],\n",
            "        [     2.2950,      2.2800],\n",
            "        [    -0.0302,      0.0000],\n",
            "        [    -0.0169,      0.0000],\n",
            "        [    -0.0067,      0.0000],\n",
            "        [    -0.0361,      0.0000],\n",
            "        [    -0.0161,      0.0000],\n",
            "        [    -0.0108,      0.0000],\n",
            "        [    -0.0194,      0.0000],\n",
            "        [    -0.0036,      0.0000],\n",
            "        [     0.5475,      0.5100],\n",
            "        [    -0.0021,      0.0000],\n",
            "        [     0.0033,      0.0000],\n",
            "        [    -0.0064,      0.0000],\n",
            "        [    -0.0187,      0.0000],\n",
            "        [     0.4702,      0.5600],\n",
            "        [    -0.0109,      0.0000],\n",
            "        [     0.0079,      0.0000],\n",
            "        [     0.3074,      0.5100],\n",
            "        [    -0.0156,      0.0000],\n",
            "        [    -0.0241,      0.0000],\n",
            "        [     1.1068,      0.7100],\n",
            "        [     4.4446,      2.3900],\n",
            "        [     0.4278,      0.3700],\n",
            "        [    -0.0052,      0.0000],\n",
            "        [    -0.0109,      0.0000],\n",
            "        [    -0.0066,      0.0000],\n",
            "        [    -0.0232,      0.0000],\n",
            "        [    -0.0030,      0.0000],\n",
            "        [    -0.0089,      0.0000],\n",
            "        [    -0.0099,      0.0000],\n",
            "        [     1.1606,      0.7100],\n",
            "        [     0.0064,      0.0000],\n",
            "        [    -0.0033,      0.0000],\n",
            "        [     0.5854,      0.5600],\n",
            "        [    -0.0041,      0.0000],\n",
            "        [    -0.0082,      0.0000],\n",
            "        [    -0.0088,      0.0000],\n",
            "        [     0.2829,      0.3700],\n",
            "        [    -0.0104,      0.0000],\n",
            "        [    -0.0095,      0.0000],\n",
            "        [     0.4167,      0.3700],\n",
            "        [    15.7062,     10.5200],\n",
            "        [     0.0856,      0.0000],\n",
            "        [    -0.0215,      0.0000],\n",
            "        [     0.6420,      0.0000],\n",
            "        [     1.5604,      3.3600],\n",
            "        [     1.4648,      0.5600],\n",
            "        [     0.8526,      0.0000],\n",
            "        [     0.0141,      0.0000],\n",
            "        [     0.7272,      0.0000],\n",
            "        [     0.9670,      0.3700],\n",
            "        [     0.8558,      0.3700],\n",
            "        [     0.7387,      0.7500],\n",
            "        [     1.1946,      1.6800],\n",
            "        [     1.0142,      0.0000],\n",
            "        [     1.6944,      0.6300],\n",
            "        [     1.1771,      2.8000],\n",
            "        [    -0.0827,      0.0000],\n",
            "        [    -0.0690,      0.0000],\n",
            "        [     0.0195,      0.0000],\n",
            "        [     0.0051,      0.0000],\n",
            "        [    -0.0462,      0.0000],\n",
            "        [    -0.0566,      0.0000],\n",
            "        [     0.0225,      0.0000],\n",
            "        [     0.0186,      0.0000],\n",
            "        [    -0.0166,      0.0000],\n",
            "        [    -0.0468,      0.0000],\n",
            "        [    -0.0242,      0.0000],\n",
            "        [     8.2809,     11.7600],\n",
            "        [     0.0170,      0.0000],\n",
            "        [     0.6749,      0.3700],\n",
            "        [     1.5054,      1.4100],\n",
            "        [     0.5379,      1.1200],\n",
            "        [    -0.0047,      0.0000],\n",
            "        [     1.0249,      2.0300],\n",
            "        [    -0.0240,      0.0000],\n",
            "        [     0.2107,      1.3300],\n",
            "        [     0.6340,      0.0000],\n",
            "        [     1.1258,      1.1200],\n",
            "        [     0.8993,      1.1200],\n",
            "        [     0.7698,      0.7500],\n",
            "        [     1.5269,      2.5100],\n",
            "        [    -0.0114,      0.0000],\n",
            "        [    53.7436,     56.8800],\n",
            "        [     3.5488,      2.2400],\n",
            "        [     4.4879,      2.2400],\n",
            "        [     3.8899,      0.7500],\n",
            "        [     6.5894,      3.7300],\n",
            "        [     2.7269,      2.9900],\n",
            "        [     3.3157,      2.2400],\n",
            "        [     6.3967,      2.6100],\n",
            "        [     6.2717,      3.9200],\n",
            "        [     6.3047,      4.9500],\n",
            "        [     5.7391,      7.0700],\n",
            "        [     5.4458,      2.2400],\n",
            "        [     7.0015,      3.9200],\n",
            "        [     4.9276,      3.3600],\n",
            "        [     5.5695,     12.0100],\n",
            "        [     5.1006,      2.6100]], device='cuda:0', grad_fn=<CatBackward0>)\n",
            "tensor([[    18.3306,     13.6500],\n",
            "        [     0.0646,      0.0000],\n",
            "        [     1.8088,      1.6800],\n",
            "        [     1.1966,      0.0000],\n",
            "        [     2.1650,      0.7100],\n",
            "        [     2.1952,      2.1200],\n",
            "        [     1.9730,      1.2500],\n",
            "        [     0.2886,      0.0000],\n",
            "        [     0.0778,      0.0000],\n",
            "        [     0.0684,      0.0000],\n",
            "        [     1.8359,      0.6300],\n",
            "        [     0.0644,      0.0000],\n",
            "        [     1.4834,      2.0300],\n",
            "        [     1.2473,      0.5100],\n",
            "        [     0.0556,      0.0000],\n",
            "        [     0.0688,      0.0000],\n",
            "        [     0.2129,      0.0000],\n",
            "        [     1.7126,      1.1200],\n",
            "        [     2.5866,      2.1200],\n",
            "        [     1.1232,      1.4900],\n",
            "        [    13.8795,     12.6900],\n",
            "        [     0.0283,      0.0000],\n",
            "        [     1.3179,      1.0100],\n",
            "        [     0.0424,      0.0000],\n",
            "        [     1.6219,      0.5600],\n",
            "        [     1.3743,      0.7500],\n",
            "        [     1.5103,      0.5100],\n",
            "        [     0.7983,      0.3300],\n",
            "        [     0.1491,      0.0000],\n",
            "        [     1.3516,      0.5100],\n",
            "        [     0.0384,      0.0000],\n",
            "        [     1.2743,      1.0100],\n",
            "        [     1.8208,      2.2400],\n",
            "        [     2.7960,      2.1200],\n",
            "        [     1.6563,      1.1200],\n",
            "        [     0.9483,      0.3700],\n",
            "        [     0.9533,      0.7500],\n",
            "        [     1.0861,      0.0000],\n",
            "        [     2.4337,      1.4100],\n",
            "        [     6.5090,      7.4800],\n",
            "        [     0.7442,      0.7500],\n",
            "        [     0.6037,      0.0000],\n",
            "        [     0.9053,      2.0300],\n",
            "        [     0.6167,      0.3300],\n",
            "        [     0.9791,      2.2400],\n",
            "        [     0.0199,      0.0000],\n",
            "        [     0.1674,      0.3700],\n",
            "        [     0.7247,      0.0000],\n",
            "        [     0.9703,      1.0100],\n",
            "        [     1.1011,      0.0000],\n",
            "        [     0.7772,      0.0000],\n",
            "        [     0.7627,      0.7500],\n",
            "        [     0.6680,      0.0000],\n",
            "        [     0.0131,      0.0000],\n",
            "        [     0.0147,      0.0000],\n",
            "        [     0.0306,      0.0000],\n",
            "        [     0.0125,      0.0000],\n",
            "        [     0.0234,      0.0000],\n",
            "        [     0.0113,      0.0000],\n",
            "        [     0.0127,      0.0000],\n",
            "        [     0.0134,      0.0000],\n",
            "        [     0.0094,      0.0000],\n",
            "        [     0.0195,      0.0000],\n",
            "        [     0.0070,      0.0000],\n",
            "        [     0.0254,      0.0000],\n",
            "        [     0.0179,      0.0000],\n",
            "        [     0.0107,      0.0000],\n",
            "        [    -0.1515,      0.0000],\n",
            "        [     0.0101,      0.0000],\n",
            "        [     0.0018,      0.0000],\n",
            "        [     0.0057,      0.0000],\n",
            "        [     0.0024,      0.0000],\n",
            "        [     0.0086,      0.0000],\n",
            "        [    -0.0307,      0.0000],\n",
            "        [     0.0160,      0.0000],\n",
            "        [     0.0558,      0.0000],\n",
            "        [     0.0234,      0.0000],\n",
            "        [     0.0077,      0.0000],\n",
            "        [     0.0223,      0.0000],\n",
            "        [     0.0085,      0.0000],\n",
            "        [     0.0108,      0.0000],\n",
            "        [     0.0075,      0.0000],\n",
            "        [     0.0166,      0.0000],\n",
            "        [    -0.0107,      0.0000],\n",
            "        [     0.0065,      0.0000],\n",
            "        [    -0.0190,      0.0000],\n",
            "        [     7.7966,      3.3100],\n",
            "        [    -0.0440,      0.0000],\n",
            "        [     1.3450,      0.7100],\n",
            "        [     0.8342,      0.3700],\n",
            "        [     0.0060,      0.0000],\n",
            "        [     1.8010,      0.7100],\n",
            "        [     0.0015,      0.0000],\n",
            "        [     1.1598,      0.0000],\n",
            "        [     0.0145,      0.0000],\n",
            "        [     0.9490,      0.5100],\n",
            "        [    -0.0288,      0.0000],\n",
            "        [     0.0001,      0.0000],\n",
            "        [    -0.0088,      0.0000],\n",
            "        [    -0.0338,      0.0000],\n",
            "        [    -0.0053,      0.0000],\n",
            "        [     0.7605,      1.0100],\n",
            "        [    -0.0223,      0.0000],\n",
            "        [     0.7252,      0.0000],\n",
            "        [    -0.0113,      0.0000],\n",
            "        [    -0.0085,      0.0000],\n",
            "        [     4.3739,      3.6700],\n",
            "        [     0.6227,      0.3700],\n",
            "        [     0.6307,      0.5100],\n",
            "        [    -0.0017,      0.0000],\n",
            "        [     0.4924,      0.3700],\n",
            "        [     0.0160,      0.0000],\n",
            "        [     0.0117,      0.0000],\n",
            "        [     0.0019,      0.0000],\n",
            "        [     0.0262,      0.0000],\n",
            "        [     1.2202,      0.7100],\n",
            "        [     0.0098,      0.0000],\n",
            "        [     1.0460,      0.7100],\n",
            "        [     0.0116,      0.0000],\n",
            "        [     0.0247,      0.0000],\n",
            "        [     0.0105,      0.0000],\n",
            "        [     0.0137,      0.0000],\n",
            "        [     0.0174,      0.0000],\n",
            "        [     1.0166,      0.6300],\n",
            "        [     0.0314,      0.0000],\n",
            "        [     0.4191,      0.3700],\n",
            "        [     0.0290,      0.0000],\n",
            "        [    10.5051,     14.2700],\n",
            "        [     1.4102,      1.6800],\n",
            "        [     0.5788,      0.0000],\n",
            "        [     1.0015,      1.1200],\n",
            "        [     1.0662,      0.5100],\n",
            "        [     0.6018,      0.7500],\n",
            "        [     1.0772,      1.5200],\n",
            "        [     1.2912,      1.1200],\n",
            "        [     1.5210,      3.1300],\n",
            "        [     1.7006,      1.8800],\n",
            "        [     0.7854,      0.7500],\n",
            "        [     1.5054,      1.2500],\n",
            "        [     1.2717,      0.5600],\n",
            "        [    17.7194,     13.5200],\n",
            "        [     1.1293,      0.3700],\n",
            "        [     1.1346,      0.0000],\n",
            "        [     1.9648,      0.6300],\n",
            "        [     1.1444,      0.0000],\n",
            "        [     1.9052,      2.8000],\n",
            "        [     1.8311,      2.8000],\n",
            "        [     2.0572,      1.8800],\n",
            "        [     0.9243,      0.0000],\n",
            "        [     1.6698,      3.9200],\n",
            "        [     1.1908,      1.1200],\n",
            "        [    15.2288,     15.8800],\n",
            "        [     1.3445,      1.0100],\n",
            "        [     0.9784,      1.8700],\n",
            "        [     0.5936,      1.1200],\n",
            "        [     0.7947,      0.0000],\n",
            "        [     1.5026,      1.6800],\n",
            "        [     1.9599,      1.4100],\n",
            "        [     1.3121,      0.5600],\n",
            "        [    -0.0471,      0.0000],\n",
            "        [     1.3398,      2.2400],\n",
            "        [     1.9245,      2.1200],\n",
            "        [     1.0454,      0.5100],\n",
            "        [     1.8058,      2.8000],\n",
            "        [     1.3490,      0.5600],\n",
            "        [     0.0173,      0.0000],\n",
            "        [    -0.0552,      0.0000],\n",
            "        [    20.3316,     22.8700],\n",
            "        [     1.3149,      1.1200],\n",
            "        [     1.8795,      1.1200],\n",
            "        [    -0.0521,      0.0000],\n",
            "        [     2.0644,      3.5300],\n",
            "        [     1.6107,      0.7500],\n",
            "        [     1.7193,      2.0300],\n",
            "        [     2.1438,      2.1200],\n",
            "        [     1.6013,      0.5600],\n",
            "        [     1.7955,      0.5600],\n",
            "        [     1.0950,      1.1200],\n",
            "        [     2.1075,      0.6300],\n",
            "        [     1.2593,      1.8700],\n",
            "        [     1.4681,      3.5500],\n",
            "        [     1.5329,      0.7500],\n",
            "        [     1.4080,      1.1200],\n",
            "        [     1.1509,      0.3700],\n",
            "        [     1.7881,      1.6800],\n",
            "        [    35.7904,     34.0900],\n",
            "        [     1.8563,      1.4900],\n",
            "        [     1.2214,      1.4900],\n",
            "        [     1.2334,      1.0100],\n",
            "        [     2.0837,      2.6100],\n",
            "        [     2.0756,      1.4900],\n",
            "        [     1.7498,      2.6100],\n",
            "        [     1.7222,      2.6100],\n",
            "        [     1.3076,      3.0400],\n",
            "        [     1.9063,      3.3600],\n",
            "        [     1.6189,      1.0100],\n",
            "        [     2.7337,      3.1300],\n",
            "        [     2.5787,      2.1200],\n",
            "        [     3.3030,      2.1200],\n",
            "        [     1.3139,      1.8700],\n",
            "        [     1.6017,      1.8700],\n",
            "        [     1.4949,      2.2400]], device='cuda:0', grad_fn=<CatBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n4rY-r30coDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SecW8YxzcoDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sRlb8Zs-coDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PxJbwLD4coDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qJkPXM-fcoDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4koFooORcoDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VfGznCq1coDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oo2nLp2WcoDH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}