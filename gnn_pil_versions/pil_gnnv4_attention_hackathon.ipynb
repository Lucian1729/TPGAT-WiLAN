{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-06T10:08:11.655513Z",
          "iopub.status.busy": "2023-07-06T10:08:11.655094Z",
          "iopub.status.idle": "2023-07-06T10:10:02.592058Z",
          "shell.execute_reply": "2023-07-06T10:10:02.590896Z",
          "shell.execute_reply.started": "2023-07-06T10:08:11.655481Z"
        },
        "id": "8vHlenqqcoB8",
        "outputId": "18256feb-fed7-4839-ba9d-10e9a38e393b",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu117\n",
            "Collecting torch==1.13.1+cu117\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torch-1.13.1%2Bcu117-cp310-cp310-linux_x86_64.whl (1801.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m420.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.14.1+cu117\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torchvision-0.14.1%2Bcu117-cp310-cp310-linux_x86_64.whl (24.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.3/24.3 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.13.1\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torchaudio-0.13.1%2Bcu117-cp310-cp310-linux_x86_64.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1+cu117) (4.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1+cu117) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1+cu117) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1+cu117) (8.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (3.4)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.15.2+cu118\n",
            "    Uninstalling torchvision-0.15.2+cu118:\n",
            "      Successfully uninstalled torchvision-0.15.2+cu118\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.0.2+cu118\n",
            "    Uninstalling torchaudio-2.0.2+cu118:\n",
            "      Successfully uninstalled torchaudio-2.0.2+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.13.1+cu117 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.13.1+cu117 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.13.1+cu117 torchaudio-0.13.1+cu117 torchvision-0.14.1+cu117\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-06T10:10:02.596163Z",
          "iopub.status.busy": "2023-07-06T10:10:02.595858Z",
          "iopub.status.idle": "2023-07-06T10:10:17.089867Z",
          "shell.execute_reply": "2023-07-06T10:10:17.088572Z",
          "shell.execute_reply.started": "2023-07-06T10:10:02.596135Z"
        },
        "id": "iZgyOuL7coCG",
        "outputId": "668ab4fe-7ade-4340-fb94-dac12f7d2498",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchdata==0.5.1\n",
            "  Downloading torchdata-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchtext==0.14.1\n",
            "  Downloading torchtext-0.14.1-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.5.1) (1.26.16)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata==0.5.1) (2.27.1)\n",
            "Collecting portalocker>=2.0.0 (from torchdata==0.5.1)\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.5.1) (1.13.1+cu117)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.14.1) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.14.1) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->torchdata==0.5.1) (4.7.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.5.1) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.5.1) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.5.1) (3.4)\n",
            "Installing collected packages: portalocker, torchtext, torchdata\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.15.2\n",
            "    Uninstalling torchtext-0.15.2:\n",
            "      Successfully uninstalled torchtext-0.15.2\n",
            "  Attempting uninstall: torchdata\n",
            "    Found existing installation: torchdata 0.6.1\n",
            "    Uninstalling torchdata-0.6.1:\n",
            "      Successfully uninstalled torchdata-0.6.1\n",
            "Successfully installed portalocker-2.7.0 torchdata-0.5.1 torchtext-0.14.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torchdata==0.5.1 torchtext==0.14.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-06T10:10:17.097502Z",
          "iopub.status.busy": "2023-07-06T10:10:17.095299Z",
          "iopub.status.idle": "2023-07-06T10:10:31.268254Z",
          "shell.execute_reply": "2023-07-06T10:10:31.267128Z",
          "shell.execute_reply.started": "2023-07-06T10:10:17.097463Z"
        },
        "id": "fgIfr5w2coCI",
        "outputId": "ed1bc046-e334-4304-909d-a33c858b6fba",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-1.13.1+cu117.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu117/torch_scatter-2.1.1%2Bpt113cu117-cp310-cp310-linux_x86_64.whl (10.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.1+pt113cu117\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.13.1+cu117.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-06T10:10:31.271935Z",
          "iopub.status.busy": "2023-07-06T10:10:31.271551Z",
          "iopub.status.idle": "2023-07-06T10:10:56.721495Z",
          "shell.execute_reply": "2023-07-06T10:10:56.720245Z",
          "shell.execute_reply.started": "2023-07-06T10:10:31.271896Z"
        },
        "id": "JgVl6k43coCK",
        "outputId": "1283794d-2bba-49e6-e5a3-e1795f0c6fbc",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/661.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch_geometric\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910459 sha256=b3260efe3a9d355777b4f3bb2ea0bd420f43076bc1de64e7ef7edb79b8ec3872\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "Successfully built torch_geometric\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T10:10:56.724219Z",
          "iopub.status.busy": "2023-07-06T10:10:56.723782Z",
          "iopub.status.idle": "2023-07-06T10:10:59.243881Z",
          "shell.execute_reply": "2023-07-06T10:10:59.242716Z",
          "shell.execute_reply.started": "2023-07-06T10:10:56.724166Z"
        },
        "id": "yN1FByVgcoCL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GCNConv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv,GATv2Conv\n",
        "from torch_scatter import scatter_mean\n",
        "from torch_geometric.data import InMemoryDataset, download_url, extract_zip\n",
        "from torch_geometric.nn import MetaLayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UCLVQJjrfGZ"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-06T10:20:24.423030Z",
          "iopub.status.busy": "2023-07-06T10:20:24.422651Z",
          "iopub.status.idle": "2023-07-06T10:20:24.430269Z",
          "shell.execute_reply": "2023-07-06T10:20:24.429163Z",
          "shell.execute_reply.started": "2023-07-06T10:20:24.423000Z"
        },
        "id": "0Pzb-vSNcoCV",
        "outputId": "a96ecc9e-bf82-4a12-8ec9-1f30ab4a0c56",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available()  else \"cpu\" )\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T10:13:51.804937Z",
          "iopub.status.busy": "2023-07-06T10:13:51.804545Z",
          "iopub.status.idle": "2023-07-06T10:13:51.811195Z",
          "shell.execute_reply": "2023-07-06T10:13:51.810142Z",
          "shell.execute_reply.started": "2023-07-06T10:13:51.804907Z"
        },
        "id": "_Pj8h3HecoCW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def get_int_map(dep):\n",
        "    dep = df.loc[df[\"deployment\"]==dep]\n",
        "    dep = dep.reset_index(drop=True)\n",
        "    return dep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "k3RVPftNcoCX"
      },
      "outputs": [],
      "source": [
        "# t1 = get_int_map(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T10:10:59.246051Z",
          "iopub.status.busy": "2023-07-06T10:10:59.245394Z",
          "iopub.status.idle": "2023-07-06T10:11:00.452826Z",
          "shell.execute_reply": "2023-07-06T10:11:00.451791Z",
          "shell.execute_reply.started": "2023-07-06T10:10:59.246013Z"
        },
        "id": "R2o8JJX7coCZ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"../TPML-WLAN-GAT/required_format_v1.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "execution": {
          "iopub.execute_input": "2023-07-06T10:11:00.454755Z",
          "iopub.status.busy": "2023-07-06T10:11:00.454402Z",
          "iopub.status.idle": "2023-07-06T10:11:00.497714Z",
          "shell.execute_reply": "2023-07-06T10:11:00.496795Z",
          "shell.execute_reply.started": "2023-07-06T10:11:00.454723Z"
        },
        "id": "K26GfkcfcoCa",
        "outputId": "c2b61cd8-8192-40f4-93e1-b8fef882065f",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>primary_channel</th>\n",
              "      <th>min_channel_allowed</th>\n",
              "      <th>max_channel_allowed</th>\n",
              "      <th>rssi</th>\n",
              "      <th>node_type</th>\n",
              "      <th>sinr</th>\n",
              "      <th>airtime_mean</th>\n",
              "      <th>deployment</th>\n",
              "      <th>channel_bonding_model</th>\n",
              "      <th>throughput</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-79.34</td>\n",
              "      <td>-103.96</td>\n",
              "      <td>-119.98</td>\n",
              "      <td>-82.35</td>\n",
              "      <td>-94.85</td>\n",
              "      <td>-111.61</td>\n",
              "      <td>-122.95</td>\n",
              "      <td>-103.96</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>-57.69</td>\n",
              "      <td>0</td>\n",
              "      <td>36.28</td>\n",
              "      <td>95.745</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>111.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-79.34</td>\n",
              "      <td>-103.96</td>\n",
              "      <td>-119.98</td>\n",
              "      <td>-82.35</td>\n",
              "      <td>-94.85</td>\n",
              "      <td>-111.61</td>\n",
              "      <td>-122.95</td>\n",
              "      <td>-103.96</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>-65.37</td>\n",
              "      <td>1</td>\n",
              "      <td>29.36</td>\n",
              "      <td>95.745</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>5.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-79.34</td>\n",
              "      <td>-103.96</td>\n",
              "      <td>-119.98</td>\n",
              "      <td>-82.35</td>\n",
              "      <td>-94.85</td>\n",
              "      <td>-111.61</td>\n",
              "      <td>-122.95</td>\n",
              "      <td>-103.96</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>-65.35</td>\n",
              "      <td>1</td>\n",
              "      <td>29.48</td>\n",
              "      <td>95.745</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>6.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-79.34</td>\n",
              "      <td>-103.96</td>\n",
              "      <td>-119.98</td>\n",
              "      <td>-82.35</td>\n",
              "      <td>-94.85</td>\n",
              "      <td>-111.61</td>\n",
              "      <td>-122.95</td>\n",
              "      <td>-103.96</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>-61.41</td>\n",
              "      <td>1</td>\n",
              "      <td>31.97</td>\n",
              "      <td>95.745</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>6.91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-79.34</td>\n",
              "      <td>-103.96</td>\n",
              "      <td>-119.98</td>\n",
              "      <td>-82.35</td>\n",
              "      <td>-94.85</td>\n",
              "      <td>-111.61</td>\n",
              "      <td>-122.95</td>\n",
              "      <td>-103.96</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>-63.52</td>\n",
              "      <td>1</td>\n",
              "      <td>31.26</td>\n",
              "      <td>95.745</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>9.99</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 27 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0    0      1       2       3      4      5       6       7  \\\n",
              "0           0  0.0 -79.34 -103.96 -119.98 -82.35 -94.85 -111.61 -122.95   \n",
              "1           1  0.0 -79.34 -103.96 -119.98 -82.35 -94.85 -111.61 -122.95   \n",
              "2           2  0.0 -79.34 -103.96 -119.98 -82.35 -94.85 -111.61 -122.95   \n",
              "3           3  0.0 -79.34 -103.96 -119.98 -82.35 -94.85 -111.61 -122.95   \n",
              "4           4  0.0 -79.34 -103.96 -119.98 -82.35 -94.85 -111.61 -122.95   \n",
              "\n",
              "        8  ...  primary_channel  min_channel_allowed  max_channel_allowed  \\\n",
              "0 -103.96  ...                4                    4                    5   \n",
              "1 -103.96  ...                4                    4                    5   \n",
              "2 -103.96  ...                4                    4                    5   \n",
              "3 -103.96  ...                4                    4                    5   \n",
              "4 -103.96  ...                4                    4                    5   \n",
              "\n",
              "    rssi  node_type   sinr  airtime_mean  deployment  channel_bonding_model  \\\n",
              "0 -57.69          0  36.28        95.745         0.0                      4   \n",
              "1 -65.37          1  29.36        95.745         0.0                      4   \n",
              "2 -65.35          1  29.48        95.745         0.0                      4   \n",
              "3 -61.41          1  31.97        95.745         0.0                      4   \n",
              "4 -63.52          1  31.26        95.745         0.0                      4   \n",
              "\n",
              "   throughput  \n",
              "0      111.77  \n",
              "1        5.79  \n",
              "2        6.11  \n",
              "3        6.91  \n",
              "4        9.99  \n",
              "\n",
              "[5 rows x 27 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ndx5diS21Fh3",
        "outputId": "f5417946-4e6e-4dc9-a7ae-3402f075ab90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "78078"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZgaLlH7tvCu1"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T10:28:27.630716Z",
          "iopub.status.busy": "2023-07-06T10:28:27.630364Z",
          "iopub.status.idle": "2023-07-06T10:28:27.648692Z",
          "shell.execute_reply": "2023-07-06T10:28:27.647731Z",
          "shell.execute_reply.started": "2023-07-06T10:28:27.630687Z"
        },
        "id": "-bXLwx5ycoCb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Creating individual graphs\n",
        "# This assumes all APs and STAs are connected to each other\n",
        "def create_graph(split,split_y,deployment):\n",
        "    dep = get_int_map(deployment)\n",
        "    dep_y = dep[\"throughput\"]\n",
        "    dep_x = dep[['0', '1', '2', '3', '4', '5', '6', '7','8', '9', '10', '11', 'wlan_code_index', 'x(m)', 'y(m)',\n",
        "            'primary_channel', 'min_channel_allowed', 'max_channel_allowed', 'rssi', 'node_type',\n",
        "            'sinr', 'airtime_mean', 'deployment']]\n",
        "    #print(dep_x)\n",
        "    dep_reset = dep.reset_index(drop=True)\n",
        "    ap_index = {}\n",
        "    out = dep_reset[dep_reset[\"node_type\"] == 0]\n",
        "    for i in range(len(out)):\n",
        "        ap_index[out.index[i]] = i\n",
        "    #print(ap_index)\n",
        "    node_features = dep_x.iloc[:,12:].values\n",
        "    # print(node_features)\n",
        "    #edge_features = dep.iloc[:,:12].values - here each node has been given an edge feature\n",
        "    # need to give each edge an edge feature\n",
        "    node_targets = dep_y.values\n",
        "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
        "    print(node_features.shape)\n",
        "    #edge_features = torch.tensor(edge_features, dtype=torch.float)\n",
        "    node_targets = torch.tensor(node_targets, dtype=torch.float)\n",
        "    # Add edges here for each deployment\n",
        "    edges = []\n",
        "    edge_features = []\n",
        "    edge_index = []\n",
        "    for i in range(len(dep)):\n",
        "        for j in range(len(dep)):\n",
        "            if (i != j and (dep[\"node_type\"].iloc[i] == 0 and dep[\"node_type\"].iloc[j] == 0)) or (i !=j and (dep[\"node_type\"].iloc[i] == 1 and dep[\"node_type\"].iloc[j] == 0)):\n",
        "                edges.append([i,j])\n",
        "    #print(edges)\n",
        "    edges2=edges\n",
        "    edges = torch.tensor(edges, dtype=torch.float)\n",
        "    #print(\"Edges: \", edges, edges.shape)\n",
        "    # edge_index = torch.tensor(edges, dtype=torch.long)\n",
        "    edge_index = torch.tensor(edges,dtype=torch.long)\n",
        "    edge_index = edge_index.t().contiguous()\n",
        "    #print(edges.detach(), edges.shape)\n",
        "    print(edges.shape[0])\n",
        "    for i in range(edges.shape[0]):\n",
        "        # print(dep.iloc[int(edges[i][0]), ap_index[int(edges[i][1])]])\n",
        "        i_pos = np.asarray([dep.at[edges2[i][0],\"x(m)\"],dep.at[edges2[i][0],\"y(m)\"]])\n",
        "        j_pos = np.asarray([dep.at[edges2[i][1],\"x(m)\"],dep.at[edges2[i][1],\"y(m)\"]])\n",
        "        distance = np.linalg.norm(i_pos - j_pos)\n",
        "        #print(i)\n",
        "        # edge_features.append([distance,dep.iloc[int(edges[i][0]), ap_index[int(edges[i][1])]]])\n",
        "        edge_type = 0\n",
        "        if int(edges2[i][0]) in ap_index.keys():\n",
        "          edge_type = 0\n",
        "        else:\n",
        "          edge_type = 1\n",
        "        edge_features.append([dep.iloc[int(edges[i][0]), ap_index[int(edges[i][1])]],distance])\n",
        "    # edge_features = np.array(edge_features)\n",
        "    # print(edge_features)\n",
        "    edge_features = torch.tensor(edge_features, dtype=torch.float)\n",
        "    #print(edge_features, edge_features.shape)\n",
        "    graph = {\n",
        "        \"edges\": edges,\n",
        "        \"edge_index\": edge_index,\n",
        "        \"node_features\": node_features,\n",
        "        \"edge_features\": edge_features,\n",
        "        \"node_targets\": node_targets\n",
        "    }\n",
        "#     print(\"*\"*10)\n",
        "#     print(edges.shape)\n",
        "#     print(edge_index.shape)\n",
        "#     print(node_features.shape)\n",
        "#     print(edge_features.shape)\n",
        "#     print(node_targets.shape)\n",
        "    return graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-06T10:19:03.496718Z",
          "iopub.status.busy": "2023-07-06T10:19:03.496147Z",
          "iopub.status.idle": "2023-07-06T10:19:05.076427Z",
          "shell.execute_reply": "2023-07-06T10:19:05.075388Z",
          "shell.execute_reply.started": "2023-07-06T10:19:03.496690Z"
        },
        "id": "LFkij5z5coCd",
        "outputId": "4e619944-48a0-48a5-c335-853ea3771a59",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([177, 11])\n",
            "2112\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'edges': tensor([[  0.,  15.],\n",
              "         [  0.,  27.],\n",
              "         [  0.,  45.],\n",
              "         ...,\n",
              "         [176., 135.],\n",
              "         [176., 150.],\n",
              "         [176., 163.]]),\n",
              " 'edge_index': tensor([[  0,   0,   0,  ..., 176, 176, 176],\n",
              "         [ 15,  27,  45,  ..., 135, 150, 163]]),\n",
              " 'node_features': tensor([[0.0000e+00, 1.0000e+01, 1.0000e+01,  ..., 3.6280e+01, 9.5745e+01,\n",
              "          0.0000e+00],\n",
              "         [0.0000e+00, 7.1300e-02, 1.0808e+01,  ..., 2.9360e+01, 9.5745e+01,\n",
              "          0.0000e+00],\n",
              "         [0.0000e+00, 1.9627e+00, 4.1427e+00,  ..., 2.9480e+01, 9.5745e+01,\n",
              "          0.0000e+00],\n",
              "         ...,\n",
              "         [1.1000e+01, 6.6595e+01, 5.5642e+01,  ..., 3.5400e+01, 9.4855e+01,\n",
              "          0.0000e+00],\n",
              "         [1.1000e+01, 6.3075e+01, 4.3863e+01,  ..., 2.9680e+01, 9.4855e+01,\n",
              "          0.0000e+00],\n",
              "         [1.1000e+01, 6.8264e+01, 5.0447e+01,  ..., 5.0690e+01, 9.4855e+01,\n",
              "          0.0000e+00]]),\n",
              " 'edge_features': tensor([[   0.0000,   20.0000],\n",
              "         [ -79.3400,   40.0000],\n",
              "         [-103.9600,   60.0000],\n",
              "         ...,\n",
              "         [-122.9900,   38.2662],\n",
              "         [-106.9700,   18.2691],\n",
              "         [ -82.3500,    1.7930]]),\n",
              " 'node_targets': tensor([1.1177e+02, 5.7900e+00, 6.1100e+00, 6.9100e+00, 9.9900e+00, 9.4400e+00,\n",
              "         7.2500e+00, 5.8800e+00, 7.3800e+00, 1.0910e+01, 1.0050e+01, 1.0260e+01,\n",
              "         7.9300e+00, 4.8800e+00, 8.9900e+00, 1.1111e+02, 8.1000e+00, 1.0440e+01,\n",
              "         9.2300e+00, 7.5700e+00, 8.4500e+00, 1.2210e+01, 1.2980e+01, 1.2360e+01,\n",
              "         6.9800e+00, 1.1830e+01, 1.0940e+01, 7.7720e+01, 2.3000e-01, 1.4600e+00,\n",
              "         7.9100e+00, 7.7600e+00, 5.3000e+00, 7.5300e+00, 7.3700e+00, 2.0000e+00,\n",
              "         7.9900e+00, 2.0700e+00, 4.7600e+00, 4.9900e+00, 7.6000e+00, 1.1500e+00,\n",
              "         2.3800e+00, 3.0000e+00, 4.2200e+00, 4.3050e+01, 3.4900e+00, 1.9800e+00,\n",
              "         2.4600e+00, 2.8000e+00, 3.1000e+00, 4.7200e+00, 4.9200e+00, 4.3000e+00,\n",
              "         3.4900e+00, 4.5800e+00, 2.8300e+00, 4.3800e+00, 7.9870e+01, 2.6900e+00,\n",
              "         6.4500e+00, 4.0700e+00, 5.6100e+00, 6.3700e+00, 6.9000e-01, 5.6100e+00,\n",
              "         1.6100e+00, 6.5300e+00, 5.3800e+00, 4.5300e+00, 3.8400e+00, 6.9900e+00,\n",
              "         5.9100e+00, 6.3700e+00, 1.6100e+00, 5.6100e+00, 5.7140e+01, 7.4500e+00,\n",
              "         6.9000e-01, 7.0700e+00, 5.5300e+00, 8.0000e-02, 7.8300e+00, 8.0000e-02,\n",
              "         3.3800e+00, 7.1400e+00, 2.0000e+00, 0.0000e+00, 4.0700e+00, 1.5000e-01,\n",
              "         3.3800e+00, 7.8300e+00, 0.0000e+00, 4.6000e-01, 3.7310e+01, 8.9100e+00,\n",
              "         8.0000e-02, 8.0000e-02, 8.4000e-01, 7.4500e+00, 7.3000e+00, 1.4500e+00,\n",
              "         2.6000e+00, 1.5000e-01, 5.4000e-01, 3.8000e-01, 7.5300e+00, 4.8080e+01,\n",
              "         2.9400e+00, 1.5700e+00, 5.6800e+00, 3.7600e+00, 4.0400e+00, 3.6300e+00,\n",
              "         2.5300e+00, 4.7200e+00, 3.6600e+00, 4.5300e+00, 1.9200e+00, 5.1500e+00,\n",
              "         3.9600e+00, 9.0930e+01, 9.3700e+00, 8.5200e+00, 9.4500e+00, 1.2520e+01,\n",
              "         5.3800e+00, 3.5300e+00, 1.4600e+00, 9.0600e+00, 7.8300e+00, 8.4000e-01,\n",
              "         1.1830e+01, 1.0060e+01, 1.0800e+00, 2.1430e+01, 3.5300e+00, 1.2300e+00,\n",
              "         1.6100e+00, 2.3000e-01, 0.0000e+00, 2.3000e-01, 3.0700e+00, 5.4000e-01,\n",
              "         6.1000e-01, 3.3000e+00, 2.5300e+00, 3.1000e-01, 8.4000e-01, 3.3800e+00,\n",
              "         1.8509e+02, 1.7890e+01, 1.6900e+01, 1.6670e+01, 1.9280e+01, 1.5440e+01,\n",
              "         1.6820e+01, 1.5670e+01, 1.6820e+01, 4.0700e+00, 1.3290e+01, 1.5670e+01,\n",
              "         1.6590e+01, 1.5509e+02, 1.4750e+01, 1.2130e+01, 1.0880e+01, 1.3060e+01,\n",
              "         1.0980e+01, 1.0530e+01, 1.3980e+01, 1.1900e+01, 1.1670e+01, 1.2240e+01,\n",
              "         1.1830e+01, 8.4600e+00, 1.2670e+01])}"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "create_graph(0,0,0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T10:19:05.079352Z",
          "iopub.status.busy": "2023-07-06T10:19:05.078366Z",
          "iopub.status.idle": "2023-07-06T10:19:05.085230Z",
          "shell.execute_reply": "2023-07-06T10:19:05.083975Z",
          "shell.execute_reply.started": "2023-07-06T10:19:05.079300Z"
        },
        "id": "ZO3ljwu7coCe",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def create_geometric_graph(graph):\n",
        "    data = Data(\n",
        "        # Input graph.\n",
        "        x=graph[\"node_features\"],\n",
        "        #pos=pos,\n",
        "        edge_index=graph[\"edge_index\"],\n",
        "        edge_attr=graph[\"edge_features\"],\n",
        "        # Output node targets.\n",
        "        y=graph[\"node_targets\"],\n",
        "        num_nodes = len(graph[\"node_features\"])\n",
        "\n",
        "    )\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-06T10:19:05.087282Z",
          "iopub.status.busy": "2023-07-06T10:19:05.086872Z",
          "iopub.status.idle": "2023-07-06T10:19:06.732846Z",
          "shell.execute_reply": "2023-07-06T10:19:06.731610Z",
          "shell.execute_reply.started": "2023-07-06T10:19:05.087249Z"
        },
        "id": "Kr7yzgfocoCf",
        "outputId": "ef987d2f-4a57-421b-dc79-0549905f3e5e",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([177, 11])\n",
            "2112\n",
            "Data(x=[177, 11], edge_index=[2, 2112], edge_attr=[2112, 2], y=[177], num_nodes=177)\n"
          ]
        }
      ],
      "source": [
        "print(create_geometric_graph(create_graph(0,0,0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYREKS0xfrSb",
        "outputId": "9701a8a6-2151-48af-a4bb-335e5d7e2027"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'rm' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'rm' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'rm' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!rm -r ../TPML-WLAN-GAT/train\n",
        "!rm -r ../TPML-WLAN-GAT/valid\n",
        "!rm -r ../TPML-WLAN-GAT/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "v8mkalUKflgL"
      },
      "outputs": [],
      "source": [
        "# With train, validation and test data.\n",
        "import random\n",
        "import os\n",
        "from torch_geometric.data import InMemoryDataset, download_url, extract_zip\n",
        "# divide into training and testing points\n",
        "class CustomDataset(InMemoryDataset):\n",
        "    def __init__(self, split=\"train\", transform=None):\n",
        "        self.data = pd.read_csv(\"../TPML-WLAN-GAT/required_format_v1.csv\")\n",
        "        self.split = split\n",
        "        super(CustomDataset, self).__init__( split, transform)\n",
        "        #self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "        #self.data = pd.read_csv(\"deployment_with_int_map.csv\")\n",
        "        #self.data, self.slices = pd.read_csv(\"deployment_with_int_map.csv\")\n",
        "\n",
        "        # print(\"In init\")\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        # print(\"In raw_file_names\")\n",
        "        return [\"../TPML-WLAN-GAT/required_format_v1.csv\"]\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        # print(\"In processed_file_names\")\n",
        "        li = ['data_train_' + str(i) + '.pt' for i in range(360)]+ ['data_valid_' + str(j) + '.pt' for j in range(360, 480)] + ['data_test_' + str(k) + '.pt' for k in range(480, 600)]\n",
        "        #print(li)\n",
        "        return ['data_train_' + str(i) + '.pt' for i in range(360)]+ ['data_valid_' + str(j) + '.pt' for j in range(360,480)] + ['data_test_' + str(k) + '.pt' for k in range(480,600)]\n",
        "\n",
        "    def _download(self):\n",
        "        '''\n",
        "        print(\"In download\")\n",
        "        path = download_url(self.url, self.raw_dir)\n",
        "        extract_zip(path, self.raw_dir)\n",
        "        # The zip file is removed\n",
        "        os.unlink(path)\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "    def process(self):\n",
        "        print(\"In process\")\n",
        "        #df = pd.read_csv(self.raw_paths[0])\n",
        "        X = self.data[['0', '1', '2', '3', '4', '5', '6', '7','8', '9', '10', '11', 'wlan_code_index', 'x(m)', 'y(m)',\n",
        "            'primary_channel', 'min_channel_allowed', 'max_channel_allowed', 'node_type','rssi',\n",
        "            'sinr', 'airtime_mean','deployment']]\n",
        "        y = self.data.loc[:, [\"throughput\", \"deployment\"]]\n",
        "#         X_train = X.iloc[:183854, :]\n",
        "#         X_valid = X.iloc[183854:245139, :]\n",
        "#         X_test = X.iloc[245139:,:]\n",
        "#         print(X_test.columns)\n",
        "#         y_train = y.iloc[:183854, :]\n",
        "#         y_valid = y.iloc[183854:245139, :]\n",
        "#         y_test = y.iloc[245139:,:]\n",
        "        graphs = []\n",
        "        # print(\"Here\")\n",
        "        l = [i for i in range(600)]\n",
        "        self.l_train = random.sample(l, 360)\n",
        "        l = [x for x in l if x not in self.l_train]\n",
        "        self.l_valid = random.sample(l, 120)\n",
        "        l = [x for x in l if x not in self.l_valid]\n",
        "        self.l_test = l\n",
        "        count = 0\n",
        "        if(self.split == \"train\"):\n",
        "\n",
        "            for i in self.l_train:\n",
        "\n",
        "                #X_train = torch.tensor(X_train.to_numpy(), dtype=torch.float)\n",
        "                #y_train = torch.tensor(y_train.to_numpy(), dtype=torch.float)\n",
        "                graph = create_graph(X, y, i)\n",
        "\n",
        "                graph = create_geometric_graph(graph)\n",
        "                graphs.append(graph)\n",
        "\n",
        "                torch.save(graph, os.path.join(self.processed_dir, f'data_train_{count}.pt'))\n",
        "                count += 1\n",
        "        elif(self.split == \"valid\"):\n",
        "            for i in self.l_valid:\n",
        "                #X_test = torch.tensor(X_test.to_numpy(), dtype=torch.float)\n",
        "                #y_test = torch.tensor(y_test.to_numpy(), dtype=torch.float)\n",
        "                graph = create_graph(X, y, i)\n",
        "                graph = create_geometric_graph(graph)\n",
        "                graphs.append(graph)\n",
        "\n",
        "                torch.save(graph, os.path.join(self.processed_dir, f'data_valid_{count}.pt'))\n",
        "                count += 1\n",
        "        else:\n",
        "            for i in self.l_test:\n",
        "                #X_test = torch.tensor(X_test.to_numpy(), dtype=torch.float)\n",
        "                #y_test = torch.tensor(y_test.to_numpy(), dtype=torch.float)\n",
        "                graph = create_graph(X, y, i)\n",
        "                graph = create_geometric_graph(graph)\n",
        "                graphs.append(graph)\n",
        "\n",
        "                torch.save(graph, os.path.join(self.processed_dir, f'data_test_{count}.pt'))\n",
        "                count += 1\n",
        "        #return graphs[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        if(self.split == \"train\"):\n",
        "            #return len(self.processed_file_names[0])\n",
        "            return 360\n",
        "        elif self.split == \"valid\":\n",
        "            return 120\n",
        "        else:\n",
        "            return 120\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #print(\"Part: \", self.processed_file_names[1])\n",
        "\n",
        "        if(self.split == \"train\"):\n",
        "            data = torch.load(os.path.join(self.processed_dir, f'data_train_{idx}.pt'))\n",
        "        elif(self.split == \"valid\"):\n",
        "            data = torch.load(os.path.join(self.processed_dir, f'data_valid_{idx}.pt'))\n",
        "        elif (self.split==\"test\"):\n",
        "            data = torch.load(os.path.join(self.processed_dir, f'data_test_{idx}.pt'))\n",
        "        return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnZO7d9AfqCN",
        "outputId": "2892dca3-8cf1-4de1-bfda-3c06f4835d47"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In process\n",
            "torch.Size([72, 11])\n",
            "568\n",
            "torch.Size([64, 11])\n",
            "504\n",
            "torch.Size([72, 11])\n",
            "568\n",
            "torch.Size([180, 11])\n",
            "2148\n",
            "torch.Size([202, 11])\n",
            "2412\n",
            "torch.Size([202, 11])\n",
            "2412\n",
            "torch.Size([178, 11])\n",
            "2124\n",
            "torch.Size([194, 11])\n",
            "2316\n",
            "torch.Size([74, 11])\n",
            "584\n",
            "torch.Size([64, 11])\n",
            "504\n",
            "torch.Size([73, 11])\n",
            "576\n",
            "torch.Size([70, 11])\n",
            "552\n",
            "torch.Size([71, 11])\n",
            "560\n",
            "torch.Size([187, 11])\n",
            "2232\n",
            "torch.Size([73, 11])\n",
            "576\n",
            "torch.Size([176, 11])\n",
            "2100\n",
            "torch.Size([196, 11])\n",
            "2340\n",
            "torch.Size([188, 11])\n",
            "2244\n",
            "torch.Size([196, 11])\n",
            "2340\n",
            "torch.Size([190, 11])\n",
            "2268\n",
            "torch.Size([66, 11])\n",
            "520\n",
            "torch.Size([191, 11])\n",
            "2280\n",
            "torch.Size([66, 11])\n",
            "520\n",
            "torch.Size([184, 11])\n",
            "2196\n",
            "torch.Size([193, 11])\n",
            "2304\n",
            "torch.Size([72, 11])\n",
            "568\n",
            "torch.Size([60, 11])\n",
            "472\n",
            "torch.Size([69, 11])\n",
            "544\n",
            "torch.Size([65, 11])\n",
            "512\n",
            "torch.Size([66, 11])\n",
            "520\n",
            "torch.Size([74, 11])\n",
            "584\n",
            "torch.Size([200, 11])\n",
            "2388\n",
            "torch.Size([198, 11])\n",
            "2364\n",
            "torch.Size([170, 11])\n",
            "2028\n",
            "torch.Size([76, 11])\n",
            "600\n",
            "torch.Size([193, 11])\n",
            "2304\n",
            "torch.Size([78, 11])\n",
            "616\n",
            "torch.Size([191, 11])\n",
            "2280\n",
            "torch.Size([205, 11])\n",
            "2448\n",
            "torch.Size([191, 11])\n",
            "2280\n",
            "torch.Size([67, 11])\n",
            "528\n",
            "torch.Size([70, 11])\n",
            "552\n",
            "torch.Size([224, 11])\n",
            "2676\n",
            "torch.Size([69, 11])\n",
            "544\n",
            "torch.Size([187, 11])\n",
            "2232\n",
            "torch.Size([66, 11])\n",
            "520\n",
            "torch.Size([184, 11])\n",
            "2196\n",
            "torch.Size([72, 11])\n",
            "568\n",
            "torch.Size([198, 11])\n",
            "2364\n",
            "torch.Size([199, 11])\n",
            "2376\n",
            "torch.Size([65, 11])\n",
            "512\n",
            "torch.Size([75, 11])\n",
            "592\n",
            "torch.Size([68, 11])\n",
            "536\n",
            "torch.Size([73, 11])\n",
            "576\n",
            "torch.Size([184, 11])\n",
            "2196\n",
            "torch.Size([76, 11])\n",
            "600\n",
            "torch.Size([68, 11])\n",
            "536\n",
            "torch.Size([174, 11])\n",
            "2076\n",
            "torch.Size([76, 11])\n",
            "600\n",
            "torch.Size([184, 11])\n",
            "2196\n",
            "torch.Size([61, 11])\n",
            "480\n",
            "torch.Size([198, 11])\n",
            "2364\n",
            "torch.Size([74, 11])\n",
            "584\n",
            "torch.Size([207, 11])\n",
            "2472\n",
            "torch.Size([69, 11])\n",
            "544\n",
            "torch.Size([75, 11])\n",
            "592\n",
            "torch.Size([203, 11])\n",
            "2424\n",
            "torch.Size([68, 11])\n",
            "536\n",
            "torch.Size([211, 11])\n",
            "2520\n",
            "torch.Size([172, 11])\n",
            "2052\n",
            "torch.Size([62, 11])\n",
            "488\n",
            "torch.Size([175, 11])\n",
            "2088\n",
            "torch.Size([74, 11])\n",
            "584\n",
            "torch.Size([77, 11])\n",
            "608\n",
            "torch.Size([198, 11])\n",
            "2364\n",
            "torch.Size([195, 11])\n",
            "2328\n",
            "torch.Size([75, 11])\n",
            "592\n",
            "torch.Size([195, 11])\n",
            "2328\n",
            "torch.Size([70, 11])\n",
            "552\n",
            "torch.Size([191, 11])\n",
            "2280\n",
            "torch.Size([207, 11])\n",
            "2472\n",
            "torch.Size([75, 11])\n",
            "592\n",
            "torch.Size([215, 11])\n",
            "2568\n",
            "torch.Size([73, 11])\n",
            "576\n",
            "torch.Size([73, 11])\n",
            "576\n",
            "torch.Size([70, 11])\n",
            "552\n",
            "torch.Size([209, 11])\n",
            "2496\n",
            "torch.Size([72, 11])\n",
            "568\n",
            "torch.Size([71, 11])\n",
            "560\n",
            "torch.Size([57, 11])\n",
            "448\n",
            "torch.Size([70, 11])\n",
            "552\n",
            "torch.Size([176, 11])\n",
            "2100\n",
            "torch.Size([77, 11])\n",
            "608\n",
            "torch.Size([199, 11])\n",
            "2376\n",
            "torch.Size([195, 11])\n",
            "2328\n",
            "torch.Size([67, 11])\n",
            "528\n",
            "torch.Size([195, 11])\n",
            "2328\n",
            "torch.Size([170, 11])\n",
            "2028\n",
            "torch.Size([71, 11])\n",
            "560\n",
            "torch.Size([189, 11])\n",
            "2256\n",
            "torch.Size([73, 11])\n",
            "576\n",
            "torch.Size([68, 11])\n",
            "536\n",
            "torch.Size([70, 11])\n",
            "552\n",
            "torch.Size([71, 11])\n",
            "560\n",
            "torch.Size([187, 11])\n",
            "2232\n",
            "torch.Size([216, 11])\n",
            "2580\n",
            "torch.Size([67, 11])\n",
            "528\n",
            "torch.Size([208, 11])\n",
            "2484\n",
            "torch.Size([188, 11])\n",
            "2244\n",
            "torch.Size([197, 11])\n",
            "2352\n",
            "torch.Size([75, 11])\n",
            "592\n",
            "torch.Size([201, 11])\n",
            "2400\n",
            "torch.Size([205, 11])\n",
            "2448\n",
            "torch.Size([62, 11])\n",
            "488\n",
            "torch.Size([182, 11])\n",
            "2172\n",
            "torch.Size([69, 11])\n",
            "544\n",
            "torch.Size([73, 11])\n",
            "576\n",
            "torch.Size([194, 11])\n",
            "2316\n",
            "torch.Size([68, 11])\n",
            "536\n",
            "torch.Size([200, 11])\n",
            "2388\n",
            "torch.Size([194, 11])\n",
            "2316\n",
            "torch.Size([182, 11])\n",
            "2172\n",
            "torch.Size([196, 11])\n",
            "2340\n",
            "torch.Size([201, 11])\n",
            "2400\n",
            "torch.Size([209, 11])\n",
            "2496\n",
            "torch.Size([196, 11])\n",
            "2340\n",
            "torch.Size([71, 11])\n",
            "560\n",
            "torch.Size([184, 11])\n",
            "2196\n",
            "torch.Size([71, 11])\n",
            "560\n",
            "torch.Size([74, 11])\n",
            "584\n",
            "torch.Size([71, 11])\n",
            "560\n",
            "torch.Size([66, 11])\n",
            "520\n",
            "torch.Size([194, 11])\n",
            "2316\n",
            "torch.Size([188, 11])\n",
            "2244\n",
            "torch.Size([69, 11])\n",
            "544\n",
            "torch.Size([62, 11])\n",
            "488\n",
            "torch.Size([67, 11])\n",
            "528\n",
            "torch.Size([193, 11])\n",
            "2304\n",
            "torch.Size([202, 11])\n",
            "2412\n",
            "torch.Size([185, 11])\n",
            "2208\n",
            "torch.Size([181, 11])\n",
            "2160\n",
            "torch.Size([71, 11])\n",
            "560\n",
            "torch.Size([73, 11])\n",
            "576\n",
            "torch.Size([200, 11])\n",
            "2388\n",
            "torch.Size([194, 11])\n",
            "2316\n",
            "torch.Size([64, 11])\n",
            "504\n",
            "torch.Size([224, 11])\n",
            "2676\n",
            "torch.Size([69, 11])\n",
            "544\n",
            "torch.Size([211, 11])\n",
            "2520\n",
            "torch.Size([202, 11])\n",
            "2412\n",
            "torch.Size([203, 11])\n",
            "2424\n",
            "torch.Size([199, 11])\n",
            "2376\n",
            "torch.Size([191, 11])\n",
            "2280\n",
            "torch.Size([68, 11])\n",
            "536\n",
            "torch.Size([64, 11])\n",
            "504\n",
            "torch.Size([72, 11])\n",
            "568\n",
            "torch.Size([62, 11])\n",
            "488\n",
            "torch.Size([69, 11])\n",
            "544\n",
            "torch.Size([72, 11])\n",
            "568\n",
            "torch.Size([58, 11])\n",
            "456\n",
            "torch.Size([63, 11])\n",
            "496\n",
            "torch.Size([72, 11])\n",
            "568\n",
            "torch.Size([186, 11])\n",
            "2220\n",
            "torch.Size([165, 11])\n",
            "1968\n",
            "torch.Size([73, 11])\n",
            "576\n",
            "torch.Size([200, 11])\n",
            "2388\n",
            "torch.Size([195, 11])\n",
            "2328\n",
            "torch.Size([63, 11])\n",
            "496\n",
            "torch.Size([181, 11])\n",
            "2160\n",
            "torch.Size([71, 11])\n",
            "560\n",
            "torch.Size([216, 11])\n",
            "2580\n",
            "torch.Size([65, 11])\n",
            "512\n",
            "torch.Size([69, 11])\n",
            "544\n",
            "torch.Size([207, 11])\n",
            "2472\n",
            "torch.Size([72, 11])\n",
            "568\n",
            "torch.Size([189, 11])\n",
            "2256\n",
            "torch.Size([63, 11])\n",
            "496\n",
            "torch.Size([75, 11])\n",
            "592\n",
            "torch.Size([62, 11])\n",
            "488\n",
            "torch.Size([188, 11])\n",
            "2244\n",
            "torch.Size([198, 11])\n",
            "2364\n",
            "torch.Size([67, 11])\n",
            "528\n",
            "torch.Size([72, 11])\n",
            "568\n",
            "torch.Size([194, 11])\n",
            "2316\n",
            "torch.Size([190, 11])\n",
            "2268\n",
            "torch.Size([66, 11])\n",
            "520\n",
            "torch.Size([174, 11])\n",
            "2076\n",
            "torch.Size([215, 11])\n",
            "2568\n",
            "torch.Size([68, 11])\n",
            "536\n",
            "torch.Size([72, 11])\n",
            "568\n",
            "torch.Size([204, 11])\n",
            "2436\n",
            "torch.Size([71, 11])\n",
            "560\n",
            "torch.Size([67, 11])\n",
            "528\n",
            "torch.Size([210, 11])\n",
            "2508\n",
            "torch.Size([63, 11])\n",
            "496\n",
            "torch.Size([183, 11])\n",
            "2184\n",
            "torch.Size([186, 11])\n",
            "2220\n",
            "torch.Size([70, 11])\n",
            "552\n",
            "torch.Size([73, 11])\n",
            "576\n",
            "torch.Size([199, 11])\n",
            "2376\n",
            "torch.Size([62, 11])\n",
            "488\n",
            "torch.Size([71, 11])\n",
            "560\n",
            "torch.Size([173, 11])\n",
            "2064\n",
            "torch.Size([201, 11])\n",
            "2400\n",
            "torch.Size([179, 11])\n",
            "2136\n",
            "torch.Size([66, 11])\n",
            "520\n",
            "torch.Size([189, 11])\n",
            "2256\n",
            "torch.Size([71, 11])\n",
            "560\n",
            "torch.Size([68, 11])\n",
            "536\n",
            "torch.Size([67, 11])\n",
            "528\n",
            "torch.Size([174, 11])\n",
            "2076\n",
            "torch.Size([71, 11])\n",
            "560\n",
            "torch.Size([195, 11])\n",
            "2328\n",
            "torch.Size([199, 11])\n",
            "2376\n",
            "torch.Size([181, 11])\n",
            "2160\n",
            "torch.Size([205, 11])\n",
            "2448\n",
            "torch.Size([196, 11])\n",
            "2340\n",
            "torch.Size([203, 11])\n",
            "2424\n",
            "torch.Size([73, 11])\n",
            "576\n",
            "torch.Size([165, 11])\n",
            "1968\n",
            "torch.Size([70, 11])\n",
            "552\n",
            "torch.Size([65, 11])\n",
            "512\n",
            "torch.Size([180, 11])\n",
            "2148\n",
            "torch.Size([198, 11])\n",
            "2364\n",
            "torch.Size([186, 11])\n",
            "2220\n",
            "torch.Size([71, 11])\n",
            "560\n",
            "torch.Size([67, 11])\n",
            "528\n",
            "torch.Size([68, 11])\n",
            "536\n",
            "torch.Size([66, 11])\n",
            "520\n",
            "torch.Size([69, 11])\n",
            "544\n",
            "torch.Size([175, 11])\n",
            "2088\n",
            "torch.Size([177, 11])\n",
            "2112\n",
            "torch.Size([76, 11])\n",
            "600\n",
            "torch.Size([75, 11])\n",
            "592\n",
            "torch.Size([192, 11])\n",
            "2292\n",
            "torch.Size([78, 11])\n",
            "616\n",
            "torch.Size([62, 11])\n",
            "488\n",
            "torch.Size([208, 11])\n",
            "2484\n",
            "torch.Size([68, 11])\n",
            "536\n",
            "torch.Size([175, 11])\n",
            "2088\n",
            "torch.Size([206, 11])\n",
            "2460\n",
            "torch.Size([170, 11])\n",
            "2028\n",
            "torch.Size([200, 11])\n",
            "2388\n",
            "torch.Size([171, 11])\n",
            "2040\n",
            "torch.Size([196, 11])\n",
            "2340\n",
            "torch.Size([182, 11])\n",
            "2172\n",
            "torch.Size([60, 11])\n",
            "472\n",
            "torch.Size([203, 11])\n",
            "2424\n",
            "torch.Size([67, 11])\n",
            "528\n",
            "torch.Size([185, 11])\n",
            "2208\n",
            "torch.Size([198, 11])\n",
            "2364\n",
            "torch.Size([70, 11])\n",
            "552\n",
            "torch.Size([68, 11])\n",
            "536\n",
            "torch.Size([69, 11])\n",
            "544\n",
            "torch.Size([183, 11])\n",
            "2184\n",
            "torch.Size([77, 11])\n",
            "608\n",
            "torch.Size([178, 11])\n",
            "2124\n",
            "torch.Size([65, 11])\n",
            "512\n",
            "torch.Size([70, 11])\n",
            "552\n",
            "torch.Size([185, 11])\n",
            "2208\n",
            "torch.Size([180, 11])\n",
            "2148\n",
            "torch.Size([200, 11])\n",
            "2388\n",
            "torch.Size([79, 11])\n",
            "624\n",
            "torch.Size([186, 11])\n",
            "2220\n",
            "torch.Size([192, 11])\n",
            "2292\n",
            "torch.Size([177, 11])\n",
            "2112\n",
            "torch.Size([186, 11])\n",
            "2220\n",
            "torch.Size([195, 11])\n",
            "2328\n",
            "torch.Size([64, 11])\n",
            "504\n",
            "torch.Size([179, 11])\n",
            "2136\n",
            "torch.Size([186, 11])\n",
            "2220\n",
            "torch.Size([68, 11])\n",
            "536\n",
            "torch.Size([74, 11])\n",
            "584\n",
            "torch.Size([207, 11])\n",
            "2472\n",
            "torch.Size([183, 11])\n",
            "2184\n",
            "torch.Size([224, 11])\n",
            "2676\n",
            "torch.Size([60, 11])\n",
            "472\n",
            "torch.Size([196, 11])\n",
            "2340\n",
            "torch.Size([74, 11])\n",
            "584\n",
            "torch.Size([65, 11])\n",
            "512\n",
            "torch.Size([62, 11])\n",
            "488\n",
            "torch.Size([70, 11])\n",
            "552\n",
            "torch.Size([66, 11])\n",
            "520\n",
            "torch.Size([67, 11])\n",
            "528\n",
            "torch.Size([67, 11])\n",
            "528\n",
            "torch.Size([65, 11])\n",
            "512\n",
            "torch.Size([68, 11])\n",
            "536\n",
            "torch.Size([73, 11])\n",
            "576\n",
            "torch.Size([187, 11])\n",
            "2232\n",
            "torch.Size([65, 11])\n",
            "512\n",
            "torch.Size([62, 11])\n",
            "488\n",
            "torch.Size([75, 11])\n",
            "592\n",
            "torch.Size([190, 11])\n",
            "2268\n",
            "torch.Size([191, 11])\n",
            "2280\n",
            "torch.Size([197, 11])\n",
            "2352\n",
            "torch.Size([68, 11])\n",
            "536\n",
            "torch.Size([69, 11])\n",
            "544\n",
            "torch.Size([69, 11])\n",
            "544\n",
            "torch.Size([192, 11])\n",
            "2292\n",
            "torch.Size([189, 11])\n",
            "2256\n",
            "torch.Size([170, 11])\n",
            "2028\n",
            "torch.Size([69, 11])\n",
            "544\n",
            "torch.Size([198, 11])\n",
            "2364\n",
            "torch.Size([191, 11])\n",
            "2280\n",
            "torch.Size([183, 11])\n",
            "2184\n",
            "torch.Size([61, 11])\n",
            "480\n",
            "torch.Size([182, 11])\n",
            "2172\n",
            "torch.Size([195, 11])\n",
            "2328\n",
            "torch.Size([194, 11])\n",
            "2316\n",
            "torch.Size([169, 11])\n",
            "2016\n",
            "torch.Size([69, 11])\n",
            "544\n",
            "torch.Size([213, 11])\n",
            "2544\n",
            "torch.Size([186, 11])\n",
            "2220\n",
            "torch.Size([74, 11])\n",
            "584\n",
            "torch.Size([67, 11])\n",
            "528\n",
            "torch.Size([70, 11])\n",
            "552\n",
            "torch.Size([168, 11])\n",
            "2004\n",
            "torch.Size([181, 11])\n",
            "2160\n",
            "torch.Size([76, 11])\n",
            "600\n",
            "torch.Size([178, 11])\n",
            "2124\n",
            "torch.Size([81, 11])\n",
            "640\n",
            "torch.Size([194, 11])\n",
            "2316\n",
            "torch.Size([196, 11])\n",
            "2340\n",
            "torch.Size([200, 11])\n",
            "2388\n",
            "torch.Size([177, 11])\n",
            "2112\n",
            "torch.Size([188, 11])\n",
            "2244\n",
            "torch.Size([65, 11])\n",
            "512\n",
            "torch.Size([192, 11])\n",
            "2292\n",
            "torch.Size([66, 11])\n",
            "520\n",
            "torch.Size([206, 11])\n",
            "2460\n",
            "torch.Size([174, 11])\n",
            "2076\n",
            "torch.Size([68, 11])\n",
            "536\n",
            "torch.Size([72, 11])\n",
            "568\n",
            "torch.Size([197, 11])\n",
            "2352\n",
            "torch.Size([65, 11])\n",
            "512\n",
            "torch.Size([75, 11])\n",
            "592\n",
            "torch.Size([66, 11])\n",
            "520\n",
            "torch.Size([159, 11])\n",
            "1896\n",
            "torch.Size([70, 11])\n",
            "552\n",
            "torch.Size([199, 11])\n",
            "2376\n",
            "torch.Size([182, 11])\n",
            "2172\n",
            "torch.Size([192, 11])\n",
            "2292\n",
            "torch.Size([188, 11])\n",
            "2244\n",
            "torch.Size([190, 11])\n",
            "2268\n",
            "torch.Size([190, 11])\n",
            "2268\n",
            "torch.Size([71, 11])\n",
            "560\n",
            "torch.Size([168, 11])\n",
            "2004\n",
            "torch.Size([203, 11])\n",
            "2424\n",
            "torch.Size([75, 11])\n",
            "592\n",
            "torch.Size([70, 11])\n",
            "552\n",
            "torch.Size([217, 11])\n",
            "2592\n",
            "torch.Size([68, 11])\n",
            "536\n",
            "torch.Size([196, 11])\n",
            "2340\n",
            "torch.Size([77, 11])\n",
            "608\n",
            "torch.Size([200, 11])\n",
            "2388\n",
            "torch.Size([198, 11])\n",
            "2364\n",
            "torch.Size([173, 11])\n",
            "2064\n",
            "torch.Size([65, 11])\n",
            "512\n",
            "torch.Size([70, 11])\n",
            "552\n",
            "torch.Size([217, 11])\n",
            "2592\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Done!\n",
            "Processing...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In process\n",
            "torch.Size([199, 11])\n",
            "2376\n",
            "torch.Size([62, 11])\n",
            "488\n",
            "torch.Size([68, 11])\n",
            "536\n",
            "torch.Size([65, 11])\n",
            "512\n",
            "torch.Size([206, 11])\n",
            "2460\n",
            "torch.Size([200, 11])\n",
            "2388\n",
            "torch.Size([177, 11])\n",
            "2112\n",
            "torch.Size([180, 11])\n",
            "2148\n",
            "torch.Size([199, 11])\n",
            "2376\n",
            "torch.Size([63, 11])\n",
            "496\n",
            "torch.Size([65, 11])\n",
            "512\n",
            "torch.Size([193, 11])\n",
            "2304\n",
            "torch.Size([73, 11])\n",
            "576\n",
            "torch.Size([203, 11])\n",
            "2424\n",
            "torch.Size([202, 11])\n",
            "2412\n",
            "torch.Size([168, 11])\n",
            "2004\n",
            "torch.Size([74, 11])\n",
            "584\n",
            "torch.Size([74, 11])\n",
            "584\n",
            "torch.Size([179, 11])\n",
            "2136\n",
            "torch.Size([70, 11])\n",
            "552\n",
            "torch.Size([180, 11])\n",
            "2148\n",
            "torch.Size([182, 11])\n",
            "2172\n",
            "torch.Size([205, 11])\n",
            "2448\n",
            "torch.Size([73, 11])\n",
            "576\n",
            "torch.Size([187, 11])\n",
            "2232\n",
            "torch.Size([62, 11])\n",
            "488\n",
            "torch.Size([194, 11])\n",
            "2316\n",
            "torch.Size([178, 11])\n",
            "2124\n",
            "torch.Size([198, 11])\n",
            "2364\n",
            "torch.Size([71, 11])\n",
            "560\n",
            "torch.Size([193, 11])\n",
            "2304\n",
            "torch.Size([66, 11])\n",
            "520\n",
            "torch.Size([74, 11])\n",
            "584\n",
            "torch.Size([70, 11])\n",
            "552\n",
            "torch.Size([77, 11])\n",
            "608\n",
            "torch.Size([67, 11])\n",
            "528\n",
            "torch.Size([224, 11])\n",
            "2676\n",
            "torch.Size([204, 11])\n",
            "2436\n",
            "torch.Size([65, 11])\n",
            "512\n",
            "torch.Size([186, 11])\n",
            "2220\n",
            "torch.Size([186, 11])\n",
            "2220\n",
            "torch.Size([66, 11])\n",
            "520\n",
            "torch.Size([69, 11])\n",
            "544\n",
            "torch.Size([179, 11])\n",
            "2136\n",
            "torch.Size([191, 11])\n",
            "2280\n",
            "torch.Size([69, 11])\n",
            "544\n",
            "torch.Size([191, 11])\n",
            "2280\n",
            "torch.Size([70, 11])\n",
            "552\n",
            "torch.Size([192, 11])\n",
            "2292\n",
            "torch.Size([202, 11])\n",
            "2412\n",
            "torch.Size([183, 11])\n",
            "2184\n",
            "torch.Size([196, 11])\n",
            "2340\n",
            "torch.Size([69, 11])\n",
            "544\n",
            "torch.Size([71, 11])\n",
            "560\n",
            "torch.Size([69, 11])\n",
            "544\n",
            "torch.Size([217, 11])\n",
            "2592\n",
            "torch.Size([194, 11])\n",
            "2316\n",
            "torch.Size([187, 11])\n",
            "2232\n",
            "torch.Size([78, 11])\n",
            "616\n",
            "torch.Size([66, 11])\n",
            "520\n",
            "torch.Size([194, 11])\n",
            "2316\n",
            "torch.Size([66, 11])\n",
            "520\n",
            "torch.Size([214, 11])\n",
            "2556\n",
            "torch.Size([187, 11])\n",
            "2232\n",
            "torch.Size([194, 11])\n",
            "2316\n",
            "torch.Size([186, 11])\n",
            "2220\n",
            "torch.Size([72, 11])\n",
            "568\n",
            "torch.Size([69, 11])\n",
            "544\n",
            "torch.Size([58, 11])\n",
            "456\n",
            "torch.Size([179, 11])\n",
            "2136\n",
            "torch.Size([64, 11])\n",
            "504\n",
            "torch.Size([72, 11])\n",
            "568\n",
            "torch.Size([193, 11])\n",
            "2304\n",
            "torch.Size([68, 11])\n",
            "536\n",
            "torch.Size([68, 11])\n",
            "536\n",
            "torch.Size([195, 11])\n",
            "2328\n",
            "torch.Size([68, 11])\n",
            "536\n",
            "torch.Size([185, 11])\n",
            "2208\n",
            "torch.Size([65, 11])\n",
            "512\n",
            "torch.Size([194, 11])\n",
            "2316\n",
            "torch.Size([178, 11])\n",
            "2124\n",
            "torch.Size([65, 11])\n",
            "512\n",
            "torch.Size([195, 11])\n",
            "2328\n",
            "torch.Size([177, 11])\n",
            "2112\n",
            "torch.Size([204, 11])\n",
            "2436\n",
            "torch.Size([191, 11])\n",
            "2280\n",
            "torch.Size([194, 11])\n",
            "2316\n",
            "torch.Size([199, 11])\n",
            "2376\n",
            "torch.Size([76, 11])\n",
            "600\n",
            "torch.Size([70, 11])\n",
            "552\n",
            "torch.Size([198, 11])\n",
            "2364\n",
            "torch.Size([77, 11])\n",
            "608\n",
            "torch.Size([75, 11])\n",
            "592\n",
            "torch.Size([186, 11])\n",
            "2220\n",
            "torch.Size([209, 11])\n",
            "2496\n",
            "torch.Size([73, 11])\n",
            "576\n",
            "torch.Size([191, 11])\n",
            "2280\n",
            "torch.Size([170, 11])\n",
            "2028\n",
            "torch.Size([69, 11])\n",
            "544\n",
            "torch.Size([70, 11])\n",
            "552\n",
            "torch.Size([187, 11])\n",
            "2232\n",
            "torch.Size([192, 11])\n",
            "2292\n",
            "torch.Size([62, 11])\n",
            "488\n",
            "torch.Size([175, 11])\n",
            "2088\n",
            "torch.Size([176, 11])\n",
            "2100\n",
            "torch.Size([185, 11])\n",
            "2208\n",
            "torch.Size([206, 11])\n",
            "2460\n",
            "torch.Size([71, 11])\n",
            "560\n",
            "torch.Size([196, 11])\n",
            "2340\n",
            "torch.Size([65, 11])\n",
            "512\n",
            "torch.Size([186, 11])\n",
            "2220\n",
            "torch.Size([186, 11])\n",
            "2220\n",
            "torch.Size([185, 11])\n",
            "2208\n",
            "torch.Size([188, 11])\n",
            "2244\n",
            "torch.Size([174, 11])\n",
            "2076\n",
            "torch.Size([66, 11])\n",
            "520\n",
            "torch.Size([73, 11])\n",
            "576\n",
            "torch.Size([66, 11])\n",
            "520\n",
            "torch.Size([73, 11])\n",
            "576\n",
            "torch.Size([208, 11])\n",
            "2484\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Done!\n",
            "Processing...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In process\n",
            "torch.Size([169, 11])\n",
            "2016\n",
            "torch.Size([224, 11])\n",
            "2676\n",
            "torch.Size([168, 11])\n",
            "2004\n",
            "torch.Size([192, 11])\n",
            "2292\n",
            "torch.Size([186, 11])\n",
            "2220\n",
            "torch.Size([183, 11])\n",
            "2184\n",
            "torch.Size([195, 11])\n",
            "2328\n",
            "torch.Size([183, 11])\n",
            "2184\n",
            "torch.Size([203, 11])\n",
            "2424\n",
            "torch.Size([185, 11])\n",
            "2208\n",
            "torch.Size([217, 11])\n",
            "2592\n",
            "torch.Size([175, 11])\n",
            "2088\n",
            "torch.Size([177, 11])\n",
            "2112\n",
            "torch.Size([215, 11])\n",
            "2568\n",
            "torch.Size([181, 11])\n",
            "2160\n",
            "torch.Size([198, 11])\n",
            "2364\n",
            "torch.Size([181, 11])\n",
            "2160\n",
            "torch.Size([192, 11])\n",
            "2292\n",
            "torch.Size([186, 11])\n",
            "2220\n",
            "torch.Size([189, 11])\n",
            "2256\n",
            "torch.Size([196, 11])\n",
            "2340\n",
            "torch.Size([185, 11])\n",
            "2208\n",
            "torch.Size([182, 11])\n",
            "2172\n",
            "torch.Size([199, 11])\n",
            "2376\n",
            "torch.Size([190, 11])\n",
            "2268\n",
            "torch.Size([203, 11])\n",
            "2424\n",
            "torch.Size([179, 11])\n",
            "2136\n",
            "torch.Size([216, 11])\n",
            "2580\n",
            "torch.Size([207, 11])\n",
            "2472\n",
            "torch.Size([203, 11])\n",
            "2424\n",
            "torch.Size([207, 11])\n",
            "2472\n",
            "torch.Size([209, 11])\n",
            "2496\n",
            "torch.Size([197, 11])\n",
            "2352\n",
            "torch.Size([194, 11])\n",
            "2316\n",
            "torch.Size([189, 11])\n",
            "2256\n",
            "torch.Size([200, 11])\n",
            "2388\n",
            "torch.Size([194, 11])\n",
            "2316\n",
            "torch.Size([187, 11])\n",
            "2232\n",
            "torch.Size([171, 11])\n",
            "2040\n",
            "torch.Size([198, 11])\n",
            "2364\n",
            "torch.Size([173, 11])\n",
            "2064\n",
            "torch.Size([173, 11])\n",
            "2064\n",
            "torch.Size([210, 11])\n",
            "2508\n",
            "torch.Size([205, 11])\n",
            "2448\n",
            "torch.Size([192, 11])\n",
            "2292\n",
            "torch.Size([187, 11])\n",
            "2232\n",
            "torch.Size([201, 11])\n",
            "2400\n",
            "torch.Size([179, 11])\n",
            "2136\n",
            "torch.Size([185, 11])\n",
            "2208\n",
            "torch.Size([199, 11])\n",
            "2376\n",
            "torch.Size([178, 11])\n",
            "2124\n",
            "torch.Size([189, 11])\n",
            "2256\n",
            "torch.Size([169, 11])\n",
            "2016\n",
            "torch.Size([175, 11])\n",
            "2088\n",
            "torch.Size([174, 11])\n",
            "2076\n",
            "torch.Size([184, 11])\n",
            "2196\n",
            "torch.Size([167, 11])\n",
            "1992\n",
            "torch.Size([211, 11])\n",
            "2520\n",
            "torch.Size([200, 11])\n",
            "2388\n",
            "torch.Size([66, 11])\n",
            "520\n",
            "torch.Size([71, 11])\n",
            "560\n",
            "torch.Size([66, 11])\n",
            "520\n",
            "torch.Size([81, 11])\n",
            "640\n",
            "torch.Size([64, 11])\n",
            "504\n",
            "torch.Size([61, 11])\n",
            "480\n",
            "torch.Size([71, 11])\n",
            "560\n",
            "torch.Size([66, 11])\n",
            "520\n",
            "torch.Size([68, 11])\n",
            "536\n",
            "torch.Size([70, 11])\n",
            "552\n",
            "torch.Size([62, 11])\n",
            "488\n",
            "torch.Size([72, 11])\n",
            "568\n",
            "torch.Size([68, 11])\n",
            "536\n",
            "torch.Size([60, 11])\n",
            "472\n",
            "torch.Size([71, 11])\n",
            "560\n",
            "torch.Size([76, 11])\n",
            "600\n",
            "torch.Size([64, 11])\n",
            "504\n",
            "torch.Size([65, 11])\n",
            "512\n",
            "torch.Size([67, 11])\n",
            "528\n",
            "torch.Size([70, 11])\n",
            "552\n",
            "torch.Size([70, 11])\n",
            "552\n",
            "torch.Size([71, 11])\n",
            "560\n",
            "torch.Size([75, 11])\n",
            "592\n",
            "torch.Size([70, 11])\n",
            "552\n",
            "torch.Size([63, 11])\n",
            "496\n",
            "torch.Size([71, 11])\n",
            "560\n",
            "torch.Size([68, 11])\n",
            "536\n",
            "torch.Size([61, 11])\n",
            "480\n",
            "torch.Size([72, 11])\n",
            "568\n",
            "torch.Size([73, 11])\n",
            "576\n",
            "torch.Size([69, 11])\n",
            "544\n",
            "torch.Size([62, 11])\n",
            "488\n",
            "torch.Size([75, 11])\n",
            "592\n",
            "torch.Size([71, 11])\n",
            "560\n",
            "torch.Size([73, 11])\n",
            "576\n",
            "torch.Size([68, 11])\n",
            "536\n",
            "torch.Size([66, 11])\n",
            "520\n",
            "torch.Size([76, 11])\n",
            "600\n",
            "torch.Size([74, 11])\n",
            "584\n",
            "torch.Size([74, 11])\n",
            "584\n",
            "torch.Size([63, 11])\n",
            "496\n",
            "torch.Size([72, 11])\n",
            "568\n",
            "torch.Size([63, 11])\n",
            "496\n",
            "torch.Size([69, 11])\n",
            "544\n",
            "torch.Size([66, 11])\n",
            "520\n",
            "torch.Size([66, 11])\n",
            "520\n",
            "torch.Size([65, 11])\n",
            "512\n",
            "torch.Size([68, 11])\n",
            "536\n",
            "torch.Size([60, 11])\n",
            "472\n",
            "torch.Size([73, 11])\n",
            "576\n",
            "torch.Size([72, 11])\n",
            "568\n",
            "torch.Size([68, 11])\n",
            "536\n",
            "torch.Size([70, 11])\n",
            "552\n",
            "torch.Size([78, 11])\n",
            "616\n",
            "torch.Size([70, 11])\n",
            "552\n",
            "torch.Size([67, 11])\n",
            "528\n",
            "torch.Size([65, 11])\n",
            "512\n",
            "torch.Size([66, 11])\n",
            "520\n",
            "torch.Size([77, 11])\n",
            "608\n",
            "torch.Size([64, 11])\n",
            "504\n",
            "torch.Size([76, 11])\n",
            "600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "dataset_train = CustomDataset( split='train')\n",
        "dataset_valid = CustomDataset( split='valid')\n",
        "dataset_test = CustomDataset( split='test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxoZSG8PGyuR"
      },
      "source": [
        "# MODS TO MODEL START HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "3YvaqZlMxQAd"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_scatter import scatter_mean\n",
        "from torch_geometric.nn import MetaLayer\n",
        "\n",
        "class EdgeModel(torch.nn.Module):\n",
        "    def __init__(self, n_node_features, n_edge_features, hiddens, n_targets):\n",
        "        super().__init__()\n",
        "        self.edge_mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(2 * n_node_features + n_edge_features, hiddens),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            torch.nn.Linear(hiddens, n_targets),\n",
        "        )\n",
        "\n",
        "    def forward(self, src, dest, edge_attr, u=None, batch=None):\n",
        "        out = torch.cat([src, dest, edge_attr], 1)\n",
        "        out = self.edge_mlp(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class NodeModel(torch.nn.Module):\n",
        "    def __init__(self, n_node_features, hiddens, n_targets):\n",
        "        super(NodeModel, self).__init__()\n",
        "        self.node_mlp_1 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(n_node_features + hiddens, hiddens),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            torch.nn.Linear(hiddens, hiddens),\n",
        "        )\n",
        "        self.node_mlp_2 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(n_node_features + hiddens, hiddens),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            torch.nn.Linear(hiddens, n_targets),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
        "        #print(\"In node model\")\n",
        "        row, col = edge_index\n",
        "        out = torch.cat([x[col], edge_attr], dim=1)\n",
        "        out = self.node_mlp_1(out)\n",
        "        out = scatter_mean(out, row, dim=0, dim_size=x.size(0))\n",
        "        out = torch.cat([x, out], dim=1)\n",
        "        out = self.node_mlp_2(out)\n",
        "        #print(\"Exit node model\")\n",
        "        return out\n",
        "\n",
        "class MetaNet(torch.nn.Module):\n",
        "    def __init__(self, n_node_features, n_edge_features, num_hidden):\n",
        "        super(MetaNet, self).__init__()\n",
        "\n",
        "        # Input Layer\n",
        "        self.input = MetaLayer(\n",
        "            edge_model=EdgeModel(\n",
        "                n_node_features=n_node_features, n_edge_features=n_edge_features,\n",
        "                hiddens=num_hidden, n_targets=num_hidden),\n",
        "            node_model=NodeModel(n_node_features=n_node_features, hiddens=num_hidden, n_targets=num_hidden)\n",
        "            )\n",
        "\n",
        "        # Output Layer\n",
        "        self.output = MetaLayer(\n",
        "            edge_model=EdgeModel(\n",
        "                n_node_features=num_hidden, n_edge_features=num_hidden,\n",
        "                hiddens=num_hidden, n_targets=num_hidden),\n",
        "            node_model=NodeModel(n_node_features=num_hidden, hiddens=num_hidden, n_targets=1)\n",
        "        )\n",
        "\n",
        "        # Attention Mechanism\n",
        "        self.attention = GATv2Conv(in_channels=num_hidden,\n",
        "                                   out_channels=num_hidden,\n",
        "                                   heads=2,\n",
        "                                   concat=False,\n",
        "                                   dropout=0.2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_attr, y = data.x, data.edge_index, data.edge_attr, data.y\n",
        "        x, edge_attr, _ = self.input(x, edge_index, edge_attr)\n",
        "        x = F.relu(x)\n",
        "        # x = F.dropout(x, p=0.2, training=self.training)\n",
        "\n",
        "        # Attention Mechanism\n",
        "        # x = x.unsqueeze(0)\n",
        "        # x,_ = self.attention(x,x,x)\n",
        "        x = self.attention(x, edge_index)\n",
        "        # x = x.squeeze(0)\n",
        "\n",
        "        x, edge_attr, _ = self.output(x, edge_index, edge_attr)\n",
        "        #x = F.dropout(x, p=0.5, training=self.training)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAnmOvhoG0XI"
      },
      "source": [
        "# MODS TO MODEL END HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T10:20:13.786261Z",
          "iopub.status.busy": "2023-07-06T10:20:13.785196Z",
          "iopub.status.idle": "2023-07-06T10:20:13.791675Z",
          "shell.execute_reply": "2023-07-06T10:20:13.790368Z",
          "shell.execute_reply.started": "2023-07-06T10:20:13.786213Z"
        },
        "id": "bPRRP8jlcoCh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "num_node_features = 11\n",
        "num_edge_features = 2\n",
        "num_hidden = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T10:20:32.677829Z",
          "iopub.status.busy": "2023-07-06T10:20:32.677457Z",
          "iopub.status.idle": "2023-07-06T10:20:34.309011Z",
          "shell.execute_reply": "2023-07-06T10:20:34.308022Z",
          "shell.execute_reply.started": "2023-07-06T10:20:32.677800Z"
        },
        "id": "u_oMx_LLcoCh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model = MetaNet(num_node_features, num_edge_features, num_hidden).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "G3dzLC61ETfJ"
      },
      "outputs": [],
      "source": [
        "# model  = Net(num_node_features, num_hidden).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2B_GYSk3nZ4",
        "outputId": "0acae69c-a927-44a5-e9ab-3cb5cbcc2088"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MetaNet(\n",
              "  (input): MetaLayer(\n",
              "    edge_model=EdgeModel(\n",
              "    (edge_mlp): Sequential(\n",
              "      (0): Linear(in_features=24, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "  ),\n",
              "    node_model=NodeModel(\n",
              "    (node_mlp_1): Sequential(\n",
              "      (0): Linear(in_features=139, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "    (node_mlp_2): Sequential(\n",
              "      (0): Linear(in_features=139, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "  ),\n",
              "    global_model=None\n",
              "  )\n",
              "  (output): MetaLayer(\n",
              "    edge_model=EdgeModel(\n",
              "    (edge_mlp): Sequential(\n",
              "      (0): Linear(in_features=384, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "  ),\n",
              "    node_model=NodeModel(\n",
              "    (node_mlp_1): Sequential(\n",
              "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "    (node_mlp_2): Sequential(\n",
              "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
              "    )\n",
              "  ),\n",
              "    global_model=None\n",
              "  )\n",
              "  (attention): GATv2Conv(128, 128, heads=2)\n",
              ")"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T10:20:46.697896Z",
          "iopub.status.busy": "2023-07-06T10:20:46.697337Z",
          "iopub.status.idle": "2023-07-06T10:20:46.703235Z",
          "shell.execute_reply": "2023-07-06T10:20:46.702092Z",
          "shell.execute_reply.started": "2023-07-06T10:20:46.697865Z"
        },
        "id": "tyIe36JGcoCh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(lr=1e-4,params=model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T10:20:46.966933Z",
          "iopub.status.busy": "2023-07-06T10:20:46.966338Z",
          "iopub.status.idle": "2023-07-06T10:20:46.975716Z",
          "shell.execute_reply": "2023-07-06T10:20:46.974673Z",
          "shell.execute_reply.started": "2023-07-06T10:20:46.966903Z"
        },
        "id": "kOdrIldDcoCi",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def train(dataset):\n",
        "    # Monitor training.\n",
        "    losses = []\n",
        "\n",
        "    # Put model in training mode!\n",
        "    model.train()\n",
        "    i=0\n",
        "    for i, batch in enumerate(dataset):\n",
        "        #print(\"misaa\")\n",
        "        # Training step.\n",
        "\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch)\n",
        "        loss = torch.sqrt(F.mse_loss(out.squeeze(), batch.y.squeeze()))\n",
        "        #print(f\"Training oss for {i}: {loss}\")\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Monitoring\n",
        "        losses.append(loss.item())\n",
        "        if(i == 359): break\n",
        "    # Return training metrics.\n",
        "    return losses\n",
        "\n",
        "\n",
        "def evaluate(dataset):\n",
        "    # Monitor evaluation.\n",
        "    losses = []\n",
        "\n",
        "    # Validation (1)\n",
        "    model.eval()\n",
        "    i = 0\n",
        "    for i, batch in enumerate(dataset):\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        # Calculate validation losses.\n",
        "        out = model(batch)\n",
        "        loss = torch.sqrt(F.mse_loss(out.squeeze(), batch.y.squeeze()))\n",
        "\n",
        "        # Metric logging.\n",
        "        losses.append(loss.item())\n",
        "        if(i == 119): break\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "jrn8HVXecoCr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import shuffle\n",
        "from torch_geometric.data import DataLoader\n",
        "train_loader = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(dataset_valid, batch_size=3, shuffle=True)\n",
        "test_loader = DataLoader(dataset_test, batch_size=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lip8xSKacoCr",
        "outputId": "3caba936-03f1-4530-aa3c-d498ebe39e54",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataBatch(x=[4414, 11], edge_index=[2, 48704], edge_attr=[48704, 2], y=[4414], num_nodes=4414, batch=[4414], ptr=[33])\n"
          ]
        }
      ],
      "source": [
        "for batch in train_loader:\n",
        "    print(batch)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeVD_pAqcoCr",
        "outputId": "11bfd4e3-9a29-4931-d341-1a80ddce5265",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Len of Training loss: 12, Average loss: 26.11988131205241\n",
            "Len of Validation loss: 40, Average loss: 25.575543451309205\n",
            "Epoch: 1, Len of Training loss: 12, Average loss: 25.455668290456135\n",
            "Len of Validation loss: 40, Average loss: 25.118904066085815\n",
            "Epoch: 2, Len of Training loss: 12, Average loss: 25.662170092264812\n",
            "Len of Validation loss: 40, Average loss: 24.933530235290526\n",
            "Epoch: 3, Len of Training loss: 12, Average loss: 25.488548119862873\n",
            "Len of Validation loss: 40, Average loss: 25.086907196044923\n",
            "Epoch: 4, Len of Training loss: 12, Average loss: 25.17410675684611\n",
            "Len of Validation loss: 40, Average loss: 24.870771169662476\n",
            "Epoch: 5, Len of Training loss: 12, Average loss: 24.679601669311523\n",
            "Len of Validation loss: 40, Average loss: 24.720238208770752\n",
            "Epoch: 6, Len of Training loss: 12, Average loss: 24.565273920694988\n",
            "Len of Validation loss: 40, Average loss: 24.277456045150757\n",
            "Epoch: 7, Len of Training loss: 12, Average loss: 24.37552785873413\n",
            "Len of Validation loss: 40, Average loss: 24.008088493347167\n",
            "Epoch: 8, Len of Training loss: 12, Average loss: 23.918114185333252\n",
            "Len of Validation loss: 40, Average loss: 23.991776037216187\n",
            "Epoch: 9, Len of Training loss: 12, Average loss: 23.872381528218586\n",
            "Len of Validation loss: 40, Average loss: 23.605943727493287\n",
            "Epoch: 10, Len of Training loss: 12, Average loss: 23.212267716725666\n",
            "Len of Validation loss: 40, Average loss: 22.530891990661623\n",
            "Epoch: 11, Len of Training loss: 12, Average loss: 22.8395250638326\n",
            "Len of Validation loss: 40, Average loss: 22.138096284866332\n",
            "Epoch: 12, Len of Training loss: 12, Average loss: 22.296640555063885\n",
            "Len of Validation loss: 40, Average loss: 21.33690230846405\n",
            "Epoch: 13, Len of Training loss: 12, Average loss: 21.687875270843506\n",
            "Len of Validation loss: 40, Average loss: 21.161470770835876\n",
            "Epoch: 14, Len of Training loss: 12, Average loss: 21.14750607808431\n",
            "Len of Validation loss: 40, Average loss: 20.59306631088257\n",
            "Epoch: 15, Len of Training loss: 12, Average loss: 20.835527261098225\n",
            "Len of Validation loss: 40, Average loss: 20.21102261543274\n",
            "Epoch: 16, Len of Training loss: 12, Average loss: 20.506367524464924\n",
            "Len of Validation loss: 40, Average loss: 21.93172969818115\n",
            "Epoch: 17, Len of Training loss: 12, Average loss: 20.773205598195393\n",
            "Len of Validation loss: 40, Average loss: 19.734506440162658\n",
            "Epoch: 18, Len of Training loss: 12, Average loss: 19.823469320933025\n",
            "Len of Validation loss: 40, Average loss: 18.76793780326843\n",
            "Epoch: 19, Len of Training loss: 12, Average loss: 19.407949606577557\n",
            "Len of Validation loss: 40, Average loss: 18.436038899421693\n",
            "Epoch: 20, Len of Training loss: 12, Average loss: 19.054256121317547\n",
            "Len of Validation loss: 40, Average loss: 17.73955044746399\n",
            "Epoch: 21, Len of Training loss: 12, Average loss: 18.433186372121174\n",
            "Len of Validation loss: 40, Average loss: 17.351713442802428\n",
            "Epoch: 22, Len of Training loss: 12, Average loss: 17.729247570037842\n",
            "Len of Validation loss: 40, Average loss: 16.54815833568573\n",
            "Epoch: 23, Len of Training loss: 12, Average loss: 17.68848220507304\n",
            "Len of Validation loss: 40, Average loss: 17.059689807891846\n",
            "Epoch: 24, Len of Training loss: 12, Average loss: 17.492016553878784\n",
            "Len of Validation loss: 40, Average loss: 15.982792091369628\n",
            "Epoch: 25, Len of Training loss: 12, Average loss: 16.834291378657024\n",
            "Len of Validation loss: 40, Average loss: 15.243610143661499\n",
            "Epoch: 26, Len of Training loss: 12, Average loss: 16.240978399912517\n",
            "Len of Validation loss: 40, Average loss: 16.175952529907228\n",
            "Epoch: 27, Len of Training loss: 12, Average loss: 16.213233550389607\n",
            "Len of Validation loss: 40, Average loss: 14.986691045761109\n",
            "Epoch: 28, Len of Training loss: 12, Average loss: 15.545711437861124\n",
            "Len of Validation loss: 40, Average loss: 14.232480478286742\n",
            "Epoch: 29, Len of Training loss: 12, Average loss: 15.127208868662516\n",
            "Len of Validation loss: 40, Average loss: 14.315550994873046\n",
            "Epoch: 30, Len of Training loss: 12, Average loss: 15.101674874623617\n",
            "Len of Validation loss: 40, Average loss: 13.759136366844178\n",
            "Epoch: 31, Len of Training loss: 12, Average loss: 14.898167689641317\n",
            "Len of Validation loss: 40, Average loss: 13.626507830619811\n",
            "Epoch: 32, Len of Training loss: 12, Average loss: 14.240657885869345\n",
            "Len of Validation loss: 40, Average loss: 12.841558384895325\n",
            "Epoch: 33, Len of Training loss: 12, Average loss: 13.801780303319296\n",
            "Len of Validation loss: 40, Average loss: 12.631167221069337\n",
            "Epoch: 34, Len of Training loss: 12, Average loss: 13.56252153714498\n",
            "Len of Validation loss: 40, Average loss: 12.352122926712036\n",
            "Epoch: 35, Len of Training loss: 12, Average loss: 13.240024407704672\n",
            "Len of Validation loss: 40, Average loss: 11.973387527465821\n",
            "Epoch: 36, Len of Training loss: 12, Average loss: 12.98884884516398\n",
            "Len of Validation loss: 40, Average loss: 11.950410735607147\n",
            "Epoch: 37, Len of Training loss: 12, Average loss: 12.932994365692139\n",
            "Len of Validation loss: 40, Average loss: 11.56868498325348\n",
            "Epoch: 38, Len of Training loss: 12, Average loss: 12.469825347264608\n",
            "Len of Validation loss: 40, Average loss: 11.281699562072754\n",
            "Epoch: 39, Len of Training loss: 12, Average loss: 12.374939123789469\n",
            "Len of Validation loss: 40, Average loss: 11.232071435451507\n",
            "Epoch: 40, Len of Training loss: 12, Average loss: 12.402014891306559\n",
            "Len of Validation loss: 40, Average loss: 11.117913925647736\n",
            "Epoch: 41, Len of Training loss: 12, Average loss: 12.1123898824056\n",
            "Len of Validation loss: 40, Average loss: 11.21272257566452\n",
            "Epoch: 42, Len of Training loss: 12, Average loss: 11.728694756825766\n",
            "Len of Validation loss: 40, Average loss: 10.874131178855896\n",
            "Epoch: 43, Len of Training loss: 12, Average loss: 11.790811777114868\n",
            "Len of Validation loss: 40, Average loss: 11.025227510929108\n",
            "Epoch: 44, Len of Training loss: 12, Average loss: 11.777700265248617\n",
            "Len of Validation loss: 40, Average loss: 10.730370557308197\n",
            "Epoch: 45, Len of Training loss: 12, Average loss: 11.513667901357016\n",
            "Len of Validation loss: 40, Average loss: 10.27398203611374\n",
            "Epoch: 46, Len of Training loss: 12, Average loss: 11.359729051589966\n",
            "Len of Validation loss: 40, Average loss: 10.542798602581025\n",
            "Epoch: 47, Len of Training loss: 12, Average loss: 11.561487038930258\n",
            "Len of Validation loss: 40, Average loss: 11.805340051651001\n",
            "Epoch: 48, Len of Training loss: 12, Average loss: 11.211488326390585\n",
            "Len of Validation loss: 40, Average loss: 10.168083107471466\n",
            "Epoch: 49, Len of Training loss: 12, Average loss: 10.766624768575033\n",
            "Len of Validation loss: 40, Average loss: 10.009965479373932\n",
            "Epoch: 50, Len of Training loss: 12, Average loss: 11.003328164418539\n",
            "Len of Validation loss: 40, Average loss: 11.40494476556778\n",
            "Epoch: 51, Len of Training loss: 12, Average loss: 11.270145654678345\n",
            "Len of Validation loss: 40, Average loss: 10.696935248374938\n",
            "Epoch: 52, Len of Training loss: 12, Average loss: 11.607405185699463\n",
            "Len of Validation loss: 40, Average loss: 10.256826412677764\n",
            "Epoch: 53, Len of Training loss: 12, Average loss: 11.10208257039388\n",
            "Len of Validation loss: 40, Average loss: 11.014328145980835\n",
            "Epoch: 54, Len of Training loss: 12, Average loss: 10.856786092122396\n",
            "Len of Validation loss: 40, Average loss: 9.866785752773286\n",
            "Epoch: 55, Len of Training loss: 12, Average loss: 10.437446276346842\n",
            "Len of Validation loss: 40, Average loss: 9.76649489402771\n",
            "Epoch: 56, Len of Training loss: 12, Average loss: 10.59676710764567\n",
            "Len of Validation loss: 40, Average loss: 9.553781712055207\n",
            "Epoch: 57, Len of Training loss: 12, Average loss: 10.388660669326782\n",
            "Len of Validation loss: 40, Average loss: 9.42688009738922\n",
            "Epoch: 58, Len of Training loss: 12, Average loss: 10.008277734120687\n",
            "Len of Validation loss: 40, Average loss: 9.557761752605439\n",
            "Epoch: 59, Len of Training loss: 12, Average loss: 10.25679580370585\n",
            "Len of Validation loss: 40, Average loss: 9.52037546634674\n",
            "Epoch: 60, Len of Training loss: 12, Average loss: 9.974582433700562\n",
            "Len of Validation loss: 40, Average loss: 9.488951349258423\n",
            "Epoch: 61, Len of Training loss: 12, Average loss: 10.169376293818155\n",
            "Len of Validation loss: 40, Average loss: 9.977351677417754\n",
            "Epoch: 62, Len of Training loss: 12, Average loss: 9.91445279121399\n",
            "Len of Validation loss: 40, Average loss: 9.228789341449737\n",
            "Epoch: 63, Len of Training loss: 12, Average loss: 9.836687803268433\n",
            "Len of Validation loss: 40, Average loss: 9.007493102550507\n",
            "Epoch: 64, Len of Training loss: 12, Average loss: 9.77195676167806\n",
            "Len of Validation loss: 40, Average loss: 9.000388622283936\n",
            "Epoch: 65, Len of Training loss: 12, Average loss: 9.787821372350058\n",
            "Len of Validation loss: 40, Average loss: 8.970634722709656\n",
            "Epoch: 66, Len of Training loss: 12, Average loss: 9.54650354385376\n",
            "Len of Validation loss: 40, Average loss: 8.839656937122346\n",
            "Epoch: 67, Len of Training loss: 12, Average loss: 9.552630027135214\n",
            "Len of Validation loss: 40, Average loss: 9.911560797691346\n",
            "Epoch: 68, Len of Training loss: 12, Average loss: 9.750034610430399\n",
            "Len of Validation loss: 40, Average loss: 9.305850076675416\n",
            "Epoch: 69, Len of Training loss: 12, Average loss: 9.662144621213278\n",
            "Len of Validation loss: 40, Average loss: 9.991898798942566\n",
            "Epoch: 70, Len of Training loss: 12, Average loss: 9.697948137919107\n",
            "Len of Validation loss: 40, Average loss: 8.623521101474761\n",
            "Epoch: 71, Len of Training loss: 12, Average loss: 9.665591160456339\n",
            "Len of Validation loss: 40, Average loss: 9.19544631242752\n",
            "Epoch: 72, Len of Training loss: 12, Average loss: 10.087563276290894\n",
            "Len of Validation loss: 40, Average loss: 8.804088294506073\n",
            "Epoch: 73, Len of Training loss: 12, Average loss: 9.55648640791575\n",
            "Len of Validation loss: 40, Average loss: 9.103666067123413\n",
            "Epoch: 74, Len of Training loss: 12, Average loss: 9.359180370966593\n",
            "Len of Validation loss: 40, Average loss: 9.39185322523117\n",
            "Epoch: 75, Len of Training loss: 12, Average loss: 9.487301627794901\n",
            "Len of Validation loss: 40, Average loss: 8.852410566806793\n",
            "Epoch: 76, Len of Training loss: 12, Average loss: 9.297562718391418\n",
            "Len of Validation loss: 40, Average loss: 8.423047494888305\n",
            "Epoch: 77, Len of Training loss: 12, Average loss: 8.804623007774353\n",
            "Len of Validation loss: 40, Average loss: 8.440147089958192\n",
            "Epoch: 78, Len of Training loss: 12, Average loss: 8.845495541890463\n",
            "Len of Validation loss: 40, Average loss: 8.427876448631286\n",
            "Epoch: 79, Len of Training loss: 12, Average loss: 8.832260568936666\n",
            "Len of Validation loss: 40, Average loss: 8.317887544631958\n",
            "Epoch: 80, Len of Training loss: 12, Average loss: 8.725817362467447\n",
            "Len of Validation loss: 40, Average loss: 8.319648802280426\n",
            "Epoch: 81, Len of Training loss: 12, Average loss: 8.755108873049418\n",
            "Len of Validation loss: 40, Average loss: 8.320916563272476\n",
            "Epoch: 82, Len of Training loss: 12, Average loss: 8.795495311419169\n",
            "Len of Validation loss: 40, Average loss: 8.645759880542755\n",
            "Epoch: 83, Len of Training loss: 12, Average loss: 9.072513063748678\n",
            "Len of Validation loss: 40, Average loss: 8.133427172899246\n",
            "Epoch: 84, Len of Training loss: 12, Average loss: 8.818705717722574\n",
            "Len of Validation loss: 40, Average loss: 8.569727563858033\n",
            "Epoch: 85, Len of Training loss: 12, Average loss: 8.83921194076538\n",
            "Len of Validation loss: 40, Average loss: 9.646962594985961\n",
            "Epoch: 86, Len of Training loss: 12, Average loss: 8.958775242169699\n",
            "Len of Validation loss: 40, Average loss: 8.251799583435059\n",
            "Epoch: 87, Len of Training loss: 12, Average loss: 8.34282124042511\n",
            "Len of Validation loss: 40, Average loss: 8.306273150444031\n",
            "Epoch: 88, Len of Training loss: 12, Average loss: 8.72123392422994\n",
            "Len of Validation loss: 40, Average loss: 8.175601387023926\n",
            "Epoch: 89, Len of Training loss: 12, Average loss: 8.435261249542236\n",
            "Len of Validation loss: 40, Average loss: 8.001251083612441\n",
            "Epoch: 90, Len of Training loss: 12, Average loss: 8.172327359517416\n",
            "Len of Validation loss: 40, Average loss: 8.324760377407074\n",
            "Epoch: 91, Len of Training loss: 12, Average loss: 8.536909858385721\n",
            "Len of Validation loss: 40, Average loss: 8.152199095487594\n",
            "Epoch: 92, Len of Training loss: 12, Average loss: 8.706066052118937\n",
            "Len of Validation loss: 40, Average loss: 9.544607305526734\n",
            "Epoch: 93, Len of Training loss: 12, Average loss: 9.069178183873495\n",
            "Len of Validation loss: 40, Average loss: 8.109744000434876\n",
            "Epoch: 94, Len of Training loss: 12, Average loss: 8.483286698659262\n",
            "Len of Validation loss: 40, Average loss: 8.349749290943146\n",
            "Epoch: 95, Len of Training loss: 12, Average loss: 8.822214365005493\n",
            "Len of Validation loss: 40, Average loss: 9.238704514503478\n",
            "Epoch: 96, Len of Training loss: 12, Average loss: 8.847792545954386\n",
            "Len of Validation loss: 40, Average loss: 8.051293981075286\n",
            "Epoch: 97, Len of Training loss: 12, Average loss: 8.388902147610983\n",
            "Len of Validation loss: 40, Average loss: 8.15098958015442\n",
            "Epoch: 98, Len of Training loss: 12, Average loss: 8.461846510569254\n",
            "Len of Validation loss: 40, Average loss: 8.040500664710999\n",
            "Epoch: 99, Len of Training loss: 12, Average loss: 8.260637124379477\n",
            "Len of Validation loss: 40, Average loss: 8.018223071098328\n",
            "Epoch: 100, Len of Training loss: 12, Average loss: 8.262404561042786\n",
            "Len of Validation loss: 40, Average loss: 8.494604551792145\n",
            "Epoch: 101, Len of Training loss: 12, Average loss: 8.410927097002665\n",
            "Len of Validation loss: 40, Average loss: 8.201691830158234\n",
            "Epoch: 102, Len of Training loss: 12, Average loss: 8.35189958413442\n",
            "Len of Validation loss: 40, Average loss: 7.751775085926056\n",
            "Epoch: 103, Len of Training loss: 12, Average loss: 8.304603894551596\n",
            "Len of Validation loss: 40, Average loss: 8.29556609392166\n",
            "Epoch: 104, Len of Training loss: 12, Average loss: 8.35425098737081\n",
            "Len of Validation loss: 40, Average loss: 8.215635704994202\n",
            "Epoch: 105, Len of Training loss: 12, Average loss: 8.327116886774698\n",
            "Len of Validation loss: 40, Average loss: 7.873779356479645\n",
            "Epoch: 106, Len of Training loss: 12, Average loss: 8.038845896720886\n",
            "Len of Validation loss: 40, Average loss: 7.742307651042938\n",
            "Epoch: 107, Len of Training loss: 12, Average loss: 8.060043692588806\n",
            "Len of Validation loss: 40, Average loss: 8.15295888185501\n",
            "Epoch: 108, Len of Training loss: 12, Average loss: 8.006344238917032\n",
            "Len of Validation loss: 40, Average loss: 7.675030672550202\n",
            "Epoch: 109, Len of Training loss: 12, Average loss: 8.178961912790934\n",
            "Len of Validation loss: 40, Average loss: 7.6817796409130095\n",
            "Epoch: 110, Len of Training loss: 12, Average loss: 7.970063209533691\n",
            "Len of Validation loss: 40, Average loss: 7.551608049869538\n",
            "Epoch: 111, Len of Training loss: 12, Average loss: 7.8940323193868\n",
            "Len of Validation loss: 40, Average loss: 8.420490658283233\n",
            "Epoch: 112, Len of Training loss: 12, Average loss: 7.904134909311931\n",
            "Len of Validation loss: 40, Average loss: 8.516094183921814\n",
            "Epoch: 113, Len of Training loss: 12, Average loss: 8.920286854108175\n",
            "Len of Validation loss: 40, Average loss: 7.583051383495331\n",
            "Epoch: 114, Len of Training loss: 12, Average loss: 8.07827603816986\n",
            "Len of Validation loss: 40, Average loss: 8.51258510351181\n",
            "Epoch: 115, Len of Training loss: 12, Average loss: 8.246209661165873\n",
            "Len of Validation loss: 40, Average loss: 7.809867250919342\n",
            "Epoch: 116, Len of Training loss: 12, Average loss: 7.842913428942363\n",
            "Len of Validation loss: 40, Average loss: 7.5593894302845\n",
            "Epoch: 117, Len of Training loss: 12, Average loss: 7.891053676605225\n",
            "Len of Validation loss: 40, Average loss: 7.8448398351669315\n",
            "Epoch: 118, Len of Training loss: 12, Average loss: 7.822984178860982\n",
            "Len of Validation loss: 40, Average loss: 7.444652926921845\n",
            "Epoch: 119, Len of Training loss: 12, Average loss: 7.723477005958557\n",
            "Len of Validation loss: 40, Average loss: 7.8379913687705995\n",
            "Epoch: 120, Len of Training loss: 12, Average loss: 7.970503926277161\n",
            "Len of Validation loss: 40, Average loss: 7.786259818077087\n",
            "Epoch: 121, Len of Training loss: 12, Average loss: 7.920323928197225\n",
            "Len of Validation loss: 40, Average loss: 9.708960664272308\n",
            "Epoch: 122, Len of Training loss: 12, Average loss: 8.234623233477274\n",
            "Len of Validation loss: 40, Average loss: 7.518887555599212\n",
            "Epoch: 123, Len of Training loss: 12, Average loss: 7.8528367678324384\n",
            "Len of Validation loss: 40, Average loss: 7.22872896194458\n",
            "Epoch: 124, Len of Training loss: 12, Average loss: 7.661450187365214\n",
            "Len of Validation loss: 40, Average loss: 7.542538022994995\n",
            "Epoch: 125, Len of Training loss: 12, Average loss: 7.863650520642598\n",
            "Len of Validation loss: 40, Average loss: 7.683890151977539\n",
            "Epoch: 126, Len of Training loss: 12, Average loss: 7.66190763314565\n",
            "Len of Validation loss: 40, Average loss: 7.330887126922607\n",
            "Epoch: 127, Len of Training loss: 12, Average loss: 7.735759178797404\n",
            "Len of Validation loss: 40, Average loss: 7.131875711679458\n",
            "Epoch: 128, Len of Training loss: 12, Average loss: 7.570005575815837\n",
            "Len of Validation loss: 40, Average loss: 7.67931250333786\n",
            "Epoch: 129, Len of Training loss: 12, Average loss: 7.554781834284465\n",
            "Len of Validation loss: 40, Average loss: 7.330950510501862\n",
            "Epoch: 130, Len of Training loss: 12, Average loss: 7.512033700942993\n",
            "Len of Validation loss: 40, Average loss: 7.22850490808487\n",
            "Epoch: 131, Len of Training loss: 12, Average loss: 7.550606330235799\n",
            "Len of Validation loss: 40, Average loss: 7.276666361093521\n",
            "Epoch: 132, Len of Training loss: 12, Average loss: 8.0111745595932\n",
            "Len of Validation loss: 40, Average loss: 7.626779735088348\n",
            "Epoch: 133, Len of Training loss: 12, Average loss: 7.742934743563334\n",
            "Len of Validation loss: 40, Average loss: 7.407620322704315\n",
            "Epoch: 134, Len of Training loss: 12, Average loss: 7.534990111986796\n",
            "Len of Validation loss: 40, Average loss: 7.379457950592041\n",
            "Epoch: 135, Len of Training loss: 12, Average loss: 8.033618887265524\n",
            "Len of Validation loss: 40, Average loss: 8.12580178976059\n",
            "Epoch: 136, Len of Training loss: 12, Average loss: 8.411495129267374\n",
            "Len of Validation loss: 40, Average loss: 7.635153084993362\n",
            "Epoch: 137, Len of Training loss: 12, Average loss: 7.6892209847768145\n",
            "Len of Validation loss: 40, Average loss: 8.053172624111175\n",
            "Epoch: 138, Len of Training loss: 12, Average loss: 7.666821241378784\n",
            "Len of Validation loss: 40, Average loss: 7.250940757989883\n",
            "Epoch: 139, Len of Training loss: 12, Average loss: 8.210328300793966\n",
            "Len of Validation loss: 40, Average loss: 8.32336221933365\n",
            "Epoch: 140, Len of Training loss: 12, Average loss: 8.027029434839884\n",
            "Len of Validation loss: 40, Average loss: 7.049081724882126\n",
            "Epoch: 141, Len of Training loss: 12, Average loss: 7.497447490692139\n",
            "Len of Validation loss: 40, Average loss: 7.178706467151642\n",
            "Epoch: 142, Len of Training loss: 12, Average loss: 7.777064243952434\n",
            "Len of Validation loss: 40, Average loss: 7.546072494983673\n",
            "Epoch: 143, Len of Training loss: 12, Average loss: 8.160009225209555\n",
            "Len of Validation loss: 40, Average loss: 7.2958694219589235\n",
            "Epoch: 144, Len of Training loss: 12, Average loss: 7.568947434425354\n",
            "Len of Validation loss: 40, Average loss: 7.374149525165558\n",
            "Epoch: 145, Len of Training loss: 12, Average loss: 7.510427713394165\n",
            "Len of Validation loss: 40, Average loss: 7.256817626953125\n",
            "Epoch: 146, Len of Training loss: 12, Average loss: 7.355728944142659\n",
            "Len of Validation loss: 40, Average loss: 7.341388237476349\n",
            "Epoch: 147, Len of Training loss: 12, Average loss: 7.430928309758504\n",
            "Len of Validation loss: 40, Average loss: 7.418365621566773\n",
            "Epoch: 148, Len of Training loss: 12, Average loss: 7.54040801525116\n",
            "Len of Validation loss: 40, Average loss: 7.2758201360702515\n",
            "Epoch: 149, Len of Training loss: 12, Average loss: 7.840379277865092\n",
            "Len of Validation loss: 40, Average loss: 7.41102545261383\n",
            "Epoch: 150, Len of Training loss: 12, Average loss: 7.390722950299581\n",
            "Len of Validation loss: 40, Average loss: 7.285008645057678\n",
            "Epoch: 151, Len of Training loss: 12, Average loss: 7.439838131268819\n",
            "Len of Validation loss: 40, Average loss: 7.448076379299164\n",
            "Epoch: 152, Len of Training loss: 12, Average loss: 8.4575887521108\n",
            "Len of Validation loss: 40, Average loss: 7.185612416267395\n",
            "Epoch: 153, Len of Training loss: 12, Average loss: 7.600428024927775\n",
            "Len of Validation loss: 40, Average loss: 7.14796359539032\n",
            "Epoch: 154, Len of Training loss: 12, Average loss: 7.308693726857503\n",
            "Len of Validation loss: 40, Average loss: 7.3041027128696445\n",
            "Epoch: 155, Len of Training loss: 12, Average loss: 7.421933531761169\n",
            "Len of Validation loss: 40, Average loss: 7.380048060417176\n",
            "Epoch: 156, Len of Training loss: 12, Average loss: 7.669873555501302\n",
            "Len of Validation loss: 40, Average loss: 9.040474307537078\n",
            "Epoch: 157, Len of Training loss: 12, Average loss: 7.683977286020915\n",
            "Len of Validation loss: 40, Average loss: 7.48434146642685\n",
            "Epoch: 158, Len of Training loss: 12, Average loss: 8.094436208407084\n",
            "Len of Validation loss: 40, Average loss: 7.643329060077667\n",
            "Epoch: 159, Len of Training loss: 12, Average loss: 7.858813762664795\n",
            "Len of Validation loss: 40, Average loss: 7.090255272388458\n",
            "Epoch: 160, Len of Training loss: 12, Average loss: 7.414851586023967\n",
            "Len of Validation loss: 40, Average loss: 7.1332915186882015\n",
            "Epoch: 161, Len of Training loss: 12, Average loss: 7.216536283493042\n",
            "Len of Validation loss: 40, Average loss: 6.966196924448013\n",
            "Epoch: 162, Len of Training loss: 12, Average loss: 7.259127775828044\n",
            "Len of Validation loss: 40, Average loss: 6.962134933471679\n",
            "Epoch: 163, Len of Training loss: 12, Average loss: 7.274444540341695\n",
            "Len of Validation loss: 40, Average loss: 7.150109767913818\n",
            "Epoch: 164, Len of Training loss: 12, Average loss: 7.153643170992534\n",
            "Len of Validation loss: 40, Average loss: 6.934502410888672\n",
            "Epoch: 165, Len of Training loss: 12, Average loss: 7.293181737263997\n",
            "Len of Validation loss: 40, Average loss: 8.337685441970825\n",
            "Epoch: 166, Len of Training loss: 12, Average loss: 7.79194708665212\n",
            "Len of Validation loss: 40, Average loss: 9.18379237651825\n",
            "Epoch: 167, Len of Training loss: 12, Average loss: 7.676588257153829\n",
            "Len of Validation loss: 40, Average loss: 7.103078663349152\n",
            "Epoch: 168, Len of Training loss: 12, Average loss: 7.286468863487244\n",
            "Len of Validation loss: 40, Average loss: 6.828071010112763\n",
            "Epoch: 169, Len of Training loss: 12, Average loss: 7.249234159787496\n",
            "Len of Validation loss: 40, Average loss: 7.392715501785278\n",
            "Epoch: 170, Len of Training loss: 12, Average loss: 7.1736830075581866\n",
            "Len of Validation loss: 40, Average loss: 7.847656881809234\n",
            "Epoch: 171, Len of Training loss: 12, Average loss: 7.292834798494975\n",
            "Len of Validation loss: 40, Average loss: 7.352783763408661\n",
            "Epoch: 172, Len of Training loss: 12, Average loss: 7.804601947466533\n",
            "Len of Validation loss: 40, Average loss: 6.91563401222229\n",
            "Epoch: 173, Len of Training loss: 12, Average loss: 7.413235942522685\n",
            "Len of Validation loss: 40, Average loss: 6.907462644577026\n",
            "Epoch: 174, Len of Training loss: 12, Average loss: 7.320961316426595\n",
            "Len of Validation loss: 40, Average loss: 6.969854688644409\n",
            "Epoch: 175, Len of Training loss: 12, Average loss: 7.562248428662618\n",
            "Len of Validation loss: 40, Average loss: 7.092274379730225\n",
            "Epoch: 176, Len of Training loss: 12, Average loss: 7.175777316093445\n",
            "Len of Validation loss: 40, Average loss: 7.166904711723328\n",
            "Epoch: 177, Len of Training loss: 12, Average loss: 7.154103914896647\n",
            "Len of Validation loss: 40, Average loss: 6.9367320239543915\n",
            "Epoch: 178, Len of Training loss: 12, Average loss: 7.1785874764124555\n",
            "Len of Validation loss: 40, Average loss: 7.457219684123993\n",
            "Epoch: 179, Len of Training loss: 12, Average loss: 7.122608383496602\n",
            "Len of Validation loss: 40, Average loss: 7.111115795373917\n",
            "Epoch: 180, Len of Training loss: 12, Average loss: 7.340888937314351\n",
            "Len of Validation loss: 40, Average loss: 6.947290968894959\n",
            "Epoch: 181, Len of Training loss: 12, Average loss: 7.209908843040466\n",
            "Len of Validation loss: 40, Average loss: 7.0705453097820286\n",
            "Epoch: 182, Len of Training loss: 12, Average loss: 7.099101185798645\n",
            "Len of Validation loss: 40, Average loss: 7.0298549175262455\n",
            "Epoch: 183, Len of Training loss: 12, Average loss: 7.143421371777852\n",
            "Len of Validation loss: 40, Average loss: 7.208593416213989\n",
            "Epoch: 184, Len of Training loss: 12, Average loss: 7.152449170748393\n",
            "Len of Validation loss: 40, Average loss: 7.053386437892914\n",
            "Epoch: 185, Len of Training loss: 12, Average loss: 7.140548467636108\n",
            "Len of Validation loss: 40, Average loss: 6.887847822904587\n",
            "Epoch: 186, Len of Training loss: 12, Average loss: 7.1674290498097735\n",
            "Len of Validation loss: 40, Average loss: 6.857758331298828\n",
            "Epoch: 187, Len of Training loss: 12, Average loss: 7.0704483191172285\n",
            "Len of Validation loss: 40, Average loss: 7.074727886915207\n",
            "Epoch: 188, Len of Training loss: 12, Average loss: 7.261114517847697\n",
            "Len of Validation loss: 40, Average loss: 8.556555116176606\n",
            "Epoch: 189, Len of Training loss: 12, Average loss: 7.244233012199402\n",
            "Len of Validation loss: 40, Average loss: 7.378888118267059\n",
            "Epoch: 190, Len of Training loss: 12, Average loss: 7.159931461016337\n",
            "Len of Validation loss: 40, Average loss: 6.777564299106598\n",
            "Epoch: 191, Len of Training loss: 12, Average loss: 7.063578446706136\n",
            "Len of Validation loss: 40, Average loss: 6.845679074525833\n",
            "Epoch: 192, Len of Training loss: 12, Average loss: 7.0378427902857466\n",
            "Len of Validation loss: 40, Average loss: 8.081342697143555\n",
            "Epoch: 193, Len of Training loss: 12, Average loss: 7.2247691949208575\n",
            "Len of Validation loss: 40, Average loss: 7.3990169882774355\n",
            "Epoch: 194, Len of Training loss: 12, Average loss: 7.1412603457768755\n",
            "Len of Validation loss: 40, Average loss: 7.05497944355011\n",
            "Epoch: 195, Len of Training loss: 12, Average loss: 7.515211423238118\n",
            "Len of Validation loss: 40, Average loss: 7.114592587947845\n",
            "Epoch: 196, Len of Training loss: 12, Average loss: 7.404836376508077\n",
            "Len of Validation loss: 40, Average loss: 6.998733234405518\n",
            "Epoch: 197, Len of Training loss: 12, Average loss: 7.09816853205363\n",
            "Len of Validation loss: 40, Average loss: 6.7091195523738865\n",
            "Epoch: 198, Len of Training loss: 12, Average loss: 7.396284143129985\n",
            "Len of Validation loss: 40, Average loss: 7.049688923358917\n",
            "Epoch: 199, Len of Training loss: 12, Average loss: 7.475271821022034\n",
            "Len of Validation loss: 40, Average loss: 7.304202020168304\n",
            "Epoch: 200, Len of Training loss: 12, Average loss: 7.127444426218669\n",
            "Len of Validation loss: 40, Average loss: 7.396259862184524\n",
            "Epoch: 201, Len of Training loss: 12, Average loss: 7.087787588437398\n",
            "Len of Validation loss: 40, Average loss: 7.083479583263397\n",
            "Epoch: 202, Len of Training loss: 12, Average loss: 7.29206395149231\n",
            "Len of Validation loss: 40, Average loss: 7.06793692111969\n",
            "Epoch: 203, Len of Training loss: 12, Average loss: 7.4311937888463335\n",
            "Len of Validation loss: 40, Average loss: 7.803174901008606\n",
            "Epoch: 204, Len of Training loss: 12, Average loss: 7.73816192150116\n",
            "Len of Validation loss: 40, Average loss: 7.937874221801758\n",
            "Epoch: 205, Len of Training loss: 12, Average loss: 8.100451747576395\n",
            "Len of Validation loss: 40, Average loss: 6.814132308959961\n",
            "Epoch: 206, Len of Training loss: 12, Average loss: 7.295714020729065\n",
            "Len of Validation loss: 40, Average loss: 6.910278528928757\n",
            "Epoch: 207, Len of Training loss: 12, Average loss: 6.960213979085286\n",
            "Len of Validation loss: 40, Average loss: 6.918072843551636\n",
            "Epoch: 208, Len of Training loss: 12, Average loss: 7.560620546340942\n",
            "Len of Validation loss: 40, Average loss: 7.287526565790176\n",
            "Epoch: 209, Len of Training loss: 12, Average loss: 7.056223352750142\n",
            "Len of Validation loss: 40, Average loss: 7.014868587255478\n",
            "Epoch: 210, Len of Training loss: 12, Average loss: 6.9978476365407305\n",
            "Len of Validation loss: 40, Average loss: 6.805441415309906\n",
            "Epoch: 211, Len of Training loss: 12, Average loss: 6.933956106503804\n",
            "Len of Validation loss: 40, Average loss: 7.216771805286408\n",
            "Epoch: 212, Len of Training loss: 12, Average loss: 7.417300740877788\n",
            "Len of Validation loss: 40, Average loss: 8.083925712108613\n",
            "Epoch: 213, Len of Training loss: 12, Average loss: 7.066822608311971\n",
            "Len of Validation loss: 40, Average loss: 7.853337776660919\n",
            "Epoch: 214, Len of Training loss: 12, Average loss: 7.182355364163716\n",
            "Len of Validation loss: 40, Average loss: 7.826596176624298\n",
            "Epoch: 215, Len of Training loss: 12, Average loss: 7.156610369682312\n",
            "Len of Validation loss: 40, Average loss: 7.759319996833801\n",
            "Epoch: 216, Len of Training loss: 12, Average loss: 7.004821499188741\n",
            "Len of Validation loss: 40, Average loss: 7.741646027565002\n",
            "Epoch: 217, Len of Training loss: 12, Average loss: 7.064463655153911\n",
            "Len of Validation loss: 40, Average loss: 6.613968861103058\n",
            "Epoch: 218, Len of Training loss: 12, Average loss: 7.283777952194214\n",
            "Len of Validation loss: 40, Average loss: 7.111252999305725\n",
            "Epoch: 219, Len of Training loss: 12, Average loss: 7.266446073849996\n",
            "Len of Validation loss: 40, Average loss: 6.638431805372238\n",
            "Epoch: 220, Len of Training loss: 12, Average loss: 7.370688239733378\n",
            "Len of Validation loss: 40, Average loss: 6.635131299495697\n",
            "Epoch: 221, Len of Training loss: 12, Average loss: 7.047825654347737\n",
            "Len of Validation loss: 40, Average loss: 6.797578448057175\n",
            "Epoch: 222, Len of Training loss: 12, Average loss: 7.036257266998291\n",
            "Len of Validation loss: 40, Average loss: 6.7621163606643675\n",
            "Epoch: 223, Len of Training loss: 12, Average loss: 6.9044274886449175\n",
            "Len of Validation loss: 40, Average loss: 6.674405437707901\n",
            "Epoch: 224, Len of Training loss: 12, Average loss: 6.83952792485555\n",
            "Len of Validation loss: 40, Average loss: 6.67060438990593\n",
            "Epoch: 225, Len of Training loss: 12, Average loss: 7.079028884569804\n",
            "Len of Validation loss: 40, Average loss: 7.341722238063812\n",
            "Epoch: 226, Len of Training loss: 12, Average loss: 6.975183606147766\n",
            "Len of Validation loss: 40, Average loss: 7.127471363544464\n",
            "Epoch: 227, Len of Training loss: 12, Average loss: 7.061470190684001\n",
            "Len of Validation loss: 40, Average loss: 7.000661265850067\n",
            "Epoch: 228, Len of Training loss: 12, Average loss: 6.8954659303029375\n",
            "Len of Validation loss: 40, Average loss: 6.753608131408692\n",
            "Epoch: 229, Len of Training loss: 12, Average loss: 6.837915341059367\n",
            "Len of Validation loss: 40, Average loss: 6.599574112892151\n",
            "Epoch: 230, Len of Training loss: 12, Average loss: 7.193259398142497\n",
            "Len of Validation loss: 40, Average loss: 7.469735276699066\n",
            "Epoch: 231, Len of Training loss: 12, Average loss: 7.420766711235046\n",
            "Len of Validation loss: 40, Average loss: 6.603037643432617\n",
            "Epoch: 232, Len of Training loss: 12, Average loss: 7.065119028091431\n",
            "Len of Validation loss: 40, Average loss: 7.1150046467781065\n",
            "Epoch: 233, Len of Training loss: 12, Average loss: 6.738030393918355\n",
            "Len of Validation loss: 40, Average loss: 6.772996664047241\n",
            "Epoch: 234, Len of Training loss: 12, Average loss: 6.940567255020142\n",
            "Len of Validation loss: 40, Average loss: 6.781247401237488\n",
            "Epoch: 235, Len of Training loss: 12, Average loss: 6.8605897426605225\n",
            "Len of Validation loss: 40, Average loss: 6.690745842456818\n",
            "Epoch: 236, Len of Training loss: 12, Average loss: 6.87213659286499\n",
            "Len of Validation loss: 40, Average loss: 6.69106742143631\n",
            "Epoch: 237, Len of Training loss: 12, Average loss: 6.817187786102295\n",
            "Len of Validation loss: 40, Average loss: 6.780017459392548\n",
            "Epoch: 238, Len of Training loss: 12, Average loss: 6.8001236120859785\n",
            "Len of Validation loss: 40, Average loss: 6.834662461280823\n",
            "Epoch: 239, Len of Training loss: 12, Average loss: 6.827099283536275\n",
            "Len of Validation loss: 40, Average loss: 6.774185746908188\n",
            "Epoch: 240, Len of Training loss: 12, Average loss: 6.943525989850362\n",
            "Len of Validation loss: 40, Average loss: 6.795381426811218\n",
            "Epoch: 241, Len of Training loss: 12, Average loss: 6.920497417449951\n",
            "Len of Validation loss: 40, Average loss: 6.829263603687286\n",
            "Epoch: 242, Len of Training loss: 12, Average loss: 6.806772907574971\n",
            "Len of Validation loss: 40, Average loss: 7.0071215510368345\n",
            "Epoch: 243, Len of Training loss: 12, Average loss: 6.863865852355957\n",
            "Len of Validation loss: 40, Average loss: 6.510265183448792\n",
            "Epoch: 244, Len of Training loss: 12, Average loss: 7.110835631688436\n",
            "Len of Validation loss: 40, Average loss: 6.663375169038773\n",
            "Epoch: 245, Len of Training loss: 12, Average loss: 7.110481142997742\n",
            "Len of Validation loss: 40, Average loss: 6.643544757366181\n",
            "Epoch: 246, Len of Training loss: 12, Average loss: 6.903396526972453\n",
            "Len of Validation loss: 40, Average loss: 7.872881805896759\n",
            "Epoch: 247, Len of Training loss: 12, Average loss: 6.87203844388326\n",
            "Len of Validation loss: 40, Average loss: 6.6315062046051025\n",
            "Epoch: 248, Len of Training loss: 12, Average loss: 6.847173452377319\n",
            "Len of Validation loss: 40, Average loss: 6.635214376449585\n",
            "Epoch: 249, Len of Training loss: 12, Average loss: 7.203606287638347\n",
            "Len of Validation loss: 40, Average loss: 6.740051960945129\n",
            "Epoch: 250, Len of Training loss: 12, Average loss: 7.329570094744365\n",
            "Len of Validation loss: 40, Average loss: 6.806612515449524\n",
            "Epoch: 251, Len of Training loss: 12, Average loss: 7.190866033236186\n",
            "Len of Validation loss: 40, Average loss: 8.911305844783783\n",
            "Epoch: 252, Len of Training loss: 12, Average loss: 7.150084137916565\n",
            "Len of Validation loss: 40, Average loss: 8.086888098716736\n",
            "Epoch: 253, Len of Training loss: 12, Average loss: 7.059253851572673\n",
            "Len of Validation loss: 40, Average loss: 7.282011783123016\n",
            "Epoch: 254, Len of Training loss: 12, Average loss: 6.790539701779683\n",
            "Len of Validation loss: 40, Average loss: 6.9121331691741945\n",
            "Epoch: 255, Len of Training loss: 12, Average loss: 6.792750914891561\n",
            "Len of Validation loss: 40, Average loss: 6.569147217273712\n",
            "Epoch: 256, Len of Training loss: 12, Average loss: 6.904398520787557\n",
            "Len of Validation loss: 40, Average loss: 6.756062853336334\n",
            "Epoch: 257, Len of Training loss: 12, Average loss: 6.891346573829651\n",
            "Len of Validation loss: 40, Average loss: 6.590879428386688\n",
            "Epoch: 258, Len of Training loss: 12, Average loss: 6.885015408198039\n",
            "Len of Validation loss: 40, Average loss: 6.908369028568268\n",
            "Epoch: 259, Len of Training loss: 12, Average loss: 6.675937215487163\n",
            "Len of Validation loss: 40, Average loss: 6.705085361003876\n",
            "Epoch: 260, Len of Training loss: 12, Average loss: 7.0698228279749555\n",
            "Len of Validation loss: 40, Average loss: 6.666296035051346\n",
            "Epoch: 261, Len of Training loss: 12, Average loss: 6.726810852686564\n",
            "Len of Validation loss: 40, Average loss: 6.778519976139068\n",
            "Epoch: 262, Len of Training loss: 12, Average loss: 6.777977665265401\n",
            "Len of Validation loss: 40, Average loss: 6.528423714637756\n",
            "Epoch: 263, Len of Training loss: 12, Average loss: 6.712677915891011\n",
            "Len of Validation loss: 40, Average loss: 6.927025008201599\n",
            "Epoch: 264, Len of Training loss: 12, Average loss: 6.919835329055786\n",
            "Len of Validation loss: 40, Average loss: 6.652277421951294\n",
            "Epoch: 265, Len of Training loss: 12, Average loss: 7.582936406135559\n",
            "Len of Validation loss: 40, Average loss: 7.2322587966918945\n",
            "Epoch: 266, Len of Training loss: 12, Average loss: 7.020477692286174\n",
            "Len of Validation loss: 40, Average loss: 7.25298513174057\n",
            "Epoch: 267, Len of Training loss: 12, Average loss: 6.918630321820577\n",
            "Len of Validation loss: 40, Average loss: 7.326507580280304\n",
            "Epoch: 268, Len of Training loss: 12, Average loss: 7.02706249554952\n",
            "Len of Validation loss: 40, Average loss: 7.021663504838943\n",
            "Epoch: 269, Len of Training loss: 12, Average loss: 6.85262409845988\n",
            "Len of Validation loss: 40, Average loss: 6.621804213523864\n",
            "Epoch: 270, Len of Training loss: 12, Average loss: 6.547833442687988\n",
            "Len of Validation loss: 40, Average loss: 6.561876964569092\n",
            "Epoch: 271, Len of Training loss: 12, Average loss: 6.5654447476069135\n",
            "Len of Validation loss: 40, Average loss: 6.479763340950012\n",
            "Epoch: 272, Len of Training loss: 12, Average loss: 6.577531178792317\n",
            "Len of Validation loss: 40, Average loss: 6.684638398885727\n",
            "Epoch: 273, Len of Training loss: 12, Average loss: 6.630523165067037\n",
            "Len of Validation loss: 40, Average loss: 6.651592767238617\n",
            "Epoch: 274, Len of Training loss: 12, Average loss: 6.7104824384053545\n",
            "Len of Validation loss: 40, Average loss: 6.549863529205322\n",
            "Epoch: 275, Len of Training loss: 12, Average loss: 7.097057541211446\n",
            "Len of Validation loss: 40, Average loss: 6.633711701631546\n",
            "Epoch: 276, Len of Training loss: 12, Average loss: 6.9667039314905805\n",
            "Len of Validation loss: 40, Average loss: 6.874343240261078\n",
            "Epoch: 277, Len of Training loss: 12, Average loss: 6.981677969296773\n",
            "Len of Validation loss: 40, Average loss: 6.7980372428894045\n",
            "Epoch: 278, Len of Training loss: 12, Average loss: 6.743702570597331\n",
            "Len of Validation loss: 40, Average loss: 7.064786207675934\n",
            "Epoch: 279, Len of Training loss: 12, Average loss: 6.787064750989278\n",
            "Len of Validation loss: 40, Average loss: 6.821411287784576\n",
            "Epoch: 280, Len of Training loss: 12, Average loss: 6.786746184031169\n",
            "Len of Validation loss: 40, Average loss: 6.6073599576950075\n",
            "Epoch: 281, Len of Training loss: 12, Average loss: 6.819038510322571\n",
            "Len of Validation loss: 40, Average loss: 6.470787453651428\n",
            "Epoch: 282, Len of Training loss: 12, Average loss: 6.828357696533203\n",
            "Len of Validation loss: 40, Average loss: 6.753644400835038\n",
            "Epoch: 283, Len of Training loss: 12, Average loss: 6.7873881260554\n",
            "Len of Validation loss: 40, Average loss: 6.867580842971802\n",
            "Epoch: 284, Len of Training loss: 12, Average loss: 7.0693918863932295\n",
            "Len of Validation loss: 40, Average loss: 6.877351999282837\n",
            "Epoch: 285, Len of Training loss: 12, Average loss: 6.733173926671346\n",
            "Len of Validation loss: 40, Average loss: 6.585490506887436\n",
            "Epoch: 286, Len of Training loss: 12, Average loss: 6.682928999265035\n",
            "Len of Validation loss: 40, Average loss: 6.480431306362152\n",
            "Epoch: 287, Len of Training loss: 12, Average loss: 6.539556543032329\n",
            "Len of Validation loss: 40, Average loss: 6.554492074251175\n",
            "Epoch: 288, Len of Training loss: 12, Average loss: 6.630160013834636\n",
            "Len of Validation loss: 40, Average loss: 6.592748290300369\n",
            "Epoch: 289, Len of Training loss: 12, Average loss: 6.615038832028707\n",
            "Len of Validation loss: 40, Average loss: 6.7644310057163235\n",
            "Epoch: 290, Len of Training loss: 12, Average loss: 6.563327471415202\n",
            "Len of Validation loss: 40, Average loss: 6.659911167621613\n",
            "Epoch: 291, Len of Training loss: 12, Average loss: 6.698893189430237\n",
            "Len of Validation loss: 40, Average loss: 6.452344155311584\n",
            "Epoch: 292, Len of Training loss: 12, Average loss: 6.797242323557536\n",
            "Len of Validation loss: 40, Average loss: 7.3146077513694765\n",
            "Epoch: 293, Len of Training loss: 12, Average loss: 6.81784991423289\n",
            "Len of Validation loss: 40, Average loss: 6.613991737365723\n",
            "Epoch: 294, Len of Training loss: 12, Average loss: 6.878671050071716\n",
            "Len of Validation loss: 40, Average loss: 7.210283577442169\n",
            "Epoch: 295, Len of Training loss: 12, Average loss: 6.7446006536483765\n",
            "Len of Validation loss: 40, Average loss: 6.780736684799194\n",
            "Epoch: 296, Len of Training loss: 12, Average loss: 6.477514028549194\n",
            "Len of Validation loss: 40, Average loss: 6.77487633228302\n",
            "Epoch: 297, Len of Training loss: 12, Average loss: 6.822605172793071\n",
            "Len of Validation loss: 40, Average loss: 6.663332313299179\n",
            "Epoch: 298, Len of Training loss: 12, Average loss: 6.784896930058797\n",
            "Len of Validation loss: 40, Average loss: 7.604739201068878\n",
            "Epoch: 299, Len of Training loss: 12, Average loss: 6.869173010190328\n",
            "Len of Validation loss: 40, Average loss: 7.71876448392868\n",
            "Epoch: 300, Len of Training loss: 12, Average loss: 6.968755046526591\n",
            "Len of Validation loss: 40, Average loss: 6.871330916881561\n",
            "Epoch: 301, Len of Training loss: 12, Average loss: 7.0720661878585815\n",
            "Len of Validation loss: 40, Average loss: 7.296870911121369\n",
            "Epoch: 302, Len of Training loss: 12, Average loss: 6.862783153851827\n",
            "Len of Validation loss: 40, Average loss: 6.582968080043793\n",
            "Epoch: 303, Len of Training loss: 12, Average loss: 6.745334545771281\n",
            "Len of Validation loss: 40, Average loss: 6.653689360618591\n",
            "Epoch: 304, Len of Training loss: 12, Average loss: 6.641574144363403\n",
            "Len of Validation loss: 40, Average loss: 7.081060016155243\n",
            "Epoch: 305, Len of Training loss: 12, Average loss: 6.5944801568984985\n",
            "Len of Validation loss: 40, Average loss: 6.524138414859772\n",
            "Epoch: 306, Len of Training loss: 12, Average loss: 6.53365155061086\n",
            "Len of Validation loss: 40, Average loss: 6.346162331104279\n",
            "Epoch: 307, Len of Training loss: 12, Average loss: 6.63176413377126\n",
            "Len of Validation loss: 40, Average loss: 7.066059947013855\n",
            "Epoch: 308, Len of Training loss: 12, Average loss: 6.723493576049805\n",
            "Len of Validation loss: 40, Average loss: 6.783271712064743\n",
            "Epoch: 309, Len of Training loss: 12, Average loss: 6.703645388285319\n",
            "Len of Validation loss: 40, Average loss: 7.199633705615997\n",
            "Epoch: 310, Len of Training loss: 12, Average loss: 6.9755449295043945\n",
            "Len of Validation loss: 40, Average loss: 6.679989922046661\n",
            "Epoch: 311, Len of Training loss: 12, Average loss: 6.882875720659892\n",
            "Len of Validation loss: 40, Average loss: 6.544148600101471\n",
            "Epoch: 312, Len of Training loss: 12, Average loss: 6.642486095428467\n",
            "Len of Validation loss: 40, Average loss: 6.513745027780533\n",
            "Epoch: 313, Len of Training loss: 12, Average loss: 6.605909864107768\n",
            "Len of Validation loss: 40, Average loss: 6.51593080163002\n",
            "Epoch: 314, Len of Training loss: 12, Average loss: 6.631411671638489\n",
            "Len of Validation loss: 40, Average loss: 6.848920941352844\n",
            "Epoch: 315, Len of Training loss: 12, Average loss: 6.483244975407918\n",
            "Len of Validation loss: 40, Average loss: 6.823740589618683\n",
            "Epoch: 316, Len of Training loss: 12, Average loss: 6.5788774490356445\n",
            "Len of Validation loss: 40, Average loss: 7.072062563896179\n",
            "Epoch: 317, Len of Training loss: 12, Average loss: 6.568166812260945\n",
            "Len of Validation loss: 40, Average loss: 7.043112337589264\n",
            "Epoch: 318, Len of Training loss: 12, Average loss: 6.527289827664693\n",
            "Len of Validation loss: 40, Average loss: 6.81077219247818\n",
            "Epoch: 319, Len of Training loss: 12, Average loss: 6.499724388122559\n",
            "Len of Validation loss: 40, Average loss: 6.5873460412025455\n",
            "Epoch: 320, Len of Training loss: 12, Average loss: 6.673781752586365\n",
            "Len of Validation loss: 40, Average loss: 6.738808381557464\n",
            "Epoch: 321, Len of Training loss: 12, Average loss: 6.629145781199138\n",
            "Len of Validation loss: 40, Average loss: 7.279076480865479\n",
            "Epoch: 322, Len of Training loss: 12, Average loss: 6.861807703971863\n",
            "Len of Validation loss: 40, Average loss: 6.641208964586258\n",
            "Epoch: 323, Len of Training loss: 12, Average loss: 6.579288641611735\n",
            "Len of Validation loss: 40, Average loss: 6.99520251750946\n",
            "Epoch: 324, Len of Training loss: 12, Average loss: 6.63271427154541\n",
            "Len of Validation loss: 40, Average loss: 7.914085435867309\n",
            "Epoch: 325, Len of Training loss: 12, Average loss: 6.591460903485616\n",
            "Len of Validation loss: 40, Average loss: 6.655551987886429\n",
            "Epoch: 326, Len of Training loss: 12, Average loss: 6.570734818776448\n",
            "Len of Validation loss: 40, Average loss: 6.548767918348313\n",
            "Epoch: 327, Len of Training loss: 12, Average loss: 6.506366968154907\n",
            "Len of Validation loss: 40, Average loss: 6.548041540384292\n",
            "Epoch: 328, Len of Training loss: 12, Average loss: 6.826627016067505\n",
            "Len of Validation loss: 40, Average loss: 7.412904441356659\n",
            "Epoch: 329, Len of Training loss: 12, Average loss: 6.6347031990687055\n",
            "Len of Validation loss: 40, Average loss: 6.763341861963272\n",
            "Epoch: 330, Len of Training loss: 12, Average loss: 6.550664305686951\n",
            "Len of Validation loss: 40, Average loss: 6.888334655761719\n",
            "Epoch: 331, Len of Training loss: 12, Average loss: 6.629674434661865\n",
            "Len of Validation loss: 40, Average loss: 6.639851206541062\n",
            "Epoch: 332, Len of Training loss: 12, Average loss: 6.654179732004802\n",
            "Len of Validation loss: 40, Average loss: 7.030622732639313\n",
            "Epoch: 333, Len of Training loss: 12, Average loss: 7.114471832911174\n",
            "Len of Validation loss: 40, Average loss: 6.9384414732456205\n",
            "Epoch: 334, Len of Training loss: 12, Average loss: 6.87323792775472\n",
            "Len of Validation loss: 40, Average loss: 6.914775276184082\n",
            "Epoch: 335, Len of Training loss: 12, Average loss: 6.510417699813843\n",
            "Len of Validation loss: 40, Average loss: 6.803071707487106\n",
            "Epoch: 336, Len of Training loss: 12, Average loss: 6.625075856844584\n",
            "Len of Validation loss: 40, Average loss: 6.3224482357501985\n",
            "Epoch: 337, Len of Training loss: 12, Average loss: 6.728533903757731\n",
            "Len of Validation loss: 40, Average loss: 7.11867847442627\n",
            "Epoch: 338, Len of Training loss: 12, Average loss: 6.698801040649414\n",
            "Len of Validation loss: 40, Average loss: 6.577156031131745\n",
            "Epoch: 339, Len of Training loss: 12, Average loss: 6.916717092196147\n",
            "Len of Validation loss: 40, Average loss: 6.7157238006591795\n",
            "Epoch: 340, Len of Training loss: 12, Average loss: 6.798915187517802\n",
            "Len of Validation loss: 40, Average loss: 6.789023208618164\n",
            "Epoch: 341, Len of Training loss: 12, Average loss: 6.5962700843811035\n",
            "Len of Validation loss: 40, Average loss: 6.647235184907913\n",
            "Epoch: 342, Len of Training loss: 12, Average loss: 6.344553232192993\n",
            "Len of Validation loss: 40, Average loss: 6.606589126586914\n",
            "Epoch: 343, Len of Training loss: 12, Average loss: 6.735065301259358\n",
            "Len of Validation loss: 40, Average loss: 6.70250836610794\n",
            "Epoch: 344, Len of Training loss: 12, Average loss: 6.643059213956197\n",
            "Len of Validation loss: 40, Average loss: 6.525095450878143\n",
            "Epoch: 345, Len of Training loss: 12, Average loss: 6.642613887786865\n",
            "Len of Validation loss: 40, Average loss: 7.578072345256805\n",
            "Epoch: 346, Len of Training loss: 12, Average loss: 6.754867235819499\n",
            "Len of Validation loss: 40, Average loss: 7.827171349525452\n",
            "Epoch: 347, Len of Training loss: 12, Average loss: 6.917460401852925\n",
            "Len of Validation loss: 40, Average loss: 7.032489013671875\n",
            "Epoch: 348, Len of Training loss: 12, Average loss: 6.607827663421631\n",
            "Len of Validation loss: 40, Average loss: 6.800778090953827\n",
            "Epoch: 349, Len of Training loss: 12, Average loss: 6.532416582107544\n",
            "Len of Validation loss: 40, Average loss: 6.623091065883637\n",
            "Epoch: 350, Len of Training loss: 12, Average loss: 6.627764066060384\n",
            "Len of Validation loss: 40, Average loss: 6.488003396987915\n",
            "Epoch: 351, Len of Training loss: 12, Average loss: 6.715859254201253\n",
            "Len of Validation loss: 40, Average loss: 7.532183682918548\n",
            "Epoch: 352, Len of Training loss: 12, Average loss: 6.9369224309921265\n",
            "Len of Validation loss: 40, Average loss: 6.746744149923325\n",
            "Epoch: 353, Len of Training loss: 12, Average loss: 6.733000834782918\n",
            "Len of Validation loss: 40, Average loss: 7.078604066371918\n",
            "Epoch: 354, Len of Training loss: 12, Average loss: 6.660804033279419\n",
            "Len of Validation loss: 40, Average loss: 7.710758519172669\n",
            "Epoch: 355, Len of Training loss: 12, Average loss: 6.735632300376892\n",
            "Len of Validation loss: 40, Average loss: 6.867975950241089\n",
            "Epoch: 356, Len of Training loss: 12, Average loss: 6.482392589251201\n",
            "Len of Validation loss: 40, Average loss: 6.531616264581681\n",
            "Epoch: 357, Len of Training loss: 12, Average loss: 6.809613545735677\n",
            "Len of Validation loss: 40, Average loss: 6.53449182510376\n",
            "Epoch: 358, Len of Training loss: 12, Average loss: 6.469689687093099\n",
            "Len of Validation loss: 40, Average loss: 6.711011064052582\n",
            "Epoch: 359, Len of Training loss: 12, Average loss: 6.4377661148707075\n",
            "Len of Validation loss: 40, Average loss: 6.438926249742508\n",
            "Epoch: 360, Len of Training loss: 12, Average loss: 6.5145606597264605\n",
            "Len of Validation loss: 40, Average loss: 6.524316567182541\n",
            "Epoch: 361, Len of Training loss: 12, Average loss: 6.583263476689656\n",
            "Len of Validation loss: 40, Average loss: 6.938653069734573\n",
            "Epoch: 362, Len of Training loss: 12, Average loss: 6.639980594317119\n",
            "Len of Validation loss: 40, Average loss: 7.071447432041168\n",
            "Epoch: 363, Len of Training loss: 12, Average loss: 6.428346435228984\n",
            "Len of Validation loss: 40, Average loss: 6.658867758512497\n",
            "Epoch: 364, Len of Training loss: 12, Average loss: 6.468150496482849\n",
            "Len of Validation loss: 40, Average loss: 6.984246063232422\n",
            "Epoch: 365, Len of Training loss: 12, Average loss: 6.461174130439758\n",
            "Len of Validation loss: 40, Average loss: 6.87344731092453\n",
            "Epoch: 366, Len of Training loss: 12, Average loss: 6.52298370997111\n",
            "Len of Validation loss: 40, Average loss: 6.771445333957672\n",
            "Epoch: 367, Len of Training loss: 12, Average loss: 6.299787203470866\n",
            "Len of Validation loss: 40, Average loss: 6.618194806575775\n",
            "Epoch: 368, Len of Training loss: 12, Average loss: 6.402048428853353\n",
            "Len of Validation loss: 40, Average loss: 6.635160595178604\n",
            "Epoch: 369, Len of Training loss: 12, Average loss: 6.486952463785808\n",
            "Len of Validation loss: 40, Average loss: 6.613513350486755\n",
            "Epoch: 370, Len of Training loss: 12, Average loss: 6.507791996002197\n",
            "Len of Validation loss: 40, Average loss: 6.8045373320579525\n",
            "Epoch: 371, Len of Training loss: 12, Average loss: 6.797263224919637\n",
            "Len of Validation loss: 40, Average loss: 6.3627688646316525\n",
            "Epoch: 372, Len of Training loss: 12, Average loss: 6.6223892370859785\n",
            "Len of Validation loss: 40, Average loss: 6.447931253910065\n",
            "Epoch: 373, Len of Training loss: 12, Average loss: 6.426853577295939\n",
            "Len of Validation loss: 40, Average loss: 6.621472853422165\n",
            "Epoch: 374, Len of Training loss: 12, Average loss: 6.669527292251587\n",
            "Len of Validation loss: 40, Average loss: 7.638019597530365\n",
            "Epoch: 375, Len of Training loss: 12, Average loss: 6.5150158405303955\n",
            "Len of Validation loss: 40, Average loss: 6.332977217435837\n",
            "Epoch: 376, Len of Training loss: 12, Average loss: 6.311970949172974\n",
            "Len of Validation loss: 40, Average loss: 6.4193405389785765\n",
            "Epoch: 377, Len of Training loss: 12, Average loss: 6.416290561358134\n",
            "Len of Validation loss: 40, Average loss: 6.66687388420105\n",
            "Epoch: 378, Len of Training loss: 12, Average loss: 6.474135994911194\n",
            "Len of Validation loss: 40, Average loss: 6.35162895321846\n",
            "Epoch: 379, Len of Training loss: 12, Average loss: 6.5065412521362305\n",
            "Len of Validation loss: 40, Average loss: 6.465821695327759\n",
            "Epoch: 380, Len of Training loss: 12, Average loss: 6.473705252011617\n",
            "Len of Validation loss: 40, Average loss: 7.28384131193161\n",
            "Epoch: 381, Len of Training loss: 12, Average loss: 6.471818645795186\n",
            "Len of Validation loss: 40, Average loss: 6.504521077871322\n",
            "Epoch: 382, Len of Training loss: 12, Average loss: 6.446138938268025\n",
            "Len of Validation loss: 40, Average loss: 7.578908479213714\n",
            "Epoch: 383, Len of Training loss: 12, Average loss: 6.642420649528503\n",
            "Len of Validation loss: 40, Average loss: 6.653202170133591\n",
            "Epoch: 384, Len of Training loss: 12, Average loss: 6.582608898480733\n",
            "Len of Validation loss: 40, Average loss: 6.712791532278061\n",
            "Epoch: 385, Len of Training loss: 12, Average loss: 6.497393886248271\n",
            "Len of Validation loss: 40, Average loss: 6.793534874916077\n",
            "Epoch: 386, Len of Training loss: 12, Average loss: 6.438026984532674\n",
            "Len of Validation loss: 40, Average loss: 6.641734528541565\n",
            "Epoch: 387, Len of Training loss: 12, Average loss: 6.518237908681233\n",
            "Len of Validation loss: 40, Average loss: 6.58478411436081\n",
            "Epoch: 388, Len of Training loss: 12, Average loss: 6.668499231338501\n",
            "Len of Validation loss: 40, Average loss: 6.6368317067623135\n",
            "Epoch: 389, Len of Training loss: 12, Average loss: 6.439465602238973\n",
            "Len of Validation loss: 40, Average loss: 7.461092776060104\n",
            "Epoch: 390, Len of Training loss: 12, Average loss: 6.772871176401774\n",
            "Len of Validation loss: 40, Average loss: 7.528355515003204\n",
            "Epoch: 391, Len of Training loss: 12, Average loss: 6.827772816022237\n",
            "Len of Validation loss: 40, Average loss: 6.83376442193985\n",
            "Epoch: 392, Len of Training loss: 12, Average loss: 7.001849850018819\n",
            "Len of Validation loss: 40, Average loss: 6.676916545629501\n",
            "Epoch: 393, Len of Training loss: 12, Average loss: 6.514767130215962\n",
            "Len of Validation loss: 40, Average loss: 7.313849174976349\n",
            "Epoch: 394, Len of Training loss: 12, Average loss: 6.489726384480794\n",
            "Len of Validation loss: 40, Average loss: 7.973837333917618\n",
            "Epoch: 395, Len of Training loss: 12, Average loss: 6.490738868713379\n",
            "Len of Validation loss: 40, Average loss: 7.463288271427155\n",
            "Epoch: 396, Len of Training loss: 12, Average loss: 6.40332845846812\n",
            "Len of Validation loss: 40, Average loss: 6.80774462223053\n",
            "Epoch: 397, Len of Training loss: 12, Average loss: 6.363112767537435\n",
            "Len of Validation loss: 40, Average loss: 6.846489441394806\n",
            "Epoch: 398, Len of Training loss: 12, Average loss: 6.346127390861511\n",
            "Len of Validation loss: 40, Average loss: 6.638157588243485\n",
            "Epoch: 399, Len of Training loss: 12, Average loss: 6.362723350524902\n",
            "Len of Validation loss: 40, Average loss: 6.657451075315476\n",
            "Epoch: 400, Len of Training loss: 12, Average loss: 6.276887158552806\n",
            "Len of Validation loss: 40, Average loss: 7.01432911157608\n",
            "Epoch: 401, Len of Training loss: 12, Average loss: 6.452112952868144\n",
            "Len of Validation loss: 40, Average loss: 6.348303830623626\n",
            "Epoch: 402, Len of Training loss: 12, Average loss: 6.522833188374837\n",
            "Len of Validation loss: 40, Average loss: 6.242698764801025\n",
            "Epoch: 403, Len of Training loss: 12, Average loss: 6.635570526123047\n",
            "Len of Validation loss: 40, Average loss: 6.4339073359966275\n",
            "Epoch: 404, Len of Training loss: 12, Average loss: 6.20514718691508\n",
            "Len of Validation loss: 40, Average loss: 6.793261206150055\n",
            "Epoch: 405, Len of Training loss: 12, Average loss: 6.490974187850952\n",
            "Len of Validation loss: 40, Average loss: 7.2351414382457735\n",
            "Epoch: 406, Len of Training loss: 12, Average loss: 6.292213996251424\n",
            "Len of Validation loss: 40, Average loss: 6.482884120941162\n",
            "Epoch: 407, Len of Training loss: 12, Average loss: 6.535458604494731\n",
            "Len of Validation loss: 40, Average loss: 6.912745082378388\n",
            "Epoch: 408, Len of Training loss: 12, Average loss: 6.408570249875386\n",
            "Len of Validation loss: 40, Average loss: 6.491746926307679\n",
            "Epoch: 409, Len of Training loss: 12, Average loss: 6.471403400103251\n",
            "Len of Validation loss: 40, Average loss: 6.638889086246491\n",
            "Epoch: 410, Len of Training loss: 12, Average loss: 6.426274458567302\n",
            "Len of Validation loss: 40, Average loss: 6.673308885097503\n",
            "Epoch: 411, Len of Training loss: 12, Average loss: 6.37054721514384\n",
            "Len of Validation loss: 40, Average loss: 6.848916482925415\n",
            "Epoch: 412, Len of Training loss: 12, Average loss: 6.334884365399678\n",
            "Len of Validation loss: 40, Average loss: 7.008775776624679\n",
            "Epoch: 413, Len of Training loss: 12, Average loss: 6.330893278121948\n",
            "Len of Validation loss: 40, Average loss: 6.678119099140167\n",
            "Epoch: 414, Len of Training loss: 12, Average loss: 6.386259913444519\n",
            "Len of Validation loss: 40, Average loss: 6.569090330600739\n",
            "Epoch: 415, Len of Training loss: 12, Average loss: 6.387547492980957\n",
            "Len of Validation loss: 40, Average loss: 6.595378971099853\n",
            "Epoch: 416, Len of Training loss: 12, Average loss: 6.3675410350163775\n",
            "Len of Validation loss: 40, Average loss: 6.60949895977974\n",
            "Epoch: 417, Len of Training loss: 12, Average loss: 6.462209582328796\n",
            "Len of Validation loss: 40, Average loss: 6.634151822328567\n",
            "Epoch: 418, Len of Training loss: 12, Average loss: 6.408862630526225\n",
            "Len of Validation loss: 40, Average loss: 6.606647616624832\n",
            "Epoch: 419, Len of Training loss: 12, Average loss: 6.233220974604289\n",
            "Len of Validation loss: 40, Average loss: 6.594250893592834\n",
            "Epoch: 420, Len of Training loss: 12, Average loss: 6.304508646329244\n",
            "Len of Validation loss: 40, Average loss: 7.362461423873901\n",
            "Epoch: 421, Len of Training loss: 12, Average loss: 6.522458513577779\n",
            "Len of Validation loss: 40, Average loss: 6.474026387929916\n",
            "Epoch: 422, Len of Training loss: 12, Average loss: 6.2440987428029375\n",
            "Len of Validation loss: 40, Average loss: 6.6281893134117125\n",
            "Epoch: 423, Len of Training loss: 12, Average loss: 6.450704534848531\n",
            "Len of Validation loss: 40, Average loss: 6.467856097221374\n",
            "Epoch: 424, Len of Training loss: 12, Average loss: 6.62943708896637\n",
            "Len of Validation loss: 40, Average loss: 6.7663989007472995\n",
            "Epoch: 425, Len of Training loss: 12, Average loss: 6.337351202964783\n",
            "Len of Validation loss: 40, Average loss: 7.139738512039185\n",
            "Epoch: 426, Len of Training loss: 12, Average loss: 6.9925856590271\n",
            "Len of Validation loss: 40, Average loss: 6.5482497096061705\n",
            "Epoch: 427, Len of Training loss: 12, Average loss: 6.227822025616963\n",
            "Len of Validation loss: 40, Average loss: 6.915732365846634\n",
            "Epoch: 428, Len of Training loss: 12, Average loss: 6.229663729667664\n",
            "Len of Validation loss: 40, Average loss: 7.551751029491425\n",
            "Epoch: 429, Len of Training loss: 12, Average loss: 6.67661988735199\n",
            "Len of Validation loss: 40, Average loss: 6.405662935972214\n",
            "Epoch: 430, Len of Training loss: 12, Average loss: 6.588248332341512\n",
            "Len of Validation loss: 40, Average loss: 6.607124984264374\n",
            "Epoch: 431, Len of Training loss: 12, Average loss: 6.476792534192403\n",
            "Len of Validation loss: 40, Average loss: 6.554811573028564\n",
            "Epoch: 432, Len of Training loss: 12, Average loss: 6.2939451932907104\n",
            "Len of Validation loss: 40, Average loss: 6.696498626470566\n",
            "Epoch: 433, Len of Training loss: 12, Average loss: 6.160426576932271\n",
            "Len of Validation loss: 40, Average loss: 6.898734444379807\n",
            "Epoch: 434, Len of Training loss: 12, Average loss: 6.319753487904866\n",
            "Len of Validation loss: 40, Average loss: 6.625543290376664\n",
            "Epoch: 435, Len of Training loss: 12, Average loss: 6.40562117099762\n",
            "Len of Validation loss: 40, Average loss: 6.4230984330177305\n",
            "Epoch: 436, Len of Training loss: 12, Average loss: 6.300846099853516\n",
            "Len of Validation loss: 40, Average loss: 6.46640619635582\n",
            "Epoch: 437, Len of Training loss: 12, Average loss: 6.355026483535767\n",
            "Len of Validation loss: 40, Average loss: 6.360912835597992\n",
            "Epoch: 438, Len of Training loss: 12, Average loss: 6.3843575318654375\n",
            "Len of Validation loss: 40, Average loss: 6.50457319021225\n",
            "Epoch: 439, Len of Training loss: 12, Average loss: 6.3427226940790815\n",
            "Len of Validation loss: 40, Average loss: 6.357602518796921\n",
            "Epoch: 440, Len of Training loss: 12, Average loss: 6.213774919509888\n",
            "Len of Validation loss: 40, Average loss: 6.484304505586624\n",
            "Epoch: 441, Len of Training loss: 12, Average loss: 6.137566963831584\n",
            "Len of Validation loss: 40, Average loss: 7.455950701236725\n",
            "Epoch: 442, Len of Training loss: 12, Average loss: 6.469488620758057\n",
            "Len of Validation loss: 40, Average loss: 6.771504330635071\n",
            "Epoch: 443, Len of Training loss: 12, Average loss: 6.25222905476888\n",
            "Len of Validation loss: 40, Average loss: 8.178959774971009\n",
            "Epoch: 444, Len of Training loss: 12, Average loss: 6.420028765996297\n",
            "Len of Validation loss: 40, Average loss: 6.397533881664276\n",
            "Epoch: 445, Len of Training loss: 12, Average loss: 6.366140961647034\n",
            "Len of Validation loss: 40, Average loss: 7.093687474727631\n",
            "Epoch: 446, Len of Training loss: 12, Average loss: 6.541398048400879\n",
            "Len of Validation loss: 40, Average loss: 6.244228488206863\n",
            "Epoch: 447, Len of Training loss: 12, Average loss: 6.3075874249140425\n",
            "Len of Validation loss: 40, Average loss: 6.478007119894028\n",
            "Epoch: 448, Len of Training loss: 12, Average loss: 6.215364734331767\n",
            "Len of Validation loss: 40, Average loss: 6.496451187133789\n",
            "Epoch: 449, Len of Training loss: 12, Average loss: 6.311270554860433\n",
            "Len of Validation loss: 40, Average loss: 6.360994356870651\n",
            "Epoch: 450, Len of Training loss: 12, Average loss: 6.215621153513591\n",
            "Len of Validation loss: 40, Average loss: 6.35903697013855\n",
            "Epoch: 451, Len of Training loss: 12, Average loss: 6.405152002970378\n",
            "Len of Validation loss: 40, Average loss: 7.863316929340362\n",
            "Epoch: 452, Len of Training loss: 12, Average loss: 6.383632938067119\n",
            "Len of Validation loss: 40, Average loss: 7.11137204170227\n",
            "Epoch: 453, Len of Training loss: 12, Average loss: 6.294196407000224\n",
            "Len of Validation loss: 40, Average loss: 6.4808192431926726\n",
            "Epoch: 454, Len of Training loss: 12, Average loss: 6.287936607996623\n",
            "Len of Validation loss: 40, Average loss: 6.352962446212769\n",
            "Epoch: 455, Len of Training loss: 12, Average loss: 6.379024863243103\n",
            "Len of Validation loss: 40, Average loss: 6.690732049942016\n",
            "Epoch: 456, Len of Training loss: 12, Average loss: 6.386233448982239\n",
            "Len of Validation loss: 40, Average loss: 7.357347869873047\n",
            "Epoch: 457, Len of Training loss: 12, Average loss: 6.467014869054158\n",
            "Len of Validation loss: 40, Average loss: 7.221411156654358\n",
            "Epoch: 458, Len of Training loss: 12, Average loss: 6.382304310798645\n",
            "Len of Validation loss: 40, Average loss: 6.399982929229736\n",
            "Epoch: 459, Len of Training loss: 12, Average loss: 6.659770369529724\n",
            "Len of Validation loss: 40, Average loss: 7.91464456319809\n",
            "Epoch: 460, Len of Training loss: 12, Average loss: 7.2208778858184814\n",
            "Len of Validation loss: 40, Average loss: 6.431296801567077\n",
            "Epoch: 461, Len of Training loss: 12, Average loss: 6.640748302141826\n",
            "Len of Validation loss: 40, Average loss: 6.699229079484939\n",
            "Epoch: 462, Len of Training loss: 12, Average loss: 6.849020957946777\n",
            "Len of Validation loss: 40, Average loss: 6.350654900074005\n",
            "Epoch: 463, Len of Training loss: 12, Average loss: 6.414819876352946\n",
            "Len of Validation loss: 40, Average loss: 6.3205619096755985\n",
            "Epoch: 464, Len of Training loss: 12, Average loss: 6.489553332328796\n",
            "Len of Validation loss: 40, Average loss: 6.476533836126327\n",
            "Epoch: 465, Len of Training loss: 12, Average loss: 6.548019846280416\n",
            "Len of Validation loss: 40, Average loss: 6.564431172609329\n",
            "Epoch: 466, Len of Training loss: 12, Average loss: 6.649016737937927\n",
            "Len of Validation loss: 40, Average loss: 6.32795820236206\n",
            "Epoch: 467, Len of Training loss: 12, Average loss: 6.426964998245239\n",
            "Len of Validation loss: 40, Average loss: 6.36504510641098\n",
            "Epoch: 468, Len of Training loss: 12, Average loss: 6.481402476628621\n",
            "Len of Validation loss: 40, Average loss: 6.504116135835647\n",
            "Epoch: 469, Len of Training loss: 12, Average loss: 6.280800501505534\n",
            "Len of Validation loss: 40, Average loss: 6.41342014670372\n",
            "Epoch: 470, Len of Training loss: 12, Average loss: 6.518399278322856\n",
            "Len of Validation loss: 40, Average loss: 6.456575453281403\n",
            "Epoch: 471, Len of Training loss: 12, Average loss: 6.5738480885823565\n",
            "Len of Validation loss: 40, Average loss: 6.476290988922119\n",
            "Epoch: 472, Len of Training loss: 12, Average loss: 6.5058689912160235\n",
            "Len of Validation loss: 40, Average loss: 6.267026937007904\n",
            "Epoch: 473, Len of Training loss: 12, Average loss: 6.32071586449941\n",
            "Len of Validation loss: 40, Average loss: 6.747335767745971\n",
            "Epoch: 474, Len of Training loss: 12, Average loss: 6.358017245928447\n",
            "Len of Validation loss: 40, Average loss: 6.569163471460342\n",
            "Epoch: 475, Len of Training loss: 12, Average loss: 6.375138163566589\n",
            "Len of Validation loss: 40, Average loss: 7.384075736999511\n",
            "Epoch: 476, Len of Training loss: 12, Average loss: 6.575732549031575\n",
            "Len of Validation loss: 40, Average loss: 6.552419763803482\n",
            "Epoch: 477, Len of Training loss: 12, Average loss: 6.166303277015686\n",
            "Len of Validation loss: 40, Average loss: 6.9534360527992245\n",
            "Epoch: 478, Len of Training loss: 12, Average loss: 6.248863657315572\n",
            "Len of Validation loss: 40, Average loss: 6.530288207530975\n",
            "Epoch: 479, Len of Training loss: 12, Average loss: 6.190300305684407\n",
            "Len of Validation loss: 40, Average loss: 6.324893748760223\n",
            "Epoch: 480, Len of Training loss: 12, Average loss: 6.158053795496623\n",
            "Len of Validation loss: 40, Average loss: 6.528373676538467\n",
            "Epoch: 481, Len of Training loss: 12, Average loss: 6.232075572013855\n",
            "Len of Validation loss: 40, Average loss: 6.964452850818634\n",
            "Epoch: 482, Len of Training loss: 12, Average loss: 6.255880514780681\n",
            "Len of Validation loss: 40, Average loss: 7.280165600776672\n",
            "Epoch: 483, Len of Training loss: 12, Average loss: 6.743085225423177\n",
            "Len of Validation loss: 40, Average loss: 7.552585875988006\n",
            "Epoch: 484, Len of Training loss: 12, Average loss: 6.540630539258321\n",
            "Len of Validation loss: 40, Average loss: 7.073503994941712\n",
            "Epoch: 485, Len of Training loss: 12, Average loss: 6.132860779762268\n",
            "Len of Validation loss: 40, Average loss: 6.715486174821853\n",
            "Epoch: 486, Len of Training loss: 12, Average loss: 6.289109547932942\n",
            "Len of Validation loss: 40, Average loss: 6.594854253530502\n",
            "Epoch: 487, Len of Training loss: 12, Average loss: 6.158448696136475\n",
            "Len of Validation loss: 40, Average loss: 7.292958521842957\n",
            "Epoch: 488, Len of Training loss: 12, Average loss: 6.2303710381189985\n",
            "Len of Validation loss: 40, Average loss: 7.456535255908966\n",
            "Epoch: 489, Len of Training loss: 12, Average loss: 6.168428222338359\n",
            "Len of Validation loss: 40, Average loss: 7.754486012458801\n",
            "Epoch: 490, Len of Training loss: 12, Average loss: 6.30607012907664\n",
            "Len of Validation loss: 40, Average loss: 7.099105978012085\n",
            "Epoch: 491, Len of Training loss: 12, Average loss: 6.210447986920674\n",
            "Len of Validation loss: 40, Average loss: 6.863876926898956\n",
            "Epoch: 492, Len of Training loss: 12, Average loss: 6.169156352678935\n",
            "Len of Validation loss: 40, Average loss: 6.580927789211273\n",
            "Epoch: 493, Len of Training loss: 12, Average loss: 6.374186913172404\n",
            "Len of Validation loss: 40, Average loss: 6.489037835597992\n",
            "Epoch: 494, Len of Training loss: 12, Average loss: 6.38101585706075\n",
            "Len of Validation loss: 40, Average loss: 7.6841145873069765\n",
            "Epoch: 495, Len of Training loss: 12, Average loss: 6.3622645537058515\n",
            "Len of Validation loss: 40, Average loss: 7.035713577270508\n",
            "Epoch: 496, Len of Training loss: 12, Average loss: 6.433437943458557\n",
            "Len of Validation loss: 40, Average loss: 7.542288339138031\n",
            "Epoch: 497, Len of Training loss: 12, Average loss: 6.490349531173706\n",
            "Len of Validation loss: 40, Average loss: 7.0036977410316466\n",
            "Epoch: 498, Len of Training loss: 12, Average loss: 6.569048007329305\n",
            "Len of Validation loss: 40, Average loss: 7.848894000053406\n",
            "Epoch: 499, Len of Training loss: 12, Average loss: 6.450179258982341\n",
            "Len of Validation loss: 40, Average loss: 6.740845441818237\n",
            "Epoch: 500, Len of Training loss: 12, Average loss: 6.2569905916849775\n",
            "Len of Validation loss: 40, Average loss: 8.037622821331023\n",
            "Epoch: 501, Len of Training loss: 12, Average loss: 6.531518181165059\n",
            "Len of Validation loss: 40, Average loss: 7.8040327548980715\n",
            "Epoch: 502, Len of Training loss: 12, Average loss: 6.584036986033122\n",
            "Len of Validation loss: 40, Average loss: 7.753917872905731\n",
            "Epoch: 503, Len of Training loss: 12, Average loss: 6.427085836728414\n",
            "Len of Validation loss: 40, Average loss: 6.6076548278331755\n",
            "Epoch: 504, Len of Training loss: 12, Average loss: 6.558263381322225\n",
            "Len of Validation loss: 40, Average loss: 6.2536641418933865\n",
            "Epoch: 505, Len of Training loss: 12, Average loss: 6.544551412264506\n",
            "Len of Validation loss: 40, Average loss: 6.318280816078186\n",
            "Epoch: 506, Len of Training loss: 12, Average loss: 6.255933920542399\n",
            "Len of Validation loss: 40, Average loss: 6.4397037088871\n",
            "Epoch: 507, Len of Training loss: 12, Average loss: 6.541148543357849\n",
            "Len of Validation loss: 40, Average loss: 6.64993188381195\n",
            "Epoch: 508, Len of Training loss: 12, Average loss: 6.15006951491038\n",
            "Len of Validation loss: 40, Average loss: 6.766882878541947\n",
            "Epoch: 509, Len of Training loss: 12, Average loss: 6.163316249847412\n",
            "Len of Validation loss: 40, Average loss: 6.3254557132720945\n",
            "Epoch: 510, Len of Training loss: 12, Average loss: 6.359190781911214\n",
            "Len of Validation loss: 40, Average loss: 6.344883543252945\n",
            "Epoch: 511, Len of Training loss: 12, Average loss: 6.153271039326985\n",
            "Len of Validation loss: 40, Average loss: 6.326053923368454\n",
            "Epoch: 512, Len of Training loss: 12, Average loss: 6.215701103210449\n",
            "Len of Validation loss: 40, Average loss: 6.411186397075653\n",
            "Epoch: 513, Len of Training loss: 12, Average loss: 6.2995467980702715\n",
            "Len of Validation loss: 40, Average loss: 7.604587841033935\n",
            "Epoch: 514, Len of Training loss: 12, Average loss: 7.121692220369975\n",
            "Len of Validation loss: 40, Average loss: 6.196099841594696\n",
            "Epoch: 515, Len of Training loss: 12, Average loss: 6.345567901929219\n",
            "Len of Validation loss: 40, Average loss: 6.195161259174347\n",
            "Epoch: 516, Len of Training loss: 12, Average loss: 6.320367534955342\n",
            "Len of Validation loss: 40, Average loss: 6.51395258307457\n",
            "Epoch: 517, Len of Training loss: 12, Average loss: 6.2042713562647505\n",
            "Len of Validation loss: 40, Average loss: 6.295984673500061\n",
            "Epoch: 518, Len of Training loss: 12, Average loss: 6.221785147984822\n",
            "Len of Validation loss: 40, Average loss: 6.70267767906189\n",
            "Epoch: 519, Len of Training loss: 12, Average loss: 6.243225653966268\n",
            "Len of Validation loss: 40, Average loss: 8.658982837200165\n",
            "Epoch: 520, Len of Training loss: 12, Average loss: 7.018578012784322\n",
            "Len of Validation loss: 40, Average loss: 8.315634167194366\n",
            "Epoch: 521, Len of Training loss: 12, Average loss: 6.955920100212097\n",
            "Len of Validation loss: 40, Average loss: 6.37682997584343\n",
            "Epoch: 522, Len of Training loss: 12, Average loss: 6.233930826187134\n",
            "Len of Validation loss: 40, Average loss: 6.67791485786438\n",
            "Epoch: 523, Len of Training loss: 12, Average loss: 6.054454207420349\n",
            "Len of Validation loss: 40, Average loss: 6.607943272590637\n",
            "Epoch: 524, Len of Training loss: 12, Average loss: 6.165315230687459\n",
            "Len of Validation loss: 40, Average loss: 6.329865646362305\n",
            "Epoch: 525, Len of Training loss: 12, Average loss: 6.079639315605164\n",
            "Len of Validation loss: 40, Average loss: 6.525028854608536\n",
            "Epoch: 526, Len of Training loss: 12, Average loss: 6.2675133148829145\n",
            "Len of Validation loss: 40, Average loss: 6.328558003902435\n",
            "Epoch: 527, Len of Training loss: 12, Average loss: 6.431197762489319\n",
            "Len of Validation loss: 40, Average loss: 6.433414018154144\n",
            "Epoch: 528, Len of Training loss: 12, Average loss: 6.266524314880371\n",
            "Len of Validation loss: 40, Average loss: 6.294104611873626\n",
            "Epoch: 529, Len of Training loss: 12, Average loss: 6.147915403048198\n",
            "Len of Validation loss: 40, Average loss: 6.422505706548691\n",
            "Epoch: 530, Len of Training loss: 12, Average loss: 5.946894685427348\n",
            "Len of Validation loss: 40, Average loss: 6.603535532951355\n",
            "Epoch: 531, Len of Training loss: 12, Average loss: 6.488457163174947\n",
            "Len of Validation loss: 40, Average loss: 6.75021510720253\n",
            "Epoch: 532, Len of Training loss: 12, Average loss: 6.364413142204285\n",
            "Len of Validation loss: 40, Average loss: 6.3065330505371096\n",
            "Epoch: 533, Len of Training loss: 12, Average loss: 6.220280289649963\n",
            "Len of Validation loss: 40, Average loss: 6.55434820652008\n",
            "Epoch: 534, Len of Training loss: 12, Average loss: 6.254768808682759\n",
            "Len of Validation loss: 40, Average loss: 6.597651684284211\n",
            "Epoch: 535, Len of Training loss: 12, Average loss: 6.416508396466573\n",
            "Len of Validation loss: 40, Average loss: 6.457819604873658\n",
            "Epoch: 536, Len of Training loss: 12, Average loss: 6.156797329584758\n",
            "Len of Validation loss: 40, Average loss: 6.446223813295364\n",
            "Epoch: 537, Len of Training loss: 12, Average loss: 6.20634384950002\n",
            "Len of Validation loss: 40, Average loss: 6.730462765693664\n",
            "Epoch: 538, Len of Training loss: 12, Average loss: 6.49985404809316\n",
            "Len of Validation loss: 40, Average loss: 6.344963324069977\n",
            "Epoch: 539, Len of Training loss: 12, Average loss: 6.273927648862203\n",
            "Len of Validation loss: 40, Average loss: 6.591799068450928\n",
            "Epoch: 540, Len of Training loss: 12, Average loss: 6.153865575790405\n",
            "Len of Validation loss: 40, Average loss: 6.871595001220703\n",
            "Epoch: 541, Len of Training loss: 12, Average loss: 6.076333999633789\n",
            "Len of Validation loss: 40, Average loss: 7.31691267490387\n",
            "Epoch: 542, Len of Training loss: 12, Average loss: 6.116807381312053\n",
            "Len of Validation loss: 40, Average loss: 7.264148676395417\n",
            "Epoch: 543, Len of Training loss: 12, Average loss: 6.596516251564026\n",
            "Len of Validation loss: 40, Average loss: 6.935681092739105\n",
            "Epoch: 544, Len of Training loss: 12, Average loss: 6.156358400980632\n",
            "Len of Validation loss: 40, Average loss: 7.1298281908035275\n",
            "Epoch: 545, Len of Training loss: 12, Average loss: 6.105775435765584\n",
            "Len of Validation loss: 40, Average loss: 6.892463260889054\n",
            "Epoch: 546, Len of Training loss: 12, Average loss: 6.025152206420898\n",
            "Len of Validation loss: 40, Average loss: 6.443526232242585\n",
            "Epoch: 547, Len of Training loss: 12, Average loss: 6.178787509600322\n",
            "Len of Validation loss: 40, Average loss: 6.512356436252594\n",
            "Epoch: 548, Len of Training loss: 12, Average loss: 6.279024084409078\n",
            "Len of Validation loss: 40, Average loss: 6.382867413759231\n",
            "Epoch: 549, Len of Training loss: 12, Average loss: 6.224168101946513\n",
            "Len of Validation loss: 40, Average loss: 6.667584949731827\n",
            "Epoch: 550, Len of Training loss: 12, Average loss: 6.195271849632263\n",
            "Len of Validation loss: 40, Average loss: 6.342490780353546\n",
            "Epoch: 551, Len of Training loss: 12, Average loss: 6.03949232896169\n",
            "Len of Validation loss: 40, Average loss: 6.770255428552628\n",
            "Epoch: 552, Len of Training loss: 12, Average loss: 6.145812273025513\n",
            "Len of Validation loss: 40, Average loss: 6.859669178724289\n",
            "Epoch: 553, Len of Training loss: 12, Average loss: 6.2681881586710615\n",
            "Len of Validation loss: 40, Average loss: 7.696595799922943\n",
            "Epoch: 554, Len of Training loss: 12, Average loss: 6.17807141939799\n",
            "Len of Validation loss: 40, Average loss: 7.364483916759491\n",
            "Epoch: 555, Len of Training loss: 12, Average loss: 6.049699584643046\n",
            "Len of Validation loss: 40, Average loss: 6.7801530122756954\n",
            "Epoch: 556, Len of Training loss: 12, Average loss: 6.064291437466939\n",
            "Len of Validation loss: 40, Average loss: 6.6424483239650725\n",
            "Epoch: 557, Len of Training loss: 12, Average loss: 6.083508412043254\n",
            "Len of Validation loss: 40, Average loss: 6.493408715724945\n",
            "Epoch: 558, Len of Training loss: 12, Average loss: 6.224391063054402\n",
            "Len of Validation loss: 40, Average loss: 6.4942844867706295\n",
            "Epoch: 559, Len of Training loss: 12, Average loss: 6.291164398193359\n",
            "Len of Validation loss: 40, Average loss: 6.620425879955292\n",
            "Epoch: 560, Len of Training loss: 12, Average loss: 6.3072463274002075\n",
            "Len of Validation loss: 40, Average loss: 6.416511309146881\n",
            "Epoch: 561, Len of Training loss: 12, Average loss: 6.25798483689626\n",
            "Len of Validation loss: 40, Average loss: 6.412144529819488\n",
            "Epoch: 562, Len of Training loss: 12, Average loss: 6.3791007200876875\n",
            "Len of Validation loss: 40, Average loss: 6.6811541497707365\n",
            "Epoch: 563, Len of Training loss: 12, Average loss: 6.33517320950826\n",
            "Len of Validation loss: 40, Average loss: 6.697938454151154\n",
            "Epoch: 564, Len of Training loss: 12, Average loss: 6.1315135558446245\n",
            "Len of Validation loss: 40, Average loss: 6.516355562210083\n",
            "Epoch: 565, Len of Training loss: 12, Average loss: 5.928811073303223\n",
            "Len of Validation loss: 40, Average loss: 7.019847637414932\n",
            "Epoch: 566, Len of Training loss: 12, Average loss: 5.897857586542766\n",
            "Len of Validation loss: 40, Average loss: 6.699498867988586\n",
            "Epoch: 567, Len of Training loss: 12, Average loss: 6.0309393008550005\n",
            "Len of Validation loss: 40, Average loss: 7.047187453508377\n",
            "Epoch: 568, Len of Training loss: 12, Average loss: 6.472443580627441\n",
            "Len of Validation loss: 40, Average loss: 6.290872406959534\n",
            "Epoch: 569, Len of Training loss: 12, Average loss: 6.176194151242574\n",
            "Len of Validation loss: 40, Average loss: 6.1550166487693785\n",
            "Epoch: 570, Len of Training loss: 12, Average loss: 5.992598255475362\n",
            "Len of Validation loss: 40, Average loss: 6.402959513664245\n",
            "Epoch: 571, Len of Training loss: 12, Average loss: 6.147806882858276\n",
            "Len of Validation loss: 40, Average loss: 6.406979113817215\n",
            "Epoch: 572, Len of Training loss: 12, Average loss: 6.078388690948486\n",
            "Len of Validation loss: 40, Average loss: 6.426634347438812\n",
            "Epoch: 573, Len of Training loss: 12, Average loss: 6.1504857540130615\n",
            "Len of Validation loss: 40, Average loss: 6.502672708034515\n",
            "Epoch: 574, Len of Training loss: 12, Average loss: 6.076443632443746\n",
            "Len of Validation loss: 40, Average loss: 7.1450474858284\n",
            "Epoch: 575, Len of Training loss: 12, Average loss: 6.317548553148906\n",
            "Len of Validation loss: 40, Average loss: 8.190934348106385\n",
            "Epoch: 576, Len of Training loss: 12, Average loss: 6.344572186470032\n",
            "Len of Validation loss: 40, Average loss: 7.008505338430405\n",
            "Epoch: 577, Len of Training loss: 12, Average loss: 6.561635812123616\n",
            "Len of Validation loss: 40, Average loss: 6.270889955759048\n",
            "Epoch: 578, Len of Training loss: 12, Average loss: 6.553969899813334\n",
            "Len of Validation loss: 40, Average loss: 6.67992485165596\n",
            "Epoch: 579, Len of Training loss: 12, Average loss: 6.245914340019226\n",
            "Len of Validation loss: 40, Average loss: 6.676559162139893\n",
            "Epoch: 580, Len of Training loss: 12, Average loss: 6.038755456606547\n",
            "Len of Validation loss: 40, Average loss: 6.669304436445236\n",
            "Epoch: 581, Len of Training loss: 12, Average loss: 5.95721713701884\n",
            "Len of Validation loss: 40, Average loss: 7.216712498664856\n",
            "Epoch: 582, Len of Training loss: 12, Average loss: 6.108773430188497\n",
            "Len of Validation loss: 40, Average loss: 6.889334970712662\n",
            "Epoch: 583, Len of Training loss: 12, Average loss: 6.006638765335083\n",
            "Len of Validation loss: 40, Average loss: 6.598256850242615\n",
            "Epoch: 584, Len of Training loss: 12, Average loss: 6.061731815338135\n",
            "Len of Validation loss: 40, Average loss: 6.803543078899383\n",
            "Epoch: 585, Len of Training loss: 12, Average loss: 6.021554946899414\n",
            "Len of Validation loss: 40, Average loss: 6.359278321266174\n",
            "Epoch: 586, Len of Training loss: 12, Average loss: 5.951040466626485\n",
            "Len of Validation loss: 40, Average loss: 6.450701594352722\n",
            "Epoch: 587, Len of Training loss: 12, Average loss: 6.006381551424663\n",
            "Len of Validation loss: 40, Average loss: 6.998823368549347\n",
            "Epoch: 588, Len of Training loss: 12, Average loss: 6.156113942464192\n",
            "Len of Validation loss: 40, Average loss: 6.5647361934185025\n",
            "Epoch: 589, Len of Training loss: 12, Average loss: 5.934358914693196\n",
            "Len of Validation loss: 40, Average loss: 6.806535738706589\n",
            "Epoch: 590, Len of Training loss: 12, Average loss: 6.0407454172770185\n",
            "Len of Validation loss: 40, Average loss: 6.483771228790284\n",
            "Epoch: 591, Len of Training loss: 12, Average loss: 6.0612978140513105\n",
            "Len of Validation loss: 40, Average loss: 6.77206780910492\n",
            "Epoch: 592, Len of Training loss: 12, Average loss: 6.20015823841095\n",
            "Len of Validation loss: 40, Average loss: 6.697442764043808\n",
            "Epoch: 593, Len of Training loss: 12, Average loss: 5.914318760236104\n",
            "Len of Validation loss: 40, Average loss: 6.546059459447861\n",
            "Epoch: 594, Len of Training loss: 12, Average loss: 6.083296020825704\n",
            "Len of Validation loss: 40, Average loss: 6.3686776638031\n",
            "Epoch: 595, Len of Training loss: 12, Average loss: 6.2556257247924805\n",
            "Len of Validation loss: 40, Average loss: 6.680216419696808\n",
            "Epoch: 596, Len of Training loss: 12, Average loss: 6.063351154327393\n",
            "Len of Validation loss: 40, Average loss: 6.4131998240947725\n",
            "Epoch: 597, Len of Training loss: 12, Average loss: 5.979019681612651\n",
            "Len of Validation loss: 40, Average loss: 6.76939845085144\n",
            "Epoch: 598, Len of Training loss: 12, Average loss: 6.061806241671245\n",
            "Len of Validation loss: 40, Average loss: 6.593525850772858\n",
            "Epoch: 599, Len of Training loss: 12, Average loss: 6.0963014761606855\n",
            "Len of Validation loss: 40, Average loss: 6.764301145076752\n",
            "Epoch: 600, Len of Training loss: 12, Average loss: 5.978091359138489\n",
            "Len of Validation loss: 40, Average loss: 6.584209996461868\n",
            "Epoch: 601, Len of Training loss: 12, Average loss: 6.014261205991109\n",
            "Len of Validation loss: 40, Average loss: 6.381948125362396\n",
            "Epoch: 602, Len of Training loss: 12, Average loss: 6.126151283582051\n",
            "Len of Validation loss: 40, Average loss: 6.563923233747483\n",
            "Epoch: 603, Len of Training loss: 12, Average loss: 6.135996143023173\n",
            "Len of Validation loss: 40, Average loss: 6.516917794942856\n",
            "Epoch: 604, Len of Training loss: 12, Average loss: 6.077872554461162\n",
            "Len of Validation loss: 40, Average loss: 6.810992872714996\n",
            "Epoch: 605, Len of Training loss: 12, Average loss: 6.18702479203542\n",
            "Len of Validation loss: 40, Average loss: 6.5093637108802795\n",
            "Epoch: 606, Len of Training loss: 12, Average loss: 6.232510924339294\n",
            "Len of Validation loss: 40, Average loss: 6.23908434510231\n",
            "Epoch: 607, Len of Training loss: 12, Average loss: 6.402486403783162\n",
            "Len of Validation loss: 40, Average loss: 6.437702071666718\n",
            "Epoch: 608, Len of Training loss: 12, Average loss: 6.092158913612366\n",
            "Len of Validation loss: 40, Average loss: 7.248111629486084\n",
            "Epoch: 609, Len of Training loss: 12, Average loss: 6.323637008666992\n",
            "Len of Validation loss: 40, Average loss: 8.699887478351593\n",
            "Epoch: 610, Len of Training loss: 12, Average loss: 6.408084432284038\n",
            "Len of Validation loss: 40, Average loss: 6.477610528469086\n",
            "Epoch: 611, Len of Training loss: 12, Average loss: 6.157245834668477\n",
            "Len of Validation loss: 40, Average loss: 7.1576207280159\n",
            "Epoch: 612, Len of Training loss: 12, Average loss: 6.281121412913005\n",
            "Len of Validation loss: 40, Average loss: 7.955861353874207\n",
            "Epoch: 613, Len of Training loss: 12, Average loss: 6.263137062390645\n",
            "Len of Validation loss: 40, Average loss: 7.487353682518005\n",
            "Epoch: 614, Len of Training loss: 12, Average loss: 6.149606188138326\n",
            "Len of Validation loss: 40, Average loss: 7.614319539070129\n",
            "Epoch: 615, Len of Training loss: 12, Average loss: 6.171918114026387\n",
            "Len of Validation loss: 40, Average loss: 6.627263355255127\n",
            "Epoch: 616, Len of Training loss: 12, Average loss: 6.199405829111735\n",
            "Len of Validation loss: 40, Average loss: 6.710110056400299\n",
            "Epoch: 617, Len of Training loss: 12, Average loss: 6.621889551480611\n",
            "Len of Validation loss: 40, Average loss: 6.282288986444473\n",
            "Epoch: 618, Len of Training loss: 12, Average loss: 6.358427047729492\n",
            "Len of Validation loss: 40, Average loss: 6.898902010917664\n",
            "Epoch: 619, Len of Training loss: 12, Average loss: 6.060843189557393\n",
            "Len of Validation loss: 40, Average loss: 7.7932178497314455\n",
            "Epoch: 620, Len of Training loss: 12, Average loss: 6.273874322573344\n",
            "Len of Validation loss: 40, Average loss: 9.20808013677597\n",
            "Epoch: 621, Len of Training loss: 12, Average loss: 6.741605361302693\n",
            "Len of Validation loss: 40, Average loss: 6.6700463950634\n",
            "Epoch: 622, Len of Training loss: 12, Average loss: 6.226783792177836\n",
            "Len of Validation loss: 40, Average loss: 6.396652042865753\n",
            "Epoch: 623, Len of Training loss: 12, Average loss: 6.056054949760437\n",
            "Len of Validation loss: 40, Average loss: 6.37781645655632\n",
            "Epoch: 624, Len of Training loss: 12, Average loss: 6.005239049593608\n",
            "Len of Validation loss: 40, Average loss: 6.58555657863617\n",
            "Epoch: 625, Len of Training loss: 12, Average loss: 5.878597696622212\n",
            "Len of Validation loss: 40, Average loss: 6.917429864406586\n",
            "Epoch: 626, Len of Training loss: 12, Average loss: 6.007925550142924\n",
            "Len of Validation loss: 40, Average loss: 6.339187037944794\n",
            "Epoch: 627, Len of Training loss: 12, Average loss: 6.045125643412272\n",
            "Len of Validation loss: 40, Average loss: 6.320672714710236\n",
            "Epoch: 628, Len of Training loss: 12, Average loss: 6.1829447746276855\n",
            "Len of Validation loss: 40, Average loss: 6.544410145282745\n",
            "Epoch: 629, Len of Training loss: 12, Average loss: 6.036974867184957\n",
            "Len of Validation loss: 40, Average loss: 6.943346738815308\n",
            "Epoch: 630, Len of Training loss: 12, Average loss: 5.931326190630595\n",
            "Len of Validation loss: 40, Average loss: 6.498597627878189\n",
            "Epoch: 631, Len of Training loss: 12, Average loss: 5.951502680778503\n",
            "Len of Validation loss: 40, Average loss: 7.221864545345307\n",
            "Epoch: 632, Len of Training loss: 12, Average loss: 5.980686942736308\n",
            "Len of Validation loss: 40, Average loss: 8.140445065498351\n",
            "Epoch: 633, Len of Training loss: 12, Average loss: 6.297900994618733\n",
            "Len of Validation loss: 40, Average loss: 6.612950909137726\n",
            "Epoch: 634, Len of Training loss: 12, Average loss: 5.997792998949687\n",
            "Len of Validation loss: 40, Average loss: 6.733420020341873\n",
            "Epoch: 635, Len of Training loss: 12, Average loss: 6.080023527145386\n",
            "Len of Validation loss: 40, Average loss: 7.169625592231751\n",
            "Epoch: 636, Len of Training loss: 12, Average loss: 5.949394146601359\n",
            "Len of Validation loss: 40, Average loss: 6.390131050348282\n",
            "Epoch: 637, Len of Training loss: 12, Average loss: 6.182151993115743\n",
            "Len of Validation loss: 40, Average loss: 6.657671976089477\n",
            "Epoch: 638, Len of Training loss: 12, Average loss: 6.101695656776428\n",
            "Len of Validation loss: 40, Average loss: 6.462299156188965\n",
            "Epoch: 639, Len of Training loss: 12, Average loss: 6.1529777844746905\n",
            "Len of Validation loss: 40, Average loss: 6.950639867782593\n",
            "Epoch: 640, Len of Training loss: 12, Average loss: 6.092906316121419\n",
            "Len of Validation loss: 40, Average loss: 8.170934993028641\n",
            "Epoch: 641, Len of Training loss: 12, Average loss: 6.2158035437266035\n",
            "Len of Validation loss: 40, Average loss: 7.546044206619262\n",
            "Epoch: 642, Len of Training loss: 12, Average loss: 6.076820691426595\n",
            "Len of Validation loss: 40, Average loss: 6.476324760913849\n",
            "Epoch: 643, Len of Training loss: 12, Average loss: 5.964901924133301\n",
            "Len of Validation loss: 40, Average loss: 7.05745250582695\n",
            "Epoch: 644, Len of Training loss: 12, Average loss: 5.973027467727661\n",
            "Len of Validation loss: 40, Average loss: 6.604329478740692\n",
            "Epoch: 645, Len of Training loss: 12, Average loss: 5.926852385203044\n",
            "Len of Validation loss: 40, Average loss: 6.561736631393432\n",
            "Epoch: 646, Len of Training loss: 12, Average loss: 6.144747535387675\n",
            "Len of Validation loss: 40, Average loss: 6.426644504070282\n",
            "Epoch: 647, Len of Training loss: 12, Average loss: 5.926005522410075\n",
            "Len of Validation loss: 40, Average loss: 6.438382488489151\n",
            "Epoch: 648, Len of Training loss: 12, Average loss: 6.0209468603134155\n",
            "Len of Validation loss: 40, Average loss: 6.892744219303131\n",
            "Epoch: 649, Len of Training loss: 12, Average loss: 5.999977946281433\n",
            "Len of Validation loss: 40, Average loss: 6.897729390859604\n",
            "Epoch: 650, Len of Training loss: 12, Average loss: 5.965078552563985\n",
            "Len of Validation loss: 40, Average loss: 7.265231144428253\n",
            "Epoch: 651, Len of Training loss: 12, Average loss: 6.050972938537598\n",
            "Len of Validation loss: 40, Average loss: 6.744209069013595\n",
            "Epoch: 652, Len of Training loss: 12, Average loss: 6.107864022254944\n",
            "Len of Validation loss: 40, Average loss: 6.489646852016449\n",
            "Epoch: 653, Len of Training loss: 12, Average loss: 6.522759795188904\n",
            "Len of Validation loss: 40, Average loss: 8.082601284980774\n",
            "Epoch: 654, Len of Training loss: 12, Average loss: 6.18844719727834\n",
            "Len of Validation loss: 40, Average loss: 7.0296407580375675\n",
            "Epoch: 655, Len of Training loss: 12, Average loss: 6.315290570259094\n",
            "Len of Validation loss: 40, Average loss: 7.045656377077103\n",
            "Epoch: 656, Len of Training loss: 12, Average loss: 6.074151237805684\n",
            "Len of Validation loss: 40, Average loss: 6.434712970256806\n",
            "Epoch: 657, Len of Training loss: 12, Average loss: 5.902351299921672\n",
            "Len of Validation loss: 40, Average loss: 7.19443861246109\n",
            "Epoch: 658, Len of Training loss: 12, Average loss: 5.942026257514954\n",
            "Len of Validation loss: 40, Average loss: 6.568374383449554\n",
            "Epoch: 659, Len of Training loss: 12, Average loss: 5.8972751299540205\n",
            "Len of Validation loss: 40, Average loss: 6.370878374576568\n",
            "Epoch: 660, Len of Training loss: 12, Average loss: 5.912991921106975\n",
            "Len of Validation loss: 40, Average loss: 6.396070444583893\n",
            "Epoch: 661, Len of Training loss: 12, Average loss: 6.197356104850769\n",
            "Len of Validation loss: 40, Average loss: 6.447009789943695\n",
            "Epoch: 662, Len of Training loss: 12, Average loss: 6.335069576899211\n",
            "Len of Validation loss: 40, Average loss: 6.628825849294662\n",
            "Epoch: 663, Len of Training loss: 12, Average loss: 6.132336417833964\n",
            "Len of Validation loss: 40, Average loss: 6.337671029567718\n",
            "Epoch: 664, Len of Training loss: 12, Average loss: 6.376233537991841\n",
            "Len of Validation loss: 40, Average loss: 6.490796452760696\n",
            "Epoch: 665, Len of Training loss: 12, Average loss: 6.213609377543132\n",
            "Len of Validation loss: 40, Average loss: 6.344454270601273\n",
            "Epoch: 666, Len of Training loss: 12, Average loss: 6.166619499524434\n",
            "Len of Validation loss: 40, Average loss: 6.760930609703064\n",
            "Epoch: 667, Len of Training loss: 12, Average loss: 6.248635609944661\n",
            "Len of Validation loss: 40, Average loss: 7.566706049442291\n",
            "Epoch: 668, Len of Training loss: 12, Average loss: 6.302734613418579\n",
            "Len of Validation loss: 40, Average loss: 7.3027388215065\n",
            "Epoch: 669, Len of Training loss: 12, Average loss: 6.125683506329854\n",
            "Len of Validation loss: 40, Average loss: 6.599885499477386\n",
            "Epoch: 670, Len of Training loss: 12, Average loss: 6.140277862548828\n",
            "Len of Validation loss: 40, Average loss: 6.412967884540558\n",
            "Epoch: 671, Len of Training loss: 12, Average loss: 5.951391895612081\n",
            "Len of Validation loss: 40, Average loss: 6.668369406461716\n",
            "Epoch: 672, Len of Training loss: 12, Average loss: 5.935956358909607\n",
            "Len of Validation loss: 40, Average loss: 7.392976522445679\n",
            "Epoch: 673, Len of Training loss: 12, Average loss: 6.006909251213074\n",
            "Len of Validation loss: 40, Average loss: 7.131132805347443\n",
            "Epoch: 674, Len of Training loss: 12, Average loss: 6.003746708234151\n",
            "Len of Validation loss: 40, Average loss: 6.742729711532593\n",
            "Epoch: 675, Len of Training loss: 12, Average loss: 6.169138669967651\n",
            "Len of Validation loss: 40, Average loss: 6.199337977170944\n",
            "Epoch: 676, Len of Training loss: 12, Average loss: 6.201552232106526\n",
            "Len of Validation loss: 40, Average loss: 6.677001047134399\n",
            "Epoch: 677, Len of Training loss: 12, Average loss: 6.019291400909424\n",
            "Len of Validation loss: 40, Average loss: 6.664961832761764\n",
            "Epoch: 678, Len of Training loss: 12, Average loss: 5.927024960517883\n",
            "Len of Validation loss: 40, Average loss: 6.7807772040367125\n",
            "Epoch: 679, Len of Training loss: 12, Average loss: 5.892750978469849\n",
            "Len of Validation loss: 40, Average loss: 7.307594448328018\n",
            "Epoch: 680, Len of Training loss: 12, Average loss: 5.878764231999715\n",
            "Len of Validation loss: 40, Average loss: 6.637330484390259\n",
            "Epoch: 681, Len of Training loss: 12, Average loss: 5.870911280314128\n",
            "Len of Validation loss: 40, Average loss: 6.412672507762909\n",
            "Epoch: 682, Len of Training loss: 12, Average loss: 5.941719094912211\n",
            "Len of Validation loss: 40, Average loss: 6.860243737697601\n",
            "Epoch: 683, Len of Training loss: 12, Average loss: 5.983063777287801\n",
            "Len of Validation loss: 40, Average loss: 6.489875376224518\n",
            "Epoch: 684, Len of Training loss: 12, Average loss: 5.911092559496562\n",
            "Len of Validation loss: 40, Average loss: 6.252312809228897\n",
            "Epoch: 685, Len of Training loss: 12, Average loss: 5.977350989977519\n",
            "Len of Validation loss: 40, Average loss: 6.432170003652573\n",
            "Epoch: 686, Len of Training loss: 12, Average loss: 6.230947732925415\n",
            "Len of Validation loss: 40, Average loss: 6.779011017084121\n",
            "Epoch: 687, Len of Training loss: 12, Average loss: 6.044827381769816\n",
            "Len of Validation loss: 40, Average loss: 6.639907974004745\n",
            "Epoch: 688, Len of Training loss: 12, Average loss: 5.945581257343292\n",
            "Len of Validation loss: 40, Average loss: 6.277446991205215\n",
            "Epoch: 689, Len of Training loss: 12, Average loss: 5.877229491869609\n",
            "Len of Validation loss: 40, Average loss: 6.614753919839859\n",
            "Epoch: 690, Len of Training loss: 12, Average loss: 6.109806497891744\n",
            "Len of Validation loss: 40, Average loss: 6.281832981109619\n",
            "Epoch: 691, Len of Training loss: 12, Average loss: 6.108347058296204\n",
            "Len of Validation loss: 40, Average loss: 6.412444037199021\n",
            "Epoch: 692, Len of Training loss: 12, Average loss: 5.9057953755060835\n",
            "Len of Validation loss: 40, Average loss: 6.539575624465942\n",
            "Epoch: 693, Len of Training loss: 12, Average loss: 5.848199566205342\n",
            "Len of Validation loss: 40, Average loss: 6.634219324588775\n",
            "Epoch: 694, Len of Training loss: 12, Average loss: 5.8535586794217425\n",
            "Len of Validation loss: 40, Average loss: 6.506029427051544\n",
            "Epoch: 695, Len of Training loss: 12, Average loss: 6.040940682093303\n",
            "Len of Validation loss: 40, Average loss: 6.41609708070755\n",
            "Epoch: 696, Len of Training loss: 12, Average loss: 5.737805724143982\n",
            "Len of Validation loss: 40, Average loss: 6.474074101448059\n",
            "Epoch: 697, Len of Training loss: 12, Average loss: 5.779331207275391\n",
            "Len of Validation loss: 40, Average loss: 7.9658411026000975\n",
            "Epoch: 698, Len of Training loss: 12, Average loss: 6.09490446249644\n",
            "Len of Validation loss: 40, Average loss: 6.475298595428467\n",
            "Epoch: 699, Len of Training loss: 12, Average loss: 5.958484212557475\n",
            "Len of Validation loss: 40, Average loss: 6.988819682598114\n",
            "Epoch: 700, Len of Training loss: 12, Average loss: 6.246584137280782\n",
            "Len of Validation loss: 40, Average loss: 6.64718976020813\n",
            "Epoch: 701, Len of Training loss: 12, Average loss: 5.90360156695048\n",
            "Len of Validation loss: 40, Average loss: 6.354716265201569\n",
            "Epoch: 702, Len of Training loss: 12, Average loss: 5.92219078540802\n",
            "Len of Validation loss: 40, Average loss: 6.3131213068962095\n",
            "Epoch: 703, Len of Training loss: 12, Average loss: 6.083690603574117\n",
            "Len of Validation loss: 40, Average loss: 6.612109416723252\n",
            "Epoch: 704, Len of Training loss: 12, Average loss: 6.263581236203511\n",
            "Len of Validation loss: 40, Average loss: 7.048503035306931\n",
            "Epoch: 705, Len of Training loss: 12, Average loss: 6.046402017275493\n",
            "Len of Validation loss: 40, Average loss: 6.9692071914672855\n",
            "Epoch: 706, Len of Training loss: 12, Average loss: 5.832540233929952\n",
            "Len of Validation loss: 40, Average loss: 7.324722850322724\n",
            "Epoch: 707, Len of Training loss: 12, Average loss: 5.8755414088567095\n",
            "Len of Validation loss: 40, Average loss: 7.301320374011993\n",
            "Epoch: 708, Len of Training loss: 12, Average loss: 5.950848340988159\n",
            "Len of Validation loss: 40, Average loss: 6.811924332380295\n",
            "Epoch: 709, Len of Training loss: 12, Average loss: 6.119636416435242\n",
            "Len of Validation loss: 40, Average loss: 6.589810597896576\n",
            "Epoch: 710, Len of Training loss: 12, Average loss: 5.885160446166992\n",
            "Len of Validation loss: 40, Average loss: 6.358147627115249\n",
            "Epoch: 711, Len of Training loss: 12, Average loss: 5.9808112780253095\n",
            "Len of Validation loss: 40, Average loss: 6.3515356361865996\n",
            "Epoch: 712, Len of Training loss: 12, Average loss: 5.996218005816142\n",
            "Len of Validation loss: 40, Average loss: 6.627236992120743\n",
            "Epoch: 713, Len of Training loss: 12, Average loss: 5.703146199385325\n",
            "Len of Validation loss: 40, Average loss: 6.767007058858871\n",
            "Epoch: 714, Len of Training loss: 12, Average loss: 6.036784211794536\n",
            "Len of Validation loss: 40, Average loss: 7.126460146903992\n",
            "Epoch: 715, Len of Training loss: 12, Average loss: 5.815980911254883\n",
            "Len of Validation loss: 40, Average loss: 7.133460623025894\n",
            "Epoch: 716, Len of Training loss: 12, Average loss: 5.765652696291606\n",
            "Len of Validation loss: 40, Average loss: 7.183505600690841\n",
            "Epoch: 717, Len of Training loss: 12, Average loss: 5.897718787193298\n",
            "Len of Validation loss: 40, Average loss: 6.818520414829254\n",
            "Epoch: 718, Len of Training loss: 12, Average loss: 6.094107508659363\n",
            "Len of Validation loss: 40, Average loss: 6.4574972748756405\n",
            "Epoch: 719, Len of Training loss: 12, Average loss: 5.97687025864919\n",
            "Len of Validation loss: 40, Average loss: 6.616486299037933\n",
            "Epoch: 720, Len of Training loss: 12, Average loss: 5.782854159673055\n",
            "Len of Validation loss: 40, Average loss: 6.418991553783417\n",
            "Epoch: 721, Len of Training loss: 12, Average loss: 5.8266942501068115\n",
            "Len of Validation loss: 40, Average loss: 6.996481841802597\n",
            "Epoch: 722, Len of Training loss: 12, Average loss: 5.738317847251892\n",
            "Len of Validation loss: 40, Average loss: 7.007454133033752\n",
            "Epoch: 723, Len of Training loss: 12, Average loss: 5.960127472877502\n",
            "Len of Validation loss: 40, Average loss: 7.175795936584473\n",
            "Epoch: 724, Len of Training loss: 12, Average loss: 6.311861316363017\n",
            "Len of Validation loss: 40, Average loss: 7.6693660616874695\n",
            "Epoch: 725, Len of Training loss: 12, Average loss: 5.882728735605876\n",
            "Len of Validation loss: 40, Average loss: 7.325963568687439\n",
            "Epoch: 726, Len of Training loss: 12, Average loss: 6.018192172050476\n",
            "Len of Validation loss: 40, Average loss: 7.0243019461631775\n",
            "Epoch: 727, Len of Training loss: 12, Average loss: 5.861829559008281\n",
            "Len of Validation loss: 40, Average loss: 6.432188266515732\n",
            "Epoch: 728, Len of Training loss: 12, Average loss: 5.840745568275452\n",
            "Len of Validation loss: 40, Average loss: 6.7180116176605225\n",
            "Epoch: 729, Len of Training loss: 12, Average loss: 5.986580848693848\n",
            "Len of Validation loss: 40, Average loss: 6.178667193651199\n",
            "Epoch: 730, Len of Training loss: 12, Average loss: 5.880873282750447\n",
            "Len of Validation loss: 40, Average loss: 6.781368380784988\n",
            "Epoch: 731, Len of Training loss: 12, Average loss: 5.86078941822052\n",
            "Len of Validation loss: 40, Average loss: 6.885532039403915\n",
            "Epoch: 732, Len of Training loss: 12, Average loss: 5.897577325503032\n",
            "Len of Validation loss: 40, Average loss: 6.707858252525329\n",
            "Epoch: 733, Len of Training loss: 12, Average loss: 5.696994821230571\n",
            "Len of Validation loss: 40, Average loss: 7.331234669685363\n",
            "Epoch: 734, Len of Training loss: 12, Average loss: 6.184980193773906\n",
            "Len of Validation loss: 40, Average loss: 6.520473968982697\n",
            "Epoch: 735, Len of Training loss: 12, Average loss: 5.9038898547490435\n",
            "Len of Validation loss: 40, Average loss: 6.7436012625694275\n",
            "Epoch: 736, Len of Training loss: 12, Average loss: 5.734887361526489\n",
            "Len of Validation loss: 40, Average loss: 6.568460786342621\n",
            "Epoch: 737, Len of Training loss: 12, Average loss: 5.902601838111877\n",
            "Len of Validation loss: 40, Average loss: 6.6125333070755\n",
            "Epoch: 738, Len of Training loss: 12, Average loss: 5.741018176078796\n",
            "Len of Validation loss: 40, Average loss: 6.743652433156967\n",
            "Epoch: 739, Len of Training loss: 12, Average loss: 5.923652331034343\n",
            "Len of Validation loss: 40, Average loss: 6.536903285980225\n",
            "Epoch: 740, Len of Training loss: 12, Average loss: 5.993131518363953\n",
            "Len of Validation loss: 40, Average loss: 6.665537047386169\n",
            "Epoch: 741, Len of Training loss: 12, Average loss: 6.178226272265117\n",
            "Len of Validation loss: 40, Average loss: 6.473628842830658\n",
            "Epoch: 742, Len of Training loss: 12, Average loss: 6.1920173565546675\n",
            "Len of Validation loss: 40, Average loss: 6.332211112976074\n",
            "Epoch: 743, Len of Training loss: 12, Average loss: 5.986144741376241\n",
            "Len of Validation loss: 40, Average loss: 6.497884684801102\n",
            "Epoch: 744, Len of Training loss: 12, Average loss: 5.9853101174036665\n",
            "Len of Validation loss: 40, Average loss: 6.555626565217972\n",
            "Epoch: 745, Len of Training loss: 12, Average loss: 5.868680914243062\n",
            "Len of Validation loss: 40, Average loss: 7.42720707654953\n",
            "Epoch: 746, Len of Training loss: 12, Average loss: 5.93299647172292\n",
            "Len of Validation loss: 40, Average loss: 6.498179465532303\n",
            "Epoch: 747, Len of Training loss: 12, Average loss: 5.810716827710469\n",
            "Len of Validation loss: 40, Average loss: 6.517448377609253\n",
            "Epoch: 748, Len of Training loss: 12, Average loss: 5.776816924413045\n",
            "Len of Validation loss: 40, Average loss: 7.298526561260223\n",
            "Epoch: 749, Len of Training loss: 12, Average loss: 5.8457454442977905\n",
            "Len of Validation loss: 40, Average loss: 7.409582245349884\n",
            "Epoch: 750, Len of Training loss: 12, Average loss: 6.20395032564799\n",
            "Len of Validation loss: 40, Average loss: 7.570631682872772\n",
            "Epoch: 751, Len of Training loss: 12, Average loss: 6.178633610407512\n",
            "Len of Validation loss: 40, Average loss: 6.705256432294846\n",
            "Epoch: 752, Len of Training loss: 12, Average loss: 6.160861452420552\n",
            "Len of Validation loss: 40, Average loss: 6.923276364803314\n",
            "Epoch: 753, Len of Training loss: 12, Average loss: 5.799233476320903\n",
            "Len of Validation loss: 40, Average loss: 6.878871214389801\n",
            "Epoch: 754, Len of Training loss: 12, Average loss: 5.7373121579488116\n",
            "Len of Validation loss: 40, Average loss: 6.603585338592529\n",
            "Epoch: 755, Len of Training loss: 12, Average loss: 5.678282459576924\n",
            "Len of Validation loss: 40, Average loss: 7.114601647853851\n",
            "Epoch: 756, Len of Training loss: 12, Average loss: 5.716755588849385\n",
            "Len of Validation loss: 40, Average loss: 7.2001479983329775\n",
            "Epoch: 757, Len of Training loss: 12, Average loss: 5.907569766044617\n",
            "Len of Validation loss: 40, Average loss: 6.47969691157341\n",
            "Epoch: 758, Len of Training loss: 12, Average loss: 5.695334990819295\n",
            "Len of Validation loss: 40, Average loss: 6.4110127747058865\n",
            "Epoch: 759, Len of Training loss: 12, Average loss: 5.683837175369263\n",
            "Len of Validation loss: 40, Average loss: 7.142233610153198\n",
            "Epoch: 760, Len of Training loss: 12, Average loss: 5.765158772468567\n",
            "Len of Validation loss: 40, Average loss: 7.081711214780808\n",
            "Epoch: 761, Len of Training loss: 12, Average loss: 5.938919186592102\n",
            "Len of Validation loss: 40, Average loss: 6.667474752664566\n",
            "Epoch: 762, Len of Training loss: 12, Average loss: 6.345464110374451\n",
            "Len of Validation loss: 40, Average loss: 6.479070508480072\n",
            "Epoch: 763, Len of Training loss: 12, Average loss: 5.915754357973735\n",
            "Len of Validation loss: 40, Average loss: 6.92943748831749\n",
            "Epoch: 764, Len of Training loss: 12, Average loss: 5.729065974553426\n",
            "Len of Validation loss: 40, Average loss: 6.335455656051636\n",
            "Epoch: 765, Len of Training loss: 12, Average loss: 5.732656200726827\n",
            "Len of Validation loss: 40, Average loss: 6.396439397335053\n",
            "Epoch: 766, Len of Training loss: 12, Average loss: 5.884157379468282\n",
            "Len of Validation loss: 40, Average loss: 6.9539418578147885\n",
            "Epoch: 767, Len of Training loss: 12, Average loss: 6.01791783173879\n",
            "Len of Validation loss: 40, Average loss: 7.272051441669464\n",
            "Epoch: 768, Len of Training loss: 12, Average loss: 5.786716183026631\n",
            "Len of Validation loss: 40, Average loss: 6.241640460491181\n",
            "Epoch: 769, Len of Training loss: 12, Average loss: 5.875612099965413\n",
            "Len of Validation loss: 40, Average loss: 6.691192406415939\n",
            "Epoch: 770, Len of Training loss: 12, Average loss: 5.827069282531738\n",
            "Len of Validation loss: 40, Average loss: 7.431728005409241\n",
            "Epoch: 771, Len of Training loss: 12, Average loss: 5.864079634348552\n",
            "Len of Validation loss: 40, Average loss: 7.135684430599213\n",
            "Epoch: 772, Len of Training loss: 12, Average loss: 6.006053566932678\n",
            "Len of Validation loss: 40, Average loss: 7.024610102176666\n",
            "Epoch: 773, Len of Training loss: 12, Average loss: 5.850703239440918\n",
            "Len of Validation loss: 40, Average loss: 6.719471079111099\n",
            "Epoch: 774, Len of Training loss: 12, Average loss: 5.931433439254761\n",
            "Len of Validation loss: 40, Average loss: 7.354955798387527\n",
            "Epoch: 775, Len of Training loss: 12, Average loss: 5.750972668329875\n",
            "Len of Validation loss: 40, Average loss: 6.388693606853485\n",
            "Epoch: 776, Len of Training loss: 12, Average loss: 5.821653485298157\n",
            "Len of Validation loss: 40, Average loss: 6.63824383020401\n",
            "Epoch: 777, Len of Training loss: 12, Average loss: 5.849569837252299\n",
            "Len of Validation loss: 40, Average loss: 6.75841538310051\n",
            "Epoch: 778, Len of Training loss: 12, Average loss: 5.738056461016337\n",
            "Len of Validation loss: 40, Average loss: 6.827724677324295\n",
            "Epoch: 779, Len of Training loss: 12, Average loss: 5.806642850240071\n",
            "Len of Validation loss: 40, Average loss: 6.82480640411377\n",
            "Epoch: 780, Len of Training loss: 12, Average loss: 5.878720800081889\n",
            "Len of Validation loss: 40, Average loss: 6.460354197025299\n",
            "Epoch: 781, Len of Training loss: 12, Average loss: 5.718758225440979\n",
            "Len of Validation loss: 40, Average loss: 6.302410113811493\n",
            "Epoch: 782, Len of Training loss: 12, Average loss: 5.829334855079651\n",
            "Len of Validation loss: 40, Average loss: 6.831379652023315\n",
            "Epoch: 783, Len of Training loss: 12, Average loss: 5.972467223803203\n",
            "Len of Validation loss: 40, Average loss: 7.386957132816315\n",
            "Epoch: 784, Len of Training loss: 12, Average loss: 5.970340768496196\n",
            "Len of Validation loss: 40, Average loss: 7.347029650211335\n",
            "Epoch: 785, Len of Training loss: 12, Average loss: 5.964274048805237\n",
            "Len of Validation loss: 40, Average loss: 6.430944412946701\n",
            "Epoch: 786, Len of Training loss: 12, Average loss: 6.306339820226033\n",
            "Len of Validation loss: 40, Average loss: 6.819512224197387\n",
            "Epoch: 787, Len of Training loss: 12, Average loss: 6.098299423853557\n",
            "Len of Validation loss: 40, Average loss: 6.417751109600067\n",
            "Epoch: 788, Len of Training loss: 12, Average loss: 5.914363463719686\n",
            "Len of Validation loss: 40, Average loss: 6.491907632350921\n",
            "Epoch: 789, Len of Training loss: 12, Average loss: 5.811747431755066\n",
            "Len of Validation loss: 40, Average loss: 6.510252773761749\n",
            "Epoch: 790, Len of Training loss: 12, Average loss: 5.87935197353363\n",
            "Len of Validation loss: 40, Average loss: 6.422993987798691\n",
            "Epoch: 791, Len of Training loss: 12, Average loss: 5.85650630791982\n",
            "Len of Validation loss: 40, Average loss: 7.404773277044296\n",
            "Epoch: 792, Len of Training loss: 12, Average loss: 5.781119704246521\n",
            "Len of Validation loss: 40, Average loss: 6.647479206323624\n",
            "Epoch: 793, Len of Training loss: 12, Average loss: 5.675105532010396\n",
            "Len of Validation loss: 40, Average loss: 6.489722502231598\n",
            "Epoch: 794, Len of Training loss: 12, Average loss: 5.664365768432617\n",
            "Len of Validation loss: 40, Average loss: 6.535988247394561\n",
            "Epoch: 795, Len of Training loss: 12, Average loss: 5.667233149210612\n",
            "Len of Validation loss: 40, Average loss: 6.471538311243057\n",
            "Epoch: 796, Len of Training loss: 12, Average loss: 5.737630287806193\n",
            "Len of Validation loss: 40, Average loss: 6.931147789955139\n",
            "Epoch: 797, Len of Training loss: 12, Average loss: 5.782068133354187\n",
            "Len of Validation loss: 40, Average loss: 7.228060162067413\n",
            "Epoch: 798, Len of Training loss: 12, Average loss: 5.804667711257935\n",
            "Len of Validation loss: 40, Average loss: 6.772016882896423\n",
            "Epoch: 799, Len of Training loss: 12, Average loss: 5.829379518826802\n",
            "Len of Validation loss: 40, Average loss: 6.997303873300552\n",
            "Epoch: 800, Len of Training loss: 12, Average loss: 5.8286979993184405\n",
            "Len of Validation loss: 40, Average loss: 6.558451759815216\n",
            "Epoch: 801, Len of Training loss: 12, Average loss: 5.745876312255859\n",
            "Len of Validation loss: 40, Average loss: 6.775476253032684\n",
            "Epoch: 802, Len of Training loss: 12, Average loss: 5.879711389541626\n",
            "Len of Validation loss: 40, Average loss: 7.192632508277893\n",
            "Epoch: 803, Len of Training loss: 12, Average loss: 5.77216116587321\n",
            "Len of Validation loss: 40, Average loss: 6.340759962797165\n",
            "Epoch: 804, Len of Training loss: 12, Average loss: 5.951626261075337\n",
            "Len of Validation loss: 40, Average loss: 6.26112710237503\n",
            "Epoch: 805, Len of Training loss: 12, Average loss: 5.88157840569814\n",
            "Len of Validation loss: 40, Average loss: 6.443275195360184\n",
            "Epoch: 806, Len of Training loss: 12, Average loss: 5.815662741661072\n",
            "Len of Validation loss: 40, Average loss: 6.524537229537964\n",
            "Epoch: 807, Len of Training loss: 12, Average loss: 5.981868863105774\n",
            "Len of Validation loss: 40, Average loss: 6.339003759622574\n",
            "Epoch: 808, Len of Training loss: 12, Average loss: 6.065444628397624\n",
            "Len of Validation loss: 40, Average loss: 6.857712209224701\n",
            "Epoch: 809, Len of Training loss: 12, Average loss: 5.9302559693654375\n",
            "Len of Validation loss: 40, Average loss: 7.727855205535889\n",
            "Epoch: 810, Len of Training loss: 12, Average loss: 5.860334396362305\n",
            "Len of Validation loss: 40, Average loss: 6.417975497245789\n",
            "Epoch: 811, Len of Training loss: 12, Average loss: 5.916905204455058\n",
            "Len of Validation loss: 40, Average loss: 6.60420857667923\n",
            "Epoch: 812, Len of Training loss: 12, Average loss: 5.8887341022491455\n",
            "Len of Validation loss: 40, Average loss: 6.701238268613816\n",
            "Epoch: 813, Len of Training loss: 12, Average loss: 5.8249937693278\n",
            "Len of Validation loss: 40, Average loss: 6.406378144025803\n",
            "Epoch: 814, Len of Training loss: 12, Average loss: 5.871714472770691\n",
            "Len of Validation loss: 40, Average loss: 6.797414368391037\n",
            "Epoch: 815, Len of Training loss: 12, Average loss: 5.650271415710449\n",
            "Len of Validation loss: 40, Average loss: 6.456650120019913\n",
            "Epoch: 816, Len of Training loss: 12, Average loss: 6.075328548749288\n",
            "Len of Validation loss: 40, Average loss: 6.568951255083084\n",
            "Epoch: 817, Len of Training loss: 12, Average loss: 5.744394222895305\n",
            "Len of Validation loss: 40, Average loss: 6.606290352344513\n",
            "Epoch: 818, Len of Training loss: 12, Average loss: 6.1096673011779785\n",
            "Len of Validation loss: 40, Average loss: 6.434227675199509\n",
            "Epoch: 819, Len of Training loss: 12, Average loss: 5.890144069989522\n",
            "Len of Validation loss: 40, Average loss: 6.488963770866394\n",
            "Epoch: 820, Len of Training loss: 12, Average loss: 5.717412869135539\n",
            "Len of Validation loss: 40, Average loss: 7.005261123180389\n",
            "Epoch: 821, Len of Training loss: 12, Average loss: 5.683431148529053\n",
            "Len of Validation loss: 40, Average loss: 6.2333251416683195\n",
            "Epoch: 822, Len of Training loss: 12, Average loss: 5.736398061116536\n",
            "Len of Validation loss: 40, Average loss: 6.701660692691803\n",
            "Epoch: 823, Len of Training loss: 12, Average loss: 6.216408133506775\n",
            "Len of Validation loss: 40, Average loss: 6.67443596124649\n",
            "Epoch: 824, Len of Training loss: 12, Average loss: 6.129687865575154\n",
            "Len of Validation loss: 40, Average loss: 6.557481217384338\n",
            "Epoch: 825, Len of Training loss: 12, Average loss: 5.889239867528279\n",
            "Len of Validation loss: 40, Average loss: 6.739578574895859\n",
            "Epoch: 826, Len of Training loss: 12, Average loss: 6.043530027071635\n",
            "Len of Validation loss: 40, Average loss: 6.512598687410355\n",
            "Epoch: 827, Len of Training loss: 12, Average loss: 6.406846006711324\n",
            "Len of Validation loss: 40, Average loss: 6.809082424640655\n",
            "Epoch: 828, Len of Training loss: 12, Average loss: 5.757847587267558\n",
            "Len of Validation loss: 40, Average loss: 6.513659596443176\n",
            "Epoch: 829, Len of Training loss: 12, Average loss: 6.090035160382588\n",
            "Len of Validation loss: 40, Average loss: 6.234486931562424\n",
            "Epoch: 830, Len of Training loss: 12, Average loss: 5.7917234897613525\n",
            "Len of Validation loss: 40, Average loss: 6.659284597635269\n",
            "Epoch: 831, Len of Training loss: 12, Average loss: 5.699284752209981\n",
            "Len of Validation loss: 40, Average loss: 6.762664413452148\n",
            "Epoch: 832, Len of Training loss: 12, Average loss: 5.835482398668925\n",
            "Len of Validation loss: 40, Average loss: 6.846324300765991\n",
            "Epoch: 833, Len of Training loss: 12, Average loss: 5.783379952112834\n",
            "Len of Validation loss: 40, Average loss: 7.4510176420211796\n",
            "Epoch: 834, Len of Training loss: 12, Average loss: 5.850702206293742\n",
            "Len of Validation loss: 40, Average loss: 6.8702386021614075\n",
            "Epoch: 835, Len of Training loss: 12, Average loss: 5.726070205370585\n",
            "Len of Validation loss: 40, Average loss: 8.043589329719543\n",
            "Epoch: 836, Len of Training loss: 12, Average loss: 5.708895683288574\n",
            "Len of Validation loss: 40, Average loss: 6.508936721086502\n",
            "Epoch: 837, Len of Training loss: 12, Average loss: 5.742399891217549\n",
            "Len of Validation loss: 40, Average loss: 6.432887178659439\n",
            "Epoch: 838, Len of Training loss: 12, Average loss: 5.825631380081177\n",
            "Len of Validation loss: 40, Average loss: 6.79705777168274\n",
            "Epoch: 839, Len of Training loss: 12, Average loss: 5.7817080815633135\n",
            "Len of Validation loss: 40, Average loss: 6.656984508037567\n",
            "Epoch: 840, Len of Training loss: 12, Average loss: 5.681100010871887\n",
            "Len of Validation loss: 40, Average loss: 6.824630469083786\n",
            "Epoch: 841, Len of Training loss: 12, Average loss: 5.6274520556132\n",
            "Len of Validation loss: 40, Average loss: 6.6590047955513\n",
            "Epoch: 842, Len of Training loss: 12, Average loss: 5.664485692977905\n",
            "Len of Validation loss: 40, Average loss: 7.695730268955231\n",
            "Epoch: 843, Len of Training loss: 12, Average loss: 5.9374931653340655\n",
            "Len of Validation loss: 40, Average loss: 7.326121056079865\n",
            "Epoch: 844, Len of Training loss: 12, Average loss: 5.726378719011943\n",
            "Len of Validation loss: 40, Average loss: 6.692432963848114\n",
            "Epoch: 845, Len of Training loss: 12, Average loss: 5.5970918734868365\n",
            "Len of Validation loss: 40, Average loss: 6.590470063686371\n",
            "Epoch: 846, Len of Training loss: 12, Average loss: 5.581754207611084\n",
            "Len of Validation loss: 40, Average loss: 6.6383910298347475\n",
            "Epoch: 847, Len of Training loss: 12, Average loss: 5.574025710423787\n",
            "Len of Validation loss: 40, Average loss: 6.613973772525787\n",
            "Epoch: 848, Len of Training loss: 12, Average loss: 5.758223136266072\n",
            "Len of Validation loss: 40, Average loss: 6.82809037566185\n",
            "Epoch: 849, Len of Training loss: 12, Average loss: 5.831949353218079\n",
            "Len of Validation loss: 40, Average loss: 6.372988146543503\n",
            "Epoch: 850, Len of Training loss: 12, Average loss: 5.755832115809123\n",
            "Len of Validation loss: 40, Average loss: 6.48942511677742\n",
            "Epoch: 851, Len of Training loss: 12, Average loss: 5.91613241036733\n",
            "Len of Validation loss: 40, Average loss: 7.63576403260231\n",
            "Epoch: 852, Len of Training loss: 12, Average loss: 5.815352996190389\n",
            "Len of Validation loss: 40, Average loss: 6.465387612581253\n",
            "Epoch: 853, Len of Training loss: 12, Average loss: 5.688119133313497\n",
            "Len of Validation loss: 40, Average loss: 6.674914205074311\n",
            "Epoch: 854, Len of Training loss: 12, Average loss: 5.724761724472046\n",
            "Len of Validation loss: 40, Average loss: 7.3087383031845095\n",
            "Epoch: 855, Len of Training loss: 12, Average loss: 5.851624806722005\n",
            "Len of Validation loss: 40, Average loss: 7.646800935268402\n",
            "Epoch: 856, Len of Training loss: 12, Average loss: 5.844709038734436\n",
            "Len of Validation loss: 40, Average loss: 8.298624086380006\n",
            "Epoch: 857, Len of Training loss: 12, Average loss: 5.891535361607869\n",
            "Len of Validation loss: 40, Average loss: 7.027171182632446\n",
            "Epoch: 858, Len of Training loss: 12, Average loss: 5.776407957077026\n",
            "Len of Validation loss: 40, Average loss: 7.506460022926331\n",
            "Epoch: 859, Len of Training loss: 12, Average loss: 5.6186416943868\n",
            "Len of Validation loss: 40, Average loss: 7.091616475582123\n",
            "Epoch: 860, Len of Training loss: 12, Average loss: 5.624988993008931\n",
            "Len of Validation loss: 40, Average loss: 6.4873609185218815\n",
            "Epoch: 861, Len of Training loss: 12, Average loss: 5.6067259311676025\n",
            "Len of Validation loss: 40, Average loss: 6.572634470462799\n",
            "Epoch: 862, Len of Training loss: 12, Average loss: 5.661260008811951\n",
            "Len of Validation loss: 40, Average loss: 6.791848820447922\n",
            "Epoch: 863, Len of Training loss: 12, Average loss: 5.636714180310567\n",
            "Len of Validation loss: 40, Average loss: 6.908594602346421\n",
            "Epoch: 864, Len of Training loss: 12, Average loss: 5.71145776907603\n",
            "Len of Validation loss: 40, Average loss: 6.427036458253861\n",
            "Epoch: 865, Len of Training loss: 12, Average loss: 5.700709899266561\n",
            "Len of Validation loss: 40, Average loss: 6.78615140914917\n",
            "Epoch: 866, Len of Training loss: 12, Average loss: 5.763566970825195\n",
            "Len of Validation loss: 40, Average loss: 7.060054624080658\n",
            "Epoch: 867, Len of Training loss: 12, Average loss: 5.702205816904704\n",
            "Len of Validation loss: 40, Average loss: 6.861726987361908\n",
            "Epoch: 868, Len of Training loss: 12, Average loss: 5.938096523284912\n",
            "Len of Validation loss: 40, Average loss: 7.195283949375153\n",
            "Epoch: 869, Len of Training loss: 12, Average loss: 5.7990033229192095\n",
            "Len of Validation loss: 40, Average loss: 6.92205240726471\n",
            "Epoch: 870, Len of Training loss: 12, Average loss: 5.893471439679463\n",
            "Len of Validation loss: 40, Average loss: 7.561996215581894\n",
            "Epoch: 871, Len of Training loss: 12, Average loss: 5.713136076927185\n",
            "Len of Validation loss: 40, Average loss: 8.365947461128235\n",
            "Epoch: 872, Len of Training loss: 12, Average loss: 6.0675318241119385\n",
            "Len of Validation loss: 40, Average loss: 7.099730122089386\n",
            "Epoch: 873, Len of Training loss: 12, Average loss: 5.85291987657547\n",
            "Len of Validation loss: 40, Average loss: 6.449339634180069\n",
            "Epoch: 874, Len of Training loss: 12, Average loss: 5.790255347887675\n",
            "Len of Validation loss: 40, Average loss: 6.552243459224701\n",
            "Epoch: 875, Len of Training loss: 12, Average loss: 5.919014930725098\n",
            "Len of Validation loss: 40, Average loss: 6.4255571365356445\n",
            "Epoch: 876, Len of Training loss: 12, Average loss: 5.534948269526164\n",
            "Len of Validation loss: 40, Average loss: 6.401665806770325\n",
            "Epoch: 877, Len of Training loss: 12, Average loss: 5.726847012837728\n",
            "Len of Validation loss: 40, Average loss: 6.09175277352333\n",
            "Epoch: 878, Len of Training loss: 12, Average loss: 5.770629684130351\n",
            "Len of Validation loss: 40, Average loss: 6.4708637535572056\n",
            "Epoch: 879, Len of Training loss: 12, Average loss: 5.693044503529866\n",
            "Len of Validation loss: 40, Average loss: 6.60362993478775\n",
            "Epoch: 880, Len of Training loss: 12, Average loss: 5.6140449444452925\n",
            "Len of Validation loss: 40, Average loss: 6.980863809585571\n",
            "Epoch: 881, Len of Training loss: 12, Average loss: 5.536834875742595\n",
            "Len of Validation loss: 40, Average loss: 7.243156135082245\n",
            "Epoch: 882, Len of Training loss: 12, Average loss: 5.626722852389018\n",
            "Len of Validation loss: 40, Average loss: 6.494012558460236\n",
            "Epoch: 883, Len of Training loss: 12, Average loss: 5.738358537356059\n",
            "Len of Validation loss: 40, Average loss: 6.3691162109375\n",
            "Epoch: 884, Len of Training loss: 12, Average loss: 5.6313590208689375\n",
            "Len of Validation loss: 40, Average loss: 6.216706532239914\n",
            "Epoch: 885, Len of Training loss: 12, Average loss: 5.6187206109364825\n",
            "Len of Validation loss: 40, Average loss: 6.847484660148621\n",
            "Epoch: 886, Len of Training loss: 12, Average loss: 5.746518611907959\n",
            "Len of Validation loss: 40, Average loss: 7.025408226251602\n",
            "Epoch: 887, Len of Training loss: 12, Average loss: 5.680129448572795\n",
            "Len of Validation loss: 40, Average loss: 6.805499684810639\n",
            "Epoch: 888, Len of Training loss: 12, Average loss: 5.667344729105632\n",
            "Len of Validation loss: 40, Average loss: 6.771103286743164\n",
            "Epoch: 889, Len of Training loss: 12, Average loss: 5.62240735689799\n",
            "Len of Validation loss: 40, Average loss: 6.588069146871566\n",
            "Epoch: 890, Len of Training loss: 12, Average loss: 5.5274884303410845\n",
            "Len of Validation loss: 40, Average loss: 6.153673058748245\n",
            "Epoch: 891, Len of Training loss: 12, Average loss: 5.686497648557027\n",
            "Len of Validation loss: 40, Average loss: 6.27706423997879\n",
            "Epoch: 892, Len of Training loss: 12, Average loss: 5.799770871798198\n",
            "Len of Validation loss: 40, Average loss: 6.380512255430221\n",
            "Epoch: 893, Len of Training loss: 12, Average loss: 6.021564523379008\n",
            "Len of Validation loss: 40, Average loss: 6.06553807258606\n",
            "Epoch: 894, Len of Training loss: 12, Average loss: 5.852382143338521\n",
            "Len of Validation loss: 40, Average loss: 6.368569499254226\n",
            "Epoch: 895, Len of Training loss: 12, Average loss: 5.600130279858907\n",
            "Len of Validation loss: 40, Average loss: 6.837630128860473\n",
            "Epoch: 896, Len of Training loss: 12, Average loss: 5.628920356432597\n",
            "Len of Validation loss: 40, Average loss: 7.218922126293182\n",
            "Epoch: 897, Len of Training loss: 12, Average loss: 5.704065601030986\n",
            "Len of Validation loss: 40, Average loss: 7.7334188461303714\n",
            "Epoch: 898, Len of Training loss: 12, Average loss: 5.673616091410319\n",
            "Len of Validation loss: 40, Average loss: 7.656702721118927\n",
            "Epoch: 899, Len of Training loss: 12, Average loss: 5.595899343490601\n",
            "Len of Validation loss: 40, Average loss: 6.523077356815338\n",
            "Epoch: 900, Len of Training loss: 12, Average loss: 5.616895318031311\n",
            "Len of Validation loss: 40, Average loss: 7.144644355773925\n",
            "Epoch: 901, Len of Training loss: 12, Average loss: 5.573533574740092\n",
            "Len of Validation loss: 40, Average loss: 7.30669584274292\n",
            "Epoch: 902, Len of Training loss: 12, Average loss: 5.562336603800456\n",
            "Len of Validation loss: 40, Average loss: 6.44715576171875\n",
            "Epoch: 903, Len of Training loss: 12, Average loss: 5.474274079004924\n",
            "Len of Validation loss: 40, Average loss: 6.274166464805603\n",
            "Epoch: 904, Len of Training loss: 12, Average loss: 5.632789731025696\n",
            "Len of Validation loss: 40, Average loss: 6.355563873052597\n",
            "Epoch: 905, Len of Training loss: 12, Average loss: 5.527145862579346\n",
            "Len of Validation loss: 40, Average loss: 6.431966263055801\n",
            "Epoch: 906, Len of Training loss: 12, Average loss: 5.774281779925029\n",
            "Len of Validation loss: 40, Average loss: 6.77191509604454\n",
            "Epoch: 907, Len of Training loss: 12, Average loss: 5.784403284390767\n",
            "Len of Validation loss: 40, Average loss: 6.323629933595657\n",
            "Epoch: 908, Len of Training loss: 12, Average loss: 5.502810915311177\n",
            "Len of Validation loss: 40, Average loss: 6.594354635477066\n",
            "Epoch: 909, Len of Training loss: 12, Average loss: 5.697665810585022\n",
            "Len of Validation loss: 40, Average loss: 6.5466272830963135\n",
            "Epoch: 910, Len of Training loss: 12, Average loss: 5.845706780751546\n",
            "Len of Validation loss: 40, Average loss: 7.062216758728027\n",
            "Epoch: 911, Len of Training loss: 12, Average loss: 5.538531323273976\n",
            "Len of Validation loss: 40, Average loss: 7.292212963104248\n",
            "Epoch: 912, Len of Training loss: 12, Average loss: 5.7247501611709595\n",
            "Len of Validation loss: 40, Average loss: 7.731724202632904\n",
            "Epoch: 913, Len of Training loss: 12, Average loss: 5.674526611963908\n",
            "Len of Validation loss: 40, Average loss: 6.904907602071762\n",
            "Epoch: 914, Len of Training loss: 12, Average loss: 5.547230402628581\n",
            "Len of Validation loss: 40, Average loss: 6.950863140821457\n",
            "Epoch: 915, Len of Training loss: 12, Average loss: 5.599477926890056\n",
            "Len of Validation loss: 40, Average loss: 6.691925418376923\n",
            "Epoch: 916, Len of Training loss: 12, Average loss: 5.604026556015015\n",
            "Len of Validation loss: 40, Average loss: 6.139157688617706\n",
            "Epoch: 917, Len of Training loss: 12, Average loss: 5.6090322732925415\n",
            "Len of Validation loss: 40, Average loss: 6.494971907138824\n",
            "Epoch: 918, Len of Training loss: 12, Average loss: 5.4937394460042315\n",
            "Len of Validation loss: 40, Average loss: 6.436943578720093\n",
            "Epoch: 919, Len of Training loss: 12, Average loss: 5.6189310948054\n",
            "Len of Validation loss: 40, Average loss: 6.507534563541412\n",
            "Epoch: 920, Len of Training loss: 12, Average loss: 5.413054505983989\n",
            "Len of Validation loss: 40, Average loss: 6.680584758520126\n",
            "Epoch: 921, Len of Training loss: 12, Average loss: 5.726253509521484\n",
            "Len of Validation loss: 40, Average loss: 7.522446644306183\n",
            "Epoch: 922, Len of Training loss: 12, Average loss: 6.015294869740804\n",
            "Len of Validation loss: 40, Average loss: 7.536253583431244\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Asus\\Documents\\ml\\PIL\\gnn_pil_versions\\pil_gnnv4_attention_hackathon.ipynb Cell 32\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv4_attention_hackathon.ipynb#X43sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv4_attention_hackathon.ipynb#X43sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m3000\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv4_attention_hackathon.ipynb#X43sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# Train.\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv4_attention_hackathon.ipynb#X43sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     train_epoch_losses \u001b[39m=\u001b[39m train(train_loader)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv4_attention_hackathon.ipynb#X43sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, Len of Training loss: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(train_epoch_losses)\u001b[39m}\u001b[39;00m\u001b[39m, Average loss: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mfloat\u001b[39m(np\u001b[39m.\u001b[39msum(train_epoch_losses))\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(train_epoch_losses)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv4_attention_hackathon.ipynb#X43sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     train_losses\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mmean(train_epoch_losses))\n",
            "\u001b[1;32mc:\\Users\\Asus\\Documents\\ml\\PIL\\gnn_pil_versions\\pil_gnnv4_attention_hackathon.ipynb Cell 32\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv4_attention_hackathon.ipynb#X43sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv4_attention_hackathon.ipynb#X43sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m i\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv4_attention_hackathon.ipynb#X43sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataset):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv4_attention_hackathon.ipynb#X43sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m#print(\"misaa\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv4_attention_hackathon.ipynb#X43sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# Training step.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv4_attention_hackathon.ipynb#X43sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     batch \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv4_attention_hackathon.ipynb#X43sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
            "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "\u001b[1;32mc:\\Users\\Asus\\Documents\\ml\\PIL\\gnn_pil_versions\\pil_gnnv4_attention_hackathon.ipynb Cell 32\u001b[0m in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv4_attention_hackathon.ipynb#X43sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv4_attention_hackathon.ipynb#X43sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m     \u001b[39m#print(\"Part: \", self.processed_file_names[1])\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv4_attention_hackathon.ipynb#X43sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m     \u001b[39mif\u001b[39;00m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv4_attention_hackathon.ipynb#X43sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m         data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocessed_dir, \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdata_train_\u001b[39;49m\u001b[39m{\u001b[39;49;00midx\u001b[39m}\u001b[39;49;00m\u001b[39m.pt\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv4_attention_hackathon.ipynb#X43sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m     \u001b[39melif\u001b[39;00m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv4_attention_hackathon.ipynb#X43sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m         data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessed_dir, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata_valid_\u001b[39m\u001b[39m{\u001b[39;00midx\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m'\u001b[39m))\n",
            "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\serialization.py:772\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    769\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    771\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m--> 772\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    773\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    774\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    775\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    776\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n\u001b[0;32m    777\u001b[0m         \u001b[39mwith\u001b[39;00m _open_zipfile_reader(opened_file) \u001b[39mas\u001b[39;00m opened_zipfile:\n",
            "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\serialization.py:78\u001b[0m, in \u001b[0;36m_is_zipfile\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m     75\u001b[0m read_bytes \u001b[39m=\u001b[39m []\n\u001b[0;32m     76\u001b[0m start \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mtell()\n\u001b[1;32m---> 78\u001b[0m byte \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39;49mread(\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     79\u001b[0m \u001b[39mwhile\u001b[39;00m byte \u001b[39m!=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     80\u001b[0m     read_bytes\u001b[39m.\u001b[39mappend(byte)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Metrics recorder per epoch.\n",
        "train_losses = []\n",
        "\n",
        "valid_losses = []\n",
        "valid_losses_corrected = []\n",
        "\n",
        "# Training loop.\n",
        "model.train()\n",
        "for epoch in range(3000):\n",
        "    # Train.\n",
        "    train_epoch_losses = train(train_loader)\n",
        "    print(f\"Epoch: {epoch}, Len of Training loss: {len(train_epoch_losses)}, Average loss: {float(np.sum(train_epoch_losses))/len(train_epoch_losses)}\")\n",
        "    train_losses.append(np.mean(train_epoch_losses))\n",
        "\n",
        "    valid_epoch_losses= evaluate(valid_loader)\n",
        "    print(f\"Len of Validation loss: {len(valid_epoch_losses)}, Average loss: {float(np.sum(valid_epoch_losses))/len(valid_epoch_losses)}\")\n",
        "    valid_losses.append(np.mean(valid_epoch_losses))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7mjcXV3coC4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "fmZa5ypccoC5",
        "outputId": "b97486da-81f4-42b9-9426-1b79eadf16c4"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 500\n",
        "\n",
        "plt.plot(np.linspace(1, num_epochs, num_epochs).astype(int), train_losses[:500], label=\"Training loss\")\n",
        "plt.plot(np.linspace(1, num_epochs, num_epochs).astype(int), valid_losses[:500], label=\"Testing loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "MZ4PODuucoC6",
        "outputId": "0cbd4f5b-a159-462c-d24f-95fde26c0ca0"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 1500\n",
        "\n",
        "plt.plot(np.linspace(1, num_epochs, num_epochs).astype(int), train_losses[:1500], label=\"Training loss\")\n",
        "plt.plot(np.linspace(1, num_epochs, num_epochs).astype(int), valid_losses[:1500], label=\"Testing loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "OmlQIUD1PPOn",
        "outputId": "d908180d-9cc4-4f24-f4d8-10f9e4f62826"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 3000\n",
        "\n",
        "plt.plot(np.linspace(1, num_epochs, num_epochs).astype(int), train_losses[:3000], label=\"Training loss\")\n",
        "plt.plot(np.linspace(1, num_epochs, num_epochs).astype(int), valid_losses[:3000], label=\"Testing loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "iYfRwBEScoC7"
      },
      "outputs": [],
      "source": [
        "torch.save(model,\"GAT_1.5k_2D.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uy1xuS5ZcoC8",
        "outputId": "7e7910d8-3333-484a-d792-63f7c1b09019"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MetaNet(\n",
              "  (input): MetaLayer(\n",
              "    edge_model=EdgeModel(\n",
              "    (edge_mlp): Sequential(\n",
              "      (0): Linear(in_features=26, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "  ),\n",
              "    node_model=NodeModel(\n",
              "    (node_mlp_1): Sequential(\n",
              "      (0): Linear(in_features=139, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "    (node_mlp_2): Sequential(\n",
              "      (0): Linear(in_features=139, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "  ),\n",
              "    global_model=None\n",
              "  )\n",
              "  (output): MetaLayer(\n",
              "    edge_model=EdgeModel(\n",
              "    (edge_mlp): Sequential(\n",
              "      (0): Linear(in_features=384, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "  ),\n",
              "    node_model=NodeModel(\n",
              "    (node_mlp_1): Sequential(\n",
              "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "    (node_mlp_2): Sequential(\n",
              "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
              "    )\n",
              "  ),\n",
              "    global_model=None\n",
              "  )\n",
              "  (attention): MultiheadAttention(\n",
              "    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "modelv2 = torch.load(\"GAT_1.5k_2D.pth\")\n",
        "modelv2.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3JS5k_jcoC8",
        "outputId": "eb9b9b80-e924-4cf8-d09f-ff5dd9fb0390"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Len of Validation loss: 120, Average loss: 5.395848728219668\n"
          ]
        }
      ],
      "source": [
        "#evaluate the model on the test set\n",
        "test_epoch_losses= evaluate(test_loader)\n",
        "print(f\"Len of Validation loss: {len(test_epoch_losses)}, Average loss: {float(np.sum(test_epoch_losses))/len(test_epoch_losses)}\")\n",
        "valid_losses.append(np.mean(test_epoch_losses))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20xjqvi0coC9",
        "outputId": "33642ef0-d4fc-41bb-d3f5-556a2d9fbe49"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(120,)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(np.array(test_epoch_losses)).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYnsCHSpcoC-",
        "outputId": "4b425321-d360-48af-f9df-a05251ff4f40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.395848728219668\n",
            "2.2373759529341495\n",
            "2.3228966245228535\n"
          ]
        }
      ],
      "source": [
        "#calculate the mean squared error for the test set\n",
        "print(np.mean(test_epoch_losses))\n",
        "#calculate the mean absolute error for the test set\n",
        "print(np.mean(np.sqrt(test_epoch_losses)))\n",
        "#calculate the root mean squared error for the test set\n",
        "print(np.sqrt(np.mean(test_epoch_losses)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2aUkX4uMe7x",
        "outputId": "63d3bacd-7b0b-4f3a-a5d5-b5bc3c1ce615"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "38.85881631488381\n",
            "5.395848728219668\n",
            "6.233684008263798\n"
          ]
        }
      ],
      "source": [
        "squared_loss = [i**2 for i in test_epoch_losses]\n",
        "print(np.mean(squared_loss))\n",
        "#calculate the mean absolute error for the test set\n",
        "print(np.mean(np.sqrt(squared_loss)))\n",
        "#calculate the root mean squared error for the test set\n",
        "print(np.sqrt(np.mean(squared_loss)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "Fhr_Bt5CIwfy",
        "outputId": "a1571cdb-f523-48f9-90dd-558f6973fa0c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "np.set_printoptions(suppress=True)\n",
        "for data in test_loader:\n",
        "    #create a 3d plot of the access points which has node_type = 0 and the stations which has node_type=1\n",
        "    #and the edges between them\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    x = data.x[:,0].cpu().detach().numpy()\n",
        "    y = data.x[:,1].cpu().detach().numpy()\n",
        "    node_type = data.x[:,10].cpu().detach().numpy()\n",
        "    ax.scatter(x[node_type==0], y[node_type==0], c='r', marker='o')\n",
        "    ax.scatter(x[node_type==1], y[node_type==1], c='b', marker='o')\n",
        "    for i in range(len(data.edge_index[0])):\n",
        "        if data.x[ data.edge_index[0][i],10] == 0 and data.x[ data.edge_index[1][i],10] == 1:\n",
        "            x_values = [data.x[ data.edge_index[0][i],0],data.x[ data.edge_index[1][i],0]]\n",
        "            y_values = [data.x[ data.edge_index[0][i],1],data.x[ data.edge_index[1][i],1]]\n",
        "            ax.plot(x_values, y_values)\n",
        "    plt.show()\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3CHUFo5coC_"
      },
      "outputs": [],
      "source": [
        "# #evaluate model and predict on the test set\n",
        "# np.set_printoptions(suppress=True)\n",
        "# #dont print tensor in scientific notation\n",
        "# torch.set_printoptions(sci_mode=False)\n",
        "# modelv2.eval()\n",
        "# for data in test_loader:\n",
        "#     # print(data.shape)\n",
        "#     out = modelv2(data.to(device))\n",
        "#     # #print the predicted values and the actual values side by side for comparison\n",
        "\n",
        "#     print(out)\n",
        "#     print(data.y)\n",
        "#     break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku68BCZicoDA",
        "outputId": "77816810-2981-4950-f0a6-d5b44a6049ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE: 1.6629234308900376\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Set the model in evaluation mode\n",
        "modelv2.eval()\n",
        "\n",
        "# Initialize variables\n",
        "mse = 0.0\n",
        "total_samples = 0\n",
        "\n",
        "# Iterate over the test loader batches\n",
        "for data in test_loader:\n",
        "    # Pass the data through the model\n",
        "    out = modelv2(data.to(device))\n",
        "\n",
        "    # Calculate squared differences\n",
        "    squared_diff = torch.pow(out.view(-1) - data.y.view(-1), 2)\n",
        "    # print(torch.cat((out.view(-1,1),data.y.view(-1,1)),1))\n",
        "\n",
        "    # Accumulate the squared differences\n",
        "    mse += torch.sum(squared_diff).item()\n",
        "\n",
        "    # Update the total number of samples\n",
        "    total_samples += len(data.y)\n",
        "\n",
        "# Calculate the mean squared error\n",
        "mse /= total_samples\n",
        "\n",
        "# Calculate the root mean squared error\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(f\"RMSE: {rmse}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4rY-r30coDB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SecW8YxzcoDC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRlb8Zs-coDC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxJbwLD4coDD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJkPXM-fcoDD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4koFooORcoDD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfGznCq1coDD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oo2nLp2WcoDH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
