{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-06T10:08:11.655513Z",
          "iopub.status.busy": "2023-07-06T10:08:11.655094Z",
          "iopub.status.idle": "2023-07-06T10:10:02.592058Z",
          "shell.execute_reply": "2023-07-06T10:10:02.590896Z",
          "shell.execute_reply.started": "2023-07-06T10:08:11.655481Z"
        },
        "id": "8vHlenqqcoB8",
        "outputId": "06395dcf-e436-453e-c63d-6b10d2cabd64",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu117\n",
            "Collecting torch==1.13.1+cu117\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torch-1.13.1%2Bcu117-cp310-cp310-linux_x86_64.whl (1801.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m705.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.14.1+cu117\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torchvision-0.14.1%2Bcu117-cp310-cp310-linux_x86_64.whl (24.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.3/24.3 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.13.1\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torchaudio-0.13.1%2Bcu117-cp310-cp310-linux_x86_64.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m106.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1+cu117) (4.6.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1+cu117) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1+cu117) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1+cu117) (8.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (3.4)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.15.2+cu118\n",
            "    Uninstalling torchvision-0.15.2+cu118:\n",
            "      Successfully uninstalled torchvision-0.15.2+cu118\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.0.2+cu118\n",
            "    Uninstalling torchaudio-2.0.2+cu118:\n",
            "      Successfully uninstalled torchaudio-2.0.2+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.13.1+cu117 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.13.1+cu117 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.13.1+cu117 torchaudio-0.13.1+cu117 torchvision-0.14.1+cu117\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-06T10:10:02.596163Z",
          "iopub.status.busy": "2023-07-06T10:10:02.595858Z",
          "iopub.status.idle": "2023-07-06T10:10:17.089867Z",
          "shell.execute_reply": "2023-07-06T10:10:17.088572Z",
          "shell.execute_reply.started": "2023-07-06T10:10:02.596135Z"
        },
        "id": "iZgyOuL7coCG",
        "outputId": "fc55430e-7b62-4608-db80-0124629de75c",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchdata==0.5.1\n",
            "  Downloading torchdata-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchtext==0.14.1\n",
            "  Downloading torchtext-0.14.1-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.5.1) (1.26.16)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata==0.5.1) (2.27.1)\n",
            "Collecting portalocker>=2.0.0 (from torchdata==0.5.1)\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.5.1) (1.13.1+cu117)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.14.1) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.14.1) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->torchdata==0.5.1) (4.6.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.5.1) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.5.1) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.5.1) (3.4)\n",
            "Installing collected packages: portalocker, torchtext, torchdata\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.15.2\n",
            "    Uninstalling torchtext-0.15.2:\n",
            "      Successfully uninstalled torchtext-0.15.2\n",
            "  Attempting uninstall: torchdata\n",
            "    Found existing installation: torchdata 0.6.1\n",
            "    Uninstalling torchdata-0.6.1:\n",
            "      Successfully uninstalled torchdata-0.6.1\n",
            "Successfully installed portalocker-2.7.0 torchdata-0.5.1 torchtext-0.14.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torchdata==0.5.1 torchtext==0.14.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-06T10:10:17.097502Z",
          "iopub.status.busy": "2023-07-06T10:10:17.095299Z",
          "iopub.status.idle": "2023-07-06T10:10:31.268254Z",
          "shell.execute_reply": "2023-07-06T10:10:31.267128Z",
          "shell.execute_reply.started": "2023-07-06T10:10:17.097463Z"
        },
        "id": "fgIfr5w2coCI",
        "outputId": "e942d4bd-ad5e-441a-ad39-65b9c0ba91f2",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-1.13.1+cu117.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu117/torch_scatter-2.1.1%2Bpt113cu117-cp310-cp310-linux_x86_64.whl (10.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.1+pt113cu117\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.13.1+cu117.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-06T10:10:31.271935Z",
          "iopub.status.busy": "2023-07-06T10:10:31.271551Z",
          "iopub.status.idle": "2023-07-06T10:10:56.721495Z",
          "shell.execute_reply": "2023-07-06T10:10:56.720245Z",
          "shell.execute_reply.started": "2023-07-06T10:10:31.271896Z"
        },
        "id": "JgVl6k43coCK",
        "outputId": "62ca1a53-3d1c-451e-d576-31a8c521fa39",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch_geometric\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910459 sha256=21cee603fa4ba74d08a1601ec3477e2e20660967441c6c89695b7faeed7b47a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "Successfully built torch_geometric\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T10:10:56.724219Z",
          "iopub.status.busy": "2023-07-06T10:10:56.723782Z",
          "iopub.status.idle": "2023-07-06T10:10:59.243881Z",
          "shell.execute_reply": "2023-07-06T10:10:59.242716Z",
          "shell.execute_reply.started": "2023-07-06T10:10:56.724166Z"
        },
        "id": "yN1FByVgcoCL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GCNConv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv,GATv2Conv\n",
        "from torch_scatter import scatter_mean\n",
        "from torch_geometric.data import InMemoryDataset, download_url, extract_zip\n",
        "from torch_geometric.nn import MetaLayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UCLVQJjrfGZ",
        "outputId": "31ec02f5-bde0-462f-a121-ac14a3fbe18e"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-06T10:20:24.423030Z",
          "iopub.status.busy": "2023-07-06T10:20:24.422651Z",
          "iopub.status.idle": "2023-07-06T10:20:24.430269Z",
          "shell.execute_reply": "2023-07-06T10:20:24.429163Z",
          "shell.execute_reply.started": "2023-07-06T10:20:24.423000Z"
        },
        "id": "0Pzb-vSNcoCV",
        "outputId": "add690aa-44aa-4136-a97f-e67022ca4f2a",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available()  else \"cpu\" )\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T10:13:51.804937Z",
          "iopub.status.busy": "2023-07-06T10:13:51.804545Z",
          "iopub.status.idle": "2023-07-06T10:13:51.811195Z",
          "shell.execute_reply": "2023-07-06T10:13:51.810142Z",
          "shell.execute_reply.started": "2023-07-06T10:13:51.804907Z"
        },
        "id": "_Pj8h3HecoCW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def get_int_map(dep):\n",
        "    dep = df.loc[df[\"deployment\"]==dep]\n",
        "    dep = dep.reset_index(drop=True)\n",
        "    return dep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "k3RVPftNcoCX"
      },
      "outputs": [],
      "source": [
        "# t1 = get_int_map(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T10:10:59.246051Z",
          "iopub.status.busy": "2023-07-06T10:10:59.245394Z",
          "iopub.status.idle": "2023-07-06T10:11:00.452826Z",
          "shell.execute_reply": "2023-07-06T10:11:00.451791Z",
          "shell.execute_reply.started": "2023-07-06T10:10:59.246013Z"
        },
        "id": "R2o8JJX7coCZ",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/required_format.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Asus\\Documents\\ml\\PIL\\gnn_pil_versions\\pil_gnnv4_attention.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv4_attention.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39m/content/required_format.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
            "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
            "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
            "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m     f,\n\u001b[0;32m   1219\u001b[0m     mode,\n\u001b[0;32m   1220\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1221\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1222\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1223\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1224\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1225\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1226\u001b[0m )\n\u001b[0;32m   1227\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
            "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    790\u001b[0m             handle,\n\u001b[0;32m    791\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    792\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    793\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    794\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    795\u001b[0m         )\n\u001b[0;32m    796\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/required_format.csv'"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"/content/required_format.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "execution": {
          "iopub.execute_input": "2023-07-06T10:11:00.454755Z",
          "iopub.status.busy": "2023-07-06T10:11:00.454402Z",
          "iopub.status.idle": "2023-07-06T10:11:00.497714Z",
          "shell.execute_reply": "2023-07-06T10:11:00.496795Z",
          "shell.execute_reply.started": "2023-07-06T10:11:00.454723Z"
        },
        "id": "K26GfkcfcoCa",
        "outputId": "720a030c-3e5a-46cc-d33b-42855dabffd9",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c3e94311-c14f-489d-9ae5-449424caea82\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>primary_channel</th>\n",
              "      <th>min_channel_allowed</th>\n",
              "      <th>max_channel_allowed</th>\n",
              "      <th>rssi</th>\n",
              "      <th>node_type</th>\n",
              "      <th>sinr</th>\n",
              "      <th>air_time_mean</th>\n",
              "      <th>deployment</th>\n",
              "      <th>channel_bonding_model</th>\n",
              "      <th>throughput</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-79.65</td>\n",
              "      <td>-93.86</td>\n",
              "      <td>-109.01</td>\n",
              "      <td>-78.68</td>\n",
              "      <td>-85.3</td>\n",
              "      <td>-98.24</td>\n",
              "      <td>-108.79</td>\n",
              "      <td>-97.21</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>-55.42</td>\n",
              "      <td>0</td>\n",
              "      <td>35.68</td>\n",
              "      <td>25.15</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>104.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-79.65</td>\n",
              "      <td>-93.86</td>\n",
              "      <td>-109.01</td>\n",
              "      <td>-78.68</td>\n",
              "      <td>-85.3</td>\n",
              "      <td>-98.24</td>\n",
              "      <td>-108.79</td>\n",
              "      <td>-97.21</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>-62.31</td>\n",
              "      <td>1</td>\n",
              "      <td>26.99</td>\n",
              "      <td>25.15</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>7.68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-79.65</td>\n",
              "      <td>-93.86</td>\n",
              "      <td>-109.01</td>\n",
              "      <td>-78.68</td>\n",
              "      <td>-85.3</td>\n",
              "      <td>-98.24</td>\n",
              "      <td>-108.79</td>\n",
              "      <td>-97.21</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>-55.42</td>\n",
              "      <td>1</td>\n",
              "      <td>35.68</td>\n",
              "      <td>25.15</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>11.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-79.65</td>\n",
              "      <td>-93.86</td>\n",
              "      <td>-109.01</td>\n",
              "      <td>-78.68</td>\n",
              "      <td>-85.3</td>\n",
              "      <td>-98.24</td>\n",
              "      <td>-108.79</td>\n",
              "      <td>-97.21</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>-58.23</td>\n",
              "      <td>1</td>\n",
              "      <td>33.42</td>\n",
              "      <td>25.15</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>14.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-79.65</td>\n",
              "      <td>-93.86</td>\n",
              "      <td>-109.01</td>\n",
              "      <td>-78.68</td>\n",
              "      <td>-85.3</td>\n",
              "      <td>-98.24</td>\n",
              "      <td>-108.79</td>\n",
              "      <td>-97.21</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>-66.64</td>\n",
              "      <td>1</td>\n",
              "      <td>26.83</td>\n",
              "      <td>25.15</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>11.95</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 27 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c3e94311-c14f-489d-9ae5-449424caea82')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c3e94311-c14f-489d-9ae5-449424caea82 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c3e94311-c14f-489d-9ae5-449424caea82');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Unnamed: 0    0      1      2       3      4     5      6       7      8  \\\n",
              "0           0  0.0 -79.65 -93.86 -109.01 -78.68 -85.3 -98.24 -108.79 -97.21   \n",
              "1           1  0.0 -79.65 -93.86 -109.01 -78.68 -85.3 -98.24 -108.79 -97.21   \n",
              "2           2  0.0 -79.65 -93.86 -109.01 -78.68 -85.3 -98.24 -108.79 -97.21   \n",
              "3           3  0.0 -79.65 -93.86 -109.01 -78.68 -85.3 -98.24 -108.79 -97.21   \n",
              "4           4  0.0 -79.65 -93.86 -109.01 -78.68 -85.3 -98.24 -108.79 -97.21   \n",
              "\n",
              "   ...  primary_channel  min_channel_allowed  max_channel_allowed   rssi  \\\n",
              "0  ...                0                    0                    3 -55.42   \n",
              "1  ...                0                    0                    3 -62.31   \n",
              "2  ...                0                    0                    3 -55.42   \n",
              "3  ...                0                    0                    3 -58.23   \n",
              "4  ...                0                    0                    3 -66.64   \n",
              "\n",
              "   node_type   sinr  air_time_mean  deployment  channel_bonding_model  \\\n",
              "0          0  35.68          25.15         0.0                      4   \n",
              "1          1  26.99          25.15         0.0                      4   \n",
              "2          1  35.68          25.15         0.0                      4   \n",
              "3          1  33.42          25.15         0.0                      4   \n",
              "4          1  26.83          25.15         0.0                      4   \n",
              "\n",
              "   throughput  \n",
              "0      104.96  \n",
              "1        7.68  \n",
              "2       11.09  \n",
              "3       14.51  \n",
              "4       11.95  \n",
              "\n",
              "[5 rows x 27 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ndx5diS21Fh3",
        "outputId": "3308b624-b3cb-4b9f-d501-8ddb1e32eee7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "306424"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgaLlH7tvCu1"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T10:28:27.630716Z",
          "iopub.status.busy": "2023-07-06T10:28:27.630364Z",
          "iopub.status.idle": "2023-07-06T10:28:27.648692Z",
          "shell.execute_reply": "2023-07-06T10:28:27.647731Z",
          "shell.execute_reply.started": "2023-07-06T10:28:27.630687Z"
        },
        "id": "-bXLwx5ycoCb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Creating individual graphs\n",
        "# This assumes all APs and STAs are connected to each other\n",
        "def create_graph(split,split_y,deployment):\n",
        "    dep = get_int_map(deployment)\n",
        "    dep_y = dep[\"throughput\"]\n",
        "    dep_x = dep[['0', '1', '2', '3', '4', '5', '6', '7','8', '9', '10', '11', 'wlan_code_index', 'x(m)', 'y(m)','z(m)',\n",
        "            'primary_channel', 'min_channel_allowed', 'max_channel_allowed', 'rssi', 'node_type',\n",
        "            'sinr', 'air_time_mean', 'deployment',\"channel_bonding_model\"]]\n",
        "    #print(dep_x)\n",
        "    dep_reset = dep.reset_index(drop=True)\n",
        "    ap_index = {}\n",
        "    out = dep_reset[dep_reset[\"node_type\"] == 0]\n",
        "    for i in range(len(out)):\n",
        "        ap_index[out.index[i]] = i\n",
        "    #print(ap_index)\n",
        "    node_features = dep_x.iloc[:,12:].values\n",
        "    # print(node_features)\n",
        "    #edge_features = dep.iloc[:,:12].values - here each node has been given an edge feature\n",
        "    # need to give each edge an edge feature\n",
        "    node_targets = dep_y.values\n",
        "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
        "    print(node_features.shape)\n",
        "    #edge_features = torch.tensor(edge_features, dtype=torch.float)\n",
        "    node_targets = torch.tensor(node_targets, dtype=torch.float)\n",
        "    # Add edges here for each deployment\n",
        "    edges = []\n",
        "    edge_features = []\n",
        "    edge_index = []\n",
        "    for i in range(len(dep)):\n",
        "        for j in range(len(dep)):\n",
        "            if (i != j and (dep[\"node_type\"].iloc[i] == 0 and dep[\"node_type\"].iloc[j] == 0)) or (i !=j and (dep[\"node_type\"].iloc[i] == 1 and dep[\"node_type\"].iloc[j] == 0)):\n",
        "                edges.append([i,j])\n",
        "    #print(edges)\n",
        "    edges2=edges\n",
        "    edges = torch.tensor(edges, dtype=torch.float)\n",
        "    #print(\"Edges: \", edges, edges.shape)\n",
        "    # edge_index = torch.tensor(edges, dtype=torch.long)\n",
        "    edge_index = torch.tensor(edges,dtype=torch.long)\n",
        "    edge_index = edge_index.t().contiguous()\n",
        "    #print(edges.detach(), edges.shape)\n",
        "    print(edges.shape[0])\n",
        "    for i in range(edges.shape[0]):\n",
        "        # print(dep.iloc[int(edges[i][0]), ap_index[int(edges[i][1])]])\n",
        "        i_pos = np.asarray([dep.at[edges2[i][0],\"x(m)\"],dep.at[edges2[i][0],\"y(m)\"],dep.at[edges2[i][0],\"z(m)\"]])\n",
        "        j_pos = np.asarray([dep.at[edges2[i][1],\"x(m)\"],dep.at[edges2[i][1],\"y(m)\"],dep.at[edges2[i][1],\"z(m)\"]])\n",
        "        distance = np.linalg.norm(i_pos - j_pos)\n",
        "        #print(i)\n",
        "        # edge_features.append([distance,dep.iloc[int(edges[i][0]), ap_index[int(edges[i][1])]]])\n",
        "        edge_type = 0\n",
        "        if int(edges2[i][0]) in ap_index.keys():\n",
        "          edge_type = 0\n",
        "        else:\n",
        "          edge_type = 1\n",
        "        edge_features.append([edge_type,dep.at[edges2[i][0],\"rssi\"],dep.iloc[int(edges[i][0]), ap_index[int(edges[i][1])]]])\n",
        "    # edge_features = np.array(edge_features)\n",
        "    # print(edge_features)\n",
        "    edge_features = torch.tensor(edge_features, dtype=torch.float)\n",
        "    #print(edge_features, edge_features.shape)\n",
        "    graph = {\n",
        "        \"edges\": edges,\n",
        "        \"edge_index\": edge_index,\n",
        "        \"node_features\": node_features,\n",
        "        \"edge_features\": edge_features,\n",
        "        \"node_targets\": node_targets\n",
        "    }\n",
        "#     print(\"*\"*10)\n",
        "#     print(edges.shape)\n",
        "#     print(edge_index.shape)\n",
        "#     print(node_features.shape)\n",
        "#     print(edge_features.shape)\n",
        "#     print(node_targets.shape)\n",
        "    return graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-06T10:19:03.496718Z",
          "iopub.status.busy": "2023-07-06T10:19:03.496147Z",
          "iopub.status.idle": "2023-07-06T10:19:05.076427Z",
          "shell.execute_reply": "2023-07-06T10:19:05.075388Z",
          "shell.execute_reply.started": "2023-07-06T10:19:03.496690Z"
        },
        "id": "LFkij5z5coCd",
        "outputId": "f742c0fa-42fc-4cdf-8266-b42edef29693",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([170, 13])\n",
            "2028\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'edges': tensor([[  0.,  11.],\n",
              "         [  0.,  27.],\n",
              "         [  0.,  44.],\n",
              "         ...,\n",
              "         [169., 128.],\n",
              "         [169., 139.],\n",
              "         [169., 156.]]),\n",
              " 'edge_index': tensor([[  0,   0,   0,  ..., 169, 169, 169],\n",
              "         [ 11,  27,  44,  ..., 128, 139, 156]]),\n",
              " 'node_features': tensor([[ 0.0000,  7.5000,  8.3333,  ..., 25.1500,  0.0000,  4.0000],\n",
              "         [ 0.0000, 12.0627,  4.6918,  ..., 25.1500,  0.0000,  4.0000],\n",
              "         [ 0.0000,  8.2712,  4.8383,  ..., 25.1500,  0.0000,  4.0000],\n",
              "         ...,\n",
              "         [11.0000, 56.2321, 45.8161,  ..., 25.7725,  0.0000,  4.0000],\n",
              "         [11.0000, 50.4659, 43.8516,  ..., 25.7725,  0.0000,  4.0000],\n",
              "         [11.0000, 46.8457, 36.8127,  ..., 25.7725,  0.0000,  4.0000]]),\n",
              " 'edge_features': tensor([[   0.0000,  -55.4200,    0.0000],\n",
              "         [   0.0000,  -55.4200,  -79.6500],\n",
              "         [   0.0000,  -55.4200,  -93.8600],\n",
              "         ...,\n",
              "         [   1.0000,  -64.6900, -108.8900],\n",
              "         [   1.0000,  -64.6900,  -96.9100],\n",
              "         [   1.0000,  -64.6900,  -76.9500]]),\n",
              " 'node_targets': tensor([104.9600,   7.6800,  11.0900,  14.5100,  11.9500,  10.2400,  11.0900,\n",
              "           9.3900,   3.4100,  13.6500,  11.9500,  52.0500,   5.9700,   0.8500,\n",
              "           5.1200,   0.8500,   5.9700,   5.1200,   7.6800,   5.1200,   0.0000,\n",
              "           7.6800,   0.0000,   0.8500,   3.4100,   1.7100,   1.7100,  40.1100,\n",
              "           0.0000,   3.4100,   1.7100,   1.7100,   7.6800,   1.7100,   2.5600,\n",
              "           1.7100,   2.5600,   0.0000,   1.7100,   3.4100,   1.7100,   3.4100,\n",
              "           3.4100,   3.4100, 125.4400,  10.2400,  16.2100,  11.0900,  14.5100,\n",
              "           5.1200,   2.5600,   7.6800,  10.2400,  12.8000,   7.6800,  15.3600,\n",
              "          11.9500,  34.1300,   0.0000,   5.9700,   0.8500,   3.4100,   0.8500,\n",
              "           2.5600,   2.5600,   8.5300,   4.2700,   0.8500,   0.0000,   4.2700,\n",
              "          18.7700,   0.0000,   2.5600,   1.7100,   2.5600,   2.5600,   0.0000,\n",
              "           4.2700,   0.8500,   1.7100,   0.0000,   2.5600,  44.3700,   2.5600,\n",
              "           2.5600,   2.5600,   3.4100,   5.9700,   0.0000,   1.7100,  11.0900,\n",
              "           0.0000,   0.0000,  10.2400,   0.8500,   0.8500,   2.5600,  27.1700,\n",
              "           1.3300,   0.0000,   0.6700,   0.6700,   0.5100,   3.0400,   0.7600,\n",
              "           0.0000,   0.0000,   2.5600,   0.0000,   4.2700,   5.1200,   2.2800,\n",
              "           5.9700,  90.4500,   5.9700,   9.3900,   5.9700,   8.5300,  11.9500,\n",
              "           2.5600,   6.8300,   8.5300,   5.9700,   2.5600,   1.7100,   6.8300,\n",
              "           6.8300,   6.8300,  74.2400,   3.4100,  14.5100,   6.8300,   1.7100,\n",
              "           3.4100,  14.5100,   8.5300,   5.9700,   6.8300,   8.5300,  32.2800,\n",
              "           0.8500,   1.7100,   2.5600,   0.7100,   0.8500,   0.8500,   0.8500,\n",
              "           3.4100,   1.7100,   4.2700,   5.1200,   4.2700,   1.7100,   0.8500,\n",
              "           1.7100,   0.8500,  78.5100,   2.5600,   0.0000,   1.7100,  10.2400,\n",
              "           3.4100,   8.5300,  12.8000,   1.7100,   8.5300,  17.0700,   6.8300,\n",
              "           5.1200,   0.0000])}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "create_graph(0,0,0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T10:19:05.079352Z",
          "iopub.status.busy": "2023-07-06T10:19:05.078366Z",
          "iopub.status.idle": "2023-07-06T10:19:05.085230Z",
          "shell.execute_reply": "2023-07-06T10:19:05.083975Z",
          "shell.execute_reply.started": "2023-07-06T10:19:05.079300Z"
        },
        "id": "ZO3ljwu7coCe",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def create_geometric_graph(graph):\n",
        "    data = Data(\n",
        "        # Input graph.\n",
        "        x=graph[\"node_features\"],\n",
        "        #pos=pos,\n",
        "        edge_index=graph[\"edge_index\"],\n",
        "        edge_attr=graph[\"edge_features\"],\n",
        "        # Output node targets.\n",
        "        y=graph[\"node_targets\"],\n",
        "        num_nodes = len(graph[\"node_features\"])\n",
        "\n",
        "    )\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-06T10:19:05.087282Z",
          "iopub.status.busy": "2023-07-06T10:19:05.086872Z",
          "iopub.status.idle": "2023-07-06T10:19:06.732846Z",
          "shell.execute_reply": "2023-07-06T10:19:06.731610Z",
          "shell.execute_reply.started": "2023-07-06T10:19:05.087249Z"
        },
        "id": "Kr7yzgfocoCf",
        "outputId": "9dda76b5-65f3-4206-98e7-345c47cb0bee",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([170, 13])\n",
            "2028\n",
            "Data(x=[170, 13], edge_index=[2, 2028], edge_attr=[2028, 3], y=[170], num_nodes=170)\n"
          ]
        }
      ],
      "source": [
        "print(create_geometric_graph(create_graph(0,0,0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYREKS0xfrSb",
        "outputId": "6d232337-d9f5-405d-bc7b-0ca929f19e0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/train': No such file or directory\n",
            "rm: cannot remove '/content/valid': No such file or directory\n",
            "rm: cannot remove '/content/test': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!rm -r /content/train\n",
        "!rm -r /content/valid\n",
        "!rm -r /content/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8mkalUKflgL"
      },
      "outputs": [],
      "source": [
        "# With train, validation and test data.\n",
        "import random\n",
        "import os\n",
        "from torch_geometric.data import InMemoryDataset, download_url, extract_zip\n",
        "# divide into training and testing points\n",
        "class CustomDataset(InMemoryDataset):\n",
        "    def __init__(self, split=\"train\", transform=None):\n",
        "        self.data = pd.read_csv(\"/content/required_format.csv\")\n",
        "        self.split = split\n",
        "        super(CustomDataset, self).__init__( split, transform)\n",
        "        #self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "        #self.data = pd.read_csv(\"deployment_with_int_map.csv\")\n",
        "        #self.data, self.slices = pd.read_csv(\"deployment_with_int_map.csv\")\n",
        "\n",
        "        # print(\"In init\")\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        # print(\"In raw_file_names\")\n",
        "        return [\"/content/required_format.csv\"]\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        # print(\"In processed_file_names\")\n",
        "        li = ['data_train_' + str(i) + '.pt' for i in range(1152)]+ ['data_valid_' + str(j) + '.pt' for j in range(1152, 1536)] + ['data_test_' + str(k) + '.pt' for k in range(1536, 1920)]\n",
        "        #print(li)\n",
        "        return ['data_train_' + str(i) + '.pt' for i in range(1152)]+ ['data_valid_' + str(j) + '.pt' for j in range(1152,1536)] + ['data_test_' + str(k) + '.pt' for k in range(384)]\n",
        "\n",
        "    def _download(self):\n",
        "        '''\n",
        "        print(\"In download\")\n",
        "        path = download_url(self.url, self.raw_dir)\n",
        "        extract_zip(path, self.raw_dir)\n",
        "        # The zip file is removed\n",
        "        os.unlink(path)\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "    def process(self):\n",
        "        print(\"In process\")\n",
        "        #df = pd.read_csv(self.raw_paths[0])\n",
        "        X = self.data[['0', '1', '2', '3', '4', '5', '6', '7','8', '9', '10', '11', 'wlan_code_index', 'x(m)', 'y(m)','z(m)',\n",
        "            'primary_channel', 'min_channel_allowed', 'max_channel_allowed', 'node_type','rssi',\n",
        "            'sinr', 'air_time_mean','channel_bonding_model','deployment']]\n",
        "        y = self.data.loc[:, [\"throughput\", \"deployment\"]]\n",
        "#         X_train = X.iloc[:183854, :]\n",
        "#         X_valid = X.iloc[183854:245139, :]\n",
        "#         X_test = X.iloc[245139:,:]\n",
        "#         print(X_test.columns)\n",
        "#         y_train = y.iloc[:183854, :]\n",
        "#         y_valid = y.iloc[183854:245139, :]\n",
        "#         y_test = y.iloc[245139:,:]\n",
        "        graphs = []\n",
        "        # print(\"Here\")\n",
        "        l = [i for i in range(1920)]\n",
        "        self.l_train = random.sample(l, 1152)\n",
        "        l = [x for x in l if x not in self.l_train]\n",
        "        self.l_valid = random.sample(l, 384)\n",
        "        l = [x for x in l if x not in self.l_valid]\n",
        "        self.l_test = l\n",
        "        count = 0\n",
        "        if(self.split == \"train\"):\n",
        "\n",
        "            for i in self.l_train:\n",
        "\n",
        "                #X_train = torch.tensor(X_train.to_numpy(), dtype=torch.float)\n",
        "                #y_train = torch.tensor(y_train.to_numpy(), dtype=torch.float)\n",
        "                graph = create_graph(X, y, i)\n",
        "\n",
        "                graph = create_geometric_graph(graph)\n",
        "                graphs.append(graph)\n",
        "\n",
        "                torch.save(graph, os.path.join(self.processed_dir, f'data_train_{count}.pt'))\n",
        "                count += 1\n",
        "        elif(self.split == \"valid\"):\n",
        "            for i in self.l_valid:\n",
        "                #X_test = torch.tensor(X_test.to_numpy(), dtype=torch.float)\n",
        "                #y_test = torch.tensor(y_test.to_numpy(), dtype=torch.float)\n",
        "                graph = create_graph(X, y, i)\n",
        "                graph = create_geometric_graph(graph)\n",
        "                graphs.append(graph)\n",
        "\n",
        "                torch.save(graph, os.path.join(self.processed_dir, f'data_valid_{count}.pt'))\n",
        "                count += 1\n",
        "        else:\n",
        "            for i in self.l_test:\n",
        "                #X_test = torch.tensor(X_test.to_numpy(), dtype=torch.float)\n",
        "                #y_test = torch.tensor(y_test.to_numpy(), dtype=torch.float)\n",
        "                graph = create_graph(X, y, i)\n",
        "                graph = create_geometric_graph(graph)\n",
        "                graphs.append(graph)\n",
        "\n",
        "                torch.save(graph, os.path.join(self.processed_dir, f'data_test_{count}.pt'))\n",
        "                count += 1\n",
        "        #return graphs[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        if(self.split == \"train\"):\n",
        "            #return len(self.processed_file_names[0])\n",
        "            return 1152\n",
        "        elif self.split == \"valid\":\n",
        "            return 384\n",
        "        else:\n",
        "            return 384\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #print(\"Part: \", self.processed_file_names[1])\n",
        "\n",
        "        if(self.split == \"train\"):\n",
        "            data = torch.load(os.path.join(self.processed_dir, f'data_train_{idx}.pt'))\n",
        "        elif(self.split == \"valid\"):\n",
        "            data = torch.load(os.path.join(self.processed_dir, f'data_valid_{idx}.pt'))\n",
        "        elif (self.split==\"test\"):\n",
        "            data = torch.load(os.path.join(self.processed_dir, f'data_test_{idx}.pt'))\n",
        "        return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnZO7d9AfqCN",
        "outputId": "4e33e531-2167-42a8-9242-bd04305c63fc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In process\n",
            "torch.Size([175, 13])\n",
            "1740\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([176, 13])\n",
            "2100\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([196, 13])\n",
            "2340\n",
            "torch.Size([168, 13])\n",
            "1670\n",
            "torch.Size([170, 13])\n",
            "2028\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([122, 13])\n",
            "968\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([144, 13])\n",
            "1144\n",
            "torch.Size([151, 13])\n",
            "1500\n",
            "torch.Size([131, 13])\n",
            "1040\n",
            "torch.Size([196, 13])\n",
            "2340\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([203, 13])\n",
            "2424\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([131, 13])\n",
            "1040\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([115, 13])\n",
            "912\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([139, 13])\n",
            "1104\n",
            "torch.Size([145, 13])\n",
            "1152\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([147, 13])\n",
            "1460\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([106, 13])\n",
            "840\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([174, 13])\n",
            "1730\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([168, 13])\n",
            "1670\n",
            "torch.Size([208, 13])\n",
            "2484\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([178, 13])\n",
            "2124\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([182, 13])\n",
            "1810\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([140, 13])\n",
            "1112\n",
            "torch.Size([169, 13])\n",
            "2016\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([201, 13])\n",
            "2400\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([149, 13])\n",
            "1480\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([192, 13])\n",
            "2292\n",
            "torch.Size([207, 13])\n",
            "2472\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([170, 13])\n",
            "1690\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([141, 13])\n",
            "1120\n",
            "torch.Size([141, 13])\n",
            "1400\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([220, 13])\n",
            "2628\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([202, 13])\n",
            "2412\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([148, 13])\n",
            "1470\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([180, 13])\n",
            "2148\n",
            "torch.Size([178, 13])\n",
            "1770\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([110, 13])\n",
            "872\n",
            "torch.Size([139, 13])\n",
            "1104\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([182, 13])\n",
            "2172\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([144, 13])\n",
            "1430\n",
            "torch.Size([142, 13])\n",
            "1128\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([212, 13])\n",
            "2532\n",
            "torch.Size([146, 13])\n",
            "1450\n",
            "torch.Size([142, 13])\n",
            "1128\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([112, 13])\n",
            "888\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([170, 13])\n",
            "1690\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([168, 13])\n",
            "2004\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([157, 13])\n",
            "1560\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([141, 13])\n",
            "1120\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([116, 13])\n",
            "920\n",
            "torch.Size([117, 13])\n",
            "928\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([170, 13])\n",
            "1690\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([157, 13])\n",
            "1560\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([198, 13])\n",
            "2364\n",
            "torch.Size([148, 13])\n",
            "1470\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([146, 13])\n",
            "1450\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([140, 13])\n",
            "1112\n",
            "torch.Size([151, 13])\n",
            "1500\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([180, 13])\n",
            "2148\n",
            "torch.Size([117, 13])\n",
            "928\n",
            "torch.Size([208, 13])\n",
            "2484\n",
            "torch.Size([176, 13])\n",
            "1750\n",
            "torch.Size([174, 13])\n",
            "1730\n",
            "torch.Size([179, 13])\n",
            "2136\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([181, 13])\n",
            "2160\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([183, 13])\n",
            "2184\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([192, 13])\n",
            "2292\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([116, 13])\n",
            "920\n",
            "torch.Size([178, 13])\n",
            "2124\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([176, 13])\n",
            "2100\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([215, 13])\n",
            "2568\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([142, 13])\n",
            "1410\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([116, 13])\n",
            "920\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([197, 13])\n",
            "2352\n",
            "torch.Size([166, 13])\n",
            "1980\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([143, 13])\n",
            "1420\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([183, 13])\n",
            "2184\n",
            "torch.Size([198, 13])\n",
            "2364\n",
            "torch.Size([180, 13])\n",
            "2148\n",
            "torch.Size([122, 13])\n",
            "968\n",
            "torch.Size([183, 13])\n",
            "2184\n",
            "torch.Size([122, 13])\n",
            "968\n",
            "torch.Size([145, 13])\n",
            "1152\n",
            "torch.Size([144, 13])\n",
            "1430\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([147, 13])\n",
            "1460\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([180, 13])\n",
            "2148\n",
            "torch.Size([148, 13])\n",
            "1470\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([178, 13])\n",
            "2124\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([227, 13])\n",
            "2712\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([139, 13])\n",
            "1380\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([146, 13])\n",
            "1450\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([131, 13])\n",
            "1040\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([167, 13])\n",
            "1992\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([234, 13])\n",
            "2796\n",
            "torch.Size([106, 13])\n",
            "840\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([144, 13])\n",
            "1144\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([182, 13])\n",
            "2172\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([148, 13])\n",
            "1176\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([170, 13])\n",
            "1690\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([148, 13])\n",
            "1470\n",
            "torch.Size([173, 13])\n",
            "2064\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([206, 13])\n",
            "2460\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([181, 13])\n",
            "2160\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([148, 13])\n",
            "1470\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([145, 13])\n",
            "1152\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([178, 13])\n",
            "2124\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([211, 13])\n",
            "2520\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([212, 13])\n",
            "2532\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([107, 13])\n",
            "848\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([131, 13])\n",
            "1040\n",
            "torch.Size([168, 13])\n",
            "1670\n",
            "torch.Size([140, 13])\n",
            "1390\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([148, 13])\n",
            "1470\n",
            "torch.Size([140, 13])\n",
            "1390\n",
            "torch.Size([117, 13])\n",
            "928\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([201, 13])\n",
            "2400\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([157, 13])\n",
            "1560\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([151, 13])\n",
            "1500\n",
            "torch.Size([213, 13])\n",
            "2544\n",
            "torch.Size([117, 13])\n",
            "928\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([122, 13])\n",
            "968\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([183, 13])\n",
            "2184\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([179, 13])\n",
            "2136\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([122, 13])\n",
            "968\n",
            "torch.Size([170, 13])\n",
            "1690\n",
            "torch.Size([115, 13])\n",
            "912\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([199, 13])\n",
            "2376\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([131, 13])\n",
            "1040\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([202, 13])\n",
            "2412\n",
            "torch.Size([183, 13])\n",
            "1820\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([197, 13])\n",
            "2352\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([182, 13])\n",
            "2172\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([199, 13])\n",
            "2376\n",
            "torch.Size([131, 13])\n",
            "1040\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([220, 13])\n",
            "2628\n",
            "torch.Size([157, 13])\n",
            "1560\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([206, 13])\n",
            "2460\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([149, 13])\n",
            "1480\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([163, 13])\n",
            "1944\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([139, 13])\n",
            "1104\n",
            "torch.Size([196, 13])\n",
            "2340\n",
            "torch.Size([224, 13])\n",
            "2676\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([148, 13])\n",
            "1470\n",
            "torch.Size([179, 13])\n",
            "2136\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([192, 13])\n",
            "2292\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([131, 13])\n",
            "1040\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([185, 13])\n",
            "1840\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([139, 13])\n",
            "1380\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([201, 13])\n",
            "2400\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([177, 13])\n",
            "1760\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([164, 13])\n",
            "1956\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([176, 13])\n",
            "1750\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([178, 13])\n",
            "1770\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([196, 13])\n",
            "2340\n",
            "torch.Size([180, 13])\n",
            "2148\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([115, 13])\n",
            "912\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([164, 13])\n",
            "1956\n",
            "torch.Size([199, 13])\n",
            "2376\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([143, 13])\n",
            "1136\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([175, 13])\n",
            "2088\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([144, 13])\n",
            "1430\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([148, 13])\n",
            "1470\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([201, 13])\n",
            "2400\n",
            "torch.Size([192, 13])\n",
            "2292\n",
            "torch.Size([168, 13])\n",
            "1670\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([199, 13])\n",
            "2376\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([129, 13])\n",
            "1280\n",
            "torch.Size([192, 13])\n",
            "2292\n",
            "torch.Size([166, 13])\n",
            "1980\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([149, 13])\n",
            "1480\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([203, 13])\n",
            "2424\n",
            "torch.Size([157, 13])\n",
            "1560\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([108, 13])\n",
            "856\n",
            "torch.Size([180, 13])\n",
            "1790\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([115, 13])\n",
            "912\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([180, 13])\n",
            "2148\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([151, 13])\n",
            "1500\n",
            "torch.Size([138, 13])\n",
            "1370\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([116, 13])\n",
            "920\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([208, 13])\n",
            "2484\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([172, 13])\n",
            "2052\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([173, 13])\n",
            "1720\n",
            "torch.Size([114, 13])\n",
            "904\n",
            "torch.Size([164, 13])\n",
            "1956\n",
            "torch.Size([134, 13])\n",
            "1330\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([149, 13])\n",
            "1480\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([170, 13])\n",
            "1690\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([122, 13])\n",
            "968\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([221, 13])\n",
            "2640\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([179, 13])\n",
            "2136\n",
            "torch.Size([110, 13])\n",
            "872\n",
            "torch.Size([156, 13])\n",
            "1860\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([150, 13])\n",
            "1192\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([113, 13])\n",
            "896\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([172, 13])\n",
            "2052\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([146, 13])\n",
            "1450\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([179, 13])\n",
            "2136\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([141, 13])\n",
            "1120\n",
            "torch.Size([143, 13])\n",
            "1136\n",
            "torch.Size([115, 13])\n",
            "912\n",
            "torch.Size([146, 13])\n",
            "1450\n",
            "torch.Size([196, 13])\n",
            "2340\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([176, 13])\n",
            "1750\n",
            "torch.Size([181, 13])\n",
            "2160\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([197, 13])\n",
            "2352\n",
            "torch.Size([192, 13])\n",
            "2292\n",
            "torch.Size([117, 13])\n",
            "928\n",
            "torch.Size([192, 13])\n",
            "2292\n",
            "torch.Size([168, 13])\n",
            "1670\n",
            "torch.Size([175, 13])\n",
            "1740\n",
            "torch.Size([151, 13])\n",
            "1500\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([131, 13])\n",
            "1040\n",
            "torch.Size([146, 13])\n",
            "1450\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([142, 13])\n",
            "1128\n",
            "torch.Size([157, 13])\n",
            "1560\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([146, 13])\n",
            "1160\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([116, 13])\n",
            "920\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([144, 13])\n",
            "1144\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([198, 13])\n",
            "2364\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([183, 13])\n",
            "2184\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([145, 13])\n",
            "1440\n",
            "torch.Size([148, 13])\n",
            "1470\n",
            "torch.Size([212, 13])\n",
            "2532\n",
            "torch.Size([141, 13])\n",
            "1120\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([142, 13])\n",
            "1128\n",
            "torch.Size([142, 13])\n",
            "1410\n",
            "torch.Size([181, 13])\n",
            "2160\n",
            "torch.Size([174, 13])\n",
            "2076\n",
            "torch.Size([206, 13])\n",
            "2460\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([201, 13])\n",
            "2400\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([146, 13])\n",
            "1450\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([122, 13])\n",
            "968\n",
            "torch.Size([145, 13])\n",
            "1440\n",
            "torch.Size([140, 13])\n",
            "1390\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([111, 13])\n",
            "880\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([113, 13])\n",
            "896\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([115, 13])\n",
            "912\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([170, 13])\n",
            "1690\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([135, 13])\n",
            "1340\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([112, 13])\n",
            "888\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([146, 13])\n",
            "1160\n",
            "torch.Size([137, 13])\n",
            "1088\n",
            "torch.Size([209, 13])\n",
            "2496\n",
            "torch.Size([147, 13])\n",
            "1460\n",
            "torch.Size([172, 13])\n",
            "2052\n",
            "torch.Size([108, 13])\n",
            "856\n",
            "torch.Size([148, 13])\n",
            "1176\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([197, 13])\n",
            "2352\n",
            "torch.Size([139, 13])\n",
            "1104\n",
            "torch.Size([113, 13])\n",
            "896\n",
            "torch.Size([202, 13])\n",
            "2412\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([198, 13])\n",
            "2364\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([137, 13])\n",
            "1088\n",
            "torch.Size([199, 13])\n",
            "2376\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([172, 13])\n",
            "2052\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([175, 13])\n",
            "2088\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([181, 13])\n",
            "2160\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([207, 13])\n",
            "2472\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([168, 13])\n",
            "1670\n",
            "torch.Size([174, 13])\n",
            "2076\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([144, 13])\n",
            "1430\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([181, 13])\n",
            "2160\n",
            "torch.Size([182, 13])\n",
            "2172\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([202, 13])\n",
            "2412\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([199, 13])\n",
            "2376\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([145, 13])\n",
            "1152\n",
            "torch.Size([179, 13])\n",
            "2136\n",
            "torch.Size([181, 13])\n",
            "1800\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([182, 13])\n",
            "2172\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([178, 13])\n",
            "1770\n",
            "torch.Size([144, 13])\n",
            "1144\n",
            "torch.Size([192, 13])\n",
            "2292\n",
            "torch.Size([149, 13])\n",
            "1480\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([196, 13])\n",
            "2340\n",
            "torch.Size([142, 13])\n",
            "1410\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([151, 13])\n",
            "1500\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([116, 13])\n",
            "920\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([122, 13])\n",
            "968\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([199, 13])\n",
            "2376\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([179, 13])\n",
            "2136\n",
            "torch.Size([220, 13])\n",
            "2628\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([208, 13])\n",
            "2484\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([198, 13])\n",
            "2364\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([180, 13])\n",
            "2148\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([221, 13])\n",
            "2640\n",
            "torch.Size([139, 13])\n",
            "1104\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([157, 13])\n",
            "1560\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([115, 13])\n",
            "912\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([223, 13])\n",
            "2664\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([183, 13])\n",
            "1820\n",
            "torch.Size([197, 13])\n",
            "2352\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([151, 13])\n",
            "1500\n",
            "torch.Size([182, 13])\n",
            "2172\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([203, 13])\n",
            "2424\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([192, 13])\n",
            "2292\n",
            "torch.Size([198, 13])\n",
            "2364\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([179, 13])\n",
            "1780\n",
            "torch.Size([206, 13])\n",
            "2460\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([111, 13])\n",
            "880\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([178, 13])\n",
            "1770\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([122, 13])\n",
            "968\n",
            "torch.Size([149, 13])\n",
            "1480\n",
            "torch.Size([216, 13])\n",
            "2580\n",
            "torch.Size([148, 13])\n",
            "1470\n",
            "torch.Size([148, 13])\n",
            "1470\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([182, 13])\n",
            "2172\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([157, 13])\n",
            "1560\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([198, 13])\n",
            "2364\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([192, 13])\n",
            "2292\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([116, 13])\n",
            "920\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([116, 13])\n",
            "920\n",
            "torch.Size([148, 13])\n",
            "1470\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([179, 13])\n",
            "2136\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([174, 13])\n",
            "1730\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([170, 13])\n",
            "1690\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([148, 13])\n",
            "1470\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([149, 13])\n",
            "1480\n",
            "torch.Size([201, 13])\n",
            "2400\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([173, 13])\n",
            "1720\n",
            "torch.Size([139, 13])\n",
            "1380\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([139, 13])\n",
            "1380\n",
            "torch.Size([168, 13])\n",
            "1670\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([180, 13])\n",
            "1790\n",
            "torch.Size([183, 13])\n",
            "1820\n",
            "torch.Size([135, 13])\n",
            "1340\n",
            "torch.Size([122, 13])\n",
            "968\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([221, 13])\n",
            "2640\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([143, 13])\n",
            "1420\n",
            "torch.Size([196, 13])\n",
            "2340\n",
            "torch.Size([114, 13])\n",
            "904\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([210, 13])\n",
            "2508\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([141, 13])\n",
            "1120\n",
            "torch.Size([137, 13])\n",
            "1088\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([181, 13])\n",
            "2160\n",
            "torch.Size([173, 13])\n",
            "2064\n",
            "torch.Size([151, 13])\n",
            "1200\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([151, 13])\n",
            "1500\n",
            "torch.Size([109, 13])\n",
            "864\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([173, 13])\n",
            "1720\n",
            "torch.Size([143, 13])\n",
            "1420\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([180, 13])\n",
            "2148\n",
            "torch.Size([176, 13])\n",
            "2100\n",
            "torch.Size([131, 13])\n",
            "1040\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([178, 13])\n",
            "1770\n",
            "torch.Size([140, 13])\n",
            "1112\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([171, 13])\n",
            "2040\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([183, 13])\n",
            "2184\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([176, 13])\n",
            "1750\n",
            "torch.Size([202, 13])\n",
            "2412\n",
            "torch.Size([172, 13])\n",
            "2052\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([182, 13])\n",
            "1810\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([102, 13])\n",
            "808\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([151, 13])\n",
            "1500\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([143, 13])\n",
            "1420\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([216, 13])\n",
            "2580\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([196, 13])\n",
            "2340\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([198, 13])\n",
            "2364\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([198, 13])\n",
            "2364\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([183, 13])\n",
            "2184\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([178, 13])\n",
            "2124\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([183, 13])\n",
            "2184\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([173, 13])\n",
            "2064\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([174, 13])\n",
            "1730\n",
            "torch.Size([174, 13])\n",
            "2076\n",
            "torch.Size([116, 13])\n",
            "920\n",
            "torch.Size([111, 13])\n",
            "880\n",
            "torch.Size([147, 13])\n",
            "1460\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([197, 13])\n",
            "2352\n",
            "torch.Size([180, 13])\n",
            "2148\n",
            "torch.Size([180, 13])\n",
            "1790\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([169, 13])\n",
            "2016\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([196, 13])\n",
            "2340\n",
            "torch.Size([139, 13])\n",
            "1104\n",
            "torch.Size([147, 13])\n",
            "1460\n",
            "torch.Size([169, 13])\n",
            "2016\n",
            "torch.Size([206, 13])\n",
            "2460\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([117, 13])\n",
            "928\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([198, 13])\n",
            "2364\n",
            "torch.Size([176, 13])\n",
            "2100\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([142, 13])\n",
            "1128\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([207, 13])\n",
            "2472\n",
            "torch.Size([178, 13])\n",
            "2124\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([201, 13])\n",
            "2400\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([174, 13])\n",
            "2076\n",
            "torch.Size([196, 13])\n",
            "2340\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([174, 13])\n",
            "1730\n",
            "torch.Size([143, 13])\n",
            "1136\n",
            "torch.Size([206, 13])\n",
            "2460\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([197, 13])\n",
            "2352\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([179, 13])\n",
            "1780\n",
            "torch.Size([141, 13])\n",
            "1120\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([137, 13])\n",
            "1088\n",
            "torch.Size([151, 13])\n",
            "1500\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([203, 13])\n",
            "2424\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([115, 13])\n",
            "912\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([179, 13])\n",
            "2136\n",
            "torch.Size([157, 13])\n",
            "1560\n",
            "torch.Size([173, 13])\n",
            "1720\n",
            "torch.Size([206, 13])\n",
            "2460\n",
            "torch.Size([202, 13])\n",
            "2412\n",
            "torch.Size([115, 13])\n",
            "912\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([174, 13])\n",
            "1730\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([140, 13])\n",
            "1112\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([146, 13])\n",
            "1160\n",
            "torch.Size([210, 13])\n",
            "2508\n",
            "torch.Size([142, 13])\n",
            "1128\n",
            "torch.Size([156, 13])\n",
            "1550\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Done!\n",
            "Processing...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In process\n",
            "torch.Size([178, 13])\n",
            "2124\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([145, 13])\n",
            "1152\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([179, 13])\n",
            "1780\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([198, 13])\n",
            "2364\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([143, 13])\n",
            "1420\n",
            "torch.Size([176, 13])\n",
            "2100\n",
            "torch.Size([180, 13])\n",
            "2148\n",
            "torch.Size([135, 13])\n",
            "1340\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([143, 13])\n",
            "1136\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([182, 13])\n",
            "2172\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([144, 13])\n",
            "1430\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([141, 13])\n",
            "1400\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([179, 13])\n",
            "2136\n",
            "torch.Size([143, 13])\n",
            "1136\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([148, 13])\n",
            "1470\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([216, 13])\n",
            "2580\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([212, 13])\n",
            "2532\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([131, 13])\n",
            "1040\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([141, 13])\n",
            "1120\n",
            "torch.Size([174, 13])\n",
            "2076\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([145, 13])\n",
            "1440\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([223, 13])\n",
            "2664\n",
            "torch.Size([174, 13])\n",
            "1730\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([173, 13])\n",
            "2064\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([151, 13])\n",
            "1500\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([224, 13])\n",
            "2676\n",
            "torch.Size([183, 13])\n",
            "2184\n",
            "torch.Size([148, 13])\n",
            "1470\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([201, 13])\n",
            "2400\n",
            "torch.Size([122, 13])\n",
            "968\n",
            "torch.Size([149, 13])\n",
            "1480\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([182, 13])\n",
            "1810\n",
            "torch.Size([175, 13])\n",
            "1740\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([113, 13])\n",
            "896\n",
            "torch.Size([145, 13])\n",
            "1152\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([139, 13])\n",
            "1104\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([202, 13])\n",
            "2412\n",
            "torch.Size([116, 13])\n",
            "920\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([211, 13])\n",
            "2520\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([174, 13])\n",
            "1730\n",
            "torch.Size([199, 13])\n",
            "2376\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([149, 13])\n",
            "1480\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([122, 13])\n",
            "968\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([139, 13])\n",
            "1104\n",
            "torch.Size([142, 13])\n",
            "1128\n",
            "torch.Size([110, 13])\n",
            "872\n",
            "torch.Size([199, 13])\n",
            "2376\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([131, 13])\n",
            "1040\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([139, 13])\n",
            "1380\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([176, 13])\n",
            "2100\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([173, 13])\n",
            "2064\n",
            "torch.Size([145, 13])\n",
            "1152\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([176, 13])\n",
            "2100\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([212, 13])\n",
            "2532\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([140, 13])\n",
            "1112\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([138, 13])\n",
            "1370\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([143, 13])\n",
            "1420\n",
            "torch.Size([144, 13])\n",
            "1430\n",
            "torch.Size([168, 13])\n",
            "1670\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([223, 13])\n",
            "2664\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([151, 13])\n",
            "1500\n",
            "torch.Size([197, 13])\n",
            "2352\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([147, 13])\n",
            "1460\n",
            "torch.Size([199, 13])\n",
            "2376\n",
            "torch.Size([171, 13])\n",
            "2040\n",
            "torch.Size([148, 13])\n",
            "1176\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([192, 13])\n",
            "2292\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([175, 13])\n",
            "1740\n",
            "torch.Size([169, 13])\n",
            "2016\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([210, 13])\n",
            "2508\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([197, 13])\n",
            "2352\n",
            "torch.Size([209, 13])\n",
            "2496\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([229, 13])\n",
            "2736\n",
            "torch.Size([197, 13])\n",
            "2352\n",
            "torch.Size([198, 13])\n",
            "2364\n",
            "torch.Size([149, 13])\n",
            "1480\n",
            "torch.Size([116, 13])\n",
            "920\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([176, 13])\n",
            "2100\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([196, 13])\n",
            "2340\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([151, 13])\n",
            "1500\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([170, 13])\n",
            "2028\n",
            "torch.Size([208, 13])\n",
            "2484\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([114, 13])\n",
            "904\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([148, 13])\n",
            "1176\n",
            "torch.Size([208, 13])\n",
            "2484\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([170, 13])\n",
            "1690\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([173, 13])\n",
            "2064\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([149, 13])\n",
            "1480\n",
            "torch.Size([139, 13])\n",
            "1380\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([117, 13])\n",
            "928\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([108, 13])\n",
            "856\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([170, 13])\n",
            "1690\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([122, 13])\n",
            "968\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([206, 13])\n",
            "2460\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([142, 13])\n",
            "1128\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([221, 13])\n",
            "2640\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([141, 13])\n",
            "1400\n",
            "torch.Size([197, 13])\n",
            "2352\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([174, 13])\n",
            "2076\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([168, 13])\n",
            "2004\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([131, 13])\n",
            "1040\n",
            "torch.Size([115, 13])\n",
            "912\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([201, 13])\n",
            "2400\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([122, 13])\n",
            "968\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([137, 13])\n",
            "1088\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([151, 13])\n",
            "1200\n",
            "torch.Size([183, 13])\n",
            "2184\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([142, 13])\n",
            "1128\n",
            "torch.Size([179, 13])\n",
            "2136\n",
            "torch.Size([115, 13])\n",
            "912\n",
            "torch.Size([139, 13])\n",
            "1104\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([139, 13])\n",
            "1104\n",
            "torch.Size([146, 13])\n",
            "1160\n",
            "torch.Size([137, 13])\n",
            "1088\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([177, 13])\n",
            "1760\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([199, 13])\n",
            "2376\n",
            "torch.Size([137, 13])\n",
            "1088\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([146, 13])\n",
            "1450\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([180, 13])\n",
            "2148\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([110, 13])\n",
            "872\n",
            "torch.Size([172, 13])\n",
            "2052\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([203, 13])\n",
            "2424\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([181, 13])\n",
            "2160\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([183, 13])\n",
            "2184\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([139, 13])\n",
            "1380\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([212, 13])\n",
            "2532\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([168, 13])\n",
            "1670\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([146, 13])\n",
            "1450\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([108, 13])\n",
            "856\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([198, 13])\n",
            "2364\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([111, 13])\n",
            "880\n",
            "torch.Size([206, 13])\n",
            "2460\n",
            "torch.Size([148, 13])\n",
            "1470\n",
            "torch.Size([131, 13])\n",
            "1040\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([198, 13])\n",
            "2364\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([209, 13])\n",
            "2496\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([115, 13])\n",
            "912\n",
            "torch.Size([192, 13])\n",
            "2292\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([202, 13])\n",
            "2412\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([145, 13])\n",
            "1440\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([206, 13])\n",
            "2460\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([137, 13])\n",
            "1088\n",
            "torch.Size([176, 13])\n",
            "1750\n",
            "torch.Size([203, 13])\n",
            "2424\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Done!\n",
            "Processing...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In process\n",
            "torch.Size([206, 13])\n",
            "2460\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([178, 13])\n",
            "2124\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([207, 13])\n",
            "2472\n",
            "torch.Size([198, 13])\n",
            "2364\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([176, 13])\n",
            "2100\n",
            "torch.Size([201, 13])\n",
            "2400\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([208, 13])\n",
            "2484\n",
            "torch.Size([215, 13])\n",
            "2568\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([220, 13])\n",
            "2628\n",
            "torch.Size([196, 13])\n",
            "2340\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([202, 13])\n",
            "2412\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([182, 13])\n",
            "2172\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([199, 13])\n",
            "2376\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([189, 13])\n",
            "2256\n",
            "torch.Size([174, 13])\n",
            "2076\n",
            "torch.Size([208, 13])\n",
            "2484\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([167, 13])\n",
            "1992\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([164, 13])\n",
            "1956\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([196, 13])\n",
            "2340\n",
            "torch.Size([197, 13])\n",
            "2352\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([213, 13])\n",
            "2544\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([192, 13])\n",
            "2292\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([181, 13])\n",
            "2160\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([202, 13])\n",
            "2412\n",
            "torch.Size([212, 13])\n",
            "2532\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([168, 13])\n",
            "2004\n",
            "torch.Size([207, 13])\n",
            "2472\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([216, 13])\n",
            "2580\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([188, 13])\n",
            "2244\n",
            "torch.Size([229, 13])\n",
            "2736\n",
            "torch.Size([183, 13])\n",
            "2184\n",
            "torch.Size([210, 13])\n",
            "2508\n",
            "torch.Size([178, 13])\n",
            "2124\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([197, 13])\n",
            "2352\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([197, 13])\n",
            "2352\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([197, 13])\n",
            "2352\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([207, 13])\n",
            "2472\n",
            "torch.Size([199, 13])\n",
            "2376\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([221, 13])\n",
            "2640\n",
            "torch.Size([193, 13])\n",
            "2304\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([199, 13])\n",
            "2376\n",
            "torch.Size([185, 13])\n",
            "2208\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([216, 13])\n",
            "2580\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([174, 13])\n",
            "2076\n",
            "torch.Size([183, 13])\n",
            "2184\n",
            "torch.Size([196, 13])\n",
            "2340\n",
            "torch.Size([175, 13])\n",
            "2088\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([172, 13])\n",
            "2052\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([173, 13])\n",
            "2064\n",
            "torch.Size([216, 13])\n",
            "2580\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([142, 13])\n",
            "1410\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([143, 13])\n",
            "1420\n",
            "torch.Size([148, 13])\n",
            "1470\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([149, 13])\n",
            "1480\n",
            "torch.Size([173, 13])\n",
            "1720\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([174, 13])\n",
            "1730\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([178, 13])\n",
            "1770\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([145, 13])\n",
            "1440\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([179, 13])\n",
            "1780\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([180, 13])\n",
            "1790\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([151, 13])\n",
            "1500\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([151, 13])\n",
            "1500\n",
            "torch.Size([148, 13])\n",
            "1470\n",
            "torch.Size([143, 13])\n",
            "1420\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([145, 13])\n",
            "1440\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([144, 13])\n",
            "1430\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([155, 13])\n",
            "1540\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([143, 13])\n",
            "1420\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([168, 13])\n",
            "1670\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([144, 13])\n",
            "1430\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([163, 13])\n",
            "1620\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([157, 13])\n",
            "1560\n",
            "torch.Size([183, 13])\n",
            "1820\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([138, 13])\n",
            "1370\n",
            "torch.Size([146, 13])\n",
            "1450\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([144, 13])\n",
            "1430\n",
            "torch.Size([149, 13])\n",
            "1480\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([146, 13])\n",
            "1450\n",
            "torch.Size([142, 13])\n",
            "1410\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([165, 13])\n",
            "1640\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([175, 13])\n",
            "1740\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([149, 13])\n",
            "1480\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([145, 13])\n",
            "1440\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([166, 13])\n",
            "1650\n",
            "torch.Size([154, 13])\n",
            "1530\n",
            "torch.Size([135, 13])\n",
            "1340\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([139, 13])\n",
            "1380\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([144, 13])\n",
            "1430\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([137, 13])\n",
            "1360\n",
            "torch.Size([156, 13])\n",
            "1550\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([159, 13])\n",
            "1580\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([158, 13])\n",
            "1570\n",
            "torch.Size([139, 13])\n",
            "1380\n",
            "torch.Size([172, 13])\n",
            "1710\n",
            "torch.Size([164, 13])\n",
            "1630\n",
            "torch.Size([168, 13])\n",
            "1670\n",
            "torch.Size([152, 13])\n",
            "1510\n",
            "torch.Size([180, 13])\n",
            "1790\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([153, 13])\n",
            "1520\n",
            "torch.Size([182, 13])\n",
            "1810\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([169, 13])\n",
            "1680\n",
            "torch.Size([170, 13])\n",
            "1690\n",
            "torch.Size([140, 13])\n",
            "1390\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([122, 13])\n",
            "968\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([142, 13])\n",
            "1128\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([141, 13])\n",
            "1120\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([144, 13])\n",
            "1144\n",
            "torch.Size([116, 13])\n",
            "920\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([122, 13])\n",
            "968\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([122, 13])\n",
            "968\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([115, 13])\n",
            "912\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([141, 13])\n",
            "1120\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([131, 13])\n",
            "1040\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([116, 13])\n",
            "920\n",
            "torch.Size([142, 13])\n",
            "1128\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([142, 13])\n",
            "1128\n",
            "torch.Size([122, 13])\n",
            "968\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([108, 13])\n",
            "856\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([112, 13])\n",
            "888\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([131, 13])\n",
            "1040\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([143, 13])\n",
            "1136\n",
            "torch.Size([137, 13])\n",
            "1088\n",
            "torch.Size([115, 13])\n",
            "912\n",
            "torch.Size([108, 13])\n",
            "856\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([116, 13])\n",
            "920\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([142, 13])\n",
            "1128\n",
            "torch.Size([141, 13])\n",
            "1120\n",
            "torch.Size([115, 13])\n",
            "912\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([113, 13])\n",
            "896\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([111, 13])\n",
            "880\n",
            "torch.Size([122, 13])\n",
            "968\n",
            "torch.Size([106, 13])\n",
            "840\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([119, 13])\n",
            "944\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([111, 13])\n",
            "880\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([110, 13])\n",
            "872\n",
            "torch.Size([115, 13])\n",
            "912\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([141, 13])\n",
            "1120\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([139, 13])\n",
            "1104\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([131, 13])\n",
            "1040\n",
            "torch.Size([136, 13])\n",
            "1080\n",
            "torch.Size([123, 13])\n",
            "976\n",
            "torch.Size([141, 13])\n",
            "1120\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([132, 13])\n",
            "1048\n",
            "torch.Size([146, 13])\n",
            "1160\n",
            "torch.Size([133, 13])\n",
            "1056\n",
            "torch.Size([151, 13])\n",
            "1200\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([140, 13])\n",
            "1112\n",
            "torch.Size([109, 13])\n",
            "864\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([108, 13])\n",
            "856\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([129, 13])\n",
            "1024\n",
            "torch.Size([127, 13])\n",
            "1008\n",
            "torch.Size([111, 13])\n",
            "880\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([118, 13])\n",
            "936\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([138, 13])\n",
            "1096\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([120, 13])\n",
            "952\n",
            "torch.Size([112, 13])\n",
            "888\n",
            "torch.Size([140, 13])\n",
            "1112\n",
            "torch.Size([126, 13])\n",
            "1000\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([140, 13])\n",
            "1112\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([121, 13])\n",
            "960\n",
            "torch.Size([128, 13])\n",
            "1016\n",
            "torch.Size([131, 13])\n",
            "1040\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "dataset_train = CustomDataset( split='train')\n",
        "dataset_valid = CustomDataset( split='valid')\n",
        "dataset_test = CustomDataset( split='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T10:20:04.104351Z",
          "iopub.status.busy": "2023-07-06T10:20:04.103933Z",
          "iopub.status.idle": "2023-07-06T10:20:04.129439Z",
          "shell.execute_reply": "2023-07-06T10:20:04.128365Z",
          "shell.execute_reply.started": "2023-07-06T10:20:04.104294Z"
        },
        "id": "lD4l3XlKcoCf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_scatter import scatter_mean\n",
        "from torch_geometric.nn import MetaLayer\n",
        "\n",
        "class EdgeModel(torch.nn.Module):\n",
        "    def __init__(self, n_node_features, n_edge_features, hiddens, n_targets):\n",
        "        super().__init__()\n",
        "        self.edge_mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(2 * n_node_features + n_edge_features, hiddens),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hiddens, n_targets),\n",
        "        )\n",
        "\n",
        "    def forward(self, src, dest, edge_attr, u=None, batch=None):\n",
        "        #print(\"In edge model\")\n",
        "        #print(src, src.shape)\n",
        "        #print(dest, dest.shape)\n",
        "        #print(edge_attr, edge_attr.shape)\n",
        "        out = torch.cat([src, dest, edge_attr], 1)\n",
        "        out = self.edge_mlp(out)\n",
        "        #print(\"Exit edge model\")\n",
        "        return out\n",
        "\n",
        "\n",
        "class NodeModel(torch.nn.Module):\n",
        "    def __init__(self, n_node_features, hiddens, n_targets):\n",
        "        super(NodeModel, self).__init__()\n",
        "        self.node_mlp_1 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(n_node_features + hiddens, hiddens),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hiddens, hiddens),\n",
        "        )\n",
        "        self.node_mlp_2 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(n_node_features + hiddens, hiddens),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hiddens, n_targets),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
        "        #print(\"In node model\")\n",
        "        row, col = edge_index\n",
        "        out = torch.cat([x[col], edge_attr], dim=1)\n",
        "        out = self.node_mlp_1(out)\n",
        "        out = scatter_mean(out, row, dim=0, dim_size=x.size(0))\n",
        "        out = torch.cat([x, out], dim=1)\n",
        "        out = self.node_mlp_2(out)\n",
        "        #print(\"Exit node model\")\n",
        "        return out\n",
        "\n",
        "\n",
        "class MetaNet(torch.nn.Module):\n",
        "    def __init__(self, n_node_features, n_edge_features, num_hidden):\n",
        "        super(MetaNet, self).__init__()\n",
        "\n",
        "        # Input Layer\n",
        "        self.input = MetaLayer(\n",
        "            edge_model=EdgeModel(\n",
        "                n_node_features=n_node_features, n_edge_features=n_edge_features,\n",
        "                hiddens=num_hidden, n_targets=num_hidden),\n",
        "            node_model=NodeModel(n_node_features=n_node_features, hiddens=num_hidden, n_targets=num_hidden)\n",
        "            )\n",
        "\n",
        "        # Output Layer\n",
        "        self.output = MetaLayer(\n",
        "            edge_model=EdgeModel(\n",
        "                n_node_features=num_hidden, n_edge_features=num_hidden,\n",
        "                hiddens=num_hidden, n_targets=num_hidden),\n",
        "            node_model=NodeModel(n_node_features=num_hidden, hiddens=num_hidden, n_targets=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_attr, y = data.x, data.edge_index, data.edge_attr, data.y\n",
        "        #print(\"In meta model\")\n",
        "        x, edge_attr, _ = self.input(x, edge_index, edge_attr)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x, edge_attr, _ = self.output(x, edge_index, edge_attr)\n",
        "        #x = F.dropout(x, p=0.5, training=self.training)\n",
        "        #print(\"Exit meta model\")\n",
        "        return x\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, num_input, num_hidden):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.name = \"Net\"\n",
        "\n",
        "        # Input GCN layer.\n",
        "        self.conv1 = GCNConv(num_input, num_hidden)\n",
        "        self.conv2 = GCNConv(num_hidden, num_hidden)\n",
        "        self.conv3 = GCNConv(num_hidden, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, y = data.x, data.edge_index, data.y\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = F.dropout(x, training=self.training)\n",
        "\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        return x\n",
        "\n",
        "class AttentionNet(torch.nn.Module):\n",
        "    def __init__(self, num_input, num_hidden):\n",
        "        super(AttentionNet, self).__init__()\n",
        "\n",
        "        self.name = \"AttentionNet\"\n",
        "\n",
        "        # Input GCN layer.\n",
        "        self.conv1 = GATv2Conv(num_input, num_hidden,heads=2,concat=False)\n",
        "        self.conv2 = GATv2Conv(num_hidden, num_hidden,heads=2,concat=False)\n",
        "        self.conv3 = GATv2Conv(num_hidden, 1,num_heads=2,concat=False)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, y = data.x, data.edge_index, data.y\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = F.dropout(x, training=self.training)\n",
        "\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxoZSG8PGyuR"
      },
      "source": [
        "# MODS TO MODEL START HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "kWRWlqN1Gye3"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv,GATv2Conv\n",
        "from torch_scatter import scatter_mean\n",
        "from torch_geometric.nn import MetaLayer\n",
        "\n",
        "class EdgeModel(torch.nn.Module):\n",
        "    def __init__(self, n_node_features, n_edge_features, hiddens, n_targets):\n",
        "        super().__init__()\n",
        "        # self.edge_mlp = torch.nn.Sequential(\n",
        "        #     # torch.nn.Linear(2 * n_node_features + n_edge_features, hiddens),\n",
        "        #     # torch.nn.ReLU(),\n",
        "        #     # torch.nn.Linear(hiddens, n_targets),\n",
        "        #     GATv2Conv(2 * n_node_features + n_edge_features, n_targets,heads=2,concat=False),\n",
        "        #     torch.nn.PReLU(),\n",
        "        #     GATv2Conv(n_targets, n_targets,heads=2,concat=False)\n",
        "        # )\n",
        "        self.gatedge1 = GATv2Conv(2 * n_node_features + n_edge_features, n_targets,heads=2,concat=False)\n",
        "        self.prelu = torch.nn.PReLU()\n",
        "        self.gatedge2 = GATv2Conv(n_targets,n_targets,heads=2,concat=False)\n",
        "\n",
        "    def forward(self, src, dest, edge_index,edge_attr, u=None, batch=None):\n",
        "        out = torch.cat([src, dest, edge_index,edge_attr], 1)\n",
        "        out = self.gatedge1(out,edge_index)\n",
        "        out = self.prelu(out)\n",
        "        out = self.gatedge2(out,edge_index)\n",
        "        #print(\"Exit edge model\")\n",
        "        return out\n",
        "\n",
        "\n",
        "class NodeModel(torch.nn.Module):\n",
        "    def __init__(self, n_node_features, hiddens, n_targets):\n",
        "        super(NodeModel, self).__init__()\n",
        "        # self.node_mlp_1 = torch.nn.Sequential(\n",
        "        #     GATv2Conv(n_node_features + hiddens, hiddens,heads=2,concat=False),\n",
        "        #     torch.nn.PReLU(),\n",
        "        #     GATv2Conv(hiddens, hiddens,heads=2,concat=False)\n",
        "        # )\n",
        "        self.gat11 = GATv2Conv(n_node_features+hiddens,hiddens,heads=2,concat=False)\n",
        "        self.prelu = torch.nn.PReLU()\n",
        "        self.gat12 = GATv2Conv(hiddens,hiddens,heads=2,concat=False)\n",
        "\n",
        "        # self.node_mlp_2 = torch.nn.Sequential(\n",
        "        #     GATv2Conv(n_node_features + hiddens, hiddens,heads=2,concat=False),\n",
        "        #     torch.nn.PReLU(),\n",
        "        #     GATv2Conv(hiddens, n_targets,heads=2,concat=False)\n",
        "        # )\n",
        "        self.gat21 = GATv2Conv(n_node_features+hiddens,hiddens,heads=2,concat=False),\n",
        "        self.gat22 = GATv2Conv(hiddens,hiddens,heads=2,concat=False)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
        "        #print(\"In node model\")\n",
        "        row, col = edge_index\n",
        "        out = torch.cat([x[col], edge_index,edge_attr], dim=1)\n",
        "        out = self.gat11(out,edge_index)\n",
        "        out = self.prelu(out)\n",
        "        out = self.gat12(out,edge_index)\n",
        "        out = scatter_mean(out, row, dim=0, dim_size=x.size(0))\n",
        "        out = torch.cat([x, out], dim=1)\n",
        "        out = self.gat21(out,edge_index)\n",
        "        out = self.prelu(out)\n",
        "        out = self.gat22(out,edge_index)\n",
        "        #print(\"Exit node model\")\n",
        "        return out\n",
        "\n",
        "\n",
        "class MetaNet(torch.nn.Module):\n",
        "    def __init__(self, n_node_features, n_edge_features, num_hidden):\n",
        "        super(MetaNet, self).__init__()\n",
        "\n",
        "        # input layer\n",
        "        self.input = MetaLayer(EdgeModel(n_node_features, n_edge_features, num_hidden, num_hidden),\n",
        "                                 NodeModel(n_node_features, num_hidden, num_hidden))\n",
        "\n",
        "        # output layer\n",
        "        self.output = MetaLayer(EdgeModel(n_node_features, num_hidden, num_hidden, num_hidden),\n",
        "                                    NodeModel(n_node_features, num_hidden, 1))\n",
        "\n",
        "        self.name = \"MetaNet\"\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_attr, y = data.x, data.edge_index, data.edge_attr, data.y\n",
        "        #print(\"In meta model\")\n",
        "        x, edge_index, edge_attr = self.input(x, edge_index, edge_attr)\n",
        "        x = torch.nn.PReLU()(x)\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x, edge_index, edge_attr = self.output(x, edge_index, edge_attr)\n",
        "        #x = F.dropout(x, p=0.5, training=self.training)\n",
        "        #print(\"Exit meta model\")\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YvaqZlMxQAd"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_scatter import scatter_mean\n",
        "from torch_geometric.nn import MetaLayer\n",
        "\n",
        "class EdgeModel(torch.nn.Module):\n",
        "    def __init__(self, n_node_features, n_edge_features, hiddens, n_targets):\n",
        "        super().__init__()\n",
        "        self.edge_mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(2 * n_node_features + n_edge_features, hiddens),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            torch.nn.Linear(hiddens, n_targets),\n",
        "        )\n",
        "\n",
        "    def forward(self, src, dest, edge_attr, u=None, batch=None):\n",
        "        #print(\"In edge model\")\n",
        "        #print(src, src.shape)\n",
        "        #print(dest, dest.shape)\n",
        "        #print(edge_attr, edge_attr.shape)\n",
        "        out = torch.cat([src, dest, edge_attr], 1)\n",
        "        out = self.edge_mlp(out)\n",
        "        out = F.relu(out) ### ADDED THIS ###\n",
        "        #print(\"Exit edge model\")\n",
        "        return out\n",
        "\n",
        "\n",
        "class NodeModel(torch.nn.Module):\n",
        "    def __init__(self, n_node_features, hiddens, n_targets):\n",
        "        super(NodeModel, self).__init__()\n",
        "        self.node_mlp_1 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(n_node_features + hiddens, hiddens),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            torch.nn.Linear(hiddens, hiddens),\n",
        "        )\n",
        "        self.node_mlp_2 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(n_node_features + hiddens, hiddens),\n",
        "            torch.nn.LeakyReLU(),\n",
        "            torch.nn.Linear(hiddens, n_targets),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
        "        #print(\"In node model\")\n",
        "        row, col = edge_index\n",
        "        out = torch.cat([x[col], edge_attr], dim=1)\n",
        "        out = self.node_mlp_1(out)\n",
        "        out = scatter_mean(out, row, dim=0, dim_size=x.size(0))\n",
        "        out = torch.cat([x, out], dim=1)\n",
        "        out = self.node_mlp_2(out)\n",
        "        #print(\"Exit node model\")\n",
        "        return out\n",
        "\n",
        "class MetaNet(torch.nn.Module):\n",
        "    def __init__(self, n_node_features, n_edge_features, num_hidden):\n",
        "        super(MetaNet, self).__init__()\n",
        "\n",
        "        # Input Layer\n",
        "        self.input = MetaLayer(\n",
        "            edge_model=EdgeModel(\n",
        "                n_node_features=n_node_features, n_edge_features=n_edge_features,\n",
        "                hiddens=num_hidden, n_targets=num_hidden),\n",
        "            node_model=NodeModel(n_node_features=n_node_features, hiddens=num_hidden, n_targets=num_hidden)\n",
        "            )\n",
        "\n",
        "        # Output Layer\n",
        "        self.output = MetaLayer(\n",
        "            edge_model=EdgeModel(\n",
        "                n_node_features=num_hidden, n_edge_features=num_hidden,\n",
        "                hiddens=num_hidden, n_targets=num_hidden),\n",
        "            node_model=NodeModel(n_node_features=num_hidden, hiddens=num_hidden, n_targets=1)\n",
        "        )\n",
        "\n",
        "        # Attention Mechanism\n",
        "        self.attention = torch.nn.MultiheadAttention(embed_dim=num_hidden, num_heads=2, dropout=0.2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_attr, y = data.x, data.edge_index, data.edge_attr, data.y\n",
        "        x, edge_attr, _ = self.input(x, edge_index, edge_attr)\n",
        "        x = F.relu(x)\n",
        "        # x = F.dropout(x, p=0.2, training=self.training)\n",
        "\n",
        "        # Attention Mechanism\n",
        "        x = x.unsqueeze(0)\n",
        "        x, _ = self.attention(x, x, x)\n",
        "        x = x.squeeze(0)\n",
        "\n",
        "        x, edge_attr, _ = self.output(x, edge_index, edge_attr)\n",
        "        #x = F.dropout(x, p=0.5, training=self.training)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAnmOvhoG0XI"
      },
      "source": [
        "# MODS TO MODEL END HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T10:20:13.786261Z",
          "iopub.status.busy": "2023-07-06T10:20:13.785196Z",
          "iopub.status.idle": "2023-07-06T10:20:13.791675Z",
          "shell.execute_reply": "2023-07-06T10:20:13.790368Z",
          "shell.execute_reply.started": "2023-07-06T10:20:13.786213Z"
        },
        "id": "bPRRP8jlcoCh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "num_node_features = 13\n",
        "num_edge_features = 3\n",
        "num_hidden = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T10:20:32.677829Z",
          "iopub.status.busy": "2023-07-06T10:20:32.677457Z",
          "iopub.status.idle": "2023-07-06T10:20:34.309011Z",
          "shell.execute_reply": "2023-07-06T10:20:34.308022Z",
          "shell.execute_reply.started": "2023-07-06T10:20:32.677800Z"
        },
        "id": "u_oMx_LLcoCh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model = MetaNet(num_node_features, num_edge_features, num_hidden).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3dzLC61ETfJ"
      },
      "outputs": [],
      "source": [
        "# model  = Net(num_node_features, num_hidden).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2B_GYSk3nZ4",
        "outputId": "bb9c3e71-41f7-4955-bca9-fffd135be7fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MetaNet(\n",
              "  (input): MetaLayer(\n",
              "    edge_model=EdgeModel(\n",
              "    (edge_mlp): Sequential(\n",
              "      (0): Linear(in_features=29, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "  ),\n",
              "    node_model=NodeModel(\n",
              "    (node_mlp_1): Sequential(\n",
              "      (0): Linear(in_features=141, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "    (node_mlp_2): Sequential(\n",
              "      (0): Linear(in_features=141, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "  ),\n",
              "    global_model=None\n",
              "  )\n",
              "  (output): MetaLayer(\n",
              "    edge_model=EdgeModel(\n",
              "    (edge_mlp): Sequential(\n",
              "      (0): Linear(in_features=384, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "  ),\n",
              "    node_model=NodeModel(\n",
              "    (node_mlp_1): Sequential(\n",
              "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "    (node_mlp_2): Sequential(\n",
              "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
              "    )\n",
              "  ),\n",
              "    global_model=None\n",
              "  )\n",
              "  (attention): MultiheadAttention(\n",
              "    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T10:20:46.697896Z",
          "iopub.status.busy": "2023-07-06T10:20:46.697337Z",
          "iopub.status.idle": "2023-07-06T10:20:46.703235Z",
          "shell.execute_reply": "2023-07-06T10:20:46.702092Z",
          "shell.execute_reply.started": "2023-07-06T10:20:46.697865Z"
        },
        "id": "tyIe36JGcoCh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(lr=1e-4,params=model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T10:20:46.966933Z",
          "iopub.status.busy": "2023-07-06T10:20:46.966338Z",
          "iopub.status.idle": "2023-07-06T10:20:46.975716Z",
          "shell.execute_reply": "2023-07-06T10:20:46.974673Z",
          "shell.execute_reply.started": "2023-07-06T10:20:46.966903Z"
        },
        "id": "kOdrIldDcoCi",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def train(dataset):\n",
        "    # Monitor training.\n",
        "    losses = []\n",
        "\n",
        "    # Put model in training mode!\n",
        "    model.train()\n",
        "    i=0\n",
        "    for i, batch in enumerate(dataset):\n",
        "        #print(\"misaa\")\n",
        "        # Training step.\n",
        "\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch)\n",
        "        loss = torch.sqrt(F.mse_loss(out.squeeze(), batch.y.squeeze()))\n",
        "        #print(f\"Training oss for {i}: {loss}\")\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Monitoring\n",
        "        losses.append(loss.item())\n",
        "        if(i == 1151): break\n",
        "    # Return training metrics.\n",
        "    return losses\n",
        "\n",
        "\n",
        "def evaluate(dataset):\n",
        "    # Monitor evaluation.\n",
        "    losses = []\n",
        "\n",
        "    # Validation (1)\n",
        "    model.eval()\n",
        "    i = 0\n",
        "    for i, batch in enumerate(dataset):\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        # Calculate validation losses.\n",
        "        out = model(batch)\n",
        "        loss = torch.sqrt(F.mse_loss(out.squeeze(), batch.y.squeeze()))\n",
        "\n",
        "        # Metric logging.\n",
        "        losses.append(loss.item())\n",
        "        if(i == 383): break\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "jrn8HVXecoCr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import shuffle\n",
        "from torch_geometric.data import DataLoader\n",
        "train_loader = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(dataset_valid, batch_size=3, shuffle=True)\n",
        "test_loader = DataLoader(dataset_test, batch_size=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lip8xSKacoCr",
        "outputId": "bbf576b5-37a7-4f47-9256-5a65f009eb0d",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataBatch(x=[5160, 13], edge_index=[2, 54000], edge_attr=[54000, 3], y=[5160], num_nodes=5160, batch=[5160], ptr=[33])\n"
          ]
        }
      ],
      "source": [
        "for batch in train_loader:\n",
        "    print(batch)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeVD_pAqcoCr",
        "outputId": "f3f98d70-ea35-4e64-ed94-6abd31e6b378",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch: 500, Len of Training loss: 36, Average loss: 1.884957042005327\n",
            "Len of Validation loss: 128, Average loss: 1.8187359031289816\n",
            "Epoch: 501, Len of Training loss: 36, Average loss: 1.8454193505975935\n",
            "Len of Validation loss: 128, Average loss: 1.8082033637911081\n",
            "Epoch: 502, Len of Training loss: 36, Average loss: 1.8921365837256114\n",
            "Len of Validation loss: 128, Average loss: 1.7986390315927565\n",
            "Epoch: 503, Len of Training loss: 36, Average loss: 1.956673930088679\n",
            "Len of Validation loss: 128, Average loss: 1.8524290942586958\n",
            "Epoch: 504, Len of Training loss: 36, Average loss: 1.902925494644377\n",
            "Len of Validation loss: 128, Average loss: 1.870206018909812\n",
            "Epoch: 505, Len of Training loss: 36, Average loss: 1.923675970898734\n",
            "Len of Validation loss: 128, Average loss: 1.8909411504864693\n",
            "Epoch: 506, Len of Training loss: 36, Average loss: 1.886351744333903\n",
            "Len of Validation loss: 128, Average loss: 1.8828103449195623\n",
            "Epoch: 507, Len of Training loss: 36, Average loss: 1.8793538808822632\n",
            "Len of Validation loss: 128, Average loss: 1.844411184079945\n",
            "Epoch: 508, Len of Training loss: 36, Average loss: 1.888354950480991\n",
            "Len of Validation loss: 128, Average loss: 1.7805021805688739\n",
            "Epoch: 509, Len of Training loss: 36, Average loss: 1.8145627313190036\n",
            "Len of Validation loss: 128, Average loss: 1.7449388841632754\n",
            "Epoch: 510, Len of Training loss: 36, Average loss: 1.8289726111623976\n",
            "Len of Validation loss: 128, Average loss: 1.8382793464697897\n",
            "Epoch: 511, Len of Training loss: 36, Average loss: 1.8788275718688965\n",
            "Len of Validation loss: 128, Average loss: 1.8082091971300542\n",
            "Epoch: 512, Len of Training loss: 36, Average loss: 1.9123650789260864\n",
            "Len of Validation loss: 128, Average loss: 1.7766403355635703\n",
            "Epoch: 513, Len of Training loss: 36, Average loss: 1.8430783417489793\n",
            "Len of Validation loss: 128, Average loss: 1.9507579146884382\n",
            "Epoch: 514, Len of Training loss: 36, Average loss: 1.95312519537078\n",
            "Len of Validation loss: 128, Average loss: 1.8773164190351963\n",
            "Epoch: 515, Len of Training loss: 36, Average loss: 1.8638885219891865\n",
            "Len of Validation loss: 128, Average loss: 1.819769938942045\n",
            "Epoch: 516, Len of Training loss: 36, Average loss: 1.8654846913284726\n",
            "Len of Validation loss: 128, Average loss: 1.871410470455885\n",
            "Epoch: 517, Len of Training loss: 36, Average loss: 1.8463857935534582\n",
            "Len of Validation loss: 128, Average loss: 1.8639903098810464\n",
            "Epoch: 518, Len of Training loss: 36, Average loss: 1.8370333512624104\n",
            "Len of Validation loss: 128, Average loss: 1.8773388215340674\n",
            "Epoch: 519, Len of Training loss: 36, Average loss: 1.860705018043518\n",
            "Len of Validation loss: 128, Average loss: 1.8386759078130126\n",
            "Epoch: 520, Len of Training loss: 36, Average loss: 1.8474432362450495\n",
            "Len of Validation loss: 128, Average loss: 1.8631675033830106\n",
            "Epoch: 521, Len of Training loss: 36, Average loss: 1.8527657224072351\n",
            "Len of Validation loss: 128, Average loss: 1.880389271536842\n",
            "Epoch: 522, Len of Training loss: 36, Average loss: 1.9017968310250177\n",
            "Len of Validation loss: 128, Average loss: 1.8257556967437267\n",
            "Epoch: 523, Len of Training loss: 36, Average loss: 1.8481995066006978\n",
            "Len of Validation loss: 128, Average loss: 1.7834084636997432\n",
            "Epoch: 524, Len of Training loss: 36, Average loss: 1.835540897316403\n",
            "Len of Validation loss: 128, Average loss: 1.8071481101214886\n",
            "Epoch: 525, Len of Training loss: 36, Average loss: 1.8558277719550662\n",
            "Len of Validation loss: 128, Average loss: 1.8664979448076338\n",
            "Epoch: 526, Len of Training loss: 36, Average loss: 1.8832244210773044\n",
            "Len of Validation loss: 128, Average loss: 1.8292445500846952\n",
            "Epoch: 527, Len of Training loss: 36, Average loss: 1.8900584048695035\n",
            "Len of Validation loss: 128, Average loss: 1.8612672302406281\n",
            "Epoch: 528, Len of Training loss: 36, Average loss: 1.8444961607456207\n",
            "Len of Validation loss: 128, Average loss: 1.8817972240503877\n",
            "Epoch: 529, Len of Training loss: 36, Average loss: 1.841661857234107\n",
            "Len of Validation loss: 128, Average loss: 1.849346986040473\n",
            "Epoch: 530, Len of Training loss: 36, Average loss: 1.942406349711948\n",
            "Len of Validation loss: 128, Average loss: 1.8598729334771633\n",
            "Epoch: 531, Len of Training loss: 36, Average loss: 1.8439537783463795\n",
            "Len of Validation loss: 128, Average loss: 1.8209517718059942\n",
            "Epoch: 532, Len of Training loss: 36, Average loss: 1.9159973959128063\n",
            "Len of Validation loss: 128, Average loss: 2.145046636229381\n",
            "Epoch: 533, Len of Training loss: 36, Average loss: 1.8794049190150366\n",
            "Len of Validation loss: 128, Average loss: 1.8680133577436209\n",
            "Epoch: 534, Len of Training loss: 36, Average loss: 1.9046253495746188\n",
            "Len of Validation loss: 128, Average loss: 1.8441648134030402\n",
            "Epoch: 535, Len of Training loss: 36, Average loss: 1.8614139159520466\n",
            "Len of Validation loss: 128, Average loss: 1.8359744739718735\n",
            "Epoch: 536, Len of Training loss: 36, Average loss: 1.8336654173003302\n",
            "Len of Validation loss: 128, Average loss: 1.7683482966385782\n",
            "Epoch: 537, Len of Training loss: 36, Average loss: 1.8651382790671454\n",
            "Len of Validation loss: 128, Average loss: 1.8235771455802023\n",
            "Epoch: 538, Len of Training loss: 36, Average loss: 1.8414565788375006\n",
            "Len of Validation loss: 128, Average loss: 1.8169086414854974\n",
            "Epoch: 539, Len of Training loss: 36, Average loss: 1.8226152194870844\n",
            "Len of Validation loss: 128, Average loss: 1.8059841159265488\n",
            "Epoch: 540, Len of Training loss: 36, Average loss: 1.8064490722285376\n",
            "Len of Validation loss: 128, Average loss: 1.8104835823178291\n",
            "Epoch: 541, Len of Training loss: 36, Average loss: 1.88190296292305\n",
            "Len of Validation loss: 128, Average loss: 1.7881697239354253\n",
            "Epoch: 542, Len of Training loss: 36, Average loss: 1.8264199031723871\n",
            "Len of Validation loss: 128, Average loss: 1.7870326666161418\n",
            "Epoch: 543, Len of Training loss: 36, Average loss: 1.8388903141021729\n",
            "Len of Validation loss: 128, Average loss: 1.812018938595429\n",
            "Epoch: 544, Len of Training loss: 36, Average loss: 1.8463972806930542\n",
            "Len of Validation loss: 128, Average loss: 1.8597424218896776\n",
            "Epoch: 545, Len of Training loss: 36, Average loss: 1.911355631219016\n",
            "Len of Validation loss: 128, Average loss: 1.9273249877151102\n",
            "Epoch: 546, Len of Training loss: 36, Average loss: 1.8433343602551355\n",
            "Len of Validation loss: 128, Average loss: 1.9076470400905237\n",
            "Epoch: 547, Len of Training loss: 36, Average loss: 1.9020974106258817\n",
            "Len of Validation loss: 128, Average loss: 1.8154703439213336\n",
            "Epoch: 548, Len of Training loss: 36, Average loss: 1.8692812389797635\n",
            "Len of Validation loss: 128, Average loss: 1.8289628634229302\n",
            "Epoch: 549, Len of Training loss: 36, Average loss: 1.8411288526323106\n",
            "Len of Validation loss: 128, Average loss: 1.858231726102531\n",
            "Epoch: 550, Len of Training loss: 36, Average loss: 1.851664874288771\n",
            "Len of Validation loss: 128, Average loss: 1.8324562031775713\n",
            "Epoch: 551, Len of Training loss: 36, Average loss: 1.8220547702577379\n",
            "Len of Validation loss: 128, Average loss: 1.7955268239602447\n",
            "Epoch: 552, Len of Training loss: 36, Average loss: 1.81289807955424\n",
            "Len of Validation loss: 128, Average loss: 1.7650145222432911\n",
            "Epoch: 553, Len of Training loss: 36, Average loss: 1.821724984380934\n",
            "Len of Validation loss: 128, Average loss: 1.7971212710253894\n",
            "Epoch: 554, Len of Training loss: 36, Average loss: 1.856919139623642\n",
            "Len of Validation loss: 128, Average loss: 1.7877902404870838\n",
            "Epoch: 555, Len of Training loss: 36, Average loss: 1.9094122648239136\n",
            "Len of Validation loss: 128, Average loss: 1.8949644858948886\n",
            "Epoch: 556, Len of Training loss: 36, Average loss: 1.9155221150981054\n",
            "Len of Validation loss: 128, Average loss: 1.9065993372350931\n",
            "Epoch: 557, Len of Training loss: 36, Average loss: 1.8275353742970362\n",
            "Len of Validation loss: 128, Average loss: 1.7880827288608998\n",
            "Epoch: 558, Len of Training loss: 36, Average loss: 1.8626366986168756\n",
            "Len of Validation loss: 128, Average loss: 1.8339249258860946\n",
            "Epoch: 559, Len of Training loss: 36, Average loss: 1.8312413460678525\n",
            "Len of Validation loss: 128, Average loss: 1.7930634922813624\n",
            "Epoch: 560, Len of Training loss: 36, Average loss: 1.8360998233159382\n",
            "Len of Validation loss: 128, Average loss: 1.7930501149967313\n",
            "Epoch: 561, Len of Training loss: 36, Average loss: 1.8397618664635553\n",
            "Len of Validation loss: 128, Average loss: 1.8869746190030128\n",
            "Epoch: 562, Len of Training loss: 36, Average loss: 1.8400251269340515\n",
            "Len of Validation loss: 128, Average loss: 1.838715955382213\n",
            "Epoch: 563, Len of Training loss: 36, Average loss: 1.823537717262904\n",
            "Len of Validation loss: 128, Average loss: 1.8001282887998968\n",
            "Epoch: 564, Len of Training loss: 36, Average loss: 1.811948600742552\n",
            "Len of Validation loss: 128, Average loss: 1.8026004461571574\n",
            "Epoch: 565, Len of Training loss: 36, Average loss: 1.8233220477898915\n",
            "Len of Validation loss: 128, Average loss: 1.8235607503447682\n",
            "Epoch: 566, Len of Training loss: 36, Average loss: 1.8155265980296664\n",
            "Len of Validation loss: 128, Average loss: 1.779340997338295\n",
            "Epoch: 567, Len of Training loss: 36, Average loss: 1.8253464897473652\n",
            "Len of Validation loss: 128, Average loss: 2.013666474726051\n",
            "Epoch: 568, Len of Training loss: 36, Average loss: 1.860828184419208\n",
            "Len of Validation loss: 128, Average loss: 1.849497688934207\n",
            "Epoch: 569, Len of Training loss: 36, Average loss: 1.8429950045214758\n",
            "Len of Validation loss: 128, Average loss: 1.7963561383076012\n",
            "Epoch: 570, Len of Training loss: 36, Average loss: 1.8156086438231998\n",
            "Len of Validation loss: 128, Average loss: 1.815960823558271\n",
            "Epoch: 571, Len of Training loss: 36, Average loss: 1.7910696897241805\n",
            "Len of Validation loss: 128, Average loss: 1.815635172650218\n",
            "Epoch: 572, Len of Training loss: 36, Average loss: 1.8250811994075775\n",
            "Len of Validation loss: 128, Average loss: 1.8330628592520952\n",
            "Epoch: 573, Len of Training loss: 36, Average loss: 1.946582065688239\n",
            "Len of Validation loss: 128, Average loss: 1.8403884735889733\n",
            "Epoch: 574, Len of Training loss: 36, Average loss: 1.8592009014553494\n",
            "Len of Validation loss: 128, Average loss: 1.894359778612852\n",
            "Epoch: 575, Len of Training loss: 36, Average loss: 1.8318017290698156\n",
            "Len of Validation loss: 128, Average loss: 1.8193379840813577\n",
            "Epoch: 576, Len of Training loss: 36, Average loss: 1.7827129430241055\n",
            "Len of Validation loss: 128, Average loss: 1.870092336088419\n",
            "Epoch: 577, Len of Training loss: 36, Average loss: 1.8091442121399774\n",
            "Len of Validation loss: 128, Average loss: 1.782461229013279\n",
            "Epoch: 578, Len of Training loss: 36, Average loss: 1.8010562029149797\n",
            "Len of Validation loss: 128, Average loss: 1.9108321040403098\n",
            "Epoch: 579, Len of Training loss: 36, Average loss: 1.8933517237504323\n",
            "Len of Validation loss: 128, Average loss: 1.970417701639235\n",
            "Epoch: 580, Len of Training loss: 36, Average loss: 1.8517724540498521\n",
            "Len of Validation loss: 128, Average loss: 1.7959691274445504\n",
            "Epoch: 581, Len of Training loss: 36, Average loss: 1.8567823602093592\n",
            "Len of Validation loss: 128, Average loss: 1.9279028349556029\n",
            "Epoch: 582, Len of Training loss: 36, Average loss: 1.8378264506657918\n",
            "Len of Validation loss: 128, Average loss: 1.9001585468649864\n",
            "Epoch: 583, Len of Training loss: 36, Average loss: 1.8533931374549866\n",
            "Len of Validation loss: 128, Average loss: 1.9009608787018806\n",
            "Epoch: 584, Len of Training loss: 36, Average loss: 1.8177541891733806\n",
            "Len of Validation loss: 128, Average loss: 1.791463150177151\n",
            "Epoch: 585, Len of Training loss: 36, Average loss: 1.807505299647649\n",
            "Len of Validation loss: 128, Average loss: 2.03344453452155\n",
            "Epoch: 586, Len of Training loss: 36, Average loss: 1.816560082965427\n",
            "Len of Validation loss: 128, Average loss: 1.7996290768496692\n",
            "Epoch: 587, Len of Training loss: 36, Average loss: 1.8312544524669647\n",
            "Len of Validation loss: 128, Average loss: 1.8086072553414851\n",
            "Epoch: 588, Len of Training loss: 36, Average loss: 1.8362711668014526\n",
            "Len of Validation loss: 128, Average loss: 1.8122822376899421\n",
            "Epoch: 589, Len of Training loss: 36, Average loss: 1.8563790553145938\n",
            "Len of Validation loss: 128, Average loss: 1.857444591121748\n",
            "Epoch: 590, Len of Training loss: 36, Average loss: 1.88884609275394\n",
            "Len of Validation loss: 128, Average loss: 1.851850472157821\n",
            "Epoch: 591, Len of Training loss: 36, Average loss: 1.8826710614893172\n",
            "Len of Validation loss: 128, Average loss: 1.815602102316916\n",
            "Epoch: 592, Len of Training loss: 36, Average loss: 1.8177290360132854\n",
            "Len of Validation loss: 128, Average loss: 1.7888665730133653\n",
            "Epoch: 593, Len of Training loss: 36, Average loss: 1.8136016229788463\n",
            "Len of Validation loss: 128, Average loss: 1.7763069290667772\n",
            "Epoch: 594, Len of Training loss: 36, Average loss: 1.863818930255042\n",
            "Len of Validation loss: 128, Average loss: 1.8820710072759539\n",
            "Epoch: 595, Len of Training loss: 36, Average loss: 1.845737400982115\n",
            "Len of Validation loss: 128, Average loss: 1.771450960310176\n",
            "Epoch: 596, Len of Training loss: 36, Average loss: 1.7931803928481207\n",
            "Len of Validation loss: 128, Average loss: 1.8424944959115237\n",
            "Epoch: 597, Len of Training loss: 36, Average loss: 1.830368674463696\n",
            "Len of Validation loss: 128, Average loss: 1.915048990980722\n",
            "Epoch: 598, Len of Training loss: 36, Average loss: 1.796148227320777\n",
            "Len of Validation loss: 128, Average loss: 1.829193017911166\n",
            "Epoch: 599, Len of Training loss: 36, Average loss: 1.8110464215278625\n",
            "Len of Validation loss: 128, Average loss: 1.8222233976703137\n",
            "Epoch: 600, Len of Training loss: 36, Average loss: 1.8465407954321966\n",
            "Len of Validation loss: 128, Average loss: 1.7982671570498496\n",
            "Epoch: 601, Len of Training loss: 36, Average loss: 1.8382214970058866\n",
            "Len of Validation loss: 128, Average loss: 1.8632545289583504\n",
            "Epoch: 602, Len of Training loss: 36, Average loss: 1.788314892186059\n",
            "Len of Validation loss: 128, Average loss: 1.807054542703554\n",
            "Epoch: 603, Len of Training loss: 36, Average loss: 1.782248596350352\n",
            "Len of Validation loss: 128, Average loss: 1.8017941180150956\n",
            "Epoch: 604, Len of Training loss: 36, Average loss: 1.7868488000498877\n",
            "Len of Validation loss: 128, Average loss: 1.8154279659502208\n",
            "Epoch: 605, Len of Training loss: 36, Average loss: 1.7905241515901353\n",
            "Len of Validation loss: 128, Average loss: 1.7893681479617953\n",
            "Epoch: 606, Len of Training loss: 36, Average loss: 1.7965789669089847\n",
            "Len of Validation loss: 128, Average loss: 1.803466202924028\n",
            "Epoch: 607, Len of Training loss: 36, Average loss: 1.8295228713088565\n",
            "Len of Validation loss: 128, Average loss: 1.8916289159096777\n",
            "Epoch: 608, Len of Training loss: 36, Average loss: 1.8330834309260051\n",
            "Len of Validation loss: 128, Average loss: 1.8555365772917867\n",
            "Epoch: 609, Len of Training loss: 36, Average loss: 1.9361588723129697\n",
            "Len of Validation loss: 128, Average loss: 1.7776924897916615\n",
            "Epoch: 610, Len of Training loss: 36, Average loss: 1.8085218535529242\n",
            "Len of Validation loss: 128, Average loss: 1.8429050804115832\n",
            "Epoch: 611, Len of Training loss: 36, Average loss: 1.8234972688886855\n",
            "Len of Validation loss: 128, Average loss: 1.8406024472787976\n",
            "Epoch: 612, Len of Training loss: 36, Average loss: 1.8282802899678547\n",
            "Len of Validation loss: 128, Average loss: 1.773733483394608\n",
            "Epoch: 613, Len of Training loss: 36, Average loss: 1.7861388425032299\n",
            "Len of Validation loss: 128, Average loss: 1.7762281359173357\n",
            "Epoch: 614, Len of Training loss: 36, Average loss: 1.8138796952035692\n",
            "Len of Validation loss: 128, Average loss: 1.7783305691555142\n",
            "Epoch: 615, Len of Training loss: 36, Average loss: 1.767047256231308\n",
            "Len of Validation loss: 128, Average loss: 1.82343491865322\n",
            "Epoch: 616, Len of Training loss: 36, Average loss: 1.7878355648782518\n",
            "Len of Validation loss: 128, Average loss: 1.907126960111782\n",
            "Epoch: 617, Len of Training loss: 36, Average loss: 1.8062026732497745\n",
            "Len of Validation loss: 128, Average loss: 1.8958671069703996\n",
            "Epoch: 618, Len of Training loss: 36, Average loss: 1.807149115535948\n",
            "Len of Validation loss: 128, Average loss: 1.8194589619524777\n",
            "Epoch: 619, Len of Training loss: 36, Average loss: 1.772784713241789\n",
            "Len of Validation loss: 128, Average loss: 1.778184311464429\n",
            "Epoch: 620, Len of Training loss: 36, Average loss: 1.7840678592522938\n",
            "Len of Validation loss: 128, Average loss: 1.80111976666376\n",
            "Epoch: 621, Len of Training loss: 36, Average loss: 1.8164682421419356\n",
            "Len of Validation loss: 128, Average loss: 1.8006207852158695\n",
            "Epoch: 622, Len of Training loss: 36, Average loss: 1.8413346476025052\n",
            "Len of Validation loss: 128, Average loss: 1.851659839041531\n",
            "Epoch: 623, Len of Training loss: 36, Average loss: 1.7900305953290727\n",
            "Len of Validation loss: 128, Average loss: 1.799729720223695\n",
            "Epoch: 624, Len of Training loss: 36, Average loss: 1.8089833392037287\n",
            "Len of Validation loss: 128, Average loss: 1.793716651853174\n",
            "Epoch: 625, Len of Training loss: 36, Average loss: 1.8243145479096308\n",
            "Len of Validation loss: 128, Average loss: 1.8110600039362907\n",
            "Epoch: 626, Len of Training loss: 36, Average loss: 1.7915811406241522\n",
            "Len of Validation loss: 128, Average loss: 1.7707005434203893\n",
            "Epoch: 627, Len of Training loss: 36, Average loss: 1.801730861266454\n",
            "Len of Validation loss: 128, Average loss: 1.8910163845866919\n",
            "Epoch: 628, Len of Training loss: 36, Average loss: 1.8224013845125835\n",
            "Len of Validation loss: 128, Average loss: 1.9667498166672885\n",
            "Epoch: 629, Len of Training loss: 36, Average loss: 1.787729134162267\n",
            "Len of Validation loss: 128, Average loss: 1.8055807566270232\n",
            "Epoch: 630, Len of Training loss: 36, Average loss: 1.8280368281735315\n",
            "Len of Validation loss: 128, Average loss: 1.7859743325971067\n",
            "Epoch: 631, Len of Training loss: 36, Average loss: 1.814806491136551\n",
            "Len of Validation loss: 128, Average loss: 1.8091906018089503\n",
            "Epoch: 632, Len of Training loss: 36, Average loss: 1.8277261555194855\n",
            "Len of Validation loss: 128, Average loss: 1.867439104244113\n",
            "Epoch: 633, Len of Training loss: 36, Average loss: 1.847300221522649\n",
            "Len of Validation loss: 128, Average loss: 1.827653035055846\n",
            "Epoch: 634, Len of Training loss: 36, Average loss: 1.8197458121511672\n",
            "Len of Validation loss: 128, Average loss: 1.7929398627020419\n",
            "Epoch: 635, Len of Training loss: 36, Average loss: 1.8101801706684961\n",
            "Len of Validation loss: 128, Average loss: 1.8006665881257504\n",
            "Epoch: 636, Len of Training loss: 36, Average loss: 1.797670715385013\n",
            "Len of Validation loss: 128, Average loss: 1.7626621664967388\n",
            "Epoch: 637, Len of Training loss: 36, Average loss: 1.7971832785341475\n",
            "Len of Validation loss: 128, Average loss: 1.8336581652984023\n",
            "Epoch: 638, Len of Training loss: 36, Average loss: 1.7923129200935364\n",
            "Len of Validation loss: 128, Average loss: 1.847609001211822\n",
            "Epoch: 639, Len of Training loss: 36, Average loss: 1.777429050869412\n",
            "Len of Validation loss: 128, Average loss: 1.79978188383393\n",
            "Epoch: 640, Len of Training loss: 36, Average loss: 1.7829246123631795\n",
            "Len of Validation loss: 128, Average loss: 1.8392144860699773\n",
            "Epoch: 641, Len of Training loss: 36, Average loss: 1.7631412976317935\n",
            "Len of Validation loss: 128, Average loss: 1.8465055101551116\n",
            "Epoch: 642, Len of Training loss: 36, Average loss: 1.8004100024700165\n",
            "Len of Validation loss: 128, Average loss: 1.8635630272328854\n",
            "Epoch: 643, Len of Training loss: 36, Average loss: 1.7669013639291127\n",
            "Len of Validation loss: 128, Average loss: 1.8227836040314287\n",
            "Epoch: 644, Len of Training loss: 36, Average loss: 1.7570898052718904\n",
            "Len of Validation loss: 128, Average loss: 1.7793664510827512\n",
            "Epoch: 645, Len of Training loss: 36, Average loss: 1.7780257827705808\n",
            "Len of Validation loss: 128, Average loss: 1.829548496287316\n",
            "Epoch: 646, Len of Training loss: 36, Average loss: 1.7954995334148407\n",
            "Len of Validation loss: 128, Average loss: 1.8022207112517208\n",
            "Epoch: 647, Len of Training loss: 36, Average loss: 1.8505961100260417\n",
            "Len of Validation loss: 128, Average loss: 1.8017920069396496\n",
            "Epoch: 648, Len of Training loss: 36, Average loss: 1.8075381914774578\n",
            "Len of Validation loss: 128, Average loss: 1.8407964520156384\n",
            "Epoch: 649, Len of Training loss: 36, Average loss: 1.8424253000153437\n",
            "Len of Validation loss: 128, Average loss: 1.8950459659099579\n",
            "Epoch: 650, Len of Training loss: 36, Average loss: 1.904556800921758\n",
            "Len of Validation loss: 128, Average loss: 1.8199925103690475\n",
            "Epoch: 651, Len of Training loss: 36, Average loss: 1.8434748550256093\n",
            "Len of Validation loss: 128, Average loss: 1.8179290262050927\n",
            "Epoch: 652, Len of Training loss: 36, Average loss: 1.791773839129342\n",
            "Len of Validation loss: 128, Average loss: 1.8177024137694389\n",
            "Epoch: 653, Len of Training loss: 36, Average loss: 1.826729095644421\n",
            "Len of Validation loss: 128, Average loss: 1.795956787886098\n",
            "Epoch: 654, Len of Training loss: 36, Average loss: 1.8142705394162073\n",
            "Len of Validation loss: 128, Average loss: 1.804474433651194\n",
            "Epoch: 655, Len of Training loss: 36, Average loss: 1.7977007428805034\n",
            "Len of Validation loss: 128, Average loss: 1.8051255100872368\n",
            "Epoch: 656, Len of Training loss: 36, Average loss: 1.804065465927124\n",
            "Len of Validation loss: 128, Average loss: 1.8177815373055637\n",
            "Epoch: 657, Len of Training loss: 36, Average loss: 1.7572004894415538\n",
            "Len of Validation loss: 128, Average loss: 1.78032898157835\n",
            "Epoch: 658, Len of Training loss: 36, Average loss: 1.7661553687519498\n",
            "Len of Validation loss: 128, Average loss: 1.8753169146366417\n",
            "Epoch: 659, Len of Training loss: 36, Average loss: 1.8044267197450001\n",
            "Len of Validation loss: 128, Average loss: 1.9994744663126767\n",
            "Epoch: 660, Len of Training loss: 36, Average loss: 1.8508923451105754\n",
            "Len of Validation loss: 128, Average loss: 1.821550938533619\n",
            "Epoch: 661, Len of Training loss: 36, Average loss: 1.8338762554857466\n",
            "Len of Validation loss: 128, Average loss: 1.8070963746868074\n",
            "Epoch: 662, Len of Training loss: 36, Average loss: 1.7806479003694322\n",
            "Len of Validation loss: 128, Average loss: 1.7542641456238925\n",
            "Epoch: 663, Len of Training loss: 36, Average loss: 1.7460578315787845\n",
            "Len of Validation loss: 128, Average loss: 1.8125332628842443\n",
            "Epoch: 664, Len of Training loss: 36, Average loss: 1.7609318163659837\n",
            "Len of Validation loss: 128, Average loss: 1.910788122098893\n",
            "Epoch: 665, Len of Training loss: 36, Average loss: 1.7834900783167944\n",
            "Len of Validation loss: 128, Average loss: 1.8078642743639648\n",
            "Epoch: 666, Len of Training loss: 36, Average loss: 1.784982098473443\n",
            "Len of Validation loss: 128, Average loss: 1.814619586803019\n",
            "Epoch: 667, Len of Training loss: 36, Average loss: 1.7787509891721938\n",
            "Len of Validation loss: 128, Average loss: 1.799829810159281\n",
            "Epoch: 668, Len of Training loss: 36, Average loss: 1.7689393626319037\n",
            "Len of Validation loss: 128, Average loss: 1.861424705479294\n",
            "Epoch: 669, Len of Training loss: 36, Average loss: 1.7781996462080214\n",
            "Len of Validation loss: 128, Average loss: 1.9970525632379577\n",
            "Epoch: 670, Len of Training loss: 36, Average loss: 1.7905110087659624\n",
            "Len of Validation loss: 128, Average loss: 1.7917738975957036\n",
            "Epoch: 671, Len of Training loss: 36, Average loss: 1.7814121146996815\n",
            "Len of Validation loss: 128, Average loss: 1.7900451687164605\n",
            "Epoch: 672, Len of Training loss: 36, Average loss: 1.7807204325993855\n",
            "Len of Validation loss: 128, Average loss: 1.9843452221248299\n",
            "Epoch: 673, Len of Training loss: 36, Average loss: 1.816586775912179\n",
            "Len of Validation loss: 128, Average loss: 1.780429560225457\n",
            "Epoch: 674, Len of Training loss: 36, Average loss: 1.7652790082825556\n",
            "Len of Validation loss: 128, Average loss: 1.7545626640785486\n",
            "Epoch: 675, Len of Training loss: 36, Average loss: 1.7753006054295435\n",
            "Len of Validation loss: 128, Average loss: 1.879270771984011\n",
            "Epoch: 676, Len of Training loss: 36, Average loss: 1.795038554403517\n",
            "Len of Validation loss: 128, Average loss: 1.8079140642657876\n",
            "Epoch: 677, Len of Training loss: 36, Average loss: 1.8426490426063538\n",
            "Len of Validation loss: 128, Average loss: 1.9788493807427585\n",
            "Epoch: 678, Len of Training loss: 36, Average loss: 1.7993522153960333\n",
            "Len of Validation loss: 128, Average loss: 1.8003625897690654\n",
            "Epoch: 679, Len of Training loss: 36, Average loss: 1.7580648859341939\n",
            "Len of Validation loss: 128, Average loss: 1.7958382084034383\n",
            "Epoch: 680, Len of Training loss: 36, Average loss: 1.7751590344640944\n",
            "Len of Validation loss: 128, Average loss: 1.8096461044624448\n",
            "Epoch: 681, Len of Training loss: 36, Average loss: 1.7747573753197987\n",
            "Len of Validation loss: 128, Average loss: 1.7809530273079872\n",
            "Epoch: 682, Len of Training loss: 36, Average loss: 1.7984530197249518\n",
            "Len of Validation loss: 128, Average loss: 1.755286753643304\n",
            "Epoch: 683, Len of Training loss: 36, Average loss: 1.7424430482917361\n",
            "Len of Validation loss: 128, Average loss: 1.8235175113659352\n",
            "Epoch: 684, Len of Training loss: 36, Average loss: 1.7839399609300826\n",
            "Len of Validation loss: 128, Average loss: 1.8656797900330275\n",
            "Epoch: 685, Len of Training loss: 36, Average loss: 1.7774790128072102\n",
            "Len of Validation loss: 128, Average loss: 2.055205959826708\n",
            "Epoch: 686, Len of Training loss: 36, Average loss: 1.8007618122630649\n",
            "Len of Validation loss: 128, Average loss: 1.922160183545202\n",
            "Epoch: 687, Len of Training loss: 36, Average loss: 1.7782842251989577\n",
            "Len of Validation loss: 128, Average loss: 1.891021437710151\n",
            "Epoch: 688, Len of Training loss: 36, Average loss: 1.7496686975161235\n",
            "Len of Validation loss: 128, Average loss: 1.8357112337835133\n",
            "Epoch: 689, Len of Training loss: 36, Average loss: 1.8086917400360107\n",
            "Len of Validation loss: 128, Average loss: 1.756829989142716\n",
            "Epoch: 690, Len of Training loss: 36, Average loss: 1.7497878869374592\n",
            "Len of Validation loss: 128, Average loss: 1.8288888989482075\n",
            "Epoch: 691, Len of Training loss: 36, Average loss: 1.7481286360157862\n",
            "Len of Validation loss: 128, Average loss: 1.8249389985576272\n",
            "Epoch: 692, Len of Training loss: 36, Average loss: 1.8190807302792866\n",
            "Len of Validation loss: 128, Average loss: 1.8282681482378393\n",
            "Epoch: 693, Len of Training loss: 36, Average loss: 1.788261592388153\n",
            "Len of Validation loss: 128, Average loss: 1.8031678551342338\n",
            "Epoch: 694, Len of Training loss: 36, Average loss: 1.7439477543036144\n",
            "Len of Validation loss: 128, Average loss: 1.77517518424429\n",
            "Epoch: 695, Len of Training loss: 36, Average loss: 1.7742235130733914\n",
            "Len of Validation loss: 128, Average loss: 1.8619074940215796\n",
            "Epoch: 696, Len of Training loss: 36, Average loss: 1.769120905134413\n",
            "Len of Validation loss: 128, Average loss: 1.8048363793641329\n",
            "Epoch: 697, Len of Training loss: 36, Average loss: 1.7637536856863234\n",
            "Len of Validation loss: 128, Average loss: 1.8145546554587781\n",
            "Epoch: 698, Len of Training loss: 36, Average loss: 1.8257363306151495\n",
            "Len of Validation loss: 128, Average loss: 1.7955800152849406\n",
            "Epoch: 699, Len of Training loss: 36, Average loss: 1.7889516519175634\n",
            "Len of Validation loss: 128, Average loss: 1.8821011406835169\n",
            "Epoch: 700, Len of Training loss: 36, Average loss: 1.7498328453964658\n",
            "Len of Validation loss: 128, Average loss: 1.7965141243766993\n",
            "Epoch: 701, Len of Training loss: 36, Average loss: 1.748124662372801\n",
            "Len of Validation loss: 128, Average loss: 1.7942123722750694\n",
            "Epoch: 702, Len of Training loss: 36, Average loss: 1.7461653881602817\n",
            "Len of Validation loss: 128, Average loss: 2.048190122935921\n",
            "Epoch: 703, Len of Training loss: 36, Average loss: 1.7777093516455755\n",
            "Len of Validation loss: 128, Average loss: 1.804234756855294\n",
            "Epoch: 704, Len of Training loss: 36, Average loss: 1.8097080720795526\n",
            "Len of Validation loss: 128, Average loss: 1.8701764768920839\n",
            "Epoch: 705, Len of Training loss: 36, Average loss: 1.7674459318319957\n",
            "Len of Validation loss: 128, Average loss: 1.7947166273370385\n",
            "Epoch: 706, Len of Training loss: 36, Average loss: 1.7841700712839763\n",
            "Len of Validation loss: 128, Average loss: 1.8534767914097756\n",
            "Epoch: 707, Len of Training loss: 36, Average loss: 1.7269609206252627\n",
            "Len of Validation loss: 128, Average loss: 1.8278082113247365\n",
            "Epoch: 708, Len of Training loss: 36, Average loss: 1.7268582648701138\n",
            "Len of Validation loss: 128, Average loss: 1.8137772239279002\n",
            "Epoch: 709, Len of Training loss: 36, Average loss: 1.7916094594531589\n",
            "Len of Validation loss: 128, Average loss: 1.9541284728329629\n",
            "Epoch: 710, Len of Training loss: 36, Average loss: 1.7567423284053802\n",
            "Len of Validation loss: 128, Average loss: 1.7895846059545875\n",
            "Epoch: 711, Len of Training loss: 36, Average loss: 1.7570435139867995\n",
            "Len of Validation loss: 128, Average loss: 1.9516773577779531\n",
            "Epoch: 712, Len of Training loss: 36, Average loss: 1.7914007206757863\n",
            "Len of Validation loss: 128, Average loss: 1.943711255211383\n",
            "Epoch: 713, Len of Training loss: 36, Average loss: 1.7894344197379217\n",
            "Len of Validation loss: 128, Average loss: 1.901334609137848\n",
            "Epoch: 714, Len of Training loss: 36, Average loss: 1.7573126786284976\n",
            "Len of Validation loss: 128, Average loss: 1.7871684113051742\n",
            "Epoch: 715, Len of Training loss: 36, Average loss: 1.740672339995702\n",
            "Len of Validation loss: 128, Average loss: 1.8029623876791447\n",
            "Epoch: 716, Len of Training loss: 36, Average loss: 1.7150126033359103\n",
            "Len of Validation loss: 128, Average loss: 1.8071843991056085\n",
            "Epoch: 717, Len of Training loss: 36, Average loss: 1.7817398077911801\n",
            "Len of Validation loss: 128, Average loss: 1.82740607438609\n",
            "Epoch: 718, Len of Training loss: 36, Average loss: 1.7835214104917314\n",
            "Len of Validation loss: 128, Average loss: 1.8579959569033235\n",
            "Epoch: 719, Len of Training loss: 36, Average loss: 1.8082641892962985\n",
            "Len of Validation loss: 128, Average loss: 1.7948117670603096\n",
            "Epoch: 720, Len of Training loss: 36, Average loss: 1.8018317222595215\n",
            "Len of Validation loss: 128, Average loss: 1.8554617362096906\n",
            "Epoch: 721, Len of Training loss: 36, Average loss: 1.7598946789900463\n",
            "Len of Validation loss: 128, Average loss: 1.8043629026506096\n",
            "Epoch: 722, Len of Training loss: 36, Average loss: 1.8811730808681912\n",
            "Len of Validation loss: 128, Average loss: 1.888646297622472\n",
            "Epoch: 723, Len of Training loss: 36, Average loss: 1.8193311327033572\n",
            "Len of Validation loss: 128, Average loss: 1.8133641690947115\n",
            "Epoch: 724, Len of Training loss: 36, Average loss: 1.8344342311223347\n",
            "Len of Validation loss: 128, Average loss: 1.822976647876203\n",
            "Epoch: 725, Len of Training loss: 36, Average loss: 1.7716663049327002\n",
            "Len of Validation loss: 128, Average loss: 1.9022870506159961\n",
            "Epoch: 726, Len of Training loss: 36, Average loss: 1.7632058295938704\n",
            "Len of Validation loss: 128, Average loss: 1.9280653623864055\n",
            "Epoch: 727, Len of Training loss: 36, Average loss: 1.7405968341562483\n",
            "Len of Validation loss: 128, Average loss: 1.8946945478674024\n",
            "Epoch: 728, Len of Training loss: 36, Average loss: 1.76197416583697\n",
            "Len of Validation loss: 128, Average loss: 1.7877321732230484\n",
            "Epoch: 729, Len of Training loss: 36, Average loss: 1.733918332391315\n",
            "Len of Validation loss: 128, Average loss: 1.757174039259553\n",
            "Epoch: 730, Len of Training loss: 36, Average loss: 1.7649411095513239\n",
            "Len of Validation loss: 128, Average loss: 1.8747803752776235\n",
            "Epoch: 731, Len of Training loss: 36, Average loss: 1.7751029895411596\n",
            "Len of Validation loss: 128, Average loss: 1.7335648559965193\n",
            "Epoch: 732, Len of Training loss: 36, Average loss: 1.7927703559398651\n",
            "Len of Validation loss: 128, Average loss: 1.9331095832167193\n",
            "Epoch: 733, Len of Training loss: 36, Average loss: 1.7800366547372606\n",
            "Len of Validation loss: 128, Average loss: 1.8459007104393095\n",
            "Epoch: 734, Len of Training loss: 36, Average loss: 1.7824435300297208\n",
            "Len of Validation loss: 128, Average loss: 1.7859755682293326\n",
            "Epoch: 735, Len of Training loss: 36, Average loss: 1.7699111600716908\n",
            "Len of Validation loss: 128, Average loss: 1.8297096388414502\n",
            "Epoch: 736, Len of Training loss: 36, Average loss: 1.724395689037111\n",
            "Len of Validation loss: 128, Average loss: 1.774379603099078\n",
            "Epoch: 737, Len of Training loss: 36, Average loss: 1.7192928459909227\n",
            "Len of Validation loss: 128, Average loss: 1.8687451207078993\n",
            "Epoch: 738, Len of Training loss: 36, Average loss: 1.7593237625228033\n",
            "Len of Validation loss: 128, Average loss: 1.7923228240106255\n",
            "Epoch: 739, Len of Training loss: 36, Average loss: 1.7130355603165097\n",
            "Len of Validation loss: 128, Average loss: 1.794565612450242\n",
            "Epoch: 740, Len of Training loss: 36, Average loss: 1.7744510604275598\n",
            "Len of Validation loss: 128, Average loss: 1.8914113172795624\n",
            "Epoch: 741, Len of Training loss: 36, Average loss: 1.8412630558013916\n",
            "Len of Validation loss: 128, Average loss: 1.8103898982517421\n",
            "Epoch: 742, Len of Training loss: 36, Average loss: 1.7500229510996077\n",
            "Len of Validation loss: 128, Average loss: 1.833393044071272\n",
            "Epoch: 743, Len of Training loss: 36, Average loss: 1.7526637348863814\n",
            "Len of Validation loss: 128, Average loss: 1.792120395693928\n",
            "Epoch: 744, Len of Training loss: 36, Average loss: 1.7110231386290655\n",
            "Len of Validation loss: 128, Average loss: 1.865717242937535\n",
            "Epoch: 745, Len of Training loss: 36, Average loss: 1.7580409049987793\n",
            "Len of Validation loss: 128, Average loss: 1.7765062437392771\n",
            "Epoch: 746, Len of Training loss: 36, Average loss: 1.728027022547192\n",
            "Len of Validation loss: 128, Average loss: 1.78561604837887\n",
            "Epoch: 747, Len of Training loss: 36, Average loss: 1.7102449138959248\n",
            "Len of Validation loss: 128, Average loss: 1.835191432852298\n",
            "Epoch: 748, Len of Training loss: 36, Average loss: 1.7592836055490706\n",
            "Len of Validation loss: 128, Average loss: 1.8052936140447855\n",
            "Epoch: 749, Len of Training loss: 36, Average loss: 1.7606036795510187\n",
            "Len of Validation loss: 128, Average loss: 1.9557237948756665\n",
            "Epoch: 750, Len of Training loss: 36, Average loss: 1.7889503604835935\n",
            "Len of Validation loss: 128, Average loss: 1.7936753071844578\n",
            "Epoch: 751, Len of Training loss: 36, Average loss: 1.794739180141025\n",
            "Len of Validation loss: 128, Average loss: 1.7486288347281516\n",
            "Epoch: 752, Len of Training loss: 36, Average loss: 1.766859557893541\n",
            "Len of Validation loss: 128, Average loss: 1.8440107041969895\n",
            "Epoch: 753, Len of Training loss: 36, Average loss: 1.7777657210826874\n",
            "Len of Validation loss: 128, Average loss: 1.7430119181517512\n",
            "Epoch: 754, Len of Training loss: 36, Average loss: 1.7425717115402222\n",
            "Len of Validation loss: 128, Average loss: 1.7932063867338002\n",
            "Epoch: 755, Len of Training loss: 36, Average loss: 1.7556838128301833\n",
            "Len of Validation loss: 128, Average loss: 1.8205095191951841\n",
            "Epoch: 756, Len of Training loss: 36, Average loss: 1.7973577744430966\n",
            "Len of Validation loss: 128, Average loss: 1.7826739915180951\n",
            "Epoch: 757, Len of Training loss: 36, Average loss: 1.7094762126604717\n",
            "Len of Validation loss: 128, Average loss: 1.767301598098129\n",
            "Epoch: 758, Len of Training loss: 36, Average loss: 1.7578447262446086\n",
            "Len of Validation loss: 128, Average loss: 1.817481940612197\n",
            "Epoch: 759, Len of Training loss: 36, Average loss: 1.7614874641100566\n",
            "Len of Validation loss: 128, Average loss: 1.9407380553893745\n",
            "Epoch: 760, Len of Training loss: 36, Average loss: 1.7344522178173065\n",
            "Len of Validation loss: 128, Average loss: 1.8105957645457238\n",
            "Epoch: 761, Len of Training loss: 36, Average loss: 1.7114230659272935\n",
            "Len of Validation loss: 128, Average loss: 1.8315895462874323\n",
            "Epoch: 762, Len of Training loss: 36, Average loss: 1.8110379543569353\n",
            "Len of Validation loss: 128, Average loss: 1.9540275372564793\n",
            "Epoch: 763, Len of Training loss: 36, Average loss: 1.7502141396204631\n",
            "Len of Validation loss: 128, Average loss: 1.7596278944984078\n",
            "Epoch: 764, Len of Training loss: 36, Average loss: 1.7421322531170316\n",
            "Len of Validation loss: 128, Average loss: 1.8120410088449717\n",
            "Epoch: 765, Len of Training loss: 36, Average loss: 1.738516519467036\n",
            "Len of Validation loss: 128, Average loss: 1.7914966342505068\n",
            "Epoch: 766, Len of Training loss: 36, Average loss: 1.745422899723053\n",
            "Len of Validation loss: 128, Average loss: 1.8879699325188994\n",
            "Epoch: 767, Len of Training loss: 36, Average loss: 1.758580396572749\n",
            "Len of Validation loss: 128, Average loss: 1.8131575067527592\n",
            "Epoch: 768, Len of Training loss: 36, Average loss: 1.7131201922893524\n",
            "Len of Validation loss: 128, Average loss: 1.7960745037999004\n",
            "Epoch: 769, Len of Training loss: 36, Average loss: 1.7289533449543848\n",
            "Len of Validation loss: 128, Average loss: 1.847748352214694\n",
            "Epoch: 770, Len of Training loss: 36, Average loss: 1.728341966867447\n",
            "Len of Validation loss: 128, Average loss: 1.827917376998812\n",
            "Epoch: 771, Len of Training loss: 36, Average loss: 1.7492004202471838\n",
            "Len of Validation loss: 128, Average loss: 1.8065852164290845\n",
            "Epoch: 772, Len of Training loss: 36, Average loss: 1.7275013658735487\n",
            "Len of Validation loss: 128, Average loss: 1.87410489609465\n",
            "Epoch: 773, Len of Training loss: 36, Average loss: 1.72898754146364\n",
            "Len of Validation loss: 128, Average loss: 1.7997731021605432\n",
            "Epoch: 774, Len of Training loss: 36, Average loss: 1.7306941780779097\n",
            "Len of Validation loss: 128, Average loss: 1.730656137689948\n",
            "Epoch: 775, Len of Training loss: 36, Average loss: 1.7237429122130077\n",
            "Len of Validation loss: 128, Average loss: 1.808465762063861\n",
            "Epoch: 776, Len of Training loss: 36, Average loss: 1.7193306883176167\n",
            "Len of Validation loss: 128, Average loss: 1.7811877643689513\n",
            "Epoch: 777, Len of Training loss: 36, Average loss: 1.7335791852739122\n",
            "Len of Validation loss: 128, Average loss: 1.831631527049467\n",
            "Epoch: 778, Len of Training loss: 36, Average loss: 1.7615576485792797\n",
            "Len of Validation loss: 128, Average loss: 1.7738131743390113\n",
            "Epoch: 779, Len of Training loss: 36, Average loss: 1.7009190652105544\n",
            "Len of Validation loss: 128, Average loss: 1.8107853091787547\n",
            "Epoch: 780, Len of Training loss: 36, Average loss: 1.7173051204946306\n",
            "Len of Validation loss: 128, Average loss: 1.9527144287712872\n",
            "Epoch: 781, Len of Training loss: 36, Average loss: 1.7919866310225592\n",
            "Len of Validation loss: 128, Average loss: 1.8235773611813784\n",
            "Epoch: 782, Len of Training loss: 36, Average loss: 1.7528151174386342\n",
            "Len of Validation loss: 128, Average loss: 1.8074148460291326\n",
            "Epoch: 783, Len of Training loss: 36, Average loss: 1.7435283594661288\n",
            "Len of Validation loss: 128, Average loss: 1.7524131371174008\n",
            "Epoch: 784, Len of Training loss: 36, Average loss: 1.7464854651027255\n",
            "Len of Validation loss: 128, Average loss: 1.7864439175464213\n",
            "Epoch: 785, Len of Training loss: 36, Average loss: 1.7345843580034044\n",
            "Len of Validation loss: 128, Average loss: 1.8578489213250577\n",
            "Epoch: 786, Len of Training loss: 36, Average loss: 1.7987420525815752\n",
            "Len of Validation loss: 128, Average loss: 1.8739510152954608\n",
            "Epoch: 787, Len of Training loss: 36, Average loss: 1.7284818589687347\n",
            "Len of Validation loss: 128, Average loss: 1.7688250606879592\n",
            "Epoch: 788, Len of Training loss: 36, Average loss: 1.6966727541552649\n",
            "Len of Validation loss: 128, Average loss: 1.7936299610882998\n",
            "Epoch: 789, Len of Training loss: 36, Average loss: 1.7212882472409143\n",
            "Len of Validation loss: 128, Average loss: 1.7696929718367755\n",
            "Epoch: 790, Len of Training loss: 36, Average loss: 1.7034567097822826\n",
            "Len of Validation loss: 128, Average loss: 1.758980378974229\n",
            "Epoch: 791, Len of Training loss: 36, Average loss: 1.7100912001397874\n",
            "Len of Validation loss: 128, Average loss: 1.7581527538131922\n",
            "Epoch: 792, Len of Training loss: 36, Average loss: 1.700316505299674\n",
            "Len of Validation loss: 128, Average loss: 1.7671958792489022\n",
            "Epoch: 793, Len of Training loss: 36, Average loss: 1.7564571102460225\n",
            "Len of Validation loss: 128, Average loss: 1.8500787385273725\n",
            "Epoch: 794, Len of Training loss: 36, Average loss: 1.7321765787071652\n",
            "Len of Validation loss: 128, Average loss: 1.8493563374504447\n",
            "Epoch: 795, Len of Training loss: 36, Average loss: 1.7839004927211337\n",
            "Len of Validation loss: 128, Average loss: 1.8649620721116662\n",
            "Epoch: 796, Len of Training loss: 36, Average loss: 1.7374126083321042\n",
            "Len of Validation loss: 128, Average loss: 1.7744689057581127\n",
            "Epoch: 797, Len of Training loss: 36, Average loss: 1.765174878968133\n",
            "Len of Validation loss: 128, Average loss: 2.057321321684867\n",
            "Epoch: 798, Len of Training loss: 36, Average loss: 1.743807272778617\n",
            "Len of Validation loss: 128, Average loss: 1.8608565833419561\n",
            "Epoch: 799, Len of Training loss: 36, Average loss: 1.713875197701984\n",
            "Len of Validation loss: 128, Average loss: 1.777963054832071\n",
            "Epoch: 800, Len of Training loss: 36, Average loss: 1.7588663465446897\n",
            "Len of Validation loss: 128, Average loss: 1.7368157147429883\n",
            "Epoch: 801, Len of Training loss: 36, Average loss: 1.8156302405728235\n",
            "Len of Validation loss: 128, Average loss: 1.8012399822473526\n",
            "Epoch: 802, Len of Training loss: 36, Average loss: 1.697008126311832\n",
            "Len of Validation loss: 128, Average loss: 1.8139805931132287\n",
            "Epoch: 803, Len of Training loss: 36, Average loss: 1.7169292337364621\n",
            "Len of Validation loss: 128, Average loss: 1.817106812959537\n",
            "Epoch: 804, Len of Training loss: 36, Average loss: 1.722165885898802\n",
            "Len of Validation loss: 128, Average loss: 1.7555427348706871\n",
            "Epoch: 805, Len of Training loss: 36, Average loss: 1.7540398240089417\n",
            "Len of Validation loss: 128, Average loss: 1.9147825760301203\n",
            "Epoch: 806, Len of Training loss: 36, Average loss: 1.8080223864979215\n",
            "Len of Validation loss: 128, Average loss: 1.8542840396985412\n",
            "Epoch: 807, Len of Training loss: 36, Average loss: 1.7670799096425374\n",
            "Len of Validation loss: 128, Average loss: 1.8211768188048154\n",
            "Epoch: 808, Len of Training loss: 36, Average loss: 1.7075408399105072\n",
            "Len of Validation loss: 128, Average loss: 1.7693317830562592\n",
            "Epoch: 809, Len of Training loss: 36, Average loss: 1.7184952199459076\n",
            "Len of Validation loss: 128, Average loss: 1.777019841130823\n",
            "Epoch: 810, Len of Training loss: 36, Average loss: 1.6996169553862677\n",
            "Len of Validation loss: 128, Average loss: 1.7829258763231337\n",
            "Epoch: 811, Len of Training loss: 36, Average loss: 1.6981040901607938\n",
            "Len of Validation loss: 128, Average loss: 1.7856898026075214\n",
            "Epoch: 812, Len of Training loss: 36, Average loss: 1.708049539062712\n",
            "Len of Validation loss: 128, Average loss: 1.7698509416077286\n",
            "Epoch: 813, Len of Training loss: 36, Average loss: 1.6976747380362616\n",
            "Len of Validation loss: 128, Average loss: 1.8058876364957541\n",
            "Epoch: 814, Len of Training loss: 36, Average loss: 1.7144610351986356\n",
            "Len of Validation loss: 128, Average loss: 1.8523663082160056\n",
            "Epoch: 815, Len of Training loss: 36, Average loss: 1.7184652156300015\n",
            "Len of Validation loss: 128, Average loss: 1.750552237732336\n",
            "Epoch: 816, Len of Training loss: 36, Average loss: 1.736243764559428\n",
            "Len of Validation loss: 128, Average loss: 1.8053962816484272\n",
            "Epoch: 817, Len of Training loss: 36, Average loss: 1.7484148210949368\n",
            "Len of Validation loss: 128, Average loss: 1.821632384089753\n",
            "Epoch: 818, Len of Training loss: 36, Average loss: 1.6852546963426802\n",
            "Len of Validation loss: 128, Average loss: 1.8000522695947438\n",
            "Epoch: 819, Len of Training loss: 36, Average loss: 1.7240064243475597\n",
            "Len of Validation loss: 128, Average loss: 1.7792858893517405\n",
            "Epoch: 820, Len of Training loss: 36, Average loss: 1.7207932538456387\n",
            "Len of Validation loss: 128, Average loss: 1.7624541372060776\n",
            "Epoch: 821, Len of Training loss: 36, Average loss: 1.7177783747514088\n",
            "Len of Validation loss: 128, Average loss: 1.7731711263768375\n",
            "Epoch: 822, Len of Training loss: 36, Average loss: 1.751871582534578\n",
            "Len of Validation loss: 128, Average loss: 1.8130860617384315\n",
            "Epoch: 823, Len of Training loss: 36, Average loss: 1.7190541194544897\n",
            "Len of Validation loss: 128, Average loss: 1.7934376439079642\n",
            "Epoch: 824, Len of Training loss: 36, Average loss: 1.7427943266100354\n",
            "Len of Validation loss: 128, Average loss: 1.9109563475940377\n",
            "Epoch: 825, Len of Training loss: 36, Average loss: 1.7460310955842335\n",
            "Len of Validation loss: 128, Average loss: 1.8422671279404312\n",
            "Epoch: 826, Len of Training loss: 36, Average loss: 1.7274468044439952\n",
            "Len of Validation loss: 128, Average loss: 1.9083131696097553\n",
            "Epoch: 827, Len of Training loss: 36, Average loss: 1.7234409352143605\n",
            "Len of Validation loss: 128, Average loss: 1.776979363989085\n",
            "Epoch: 828, Len of Training loss: 36, Average loss: 1.7043368021647136\n",
            "Len of Validation loss: 128, Average loss: 1.8291118261404335\n",
            "Epoch: 829, Len of Training loss: 36, Average loss: 1.7430834339724646\n",
            "Len of Validation loss: 128, Average loss: 1.8070697872899473\n",
            "Epoch: 830, Len of Training loss: 36, Average loss: 1.704113142357932\n",
            "Len of Validation loss: 128, Average loss: 1.8740180265158415\n",
            "Epoch: 831, Len of Training loss: 36, Average loss: 1.7027732266320124\n",
            "Len of Validation loss: 128, Average loss: 1.8025275322142988\n",
            "Epoch: 832, Len of Training loss: 36, Average loss: 1.676819966899024\n",
            "Len of Validation loss: 128, Average loss: 1.78713199775666\n",
            "Epoch: 833, Len of Training loss: 36, Average loss: 1.6893275876839955\n",
            "Len of Validation loss: 128, Average loss: 1.8058787039481103\n",
            "Epoch: 834, Len of Training loss: 36, Average loss: 1.6973276105191972\n",
            "Len of Validation loss: 128, Average loss: 1.7675676031503826\n",
            "Epoch: 835, Len of Training loss: 36, Average loss: 1.7353820867008634\n",
            "Len of Validation loss: 128, Average loss: 1.8735764003358781\n",
            "Epoch: 836, Len of Training loss: 36, Average loss: 1.7131367458237543\n",
            "Len of Validation loss: 128, Average loss: 1.7582913499791175\n",
            "Epoch: 837, Len of Training loss: 36, Average loss: 1.701731221543418\n",
            "Len of Validation loss: 128, Average loss: 1.9052701459731907\n",
            "Epoch: 838, Len of Training loss: 36, Average loss: 1.6999432510799832\n",
            "Len of Validation loss: 128, Average loss: 1.8670578310266137\n",
            "Epoch: 839, Len of Training loss: 36, Average loss: 1.6838672955830891\n",
            "Len of Validation loss: 128, Average loss: 1.791173451114446\n",
            "Epoch: 840, Len of Training loss: 36, Average loss: 1.7256598472595215\n",
            "Len of Validation loss: 128, Average loss: 1.775272159371525\n",
            "Epoch: 841, Len of Training loss: 36, Average loss: 1.767386360300912\n",
            "Len of Validation loss: 128, Average loss: 1.831041127210483\n",
            "Epoch: 842, Len of Training loss: 36, Average loss: 1.7889010906219482\n",
            "Len of Validation loss: 128, Average loss: 1.8629618370905519\n",
            "Epoch: 843, Len of Training loss: 36, Average loss: 1.755290087726381\n",
            "Len of Validation loss: 128, Average loss: 1.9311228366568685\n",
            "Epoch: 844, Len of Training loss: 36, Average loss: 1.7317572037378948\n",
            "Len of Validation loss: 128, Average loss: 2.036794271785766\n",
            "Epoch: 845, Len of Training loss: 36, Average loss: 1.708036247226927\n",
            "Len of Validation loss: 128, Average loss: 1.8186522093601525\n",
            "Epoch: 846, Len of Training loss: 36, Average loss: 1.7014399800035689\n",
            "Len of Validation loss: 128, Average loss: 1.7749046159442514\n",
            "Epoch: 847, Len of Training loss: 36, Average loss: 1.72421614991294\n",
            "Len of Validation loss: 128, Average loss: 1.9334113630466163\n",
            "Epoch: 848, Len of Training loss: 36, Average loss: 1.7148934370941586\n",
            "Len of Validation loss: 128, Average loss: 1.7799834227189422\n",
            "Epoch: 849, Len of Training loss: 36, Average loss: 1.7037659221225314\n",
            "Len of Validation loss: 128, Average loss: 1.782451253850013\n",
            "Epoch: 850, Len of Training loss: 36, Average loss: 1.6992538041538663\n",
            "Len of Validation loss: 128, Average loss: 1.8063795887865126\n",
            "Epoch: 851, Len of Training loss: 36, Average loss: 1.687118411064148\n",
            "Len of Validation loss: 128, Average loss: 1.8513612626120448\n",
            "Epoch: 852, Len of Training loss: 36, Average loss: 1.7391053140163422\n",
            "Len of Validation loss: 128, Average loss: 1.7848429176956415\n",
            "Epoch: 853, Len of Training loss: 36, Average loss: 1.7823241651058197\n",
            "Len of Validation loss: 128, Average loss: 1.9668542579747736\n",
            "Epoch: 854, Len of Training loss: 36, Average loss: 1.6990096900198195\n",
            "Len of Validation loss: 128, Average loss: 1.760772247100249\n",
            "Epoch: 855, Len of Training loss: 36, Average loss: 1.682890448305342\n",
            "Len of Validation loss: 128, Average loss: 1.8313489304855466\n",
            "Epoch: 856, Len of Training loss: 36, Average loss: 1.7268563111623128\n",
            "Len of Validation loss: 128, Average loss: 1.7707330032717437\n",
            "Epoch: 857, Len of Training loss: 36, Average loss: 1.6756175822681851\n",
            "Len of Validation loss: 128, Average loss: 1.7943678761366755\n",
            "Epoch: 858, Len of Training loss: 36, Average loss: 1.690440810388989\n",
            "Len of Validation loss: 128, Average loss: 1.8394652912393212\n",
            "Epoch: 859, Len of Training loss: 36, Average loss: 1.7866361902819738\n",
            "Len of Validation loss: 128, Average loss: 1.8166155160870403\n",
            "Epoch: 860, Len of Training loss: 36, Average loss: 1.7571454180611505\n",
            "Len of Validation loss: 128, Average loss: 1.7689439477398992\n",
            "Epoch: 861, Len of Training loss: 36, Average loss: 1.6793489356835682\n",
            "Len of Validation loss: 128, Average loss: 1.8148050603922457\n",
            "Epoch: 862, Len of Training loss: 36, Average loss: 1.7131646573543549\n",
            "Len of Validation loss: 128, Average loss: 1.7937987504992634\n",
            "Epoch: 863, Len of Training loss: 36, Average loss: 1.7379088136884902\n",
            "Len of Validation loss: 128, Average loss: 1.8770572100766003\n",
            "Epoch: 864, Len of Training loss: 36, Average loss: 1.7477989263004727\n",
            "Len of Validation loss: 128, Average loss: 1.8057131369132549\n",
            "Epoch: 865, Len of Training loss: 36, Average loss: 1.6894061631626553\n",
            "Len of Validation loss: 128, Average loss: 1.8068666877225041\n",
            "Epoch: 866, Len of Training loss: 36, Average loss: 1.6708641813860998\n",
            "Len of Validation loss: 128, Average loss: 1.820981394732371\n",
            "Epoch: 867, Len of Training loss: 36, Average loss: 1.7093384994400873\n",
            "Len of Validation loss: 128, Average loss: 1.7855371886398643\n",
            "Epoch: 868, Len of Training loss: 36, Average loss: 1.7039066321320004\n",
            "Len of Validation loss: 128, Average loss: 1.7721019717864692\n",
            "Epoch: 869, Len of Training loss: 36, Average loss: 1.6774368716610804\n",
            "Len of Validation loss: 128, Average loss: 1.776537719881162\n",
            "Epoch: 870, Len of Training loss: 36, Average loss: 1.6926412814193301\n",
            "Len of Validation loss: 128, Average loss: 1.7724361503496766\n",
            "Epoch: 871, Len of Training loss: 36, Average loss: 1.7237943510214488\n",
            "Len of Validation loss: 128, Average loss: 1.7980713695287704\n",
            "Epoch: 872, Len of Training loss: 36, Average loss: 1.676927496989568\n",
            "Len of Validation loss: 128, Average loss: 1.7768511129543185\n",
            "Epoch: 873, Len of Training loss: 36, Average loss: 1.6597207585970561\n",
            "Len of Validation loss: 128, Average loss: 1.7247926136478782\n",
            "Epoch: 874, Len of Training loss: 36, Average loss: 1.6990074151092105\n",
            "Len of Validation loss: 128, Average loss: 1.8123118642251939\n",
            "Epoch: 875, Len of Training loss: 36, Average loss: 1.6949466798040602\n",
            "Len of Validation loss: 128, Average loss: 1.741240887902677\n",
            "Epoch: 876, Len of Training loss: 36, Average loss: 1.6934837069776323\n",
            "Len of Validation loss: 128, Average loss: 1.778111505554989\n",
            "Epoch: 877, Len of Training loss: 36, Average loss: 1.6623706354035273\n",
            "Len of Validation loss: 128, Average loss: 1.8056425480172038\n",
            "Epoch: 878, Len of Training loss: 36, Average loss: 1.6834535135163202\n",
            "Len of Validation loss: 128, Average loss: 1.7731278999708593\n",
            "Epoch: 879, Len of Training loss: 36, Average loss: 1.709313400917583\n",
            "Len of Validation loss: 128, Average loss: 1.8292858214117587\n",
            "Epoch: 880, Len of Training loss: 36, Average loss: 1.6889245377646551\n",
            "Len of Validation loss: 128, Average loss: 1.8454705879557878\n",
            "Epoch: 881, Len of Training loss: 36, Average loss: 1.7300937573115032\n",
            "Len of Validation loss: 128, Average loss: 1.8102737911976874\n",
            "Epoch: 882, Len of Training loss: 36, Average loss: 1.7548457947042253\n",
            "Len of Validation loss: 128, Average loss: 1.8611108041368425\n",
            "Epoch: 883, Len of Training loss: 36, Average loss: 1.695599357287089\n",
            "Len of Validation loss: 128, Average loss: 1.7945053554140031\n",
            "Epoch: 884, Len of Training loss: 36, Average loss: 1.6871539586120181\n",
            "Len of Validation loss: 128, Average loss: 1.7795640946133062\n",
            "Epoch: 885, Len of Training loss: 36, Average loss: 1.7158369090822008\n",
            "Len of Validation loss: 128, Average loss: 1.8160194719675928\n",
            "Epoch: 886, Len of Training loss: 36, Average loss: 1.6957996355162726\n",
            "Len of Validation loss: 128, Average loss: 1.7717772610485554\n",
            "Epoch: 887, Len of Training loss: 36, Average loss: 1.6605465726719961\n",
            "Len of Validation loss: 128, Average loss: 1.7712873842101544\n",
            "Epoch: 888, Len of Training loss: 36, Average loss: 1.712038904428482\n",
            "Len of Validation loss: 128, Average loss: 1.7866929080337286\n",
            "Epoch: 889, Len of Training loss: 36, Average loss: 1.6972785393397014\n",
            "Len of Validation loss: 128, Average loss: 1.7804831368848681\n",
            "Epoch: 890, Len of Training loss: 36, Average loss: 1.729484736919403\n",
            "Len of Validation loss: 128, Average loss: 1.877610052935779\n",
            "Epoch: 891, Len of Training loss: 36, Average loss: 1.7224819097254012\n",
            "Len of Validation loss: 128, Average loss: 1.770740132778883\n",
            "Epoch: 892, Len of Training loss: 36, Average loss: 1.6832523842652638\n",
            "Len of Validation loss: 128, Average loss: 1.7904838575050235\n",
            "Epoch: 893, Len of Training loss: 36, Average loss: 1.6605412761370342\n",
            "Len of Validation loss: 128, Average loss: 1.791385737247765\n",
            "Epoch: 894, Len of Training loss: 36, Average loss: 1.6714278095298343\n",
            "Len of Validation loss: 128, Average loss: 1.781247604638338\n",
            "Epoch: 895, Len of Training loss: 36, Average loss: 1.6843137509293027\n",
            "Len of Validation loss: 128, Average loss: 1.799024089705199\n",
            "Epoch: 896, Len of Training loss: 36, Average loss: 1.6768201920721266\n",
            "Len of Validation loss: 128, Average loss: 1.8108814631123096\n",
            "Epoch: 897, Len of Training loss: 36, Average loss: 1.7195845809247758\n",
            "Len of Validation loss: 128, Average loss: 1.8783933660015464\n",
            "Epoch: 898, Len of Training loss: 36, Average loss: 1.6768452061547174\n",
            "Len of Validation loss: 128, Average loss: 1.7843523109331727\n",
            "Epoch: 899, Len of Training loss: 36, Average loss: 1.7036887904008229\n",
            "Len of Validation loss: 128, Average loss: 1.7903875382617116\n",
            "Epoch: 900, Len of Training loss: 36, Average loss: 1.6927138401402368\n",
            "Len of Validation loss: 128, Average loss: 1.7868542177602649\n",
            "Epoch: 901, Len of Training loss: 36, Average loss: 1.6727987329165142\n",
            "Len of Validation loss: 128, Average loss: 1.7768815318122506\n",
            "Epoch: 902, Len of Training loss: 36, Average loss: 1.7051940560340881\n",
            "Len of Validation loss: 128, Average loss: 1.7788377818651497\n",
            "Epoch: 903, Len of Training loss: 36, Average loss: 1.6842368178897433\n",
            "Len of Validation loss: 128, Average loss: 1.7826111076865345\n",
            "Epoch: 904, Len of Training loss: 36, Average loss: 1.7057748403814104\n",
            "Len of Validation loss: 128, Average loss: 1.7368208263069391\n",
            "Epoch: 905, Len of Training loss: 36, Average loss: 1.6630246970388625\n",
            "Len of Validation loss: 128, Average loss: 1.8090415382757783\n",
            "Epoch: 906, Len of Training loss: 36, Average loss: 1.7128353830840852\n",
            "Len of Validation loss: 128, Average loss: 1.7734668825287372\n",
            "Epoch: 907, Len of Training loss: 36, Average loss: 1.6968020101388295\n",
            "Len of Validation loss: 128, Average loss: 1.7486543515697122\n",
            "Epoch: 908, Len of Training loss: 36, Average loss: 1.688046074575848\n",
            "Len of Validation loss: 128, Average loss: 1.8405500981025398\n",
            "Epoch: 909, Len of Training loss: 36, Average loss: 1.705011937353346\n",
            "Len of Validation loss: 128, Average loss: 1.8185493627097458\n",
            "Epoch: 910, Len of Training loss: 36, Average loss: 1.6554670300748613\n",
            "Len of Validation loss: 128, Average loss: 1.795272559626028\n",
            "Epoch: 911, Len of Training loss: 36, Average loss: 1.6862442592779796\n",
            "Len of Validation loss: 128, Average loss: 1.7557736365124583\n",
            "Epoch: 912, Len of Training loss: 36, Average loss: 1.6590698328283098\n",
            "Len of Validation loss: 128, Average loss: 1.7797876535914838\n",
            "Epoch: 913, Len of Training loss: 36, Average loss: 1.6846845878495111\n",
            "Len of Validation loss: 128, Average loss: 1.920769619056955\n",
            "Epoch: 914, Len of Training loss: 36, Average loss: 1.7240202029546101\n",
            "Len of Validation loss: 128, Average loss: 1.8652779958210886\n",
            "Epoch: 915, Len of Training loss: 36, Average loss: 1.7171330716874864\n",
            "Len of Validation loss: 128, Average loss: 1.8367426344193518\n",
            "Epoch: 916, Len of Training loss: 36, Average loss: 1.6700828638341692\n",
            "Len of Validation loss: 128, Average loss: 1.7772052946966141\n",
            "Epoch: 917, Len of Training loss: 36, Average loss: 1.7232813901371427\n",
            "Len of Validation loss: 128, Average loss: 1.7722849946003407\n",
            "Epoch: 918, Len of Training loss: 36, Average loss: 1.7004893389013078\n",
            "Len of Validation loss: 128, Average loss: 1.8230010056868196\n",
            "Epoch: 919, Len of Training loss: 36, Average loss: 1.6851419011751811\n",
            "Len of Validation loss: 128, Average loss: 1.7701258743181825\n",
            "Epoch: 920, Len of Training loss: 36, Average loss: 1.6509723166624706\n",
            "Len of Validation loss: 128, Average loss: 1.7977693958673626\n",
            "Epoch: 921, Len of Training loss: 36, Average loss: 1.670741428931554\n",
            "Len of Validation loss: 128, Average loss: 1.7726971902884543\n",
            "Epoch: 922, Len of Training loss: 36, Average loss: 1.6646215716997783\n",
            "Len of Validation loss: 128, Average loss: 1.7794731194153428\n",
            "Epoch: 923, Len of Training loss: 36, Average loss: 1.6593019796742334\n",
            "Len of Validation loss: 128, Average loss: 1.828738847747445\n",
            "Epoch: 924, Len of Training loss: 36, Average loss: 1.6831028958161671\n",
            "Len of Validation loss: 128, Average loss: 1.7418051066342741\n",
            "Epoch: 925, Len of Training loss: 36, Average loss: 1.7224222984578874\n",
            "Len of Validation loss: 128, Average loss: 1.8600242477841675\n",
            "Epoch: 926, Len of Training loss: 36, Average loss: 1.6806851128737132\n",
            "Len of Validation loss: 128, Average loss: 1.7863997998647392\n",
            "Epoch: 927, Len of Training loss: 36, Average loss: 1.6645392775535583\n",
            "Len of Validation loss: 128, Average loss: 1.8061147257685661\n",
            "Epoch: 928, Len of Training loss: 36, Average loss: 1.6607894466982946\n",
            "Len of Validation loss: 128, Average loss: 1.8080110831651837\n",
            "Epoch: 929, Len of Training loss: 36, Average loss: 1.7092839578787486\n",
            "Len of Validation loss: 128, Average loss: 1.8662212470080703\n",
            "Epoch: 930, Len of Training loss: 36, Average loss: 1.6996827258004084\n",
            "Len of Validation loss: 128, Average loss: 1.8125331522896886\n",
            "Epoch: 931, Len of Training loss: 36, Average loss: 1.6946841577688854\n",
            "Len of Validation loss: 128, Average loss: 1.7770066959783435\n",
            "Epoch: 932, Len of Training loss: 36, Average loss: 1.6815746161672804\n",
            "Len of Validation loss: 128, Average loss: 1.7518040100112557\n",
            "Epoch: 933, Len of Training loss: 36, Average loss: 1.657456202639474\n",
            "Len of Validation loss: 128, Average loss: 1.8562589883804321\n",
            "Epoch: 934, Len of Training loss: 36, Average loss: 1.6922135849793751\n",
            "Len of Validation loss: 128, Average loss: 1.7984528299421072\n",
            "Epoch: 935, Len of Training loss: 36, Average loss: 1.6726670033401914\n",
            "Len of Validation loss: 128, Average loss: 1.7845702501945198\n",
            "Epoch: 936, Len of Training loss: 36, Average loss: 1.6950546238157485\n",
            "Len of Validation loss: 128, Average loss: 1.8264371305704117\n",
            "Epoch: 937, Len of Training loss: 36, Average loss: 1.6688281761275396\n",
            "Len of Validation loss: 128, Average loss: 1.7659670431166887\n",
            "Epoch: 938, Len of Training loss: 36, Average loss: 1.647936726609866\n",
            "Len of Validation loss: 128, Average loss: 1.760080590378493\n",
            "Epoch: 939, Len of Training loss: 36, Average loss: 1.6732881367206573\n",
            "Len of Validation loss: 128, Average loss: 1.8264300492592156\n",
            "Epoch: 940, Len of Training loss: 36, Average loss: 1.6635107960965898\n",
            "Len of Validation loss: 128, Average loss: 1.7911109062843025\n",
            "Epoch: 941, Len of Training loss: 36, Average loss: 1.676634864674674\n",
            "Len of Validation loss: 128, Average loss: 1.7440271801315248\n",
            "Epoch: 942, Len of Training loss: 36, Average loss: 1.6628675758838654\n",
            "Len of Validation loss: 128, Average loss: 1.776423669885844\n",
            "Epoch: 943, Len of Training loss: 36, Average loss: 1.6741483045948877\n",
            "Len of Validation loss: 128, Average loss: 1.894614345394075\n",
            "Epoch: 944, Len of Training loss: 36, Average loss: 1.6810869425535202\n",
            "Len of Validation loss: 128, Average loss: 1.8019709582440555\n",
            "Epoch: 945, Len of Training loss: 36, Average loss: 1.6858762039078607\n",
            "Len of Validation loss: 128, Average loss: 1.7931629070080817\n",
            "Epoch: 946, Len of Training loss: 36, Average loss: 1.6911157369613647\n",
            "Len of Validation loss: 128, Average loss: 1.776477281935513\n",
            "Epoch: 947, Len of Training loss: 36, Average loss: 1.6593921018971338\n",
            "Len of Validation loss: 128, Average loss: 1.7807875426951796\n",
            "Epoch: 948, Len of Training loss: 36, Average loss: 1.6578021711773343\n",
            "Len of Validation loss: 128, Average loss: 1.8053935677744448\n",
            "Epoch: 949, Len of Training loss: 36, Average loss: 1.6667207611931696\n",
            "Len of Validation loss: 128, Average loss: 1.7473714121151716\n",
            "Epoch: 950, Len of Training loss: 36, Average loss: 1.677705768081877\n",
            "Len of Validation loss: 128, Average loss: 1.9073049570433795\n",
            "Epoch: 951, Len of Training loss: 36, Average loss: 1.7069836689366236\n",
            "Len of Validation loss: 128, Average loss: 1.8608345168177038\n",
            "Epoch: 952, Len of Training loss: 36, Average loss: 1.7102128002378676\n",
            "Len of Validation loss: 128, Average loss: 1.9204750808421522\n",
            "Epoch: 953, Len of Training loss: 36, Average loss: 1.6344114144643147\n",
            "Len of Validation loss: 128, Average loss: 1.7743871323764324\n",
            "Epoch: 954, Len of Training loss: 36, Average loss: 1.6336004038651784\n",
            "Len of Validation loss: 128, Average loss: 1.8117203020956367\n",
            "Epoch: 955, Len of Training loss: 36, Average loss: 1.6441347036096785\n",
            "Len of Validation loss: 128, Average loss: 1.8014286970719695\n",
            "Epoch: 956, Len of Training loss: 36, Average loss: 1.649628546502855\n",
            "Len of Validation loss: 128, Average loss: 1.7541464949026704\n",
            "Epoch: 957, Len of Training loss: 36, Average loss: 1.6430055797100067\n",
            "Len of Validation loss: 128, Average loss: 1.7731334178242832\n",
            "Epoch: 958, Len of Training loss: 36, Average loss: 1.6718088189760845\n",
            "Len of Validation loss: 128, Average loss: 1.7770127018447965\n",
            "Epoch: 959, Len of Training loss: 36, Average loss: 1.6507307986418407\n",
            "Len of Validation loss: 128, Average loss: 1.742550416616723\n",
            "Epoch: 960, Len of Training loss: 36, Average loss: 1.6238344543510013\n",
            "Len of Validation loss: 128, Average loss: 1.796023216098547\n",
            "Epoch: 961, Len of Training loss: 36, Average loss: 1.719904022084342\n",
            "Len of Validation loss: 128, Average loss: 1.8022789827082306\n",
            "Epoch: 962, Len of Training loss: 36, Average loss: 1.7418984174728394\n",
            "Len of Validation loss: 128, Average loss: 1.7844651993364096\n",
            "Epoch: 963, Len of Training loss: 36, Average loss: 1.6570382482475705\n",
            "Len of Validation loss: 128, Average loss: 1.7880255852360278\n",
            "Epoch: 964, Len of Training loss: 36, Average loss: 1.6967422664165497\n",
            "Len of Validation loss: 128, Average loss: 1.740833857562393\n",
            "Epoch: 965, Len of Training loss: 36, Average loss: 1.6383077601591747\n",
            "Len of Validation loss: 128, Average loss: 1.7997293192893267\n",
            "Epoch: 966, Len of Training loss: 36, Average loss: 1.6485841770966847\n",
            "Len of Validation loss: 128, Average loss: 1.7937723230570555\n",
            "Epoch: 967, Len of Training loss: 36, Average loss: 1.6548030078411102\n",
            "Len of Validation loss: 128, Average loss: 1.76769044296816\n",
            "Epoch: 968, Len of Training loss: 36, Average loss: 1.687638047668669\n",
            "Len of Validation loss: 128, Average loss: 1.7608951686415821\n",
            "Epoch: 969, Len of Training loss: 36, Average loss: 1.6527780161963568\n",
            "Len of Validation loss: 128, Average loss: 1.8437767582945526\n",
            "Epoch: 970, Len of Training loss: 36, Average loss: 1.6410339375336964\n",
            "Len of Validation loss: 128, Average loss: 1.9216703111305833\n",
            "Epoch: 971, Len of Training loss: 36, Average loss: 1.6498853464921315\n",
            "Len of Validation loss: 128, Average loss: 1.7883937200531363\n",
            "Epoch: 972, Len of Training loss: 36, Average loss: 1.6671513550811343\n",
            "Len of Validation loss: 128, Average loss: 1.755437248852104\n",
            "Epoch: 973, Len of Training loss: 36, Average loss: 1.6512826614909701\n",
            "Len of Validation loss: 128, Average loss: 1.7713770216796547\n",
            "Epoch: 974, Len of Training loss: 36, Average loss: 1.6383903357717726\n",
            "Len of Validation loss: 128, Average loss: 1.8557951969560236\n",
            "Epoch: 975, Len of Training loss: 36, Average loss: 1.6857896215385861\n",
            "Len of Validation loss: 128, Average loss: 1.8354783444665372\n",
            "Epoch: 976, Len of Training loss: 36, Average loss: 1.6669100986586676\n",
            "Len of Validation loss: 128, Average loss: 1.7418815596029162\n",
            "Epoch: 977, Len of Training loss: 36, Average loss: 1.622742772102356\n",
            "Len of Validation loss: 128, Average loss: 1.7573190878611058\n",
            "Epoch: 978, Len of Training loss: 36, Average loss: 1.6257294846905603\n",
            "Len of Validation loss: 128, Average loss: 1.8516248567029834\n",
            "Epoch: 979, Len of Training loss: 36, Average loss: 1.693846172756619\n",
            "Len of Validation loss: 128, Average loss: 1.8702352836262435\n",
            "Epoch: 980, Len of Training loss: 36, Average loss: 1.7108055882983737\n",
            "Len of Validation loss: 128, Average loss: 1.770221286918968\n",
            "Epoch: 981, Len of Training loss: 36, Average loss: 1.6435895827081468\n",
            "Len of Validation loss: 128, Average loss: 1.7788197533227503\n",
            "Epoch: 982, Len of Training loss: 36, Average loss: 1.6537212696340349\n",
            "Len of Validation loss: 128, Average loss: 1.8027671298477799\n",
            "Epoch: 983, Len of Training loss: 36, Average loss: 1.7028517921765645\n",
            "Len of Validation loss: 128, Average loss: 2.0170467547141016\n",
            "Epoch: 984, Len of Training loss: 36, Average loss: 1.676944954527749\n",
            "Len of Validation loss: 128, Average loss: 1.762699086451903\n",
            "Epoch: 985, Len of Training loss: 36, Average loss: 1.6436520483758714\n",
            "Len of Validation loss: 128, Average loss: 1.8238040131982416\n",
            "Epoch: 986, Len of Training loss: 36, Average loss: 1.6395515037907495\n",
            "Len of Validation loss: 128, Average loss: 1.7780670991633087\n",
            "Epoch: 987, Len of Training loss: 36, Average loss: 1.645525574684143\n",
            "Len of Validation loss: 128, Average loss: 1.880957732675597\n",
            "Epoch: 988, Len of Training loss: 36, Average loss: 1.6547938287258148\n",
            "Len of Validation loss: 128, Average loss: 1.8565681094769388\n",
            "Epoch: 989, Len of Training loss: 36, Average loss: 1.6203135914272733\n",
            "Len of Validation loss: 128, Average loss: 1.757281518774107\n",
            "Epoch: 990, Len of Training loss: 36, Average loss: 1.6228960322009192\n",
            "Len of Validation loss: 128, Average loss: 1.779543373035267\n",
            "Epoch: 991, Len of Training loss: 36, Average loss: 1.6199518309699164\n",
            "Len of Validation loss: 128, Average loss: 1.778225822839886\n",
            "Epoch: 992, Len of Training loss: 36, Average loss: 1.637213197019365\n",
            "Len of Validation loss: 128, Average loss: 1.7298747033346444\n",
            "Epoch: 993, Len of Training loss: 36, Average loss: 1.6275300549136267\n",
            "Len of Validation loss: 128, Average loss: 1.7510666453745216\n",
            "Epoch: 994, Len of Training loss: 36, Average loss: 1.6445507142278883\n",
            "Len of Validation loss: 128, Average loss: 1.8140028193593025\n",
            "Epoch: 995, Len of Training loss: 36, Average loss: 1.6376791811651654\n",
            "Len of Validation loss: 128, Average loss: 1.7627578303217888\n",
            "Epoch: 996, Len of Training loss: 36, Average loss: 1.693473478158315\n",
            "Len of Validation loss: 128, Average loss: 1.7825304861180484\n",
            "Epoch: 997, Len of Training loss: 36, Average loss: 1.6430539058314428\n",
            "Len of Validation loss: 128, Average loss: 1.8052497992757708\n",
            "Epoch: 998, Len of Training loss: 36, Average loss: 1.7517591714859009\n",
            "Len of Validation loss: 128, Average loss: 1.7342274449765682\n",
            "Epoch: 999, Len of Training loss: 36, Average loss: 1.610324717230267\n",
            "Len of Validation loss: 128, Average loss: 1.775955358054489\n",
            "Epoch: 1000, Len of Training loss: 36, Average loss: 1.6255464669730928\n",
            "Len of Validation loss: 128, Average loss: 1.7046412059571594\n",
            "Epoch: 1001, Len of Training loss: 36, Average loss: 1.6427526043521032\n",
            "Len of Validation loss: 128, Average loss: 1.7235239308793098\n",
            "Epoch: 1002, Len of Training loss: 36, Average loss: 1.6525170538160536\n",
            "Len of Validation loss: 128, Average loss: 1.791091363877058\n",
            "Epoch: 1003, Len of Training loss: 36, Average loss: 1.6189849641587999\n",
            "Len of Validation loss: 128, Average loss: 1.756401834776625\n",
            "Epoch: 1004, Len of Training loss: 36, Average loss: 1.6174012753698561\n",
            "Len of Validation loss: 128, Average loss: 1.7832738049328327\n",
            "Epoch: 1005, Len of Training loss: 36, Average loss: 1.6255184047751956\n",
            "Len of Validation loss: 128, Average loss: 1.740323682781309\n",
            "Epoch: 1006, Len of Training loss: 36, Average loss: 1.6399957868787978\n",
            "Len of Validation loss: 128, Average loss: 1.8947286598850042\n",
            "Epoch: 1007, Len of Training loss: 36, Average loss: 1.6665389239788055\n",
            "Len of Validation loss: 128, Average loss: 1.7883250184822828\n",
            "Epoch: 1008, Len of Training loss: 36, Average loss: 1.6449391113387213\n",
            "Len of Validation loss: 128, Average loss: 1.7523115074727684\n",
            "Epoch: 1009, Len of Training loss: 36, Average loss: 1.6523167159822252\n",
            "Len of Validation loss: 128, Average loss: 1.8408051296137273\n",
            "Epoch: 1010, Len of Training loss: 36, Average loss: 1.6371432377232447\n",
            "Len of Validation loss: 128, Average loss: 1.8138429997488856\n",
            "Epoch: 1011, Len of Training loss: 36, Average loss: 1.6613105237483978\n",
            "Len of Validation loss: 128, Average loss: 1.767216517822817\n",
            "Epoch: 1012, Len of Training loss: 36, Average loss: 1.6153124504619174\n",
            "Len of Validation loss: 128, Average loss: 1.7766914879903197\n",
            "Epoch: 1013, Len of Training loss: 36, Average loss: 1.6433403856224484\n",
            "Len of Validation loss: 128, Average loss: 1.7462680554017425\n",
            "Epoch: 1014, Len of Training loss: 36, Average loss: 1.6616997056537204\n",
            "Len of Validation loss: 128, Average loss: 1.8605707993265241\n",
            "Epoch: 1015, Len of Training loss: 36, Average loss: 1.6078902648554907\n",
            "Len of Validation loss: 128, Average loss: 1.7568110872525722\n",
            "Epoch: 1016, Len of Training loss: 36, Average loss: 1.6295864880084991\n",
            "Len of Validation loss: 128, Average loss: 1.7555315063800663\n",
            "Epoch: 1017, Len of Training loss: 36, Average loss: 1.6002845035658941\n",
            "Len of Validation loss: 128, Average loss: 1.7386576514691114\n",
            "Epoch: 1018, Len of Training loss: 36, Average loss: 1.6406454808182187\n",
            "Len of Validation loss: 128, Average loss: 1.9124136380851269\n",
            "Epoch: 1019, Len of Training loss: 36, Average loss: 1.6403825448618994\n",
            "Len of Validation loss: 128, Average loss: 1.7598477816209197\n",
            "Epoch: 1020, Len of Training loss: 36, Average loss: 1.643008142709732\n",
            "Len of Validation loss: 128, Average loss: 1.7754304863046855\n",
            "Epoch: 1021, Len of Training loss: 36, Average loss: 1.5979981687333848\n",
            "Len of Validation loss: 128, Average loss: 1.7834535678848624\n",
            "Epoch: 1022, Len of Training loss: 36, Average loss: 1.6286673115359411\n",
            "Len of Validation loss: 128, Average loss: 1.7157021381426603\n",
            "Epoch: 1023, Len of Training loss: 36, Average loss: 1.6069264113903046\n",
            "Len of Validation loss: 128, Average loss: 1.7429942567832768\n",
            "Epoch: 1024, Len of Training loss: 36, Average loss: 1.6005521019299824\n",
            "Len of Validation loss: 128, Average loss: 1.802588359452784\n",
            "Epoch: 1025, Len of Training loss: 36, Average loss: 1.6148344659143024\n",
            "Len of Validation loss: 128, Average loss: 1.7673744969069958\n",
            "Epoch: 1026, Len of Training loss: 36, Average loss: 1.6199803451697032\n",
            "Len of Validation loss: 128, Average loss: 1.7677526376210153\n",
            "Epoch: 1027, Len of Training loss: 36, Average loss: 1.6112721463044484\n",
            "Len of Validation loss: 128, Average loss: 1.7345877778716385\n",
            "Epoch: 1028, Len of Training loss: 36, Average loss: 1.6370533572302923\n",
            "Len of Validation loss: 128, Average loss: 1.7617660365067422\n",
            "Epoch: 1029, Len of Training loss: 36, Average loss: 1.6358387470245361\n",
            "Len of Validation loss: 128, Average loss: 1.7634599287994206\n",
            "Epoch: 1030, Len of Training loss: 36, Average loss: 1.5933037648598354\n",
            "Len of Validation loss: 128, Average loss: 1.7513377484865487\n",
            "Epoch: 1031, Len of Training loss: 36, Average loss: 1.6388287014431424\n",
            "Len of Validation loss: 128, Average loss: 1.74356413888745\n",
            "Epoch: 1032, Len of Training loss: 36, Average loss: 1.6240868700875177\n",
            "Len of Validation loss: 128, Average loss: 1.7357401074841619\n",
            "Epoch: 1033, Len of Training loss: 36, Average loss: 1.6143787834379408\n",
            "Len of Validation loss: 128, Average loss: 1.8031006345991045\n",
            "Epoch: 1034, Len of Training loss: 36, Average loss: 1.6277088522911072\n",
            "Len of Validation loss: 128, Average loss: 1.8050846653059125\n",
            "Epoch: 1035, Len of Training loss: 36, Average loss: 1.6139829523033566\n",
            "Len of Validation loss: 128, Average loss: 1.7274291501380503\n",
            "Epoch: 1036, Len of Training loss: 36, Average loss: 1.6238524814446766\n",
            "Len of Validation loss: 128, Average loss: 1.8420984509866685\n",
            "Epoch: 1037, Len of Training loss: 36, Average loss: 1.654535515440835\n",
            "Len of Validation loss: 128, Average loss: 1.7646680993493646\n",
            "Epoch: 1038, Len of Training loss: 36, Average loss: 1.6260059873263042\n",
            "Len of Validation loss: 128, Average loss: 1.7527529709041119\n",
            "Epoch: 1039, Len of Training loss: 36, Average loss: 1.607509712378184\n",
            "Len of Validation loss: 128, Average loss: 1.7604393362998962\n",
            "Epoch: 1040, Len of Training loss: 36, Average loss: 1.6463771015405655\n",
            "Len of Validation loss: 128, Average loss: 1.8293662981595844\n",
            "Epoch: 1041, Len of Training loss: 36, Average loss: 1.6638833151923285\n",
            "Len of Validation loss: 128, Average loss: 1.7533112629316747\n",
            "Epoch: 1042, Len of Training loss: 36, Average loss: 1.6080433792538114\n",
            "Len of Validation loss: 128, Average loss: 1.7789100394584239\n",
            "Epoch: 1043, Len of Training loss: 36, Average loss: 1.603072765800688\n",
            "Len of Validation loss: 128, Average loss: 1.7276521725580096\n",
            "Epoch: 1044, Len of Training loss: 36, Average loss: 1.6688785519864824\n",
            "Len of Validation loss: 128, Average loss: 1.7604523135814816\n",
            "Epoch: 1045, Len of Training loss: 36, Average loss: 1.669878168238534\n",
            "Len of Validation loss: 128, Average loss: 1.7840618945192546\n",
            "Epoch: 1046, Len of Training loss: 36, Average loss: 1.640099479092492\n",
            "Len of Validation loss: 128, Average loss: 1.7892111148685217\n",
            "Epoch: 1047, Len of Training loss: 36, Average loss: 1.636717094315423\n",
            "Len of Validation loss: 128, Average loss: 1.7445180239155889\n",
            "Epoch: 1048, Len of Training loss: 36, Average loss: 1.6226425402694278\n",
            "Len of Validation loss: 128, Average loss: 1.7962823770940304\n",
            "Epoch: 1049, Len of Training loss: 36, Average loss: 1.672285063399209\n",
            "Len of Validation loss: 128, Average loss: 1.7623997111804783\n",
            "Epoch: 1050, Len of Training loss: 36, Average loss: 1.6225455966260698\n",
            "Len of Validation loss: 128, Average loss: 1.7806188534013927\n",
            "Epoch: 1051, Len of Training loss: 36, Average loss: 1.6109325852659013\n",
            "Len of Validation loss: 128, Average loss: 1.867083355318755\n",
            "Epoch: 1052, Len of Training loss: 36, Average loss: 1.608659780687756\n",
            "Len of Validation loss: 128, Average loss: 1.7525089113041759\n",
            "Epoch: 1053, Len of Training loss: 36, Average loss: 1.6120200157165527\n",
            "Len of Validation loss: 128, Average loss: 1.7528757903492078\n",
            "Epoch: 1054, Len of Training loss: 36, Average loss: 1.6008474793699052\n",
            "Len of Validation loss: 128, Average loss: 1.7240755218081176\n",
            "Epoch: 1055, Len of Training loss: 36, Average loss: 1.6256238453918033\n",
            "Len of Validation loss: 128, Average loss: 1.784823581110686\n",
            "Epoch: 1056, Len of Training loss: 36, Average loss: 1.6131828162405226\n",
            "Len of Validation loss: 128, Average loss: 1.7609971563797444\n",
            "Epoch: 1057, Len of Training loss: 36, Average loss: 1.6013616522153218\n",
            "Len of Validation loss: 128, Average loss: 1.80173276248388\n",
            "Epoch: 1058, Len of Training loss: 36, Average loss: 1.6187284919950697\n",
            "Len of Validation loss: 128, Average loss: 1.7191644394770265\n",
            "Epoch: 1059, Len of Training loss: 36, Average loss: 1.622016234530343\n",
            "Len of Validation loss: 128, Average loss: 1.7516744090244174\n",
            "Epoch: 1060, Len of Training loss: 36, Average loss: 1.6248869564798143\n",
            "Len of Validation loss: 128, Average loss: 1.7736028442159295\n",
            "Epoch: 1061, Len of Training loss: 36, Average loss: 1.6186058587498136\n",
            "Len of Validation loss: 128, Average loss: 1.7585895974189043\n",
            "Epoch: 1062, Len of Training loss: 36, Average loss: 1.6374201609028711\n",
            "Len of Validation loss: 128, Average loss: 1.7389887038152665\n",
            "Epoch: 1063, Len of Training loss: 36, Average loss: 1.6237921946578555\n",
            "Len of Validation loss: 128, Average loss: 1.693694148445502\n",
            "Epoch: 1064, Len of Training loss: 36, Average loss: 1.6039916310045454\n",
            "Len of Validation loss: 128, Average loss: 1.9036743589676917\n",
            "Epoch: 1065, Len of Training loss: 36, Average loss: 1.6390032768249512\n",
            "Len of Validation loss: 128, Average loss: 1.7381051313132048\n",
            "Epoch: 1066, Len of Training loss: 36, Average loss: 1.6105197668075562\n",
            "Len of Validation loss: 128, Average loss: 1.784282555570826\n",
            "Epoch: 1067, Len of Training loss: 36, Average loss: 1.6176650938060548\n",
            "Len of Validation loss: 128, Average loss: 1.8078007036820054\n",
            "Epoch: 1068, Len of Training loss: 36, Average loss: 1.632440447807312\n",
            "Len of Validation loss: 128, Average loss: 1.7581197668332607\n",
            "Epoch: 1069, Len of Training loss: 36, Average loss: 1.6017587449815538\n",
            "Len of Validation loss: 128, Average loss: 1.736797531368211\n",
            "Epoch: 1070, Len of Training loss: 36, Average loss: 1.5948082556327183\n",
            "Len of Validation loss: 128, Average loss: 1.7189175344537944\n",
            "Epoch: 1071, Len of Training loss: 36, Average loss: 1.6071125037140317\n",
            "Len of Validation loss: 128, Average loss: 1.8342225581873208\n",
            "Epoch: 1072, Len of Training loss: 36, Average loss: 1.602464348077774\n",
            "Len of Validation loss: 128, Average loss: 1.8605483279097825\n",
            "Epoch: 1073, Len of Training loss: 36, Average loss: 1.616721745994356\n",
            "Len of Validation loss: 128, Average loss: 1.8026463091373444\n",
            "Epoch: 1074, Len of Training loss: 36, Average loss: 1.6602909631199307\n",
            "Len of Validation loss: 128, Average loss: 1.761337291682139\n",
            "Epoch: 1075, Len of Training loss: 36, Average loss: 1.6318548950884078\n",
            "Len of Validation loss: 128, Average loss: 1.8139321927446872\n",
            "Epoch: 1076, Len of Training loss: 36, Average loss: 1.6117444667551253\n",
            "Len of Validation loss: 128, Average loss: 1.7685606214217842\n",
            "Epoch: 1077, Len of Training loss: 36, Average loss: 1.6529040700859494\n",
            "Len of Validation loss: 128, Average loss: 1.8513101337011904\n",
            "Epoch: 1078, Len of Training loss: 36, Average loss: 1.6495898730225034\n",
            "Len of Validation loss: 128, Average loss: 1.6916166867595166\n",
            "Epoch: 1079, Len of Training loss: 36, Average loss: 1.650953965054618\n",
            "Len of Validation loss: 128, Average loss: 1.7367701188195497\n",
            "Epoch: 1080, Len of Training loss: 36, Average loss: 1.6145544581943088\n",
            "Len of Validation loss: 128, Average loss: 1.831840532599017\n",
            "Epoch: 1081, Len of Training loss: 36, Average loss: 1.639631946881612\n",
            "Len of Validation loss: 128, Average loss: 1.7822939972393215\n",
            "Epoch: 1082, Len of Training loss: 36, Average loss: 1.5967062364021938\n",
            "Len of Validation loss: 128, Average loss: 1.7797081083990633\n",
            "Epoch: 1083, Len of Training loss: 36, Average loss: 1.614010536008411\n",
            "Len of Validation loss: 128, Average loss: 1.7268600035458803\n",
            "Epoch: 1084, Len of Training loss: 36, Average loss: 1.6395393676227994\n",
            "Len of Validation loss: 128, Average loss: 1.7362385098822415\n",
            "Epoch: 1085, Len of Training loss: 36, Average loss: 1.631612264447742\n",
            "Len of Validation loss: 128, Average loss: 1.7739860087167472\n",
            "Epoch: 1086, Len of Training loss: 36, Average loss: 1.5754909217357635\n",
            "Len of Validation loss: 128, Average loss: 1.815783582162112\n",
            "Epoch: 1087, Len of Training loss: 36, Average loss: 1.5968485044108496\n",
            "Len of Validation loss: 128, Average loss: 1.755181589629501\n",
            "Epoch: 1088, Len of Training loss: 36, Average loss: 1.6051492393016815\n",
            "Len of Validation loss: 128, Average loss: 1.7223611259832978\n",
            "Epoch: 1089, Len of Training loss: 36, Average loss: 1.5924348400698767\n",
            "Len of Validation loss: 128, Average loss: 1.7144724114332348\n",
            "Epoch: 1090, Len of Training loss: 36, Average loss: 1.5849112106694117\n",
            "Len of Validation loss: 128, Average loss: 1.7720638671889901\n",
            "Epoch: 1091, Len of Training loss: 36, Average loss: 1.6231652398904164\n",
            "Len of Validation loss: 128, Average loss: 1.9637920670211315\n",
            "Epoch: 1092, Len of Training loss: 36, Average loss: 1.6244971752166748\n",
            "Len of Validation loss: 128, Average loss: 1.747394430451095\n",
            "Epoch: 1093, Len of Training loss: 36, Average loss: 1.5943934420744579\n",
            "Len of Validation loss: 128, Average loss: 1.7925754727330059\n",
            "Epoch: 1094, Len of Training loss: 36, Average loss: 1.6089562773704529\n",
            "Len of Validation loss: 128, Average loss: 1.7142038331367075\n",
            "Epoch: 1095, Len of Training loss: 36, Average loss: 1.6107048855887518\n",
            "Len of Validation loss: 128, Average loss: 1.7700838278979063\n",
            "Epoch: 1096, Len of Training loss: 36, Average loss: 1.6044938762982686\n",
            "Len of Validation loss: 128, Average loss: 1.7932209228165448\n",
            "Epoch: 1097, Len of Training loss: 36, Average loss: 1.6258104774687026\n",
            "Len of Validation loss: 128, Average loss: 1.7554673608392477\n",
            "Epoch: 1098, Len of Training loss: 36, Average loss: 1.6017089850372739\n",
            "Len of Validation loss: 128, Average loss: 1.834862713702023\n",
            "Epoch: 1099, Len of Training loss: 36, Average loss: 1.5957279072867498\n",
            "Len of Validation loss: 128, Average loss: 1.7497473044786602\n",
            "Epoch: 1100, Len of Training loss: 36, Average loss: 1.592108064227634\n",
            "Len of Validation loss: 128, Average loss: 1.7249103819485754\n",
            "Epoch: 1101, Len of Training loss: 36, Average loss: 1.5933357609642878\n",
            "Len of Validation loss: 128, Average loss: 1.7371659423224628\n",
            "Epoch: 1102, Len of Training loss: 36, Average loss: 1.590731617477205\n",
            "Len of Validation loss: 128, Average loss: 1.7312903576530516\n",
            "Epoch: 1103, Len of Training loss: 36, Average loss: 1.5826101700464885\n",
            "Len of Validation loss: 128, Average loss: 1.7495862543582916\n",
            "Epoch: 1104, Len of Training loss: 36, Average loss: 1.601395269234975\n",
            "Len of Validation loss: 128, Average loss: 1.7327836346812546\n",
            "Epoch: 1105, Len of Training loss: 36, Average loss: 1.642934607134925\n",
            "Len of Validation loss: 128, Average loss: 1.7950534685514867\n",
            "Epoch: 1106, Len of Training loss: 36, Average loss: 1.6145978437529669\n",
            "Len of Validation loss: 128, Average loss: 1.741929995128885\n",
            "Epoch: 1107, Len of Training loss: 36, Average loss: 1.6192197369204626\n",
            "Len of Validation loss: 128, Average loss: 1.723560580285266\n",
            "Epoch: 1108, Len of Training loss: 36, Average loss: 1.5945343607001834\n",
            "Len of Validation loss: 128, Average loss: 1.7576062949374318\n",
            "Epoch: 1109, Len of Training loss: 36, Average loss: 1.6118490894635518\n",
            "Len of Validation loss: 128, Average loss: 1.7549500330351293\n",
            "Epoch: 1110, Len of Training loss: 36, Average loss: 1.6238385604487524\n",
            "Len of Validation loss: 128, Average loss: 1.7502895947545767\n",
            "Epoch: 1111, Len of Training loss: 36, Average loss: 1.645987825261222\n",
            "Len of Validation loss: 128, Average loss: 1.7167370594106615\n",
            "Epoch: 1112, Len of Training loss: 36, Average loss: 1.5657591819763184\n",
            "Len of Validation loss: 128, Average loss: 1.7905259833205491\n",
            "Epoch: 1113, Len of Training loss: 36, Average loss: 1.600097464190589\n",
            "Len of Validation loss: 128, Average loss: 1.7782054520212114\n",
            "Epoch: 1114, Len of Training loss: 36, Average loss: 1.661134637064404\n",
            "Len of Validation loss: 128, Average loss: 1.7419702054467052\n",
            "Epoch: 1115, Len of Training loss: 36, Average loss: 1.6495795051256816\n",
            "Len of Validation loss: 128, Average loss: 1.8286507562734187\n",
            "Epoch: 1116, Len of Training loss: 36, Average loss: 1.6163506077395544\n",
            "Len of Validation loss: 128, Average loss: 1.7470039762556553\n",
            "Epoch: 1117, Len of Training loss: 36, Average loss: 1.5743348598480225\n",
            "Len of Validation loss: 128, Average loss: 1.8848964979406446\n",
            "Epoch: 1118, Len of Training loss: 36, Average loss: 1.6291393207179174\n",
            "Len of Validation loss: 128, Average loss: 1.7680245749652386\n",
            "Epoch: 1119, Len of Training loss: 36, Average loss: 1.5733920733133953\n",
            "Len of Validation loss: 128, Average loss: 1.7104226984083652\n",
            "Epoch: 1120, Len of Training loss: 36, Average loss: 1.5732447306315105\n",
            "Len of Validation loss: 128, Average loss: 1.8311741976067424\n",
            "Epoch: 1121, Len of Training loss: 36, Average loss: 1.6015741262171004\n",
            "Len of Validation loss: 128, Average loss: 1.775410855654627\n",
            "Epoch: 1122, Len of Training loss: 36, Average loss: 1.5705932643678453\n",
            "Len of Validation loss: 128, Average loss: 1.7213727249763906\n",
            "Epoch: 1123, Len of Training loss: 36, Average loss: 1.6068892280260723\n",
            "Len of Validation loss: 128, Average loss: 1.7854977222159505\n",
            "Epoch: 1124, Len of Training loss: 36, Average loss: 1.6099056369728513\n",
            "Len of Validation loss: 128, Average loss: 1.7790571034420282\n",
            "Epoch: 1125, Len of Training loss: 36, Average loss: 1.6006544563505385\n",
            "Len of Validation loss: 128, Average loss: 1.753961954265833\n",
            "Epoch: 1126, Len of Training loss: 36, Average loss: 1.6003079844845667\n",
            "Len of Validation loss: 128, Average loss: 1.7553061647340655\n",
            "Epoch: 1127, Len of Training loss: 36, Average loss: 1.5714239047633276\n",
            "Len of Validation loss: 128, Average loss: 1.7649695253930986\n",
            "Epoch: 1128, Len of Training loss: 36, Average loss: 1.5714466604921553\n",
            "Len of Validation loss: 128, Average loss: 1.7095729883294553\n",
            "Epoch: 1129, Len of Training loss: 36, Average loss: 1.589458492067125\n",
            "Len of Validation loss: 128, Average loss: 1.7481339271180332\n",
            "Epoch: 1130, Len of Training loss: 36, Average loss: 1.642476800415251\n",
            "Len of Validation loss: 128, Average loss: 1.7867849110625684\n",
            "Epoch: 1131, Len of Training loss: 36, Average loss: 1.6334853735235002\n",
            "Len of Validation loss: 128, Average loss: 1.7748340382240713\n",
            "Epoch: 1132, Len of Training loss: 36, Average loss: 1.5686434970961676\n",
            "Len of Validation loss: 128, Average loss: 1.7666948835831136\n",
            "Epoch: 1133, Len of Training loss: 36, Average loss: 1.6047639615005918\n",
            "Len of Validation loss: 128, Average loss: 1.7496817766223103\n",
            "Epoch: 1134, Len of Training loss: 36, Average loss: 1.590171731180615\n",
            "Len of Validation loss: 128, Average loss: 1.7317609852179885\n",
            "Epoch: 1135, Len of Training loss: 36, Average loss: 1.5921533107757568\n",
            "Len of Validation loss: 128, Average loss: 1.7552141600754112\n",
            "Epoch: 1136, Len of Training loss: 36, Average loss: 1.6128584808773465\n",
            "Len of Validation loss: 128, Average loss: 1.7636933610774577\n",
            "Epoch: 1137, Len of Training loss: 36, Average loss: 1.606830487648646\n",
            "Len of Validation loss: 128, Average loss: 1.8364804345183074\n",
            "Epoch: 1138, Len of Training loss: 36, Average loss: 1.5756549603409238\n",
            "Len of Validation loss: 128, Average loss: 1.7017462174408138\n",
            "Epoch: 1139, Len of Training loss: 36, Average loss: 1.6480964985158708\n",
            "Len of Validation loss: 128, Average loss: 1.814610818400979\n",
            "Epoch: 1140, Len of Training loss: 36, Average loss: 1.6365425719155207\n",
            "Len of Validation loss: 128, Average loss: 1.7142511261627078\n",
            "Epoch: 1141, Len of Training loss: 36, Average loss: 1.570507999923494\n",
            "Len of Validation loss: 128, Average loss: 1.7481035813689232\n",
            "Epoch: 1142, Len of Training loss: 36, Average loss: 1.5687284072240193\n",
            "Len of Validation loss: 128, Average loss: 1.7356715097557753\n",
            "Epoch: 1143, Len of Training loss: 36, Average loss: 1.6116900907622442\n",
            "Len of Validation loss: 128, Average loss: 1.7918122827541083\n",
            "Epoch: 1144, Len of Training loss: 36, Average loss: 1.5765960845682356\n",
            "Len of Validation loss: 128, Average loss: 1.7359456280246377\n",
            "Epoch: 1145, Len of Training loss: 36, Average loss: 1.5825474460919697\n",
            "Len of Validation loss: 128, Average loss: 1.704426406417042\n",
            "Epoch: 1146, Len of Training loss: 36, Average loss: 1.554082731405894\n",
            "Len of Validation loss: 128, Average loss: 1.7159084677696228\n",
            "Epoch: 1147, Len of Training loss: 36, Average loss: 1.575291461414761\n",
            "Len of Validation loss: 128, Average loss: 1.7388908928260207\n",
            "Epoch: 1148, Len of Training loss: 36, Average loss: 1.5749046206474304\n",
            "Len of Validation loss: 128, Average loss: 1.7363175775390118\n",
            "Epoch: 1149, Len of Training loss: 36, Average loss: 1.5881647202703688\n",
            "Len of Validation loss: 128, Average loss: 1.7485457467846572\n",
            "Epoch: 1150, Len of Training loss: 36, Average loss: 1.5714966754118602\n",
            "Len of Validation loss: 128, Average loss: 1.8098214021883905\n",
            "Epoch: 1151, Len of Training loss: 36, Average loss: 1.5524779955546062\n",
            "Len of Validation loss: 128, Average loss: 1.7653743945993483\n",
            "Epoch: 1152, Len of Training loss: 36, Average loss: 1.5630175405078464\n",
            "Len of Validation loss: 128, Average loss: 1.7869782506022602\n",
            "Epoch: 1153, Len of Training loss: 36, Average loss: 1.5808879600630865\n",
            "Len of Validation loss: 128, Average loss: 1.8163792989216745\n",
            "Epoch: 1154, Len of Training loss: 36, Average loss: 1.574486318561766\n",
            "Len of Validation loss: 128, Average loss: 1.7241954235360026\n",
            "Epoch: 1155, Len of Training loss: 36, Average loss: 1.583120498392317\n",
            "Len of Validation loss: 128, Average loss: 1.6930004532914609\n",
            "Epoch: 1156, Len of Training loss: 36, Average loss: 1.611434166630109\n",
            "Len of Validation loss: 128, Average loss: 1.7634613804984838\n",
            "Epoch: 1157, Len of Training loss: 36, Average loss: 1.6315387785434723\n",
            "Len of Validation loss: 128, Average loss: 1.7015515847597271\n",
            "Epoch: 1158, Len of Training loss: 36, Average loss: 1.5469630592399173\n",
            "Len of Validation loss: 128, Average loss: 1.7548227665247396\n",
            "Epoch: 1159, Len of Training loss: 36, Average loss: 1.5509485734833612\n",
            "Len of Validation loss: 128, Average loss: 1.7576274743769318\n",
            "Epoch: 1160, Len of Training loss: 36, Average loss: 1.5727398031287723\n",
            "Len of Validation loss: 128, Average loss: 1.7396809640340507\n",
            "Epoch: 1161, Len of Training loss: 36, Average loss: 1.5507455435064104\n",
            "Len of Validation loss: 128, Average loss: 1.7128505264408886\n",
            "Epoch: 1162, Len of Training loss: 36, Average loss: 1.5845403340127733\n",
            "Len of Validation loss: 128, Average loss: 1.7546266273129731\n",
            "Epoch: 1163, Len of Training loss: 36, Average loss: 1.566599922047721\n",
            "Len of Validation loss: 128, Average loss: 1.7082037897780538\n",
            "Epoch: 1164, Len of Training loss: 36, Average loss: 1.5531997018390231\n",
            "Len of Validation loss: 128, Average loss: 1.7565192636102438\n",
            "Epoch: 1165, Len of Training loss: 36, Average loss: 1.5500213272041745\n",
            "Len of Validation loss: 128, Average loss: 1.7237795903347433\n",
            "Epoch: 1166, Len of Training loss: 36, Average loss: 1.5696515043576558\n",
            "Len of Validation loss: 128, Average loss: 1.7600422739051282\n",
            "Epoch: 1167, Len of Training loss: 36, Average loss: 1.548660202158822\n",
            "Len of Validation loss: 128, Average loss: 1.7579334862530231\n",
            "Epoch: 1168, Len of Training loss: 36, Average loss: 1.5411423378520541\n",
            "Len of Validation loss: 128, Average loss: 1.7047989841084927\n",
            "Epoch: 1169, Len of Training loss: 36, Average loss: 1.5792779756916895\n",
            "Len of Validation loss: 128, Average loss: 1.7738411712925881\n",
            "Epoch: 1170, Len of Training loss: 36, Average loss: 1.6110488010777368\n",
            "Len of Validation loss: 128, Average loss: 1.7377831023186445\n",
            "Epoch: 1171, Len of Training loss: 36, Average loss: 1.5797474483648937\n",
            "Len of Validation loss: 128, Average loss: 1.7796994394157082\n",
            "Epoch: 1172, Len of Training loss: 36, Average loss: 1.551528341240353\n",
            "Len of Validation loss: 128, Average loss: 1.7311029962729663\n",
            "Epoch: 1173, Len of Training loss: 36, Average loss: 1.5706234408749475\n",
            "Len of Validation loss: 128, Average loss: 1.7707318160682917\n",
            "Epoch: 1174, Len of Training loss: 36, Average loss: 1.5718204511536493\n",
            "Len of Validation loss: 128, Average loss: 1.7601149964611977\n",
            "Epoch: 1175, Len of Training loss: 36, Average loss: 1.5563042362531025\n",
            "Len of Validation loss: 128, Average loss: 1.7233305792324245\n",
            "Epoch: 1176, Len of Training loss: 36, Average loss: 1.5780970884693994\n",
            "Len of Validation loss: 128, Average loss: 1.7372595702763647\n",
            "Epoch: 1177, Len of Training loss: 36, Average loss: 1.5647349688741896\n",
            "Len of Validation loss: 128, Average loss: 1.7725108591839671\n",
            "Epoch: 1178, Len of Training loss: 36, Average loss: 1.614055530892478\n",
            "Len of Validation loss: 128, Average loss: 1.7398821311071515\n",
            "Epoch: 1179, Len of Training loss: 36, Average loss: 1.6637154983149633\n",
            "Len of Validation loss: 128, Average loss: 1.7602847246453166\n",
            "Epoch: 1180, Len of Training loss: 36, Average loss: 1.5718912647830114\n",
            "Len of Validation loss: 128, Average loss: 1.7289019832387567\n",
            "Epoch: 1181, Len of Training loss: 36, Average loss: 1.5681844751040142\n",
            "Len of Validation loss: 128, Average loss: 1.7744392496533692\n",
            "Epoch: 1182, Len of Training loss: 36, Average loss: 1.5667568643887837\n",
            "Len of Validation loss: 128, Average loss: 1.6937211418990046\n",
            "Epoch: 1183, Len of Training loss: 36, Average loss: 1.5428584880299039\n",
            "Len of Validation loss: 128, Average loss: 1.689744905102998\n",
            "Epoch: 1184, Len of Training loss: 36, Average loss: 1.5381465355555217\n",
            "Len of Validation loss: 128, Average loss: 1.7141251254361123\n",
            "Epoch: 1185, Len of Training loss: 36, Average loss: 1.5512437489297655\n",
            "Len of Validation loss: 128, Average loss: 1.7475755913183093\n",
            "Epoch: 1186, Len of Training loss: 36, Average loss: 1.549019392993715\n",
            "Len of Validation loss: 128, Average loss: 1.7448955047875643\n",
            "Epoch: 1187, Len of Training loss: 36, Average loss: 1.5536254478825464\n",
            "Len of Validation loss: 128, Average loss: 1.7177795192692429\n",
            "Epoch: 1188, Len of Training loss: 36, Average loss: 1.5896809895833333\n",
            "Len of Validation loss: 128, Average loss: 1.762814586283639\n",
            "Epoch: 1189, Len of Training loss: 36, Average loss: 1.5850828455554113\n",
            "Len of Validation loss: 128, Average loss: 1.7033342374488711\n",
            "Epoch: 1190, Len of Training loss: 36, Average loss: 1.5658877227041457\n",
            "Len of Validation loss: 128, Average loss: 1.7274794331751764\n",
            "Epoch: 1191, Len of Training loss: 36, Average loss: 1.5810276170571644\n",
            "Len of Validation loss: 128, Average loss: 1.77206539362669\n",
            "Epoch: 1192, Len of Training loss: 36, Average loss: 1.540399021572537\n",
            "Len of Validation loss: 128, Average loss: 1.7862487160600722\n",
            "Epoch: 1193, Len of Training loss: 36, Average loss: 1.594108220603731\n",
            "Len of Validation loss: 128, Average loss: 1.7814239971339703\n",
            "Epoch: 1194, Len of Training loss: 36, Average loss: 1.532183398803075\n",
            "Len of Validation loss: 128, Average loss: 1.7278803219087422\n",
            "Epoch: 1195, Len of Training loss: 36, Average loss: 1.5747551222642262\n",
            "Len of Validation loss: 128, Average loss: 1.7788149039261043\n",
            "Epoch: 1196, Len of Training loss: 36, Average loss: 1.6178012920750513\n",
            "Len of Validation loss: 128, Average loss: 1.7794434141833335\n",
            "Epoch: 1197, Len of Training loss: 36, Average loss: 1.6371677418549855\n",
            "Len of Validation loss: 128, Average loss: 1.8005395140498877\n",
            "Epoch: 1198, Len of Training loss: 36, Average loss: 1.6046771142217848\n",
            "Len of Validation loss: 128, Average loss: 1.8873075130395591\n",
            "Epoch: 1199, Len of Training loss: 36, Average loss: 1.5511967109309301\n",
            "Len of Validation loss: 128, Average loss: 1.6932529241312295\n",
            "Epoch: 1200, Len of Training loss: 36, Average loss: 1.5565983520613775\n",
            "Len of Validation loss: 128, Average loss: 1.7628358961082995\n",
            "Epoch: 1201, Len of Training loss: 36, Average loss: 1.5706843932469685\n",
            "Len of Validation loss: 128, Average loss: 1.8884253925643861\n",
            "Epoch: 1202, Len of Training loss: 36, Average loss: 1.5903578996658325\n",
            "Len of Validation loss: 128, Average loss: 1.812775880098343\n",
            "Epoch: 1203, Len of Training loss: 36, Average loss: 1.5327699217531416\n",
            "Len of Validation loss: 128, Average loss: 1.7728814703878015\n",
            "Epoch: 1204, Len of Training loss: 36, Average loss: 1.5447992119524214\n",
            "Len of Validation loss: 128, Average loss: 1.7753330515697598\n",
            "Epoch: 1205, Len of Training loss: 36, Average loss: 1.5614658031198714\n",
            "Len of Validation loss: 128, Average loss: 1.7364828307181597\n",
            "Epoch: 1206, Len of Training loss: 36, Average loss: 1.5612146688832178\n",
            "Len of Validation loss: 128, Average loss: 1.8156024583149701\n",
            "Epoch: 1207, Len of Training loss: 36, Average loss: 1.6201369563738506\n",
            "Len of Validation loss: 128, Average loss: 1.7684826159384102\n",
            "Epoch: 1208, Len of Training loss: 36, Average loss: 1.5463900069395702\n",
            "Len of Validation loss: 128, Average loss: 1.7422416401095688\n",
            "Epoch: 1209, Len of Training loss: 36, Average loss: 1.5227332380082872\n",
            "Len of Validation loss: 128, Average loss: 1.6903737601824105\n",
            "Epoch: 1210, Len of Training loss: 36, Average loss: 1.5316916637950473\n",
            "Len of Validation loss: 128, Average loss: 1.7259372426196933\n",
            "Epoch: 1211, Len of Training loss: 36, Average loss: 1.564905219607883\n",
            "Len of Validation loss: 128, Average loss: 1.9267019553808495\n",
            "Epoch: 1212, Len of Training loss: 36, Average loss: 1.5700937592320972\n",
            "Len of Validation loss: 128, Average loss: 1.729707438731566\n",
            "Epoch: 1213, Len of Training loss: 36, Average loss: 1.5895453956392076\n",
            "Len of Validation loss: 128, Average loss: 1.745192774105817\n",
            "Epoch: 1214, Len of Training loss: 36, Average loss: 1.5750600662496355\n",
            "Len of Validation loss: 128, Average loss: 1.74334049760364\n",
            "Epoch: 1215, Len of Training loss: 36, Average loss: 1.5584589805867937\n",
            "Len of Validation loss: 128, Average loss: 1.7541594905778766\n",
            "Epoch: 1216, Len of Training loss: 36, Average loss: 1.5271450413597956\n",
            "Len of Validation loss: 128, Average loss: 1.6976548351813108\n",
            "Epoch: 1217, Len of Training loss: 36, Average loss: 1.52178841498163\n",
            "Len of Validation loss: 128, Average loss: 1.6949550434947014\n",
            "Epoch: 1218, Len of Training loss: 36, Average loss: 1.523721632030275\n",
            "Len of Validation loss: 128, Average loss: 1.6975904458668083\n",
            "Epoch: 1219, Len of Training loss: 36, Average loss: 1.5205192400349512\n",
            "Len of Validation loss: 128, Average loss: 1.7762747483793646\n",
            "Epoch: 1220, Len of Training loss: 36, Average loss: 1.557215091254976\n",
            "Len of Validation loss: 128, Average loss: 1.7454445518087596\n",
            "Epoch: 1221, Len of Training loss: 36, Average loss: 1.5331127146879833\n",
            "Len of Validation loss: 128, Average loss: 1.7328721382655203\n",
            "Epoch: 1222, Len of Training loss: 36, Average loss: 1.539684424797694\n",
            "Len of Validation loss: 128, Average loss: 1.723794928053394\n",
            "Epoch: 1223, Len of Training loss: 36, Average loss: 1.51785432961252\n",
            "Len of Validation loss: 128, Average loss: 1.7228989019058645\n",
            "Epoch: 1224, Len of Training loss: 36, Average loss: 1.5086272325780656\n",
            "Len of Validation loss: 128, Average loss: 1.690352980978787\n",
            "Epoch: 1225, Len of Training loss: 36, Average loss: 1.518041170305676\n",
            "Len of Validation loss: 128, Average loss: 1.715146501781419\n",
            "Epoch: 1226, Len of Training loss: 36, Average loss: 1.52680218550894\n",
            "Len of Validation loss: 128, Average loss: 1.6957550446968526\n",
            "Epoch: 1227, Len of Training loss: 36, Average loss: 1.5245517740646999\n",
            "Len of Validation loss: 128, Average loss: 1.7436519868206233\n",
            "Epoch: 1228, Len of Training loss: 36, Average loss: 1.5845635500219133\n",
            "Len of Validation loss: 128, Average loss: 1.7028417154215276\n",
            "Epoch: 1229, Len of Training loss: 36, Average loss: 1.5623071425490909\n",
            "Len of Validation loss: 128, Average loss: 1.7121585058048368\n",
            "Epoch: 1230, Len of Training loss: 36, Average loss: 1.5595948927932315\n",
            "Len of Validation loss: 128, Average loss: 1.7294180858880281\n",
            "Epoch: 1231, Len of Training loss: 36, Average loss: 1.532630248202218\n",
            "Len of Validation loss: 128, Average loss: 1.6912721286062151\n",
            "Epoch: 1232, Len of Training loss: 36, Average loss: 1.546274380551444\n",
            "Len of Validation loss: 128, Average loss: 1.724055326078087\n",
            "Epoch: 1233, Len of Training loss: 36, Average loss: 1.5325242810779147\n",
            "Len of Validation loss: 128, Average loss: 1.8758952147327363\n",
            "Epoch: 1234, Len of Training loss: 36, Average loss: 1.5507953216632206\n",
            "Len of Validation loss: 128, Average loss: 1.7905688844621181\n",
            "Epoch: 1235, Len of Training loss: 36, Average loss: 1.5255169173081715\n",
            "Len of Validation loss: 128, Average loss: 1.777708547655493\n",
            "Epoch: 1236, Len of Training loss: 36, Average loss: 1.5543794135252635\n",
            "Len of Validation loss: 128, Average loss: 1.7604156672023237\n",
            "Epoch: 1237, Len of Training loss: 36, Average loss: 1.5286165906323328\n",
            "Len of Validation loss: 128, Average loss: 1.7424395948182791\n",
            "Epoch: 1238, Len of Training loss: 36, Average loss: 1.5753176046742334\n",
            "Len of Validation loss: 128, Average loss: 1.7959083300083876\n",
            "Epoch: 1239, Len of Training loss: 36, Average loss: 1.600015229649014\n",
            "Len of Validation loss: 128, Average loss: 1.7946016208734363\n",
            "Epoch: 1240, Len of Training loss: 36, Average loss: 1.568166540728675\n",
            "Len of Validation loss: 128, Average loss: 1.8566952843684703\n",
            "Epoch: 1241, Len of Training loss: 36, Average loss: 1.5623426536719005\n",
            "Len of Validation loss: 128, Average loss: 1.7375746306497604\n",
            "Epoch: 1242, Len of Training loss: 36, Average loss: 1.618120152089331\n",
            "Len of Validation loss: 128, Average loss: 1.743244310375303\n",
            "Epoch: 1243, Len of Training loss: 36, Average loss: 1.6162496838304732\n",
            "Len of Validation loss: 128, Average loss: 1.7561245111282915\n",
            "Epoch: 1244, Len of Training loss: 36, Average loss: 1.5476345949702792\n",
            "Len of Validation loss: 128, Average loss: 1.6947451052255929\n",
            "Epoch: 1245, Len of Training loss: 36, Average loss: 1.5298581951194339\n",
            "Len of Validation loss: 128, Average loss: 1.6882074524182826\n",
            "Epoch: 1246, Len of Training loss: 36, Average loss: 1.513745903968811\n",
            "Len of Validation loss: 128, Average loss: 1.7292848592624068\n",
            "Epoch: 1247, Len of Training loss: 36, Average loss: 1.5289715462260776\n",
            "Len of Validation loss: 128, Average loss: 1.8393957982771099\n",
            "Epoch: 1248, Len of Training loss: 36, Average loss: 1.5520793365107641\n",
            "Len of Validation loss: 128, Average loss: 1.7310074251145124\n",
            "Epoch: 1249, Len of Training loss: 36, Average loss: 1.5564070906904008\n",
            "Len of Validation loss: 128, Average loss: 1.697245157789439\n",
            "Epoch: 1250, Len of Training loss: 36, Average loss: 1.5178119705782995\n",
            "Len of Validation loss: 128, Average loss: 1.7309685337822884\n",
            "Epoch: 1251, Len of Training loss: 36, Average loss: 1.531391743156645\n",
            "Len of Validation loss: 128, Average loss: 1.7153861643746495\n",
            "Epoch: 1252, Len of Training loss: 36, Average loss: 1.5397275355127122\n",
            "Len of Validation loss: 128, Average loss: 1.7867307134438306\n",
            "Epoch: 1253, Len of Training loss: 36, Average loss: 1.539902839395735\n",
            "Len of Validation loss: 128, Average loss: 1.7582355893682688\n",
            "Epoch: 1254, Len of Training loss: 36, Average loss: 1.4918609923786588\n",
            "Len of Validation loss: 128, Average loss: 1.6894611869938672\n",
            "Epoch: 1255, Len of Training loss: 36, Average loss: 1.539564503563775\n",
            "Len of Validation loss: 128, Average loss: 1.7039491801988333\n",
            "Epoch: 1256, Len of Training loss: 36, Average loss: 1.5337642166349623\n",
            "Len of Validation loss: 128, Average loss: 1.7818538702558726\n",
            "Epoch: 1257, Len of Training loss: 36, Average loss: 1.521165344450209\n",
            "Len of Validation loss: 128, Average loss: 1.6862176223658025\n",
            "Epoch: 1258, Len of Training loss: 36, Average loss: 1.5340246491962009\n",
            "Len of Validation loss: 128, Average loss: 1.7672714730724692\n",
            "Epoch: 1259, Len of Training loss: 36, Average loss: 1.5319166249699063\n",
            "Len of Validation loss: 128, Average loss: 1.7280102325603366\n",
            "Epoch: 1260, Len of Training loss: 36, Average loss: 1.5165157285001543\n",
            "Len of Validation loss: 128, Average loss: 1.7415309322532266\n",
            "Epoch: 1261, Len of Training loss: 36, Average loss: 1.5150118503305647\n",
            "Len of Validation loss: 128, Average loss: 1.7043134588748217\n",
            "Epoch: 1262, Len of Training loss: 36, Average loss: 1.5367498066690233\n",
            "Len of Validation loss: 128, Average loss: 1.6736747273243964\n",
            "Epoch: 1263, Len of Training loss: 36, Average loss: 1.5138956308364868\n",
            "Len of Validation loss: 128, Average loss: 1.7191469930112362\n",
            "Epoch: 1264, Len of Training loss: 36, Average loss: 1.5156435138649411\n",
            "Len of Validation loss: 128, Average loss: 1.7101100464351475\n",
            "Epoch: 1265, Len of Training loss: 36, Average loss: 1.5122713579071894\n",
            "Len of Validation loss: 128, Average loss: 1.6743813189677894\n",
            "Epoch: 1266, Len of Training loss: 36, Average loss: 1.5214528441429138\n",
            "Len of Validation loss: 128, Average loss: 1.698579114396125\n",
            "Epoch: 1267, Len of Training loss: 36, Average loss: 1.5211057100031111\n",
            "Len of Validation loss: 128, Average loss: 1.7376940315589309\n",
            "Epoch: 1268, Len of Training loss: 36, Average loss: 1.5257944464683533\n",
            "Len of Validation loss: 128, Average loss: 1.7131194008979946\n",
            "Epoch: 1269, Len of Training loss: 36, Average loss: 1.5038932926124997\n",
            "Len of Validation loss: 128, Average loss: 1.7261224673129618\n",
            "Epoch: 1270, Len of Training loss: 36, Average loss: 1.533435235420863\n",
            "Len of Validation loss: 128, Average loss: 1.7397920219227672\n",
            "Epoch: 1271, Len of Training loss: 36, Average loss: 1.5519918865627713\n",
            "Len of Validation loss: 128, Average loss: 1.7259013026487082\n",
            "Epoch: 1272, Len of Training loss: 36, Average loss: 1.5331964261002011\n",
            "Len of Validation loss: 128, Average loss: 1.7975067989900708\n",
            "Epoch: 1273, Len of Training loss: 36, Average loss: 1.5078135795063443\n",
            "Len of Validation loss: 128, Average loss: 1.7501415563747287\n",
            "Epoch: 1274, Len of Training loss: 36, Average loss: 1.5222336318757799\n",
            "Len of Validation loss: 128, Average loss: 1.6867730398662388\n",
            "Epoch: 1275, Len of Training loss: 36, Average loss: 1.5249997178713481\n",
            "Len of Validation loss: 128, Average loss: 1.74982201308012\n",
            "Epoch: 1276, Len of Training loss: 36, Average loss: 1.5426621701982286\n",
            "Len of Validation loss: 128, Average loss: 1.7123651925940067\n",
            "Epoch: 1277, Len of Training loss: 36, Average loss: 1.5028265085485246\n",
            "Len of Validation loss: 128, Average loss: 1.6912090440746397\n",
            "Epoch: 1278, Len of Training loss: 36, Average loss: 1.5165187219778697\n",
            "Len of Validation loss: 128, Average loss: 1.6949951858259737\n",
            "Epoch: 1279, Len of Training loss: 36, Average loss: 1.5292147364881303\n",
            "Len of Validation loss: 128, Average loss: 1.7397801247425377\n",
            "Epoch: 1280, Len of Training loss: 36, Average loss: 1.521305991543664\n",
            "Len of Validation loss: 128, Average loss: 1.7166153816506267\n",
            "Epoch: 1281, Len of Training loss: 36, Average loss: 1.5273971524503496\n",
            "Len of Validation loss: 128, Average loss: 1.8455804167315364\n",
            "Epoch: 1282, Len of Training loss: 36, Average loss: 1.5410905447271135\n",
            "Len of Validation loss: 128, Average loss: 1.7722413851879537\n",
            "Epoch: 1283, Len of Training loss: 36, Average loss: 1.5173489020930395\n",
            "Len of Validation loss: 128, Average loss: 1.8738204324617982\n",
            "Epoch: 1284, Len of Training loss: 36, Average loss: 1.5665566358301375\n",
            "Len of Validation loss: 128, Average loss: 1.7587109592277557\n",
            "Epoch: 1285, Len of Training loss: 36, Average loss: 1.5369109014670055\n",
            "Len of Validation loss: 128, Average loss: 1.7133573528844863\n",
            "Epoch: 1286, Len of Training loss: 36, Average loss: 1.485531856616338\n",
            "Len of Validation loss: 128, Average loss: 1.6937382828909904\n",
            "Epoch: 1287, Len of Training loss: 36, Average loss: 1.5087311168511708\n",
            "Len of Validation loss: 128, Average loss: 1.7476113615557551\n",
            "Epoch: 1288, Len of Training loss: 36, Average loss: 1.5246084729830425\n",
            "Len of Validation loss: 128, Average loss: 1.7907423309516162\n",
            "Epoch: 1289, Len of Training loss: 36, Average loss: 1.5397584305869207\n",
            "Len of Validation loss: 128, Average loss: 1.8572902036830783\n",
            "Epoch: 1290, Len of Training loss: 36, Average loss: 1.506740414434009\n",
            "Len of Validation loss: 128, Average loss: 1.7867307716514915\n",
            "Epoch: 1291, Len of Training loss: 36, Average loss: 1.526842811041408\n",
            "Len of Validation loss: 128, Average loss: 1.7633302242029458\n",
            "Epoch: 1292, Len of Training loss: 36, Average loss: 1.506948322057724\n",
            "Len of Validation loss: 128, Average loss: 1.6576946678105742\n",
            "Epoch: 1293, Len of Training loss: 36, Average loss: 1.489503264427185\n",
            "Len of Validation loss: 128, Average loss: 1.711241947952658\n",
            "Epoch: 1294, Len of Training loss: 36, Average loss: 1.5315213700135548\n",
            "Len of Validation loss: 128, Average loss: 1.7334130709059536\n",
            "Epoch: 1295, Len of Training loss: 36, Average loss: 1.4994869265291426\n",
            "Len of Validation loss: 128, Average loss: 1.6994835920631886\n",
            "Epoch: 1296, Len of Training loss: 36, Average loss: 1.5305529832839966\n",
            "Len of Validation loss: 128, Average loss: 1.662637570174411\n",
            "Epoch: 1297, Len of Training loss: 36, Average loss: 1.514298700624042\n",
            "Len of Validation loss: 128, Average loss: 1.7449170018080622\n",
            "Epoch: 1298, Len of Training loss: 36, Average loss: 1.4938447409205966\n",
            "Len of Validation loss: 128, Average loss: 1.7800651600118726\n",
            "Epoch: 1299, Len of Training loss: 36, Average loss: 1.4931429392761655\n",
            "Len of Validation loss: 128, Average loss: 1.7044780405703932\n",
            "Epoch: 1300, Len of Training loss: 36, Average loss: 1.4924462238947551\n",
            "Len of Validation loss: 128, Average loss: 1.6837397837080061\n",
            "Epoch: 1301, Len of Training loss: 36, Average loss: 1.5556626088089414\n",
            "Len of Validation loss: 128, Average loss: 1.8366853336337954\n",
            "Epoch: 1302, Len of Training loss: 36, Average loss: 1.6132812549670537\n",
            "Len of Validation loss: 128, Average loss: 1.8232409716583788\n",
            "Epoch: 1303, Len of Training loss: 36, Average loss: 1.5277889404031966\n",
            "Len of Validation loss: 128, Average loss: 1.71329126204364\n",
            "Epoch: 1304, Len of Training loss: 36, Average loss: 1.4895292586750455\n",
            "Len of Validation loss: 128, Average loss: 1.6764784650877118\n",
            "Epoch: 1305, Len of Training loss: 36, Average loss: 1.4899088376098208\n",
            "Len of Validation loss: 128, Average loss: 1.7319112601689994\n",
            "Epoch: 1306, Len of Training loss: 36, Average loss: 1.5160356594456568\n",
            "Len of Validation loss: 128, Average loss: 1.7608624033164233\n",
            "Epoch: 1307, Len of Training loss: 36, Average loss: 1.4858656509055033\n",
            "Len of Validation loss: 128, Average loss: 1.7286425919737667\n",
            "Epoch: 1308, Len of Training loss: 36, Average loss: 1.5031513671080272\n",
            "Len of Validation loss: 128, Average loss: 1.7423712126910686\n",
            "Epoch: 1309, Len of Training loss: 36, Average loss: 1.4950101524591446\n",
            "Len of Validation loss: 128, Average loss: 1.7341689625754952\n",
            "Epoch: 1310, Len of Training loss: 36, Average loss: 1.5091473162174225\n",
            "Len of Validation loss: 128, Average loss: 1.787123520160094\n",
            "Epoch: 1311, Len of Training loss: 36, Average loss: 1.5035688645309873\n",
            "Len of Validation loss: 128, Average loss: 1.7626859629526734\n",
            "Epoch: 1312, Len of Training loss: 36, Average loss: 1.5153685179021623\n",
            "Len of Validation loss: 128, Average loss: 1.7209957190789282\n",
            "Epoch: 1313, Len of Training loss: 36, Average loss: 1.4914105981588364\n",
            "Len of Validation loss: 128, Average loss: 1.6970113907009363\n",
            "Epoch: 1314, Len of Training loss: 36, Average loss: 1.4924506296714146\n",
            "Len of Validation loss: 128, Average loss: 1.685622381977737\n",
            "Epoch: 1315, Len of Training loss: 36, Average loss: 1.5435477958785162\n",
            "Len of Validation loss: 128, Average loss: 1.8118581091985106\n",
            "Epoch: 1316, Len of Training loss: 36, Average loss: 1.47855424715413\n",
            "Len of Validation loss: 128, Average loss: 1.7005820909980685\n",
            "Epoch: 1317, Len of Training loss: 36, Average loss: 1.483852595090866\n",
            "Len of Validation loss: 128, Average loss: 1.6973337933886796\n",
            "Epoch: 1318, Len of Training loss: 36, Average loss: 1.500389771329032\n",
            "Len of Validation loss: 128, Average loss: 1.722819761140272\n",
            "Epoch: 1319, Len of Training loss: 36, Average loss: 1.500537759727902\n",
            "Len of Validation loss: 128, Average loss: 1.7248282744549215\n",
            "Epoch: 1320, Len of Training loss: 36, Average loss: 1.514958659807841\n",
            "Len of Validation loss: 128, Average loss: 1.696391083067283\n",
            "Epoch: 1321, Len of Training loss: 36, Average loss: 1.520215888818105\n",
            "Len of Validation loss: 128, Average loss: 1.7128262463957071\n",
            "Epoch: 1322, Len of Training loss: 36, Average loss: 1.505412717660268\n",
            "Len of Validation loss: 128, Average loss: 1.6924318994861096\n",
            "Epoch: 1323, Len of Training loss: 36, Average loss: 1.4956208136346605\n",
            "Len of Validation loss: 128, Average loss: 1.731647095642984\n",
            "Epoch: 1324, Len of Training loss: 36, Average loss: 1.4811418884330325\n",
            "Len of Validation loss: 128, Average loss: 1.6725683049298823\n",
            "Epoch: 1325, Len of Training loss: 36, Average loss: 1.4938654469119177\n",
            "Len of Validation loss: 128, Average loss: 1.74288485548459\n",
            "Epoch: 1326, Len of Training loss: 36, Average loss: 1.5115417473846011\n",
            "Len of Validation loss: 128, Average loss: 1.805440103635192\n",
            "Epoch: 1327, Len of Training loss: 36, Average loss: 1.5413668553034465\n",
            "Len of Validation loss: 128, Average loss: 1.6770643416093662\n",
            "Epoch: 1328, Len of Training loss: 36, Average loss: 1.4992259972625308\n",
            "Len of Validation loss: 128, Average loss: 1.757503675762564\n",
            "Epoch: 1329, Len of Training loss: 36, Average loss: 1.4816626244121127\n",
            "Len of Validation loss: 128, Average loss: 1.7443110886961222\n",
            "Epoch: 1330, Len of Training loss: 36, Average loss: 1.4827103747261896\n",
            "Len of Validation loss: 128, Average loss: 1.6979337877128273\n",
            "Epoch: 1331, Len of Training loss: 36, Average loss: 1.4637474219004314\n",
            "Len of Validation loss: 128, Average loss: 1.697756462264806\n",
            "Epoch: 1332, Len of Training loss: 36, Average loss: 1.4789356523089938\n",
            "Len of Validation loss: 128, Average loss: 1.7727064243517816\n",
            "Epoch: 1333, Len of Training loss: 36, Average loss: 1.5490081310272217\n",
            "Len of Validation loss: 128, Average loss: 1.6854569567367435\n",
            "Epoch: 1334, Len of Training loss: 36, Average loss: 1.4952945576773748\n",
            "Len of Validation loss: 128, Average loss: 1.7020294982939959\n",
            "Epoch: 1335, Len of Training loss: 36, Average loss: 1.4946768018934462\n",
            "Len of Validation loss: 128, Average loss: 1.698828760534525\n",
            "Epoch: 1336, Len of Training loss: 36, Average loss: 1.5036475393507216\n",
            "Len of Validation loss: 128, Average loss: 1.7083450928330421\n",
            "Epoch: 1337, Len of Training loss: 36, Average loss: 1.4795830845832825\n",
            "Len of Validation loss: 128, Average loss: 1.7540809088386595\n",
            "Epoch: 1338, Len of Training loss: 36, Average loss: 1.5017847286330328\n",
            "Len of Validation loss: 128, Average loss: 1.8011207594536245\n",
            "Epoch: 1339, Len of Training loss: 36, Average loss: 1.4676064650217693\n",
            "Len of Validation loss: 128, Average loss: 1.7207139628008008\n",
            "Epoch: 1340, Len of Training loss: 36, Average loss: 1.5100081066290538\n",
            "Len of Validation loss: 128, Average loss: 1.737481627613306\n",
            "Epoch: 1341, Len of Training loss: 36, Average loss: 1.5319109559059143\n",
            "Len of Validation loss: 128, Average loss: 1.711075408384204\n",
            "Epoch: 1342, Len of Training loss: 36, Average loss: 1.4966950250996485\n",
            "Len of Validation loss: 128, Average loss: 1.7070890627801418\n",
            "Epoch: 1343, Len of Training loss: 36, Average loss: 1.4806182583173115\n",
            "Len of Validation loss: 128, Average loss: 1.7875951016321778\n",
            "Epoch: 1344, Len of Training loss: 36, Average loss: 1.4861534270975325\n",
            "Len of Validation loss: 128, Average loss: 1.7170947508420795\n",
            "Epoch: 1345, Len of Training loss: 36, Average loss: 1.518278072277705\n",
            "Len of Validation loss: 128, Average loss: 1.6828621132299304\n",
            "Epoch: 1346, Len of Training loss: 36, Average loss: 1.5014785726865132\n",
            "Len of Validation loss: 128, Average loss: 1.6596878052223474\n",
            "Epoch: 1347, Len of Training loss: 36, Average loss: 1.4969096730152767\n",
            "Len of Validation loss: 128, Average loss: 1.7029834792483598\n",
            "Epoch: 1348, Len of Training loss: 36, Average loss: 1.4612943861219618\n",
            "Len of Validation loss: 128, Average loss: 1.6916766434442252\n",
            "Epoch: 1349, Len of Training loss: 36, Average loss: 1.472448981470532\n",
            "Len of Validation loss: 128, Average loss: 1.6750825888011605\n",
            "Epoch: 1350, Len of Training loss: 36, Average loss: 1.4758547577593062\n",
            "Len of Validation loss: 128, Average loss: 1.7481048917397857\n",
            "Epoch: 1351, Len of Training loss: 36, Average loss: 1.5116624004311032\n",
            "Len of Validation loss: 128, Average loss: 1.692854160675779\n",
            "Epoch: 1352, Len of Training loss: 36, Average loss: 1.4820571210649278\n",
            "Len of Validation loss: 128, Average loss: 1.82250832952559\n",
            "Epoch: 1353, Len of Training loss: 36, Average loss: 1.5334791474872165\n",
            "Len of Validation loss: 128, Average loss: 1.84154534782283\n",
            "Epoch: 1354, Len of Training loss: 36, Average loss: 1.5405353506406148\n",
            "Len of Validation loss: 128, Average loss: 1.7083055798429996\n",
            "Epoch: 1355, Len of Training loss: 36, Average loss: 1.5123282074928284\n",
            "Len of Validation loss: 128, Average loss: 1.8339896518737078\n",
            "Epoch: 1356, Len of Training loss: 36, Average loss: 1.4924672842025757\n",
            "Len of Validation loss: 128, Average loss: 1.7382970312610269\n",
            "Epoch: 1357, Len of Training loss: 36, Average loss: 1.4704344272613525\n",
            "Len of Validation loss: 128, Average loss: 1.7151811243966222\n",
            "Epoch: 1358, Len of Training loss: 36, Average loss: 1.4878584841887157\n",
            "Len of Validation loss: 128, Average loss: 1.7766785034909844\n",
            "Epoch: 1359, Len of Training loss: 36, Average loss: 1.5526516139507294\n",
            "Len of Validation loss: 128, Average loss: 1.7488522725179791\n",
            "Epoch: 1360, Len of Training loss: 36, Average loss: 1.4901742306020525\n",
            "Len of Validation loss: 128, Average loss: 1.6973603698424995\n",
            "Epoch: 1361, Len of Training loss: 36, Average loss: 1.511643605099784\n",
            "Len of Validation loss: 128, Average loss: 1.760892882477492\n",
            "Epoch: 1362, Len of Training loss: 36, Average loss: 1.4993573096063402\n",
            "Len of Validation loss: 128, Average loss: 1.7192705117631704\n",
            "Epoch: 1363, Len of Training loss: 36, Average loss: 1.4710973186625376\n",
            "Len of Validation loss: 128, Average loss: 1.6739433992188424\n",
            "Epoch: 1364, Len of Training loss: 36, Average loss: 1.47859791914622\n",
            "Len of Validation loss: 128, Average loss: 1.7161098048090935\n",
            "Epoch: 1365, Len of Training loss: 36, Average loss: 1.4827991790241666\n",
            "Len of Validation loss: 128, Average loss: 1.6901432606391609\n",
            "Epoch: 1366, Len of Training loss: 36, Average loss: 1.4622223377227783\n",
            "Len of Validation loss: 128, Average loss: 1.677352651488036\n",
            "Epoch: 1367, Len of Training loss: 36, Average loss: 1.4701035519440968\n",
            "Len of Validation loss: 128, Average loss: 1.6930810220073909\n",
            "Epoch: 1368, Len of Training loss: 36, Average loss: 1.4635784096188016\n",
            "Len of Validation loss: 128, Average loss: 1.6410729689523578\n",
            "Epoch: 1369, Len of Training loss: 36, Average loss: 1.459731423192554\n",
            "Len of Validation loss: 128, Average loss: 1.666460000211373\n",
            "Epoch: 1370, Len of Training loss: 36, Average loss: 1.475385500325097\n",
            "Len of Validation loss: 128, Average loss: 1.705954436212778\n",
            "Epoch: 1371, Len of Training loss: 36, Average loss: 1.4770478937360976\n",
            "Len of Validation loss: 128, Average loss: 1.7732865361031145\n",
            "Epoch: 1372, Len of Training loss: 36, Average loss: 1.5039766000376806\n",
            "Len of Validation loss: 128, Average loss: 1.7192924073897302\n",
            "Epoch: 1373, Len of Training loss: 36, Average loss: 1.4771442876921759\n",
            "Len of Validation loss: 128, Average loss: 1.7067927429452538\n",
            "Epoch: 1374, Len of Training loss: 36, Average loss: 1.492425133784612\n",
            "Len of Validation loss: 128, Average loss: 1.7016193468589336\n",
            "Epoch: 1375, Len of Training loss: 36, Average loss: 1.5028044912550185\n",
            "Len of Validation loss: 128, Average loss: 1.7094780174084008\n",
            "Epoch: 1376, Len of Training loss: 36, Average loss: 1.4753467059797711\n",
            "Len of Validation loss: 128, Average loss: 1.679195845965296\n",
            "Epoch: 1377, Len of Training loss: 36, Average loss: 1.4700172709094153\n",
            "Len of Validation loss: 128, Average loss: 1.714162504998967\n",
            "Epoch: 1378, Len of Training loss: 36, Average loss: 1.4840005569987826\n",
            "Len of Validation loss: 128, Average loss: 1.7268412674311548\n",
            "Epoch: 1379, Len of Training loss: 36, Average loss: 1.468161231941647\n",
            "Len of Validation loss: 128, Average loss: 1.721840932033956\n",
            "Epoch: 1380, Len of Training loss: 36, Average loss: 1.461436875992351\n",
            "Len of Validation loss: 128, Average loss: 1.712182784685865\n",
            "Epoch: 1381, Len of Training loss: 36, Average loss: 1.4446521898110707\n",
            "Len of Validation loss: 128, Average loss: 1.6892729990649968\n",
            "Epoch: 1382, Len of Training loss: 36, Average loss: 1.448229408926434\n",
            "Len of Validation loss: 128, Average loss: 1.716216356260702\n",
            "Epoch: 1383, Len of Training loss: 36, Average loss: 1.4478375646803114\n",
            "Len of Validation loss: 128, Average loss: 1.6963503102306277\n",
            "Epoch: 1384, Len of Training loss: 36, Average loss: 1.4675822092427149\n",
            "Len of Validation loss: 128, Average loss: 1.776889648521319\n",
            "Epoch: 1385, Len of Training loss: 36, Average loss: 1.4930760380294588\n",
            "Len of Validation loss: 128, Average loss: 1.714789443416521\n",
            "Epoch: 1386, Len of Training loss: 36, Average loss: 1.4756815102365282\n",
            "Len of Validation loss: 128, Average loss: 1.7375030033290386\n",
            "Epoch: 1387, Len of Training loss: 36, Average loss: 1.5062745180394914\n",
            "Len of Validation loss: 128, Average loss: 1.7545636428985745\n",
            "Epoch: 1388, Len of Training loss: 36, Average loss: 1.476367450422711\n",
            "Len of Validation loss: 128, Average loss: 1.6516667020041496\n",
            "Epoch: 1389, Len of Training loss: 36, Average loss: 1.4369917114575703\n",
            "Len of Validation loss: 128, Average loss: 1.7069617013912648\n",
            "Epoch: 1390, Len of Training loss: 36, Average loss: 1.500357641114129\n",
            "Len of Validation loss: 128, Average loss: 1.7092827102169394\n",
            "Epoch: 1391, Len of Training loss: 36, Average loss: 1.482311748796039\n",
            "Len of Validation loss: 128, Average loss: 1.7105413263197988\n",
            "Epoch: 1392, Len of Training loss: 36, Average loss: 1.4890987541940477\n",
            "Len of Validation loss: 128, Average loss: 1.7257088767364621\n",
            "Epoch: 1393, Len of Training loss: 36, Average loss: 1.4594882163736556\n",
            "Len of Validation loss: 128, Average loss: 1.681689835852012\n",
            "Epoch: 1394, Len of Training loss: 36, Average loss: 1.467489942908287\n",
            "Len of Validation loss: 128, Average loss: 1.7545578642748296\n",
            "Epoch: 1395, Len of Training loss: 36, Average loss: 1.4504435045851602\n",
            "Len of Validation loss: 128, Average loss: 1.7035740104038268\n",
            "Epoch: 1396, Len of Training loss: 36, Average loss: 1.4457733862929874\n",
            "Len of Validation loss: 128, Average loss: 1.684878379572183\n",
            "Epoch: 1397, Len of Training loss: 36, Average loss: 1.4908362064096663\n",
            "Len of Validation loss: 128, Average loss: 1.7211962919682264\n",
            "Epoch: 1398, Len of Training loss: 36, Average loss: 1.4644586477014754\n",
            "Len of Validation loss: 128, Average loss: 1.6367938416078687\n",
            "Epoch: 1399, Len of Training loss: 36, Average loss: 1.4474003679222531\n",
            "Len of Validation loss: 128, Average loss: 1.6678967212792486\n",
            "Epoch: 1400, Len of Training loss: 36, Average loss: 1.4592924051814609\n",
            "Len of Validation loss: 128, Average loss: 1.6600677814567462\n",
            "Epoch: 1401, Len of Training loss: 36, Average loss: 1.4667473137378693\n",
            "Len of Validation loss: 128, Average loss: 1.7460564281791449\n",
            "Epoch: 1402, Len of Training loss: 36, Average loss: 1.470107255710496\n",
            "Len of Validation loss: 128, Average loss: 1.6875620052451268\n",
            "Epoch: 1403, Len of Training loss: 36, Average loss: 1.445774753888448\n",
            "Len of Validation loss: 128, Average loss: 1.7046087358612567\n",
            "Epoch: 1404, Len of Training loss: 36, Average loss: 1.44769091407458\n",
            "Len of Validation loss: 128, Average loss: 1.7242419673129916\n",
            "Epoch: 1405, Len of Training loss: 36, Average loss: 1.4876008166207209\n",
            "Len of Validation loss: 128, Average loss: 1.7403971361927688\n",
            "Epoch: 1406, Len of Training loss: 36, Average loss: 1.4456179075770907\n",
            "Len of Validation loss: 128, Average loss: 1.7773057967424393\n",
            "Epoch: 1407, Len of Training loss: 36, Average loss: 1.4446033040682476\n",
            "Len of Validation loss: 128, Average loss: 1.7084647428710014\n",
            "Epoch: 1408, Len of Training loss: 36, Average loss: 1.4513684047593012\n",
            "Len of Validation loss: 128, Average loss: 1.7685081847012043\n",
            "Epoch: 1409, Len of Training loss: 36, Average loss: 1.470741824971305\n",
            "Len of Validation loss: 128, Average loss: 1.6708168352488428\n",
            "Epoch: 1410, Len of Training loss: 36, Average loss: 1.4409668644269307\n",
            "Len of Validation loss: 128, Average loss: 1.6872274472843856\n",
            "Epoch: 1411, Len of Training loss: 36, Average loss: 1.4731605615880754\n",
            "Len of Validation loss: 128, Average loss: 1.669966971501708\n",
            "Epoch: 1412, Len of Training loss: 36, Average loss: 1.4940029333035152\n",
            "Len of Validation loss: 128, Average loss: 1.6747831897810102\n",
            "Epoch: 1413, Len of Training loss: 36, Average loss: 1.5180374284585316\n",
            "Len of Validation loss: 128, Average loss: 1.6813191045075655\n",
            "Epoch: 1414, Len of Training loss: 36, Average loss: 1.512291623486413\n",
            "Len of Validation loss: 128, Average loss: 1.7299160759430379\n",
            "Epoch: 1415, Len of Training loss: 36, Average loss: 1.4708860549661849\n",
            "Len of Validation loss: 128, Average loss: 1.6562433154322207\n",
            "Epoch: 1416, Len of Training loss: 36, Average loss: 1.470788323216968\n",
            "Len of Validation loss: 128, Average loss: 1.6961821573786438\n",
            "Epoch: 1417, Len of Training loss: 36, Average loss: 1.469912177986569\n",
            "Len of Validation loss: 128, Average loss: 1.6502029153052717\n",
            "Epoch: 1418, Len of Training loss: 36, Average loss: 1.4646279215812683\n",
            "Len of Validation loss: 128, Average loss: 1.6667483926285058\n",
            "Epoch: 1419, Len of Training loss: 36, Average loss: 1.4631867574320898\n",
            "Len of Validation loss: 128, Average loss: 1.7111880397424102\n",
            "Epoch: 1420, Len of Training loss: 36, Average loss: 1.4537936233811908\n",
            "Len of Validation loss: 128, Average loss: 1.6942413402721286\n",
            "Epoch: 1421, Len of Training loss: 36, Average loss: 1.4573315282662709\n",
            "Len of Validation loss: 128, Average loss: 1.7170935131143779\n",
            "Epoch: 1422, Len of Training loss: 36, Average loss: 1.4376193748580084\n",
            "Len of Validation loss: 128, Average loss: 1.6938345751259476\n",
            "Epoch: 1423, Len of Training loss: 36, Average loss: 1.4315278447336621\n",
            "Len of Validation loss: 128, Average loss: 1.6835874021053314\n",
            "Epoch: 1424, Len of Training loss: 36, Average loss: 1.4870178037219577\n",
            "Len of Validation loss: 128, Average loss: 1.649455398786813\n",
            "Epoch: 1425, Len of Training loss: 36, Average loss: 1.49061808652348\n",
            "Len of Validation loss: 128, Average loss: 1.7030640416778624\n",
            "Epoch: 1426, Len of Training loss: 36, Average loss: 1.4495389825767941\n",
            "Len of Validation loss: 128, Average loss: 1.7383272908627987\n",
            "Epoch: 1427, Len of Training loss: 36, Average loss: 1.4401119119591184\n",
            "Len of Validation loss: 128, Average loss: 1.6801735314074904\n",
            "Epoch: 1428, Len of Training loss: 36, Average loss: 1.4718863632943895\n",
            "Len of Validation loss: 128, Average loss: 1.670109475031495\n",
            "Epoch: 1429, Len of Training loss: 36, Average loss: 1.4653876490063138\n",
            "Len of Validation loss: 128, Average loss: 1.8429320370778441\n",
            "Epoch: 1430, Len of Training loss: 36, Average loss: 1.472251875532998\n",
            "Len of Validation loss: 128, Average loss: 1.7763633523136377\n",
            "Epoch: 1431, Len of Training loss: 36, Average loss: 1.4406222568617926\n",
            "Len of Validation loss: 128, Average loss: 1.690572902560234\n",
            "Epoch: 1432, Len of Training loss: 36, Average loss: 1.4284267160627577\n",
            "Len of Validation loss: 128, Average loss: 1.6819624698255211\n",
            "Epoch: 1433, Len of Training loss: 36, Average loss: 1.4351595011022356\n",
            "Len of Validation loss: 128, Average loss: 1.7152008940465748\n",
            "Epoch: 1434, Len of Training loss: 36, Average loss: 1.428485396835539\n",
            "Len of Validation loss: 128, Average loss: 1.665043391752988\n",
            "Epoch: 1435, Len of Training loss: 36, Average loss: 1.4429270575443904\n",
            "Len of Validation loss: 128, Average loss: 1.8218822113703936\n",
            "Epoch: 1436, Len of Training loss: 36, Average loss: 1.4964431822299957\n",
            "Len of Validation loss: 128, Average loss: 1.70514033944346\n",
            "Epoch: 1437, Len of Training loss: 36, Average loss: 1.4896326329973009\n",
            "Len of Validation loss: 128, Average loss: 1.6977418947499245\n",
            "Epoch: 1438, Len of Training loss: 36, Average loss: 1.45313079489602\n",
            "Len of Validation loss: 128, Average loss: 1.6895422195084393\n",
            "Epoch: 1439, Len of Training loss: 36, Average loss: 1.4414999120765262\n",
            "Len of Validation loss: 128, Average loss: 1.6613768744282424\n",
            "Epoch: 1440, Len of Training loss: 36, Average loss: 1.4789398279454973\n",
            "Len of Validation loss: 128, Average loss: 1.7037434063386172\n",
            "Epoch: 1441, Len of Training loss: 36, Average loss: 1.4706324769390955\n",
            "Len of Validation loss: 128, Average loss: 1.7126615685410798\n",
            "Epoch: 1442, Len of Training loss: 36, Average loss: 1.4507329265276592\n",
            "Len of Validation loss: 128, Average loss: 1.665879861684516\n",
            "Epoch: 1443, Len of Training loss: 36, Average loss: 1.436199148495992\n",
            "Len of Validation loss: 128, Average loss: 1.7281009098514915\n",
            "Epoch: 1444, Len of Training loss: 36, Average loss: 1.435352236032486\n",
            "Len of Validation loss: 128, Average loss: 1.680026275338605\n",
            "Epoch: 1445, Len of Training loss: 36, Average loss: 1.4264740977022383\n",
            "Len of Validation loss: 128, Average loss: 1.6923541261348873\n",
            "Epoch: 1446, Len of Training loss: 36, Average loss: 1.4133524199326832\n",
            "Len of Validation loss: 128, Average loss: 1.6473920978605747\n",
            "Epoch: 1447, Len of Training loss: 36, Average loss: 1.4555199609862433\n",
            "Len of Validation loss: 128, Average loss: 1.749745536595583\n",
            "Epoch: 1448, Len of Training loss: 36, Average loss: 1.462381703986062\n",
            "Len of Validation loss: 128, Average loss: 1.698728024493903\n",
            "Epoch: 1449, Len of Training loss: 36, Average loss: 1.4603105617894068\n",
            "Len of Validation loss: 128, Average loss: 1.688187403138727\n",
            "Epoch: 1450, Len of Training loss: 36, Average loss: 1.4546897113323212\n",
            "Len of Validation loss: 128, Average loss: 1.7087334864772856\n",
            "Epoch: 1451, Len of Training loss: 36, Average loss: 1.4496323698096805\n",
            "Len of Validation loss: 128, Average loss: 1.7476715841330588\n",
            "Epoch: 1452, Len of Training loss: 36, Average loss: 1.4581601950857375\n",
            "Len of Validation loss: 128, Average loss: 1.7906020851805806\n",
            "Epoch: 1453, Len of Training loss: 36, Average loss: 1.4776301516426935\n",
            "Len of Validation loss: 128, Average loss: 1.6924709260929376\n",
            "Epoch: 1454, Len of Training loss: 36, Average loss: 1.446431232823266\n",
            "Len of Validation loss: 128, Average loss: 1.6819822238758206\n",
            "Epoch: 1455, Len of Training loss: 36, Average loss: 1.4600515796078577\n",
            "Len of Validation loss: 128, Average loss: 1.817064935574308\n",
            "Epoch: 1456, Len of Training loss: 36, Average loss: 1.4601732691129048\n",
            "Len of Validation loss: 128, Average loss: 1.6766748088411987\n",
            "Epoch: 1457, Len of Training loss: 36, Average loss: 1.432235105170144\n",
            "Len of Validation loss: 128, Average loss: 1.6979752022307366\n",
            "Epoch: 1458, Len of Training loss: 36, Average loss: 1.4600526955392625\n",
            "Len of Validation loss: 128, Average loss: 1.6789872492663562\n",
            "Epoch: 1459, Len of Training loss: 36, Average loss: 1.4202641265259848\n",
            "Len of Validation loss: 128, Average loss: 1.6624278239905834\n",
            "Epoch: 1460, Len of Training loss: 36, Average loss: 1.4526953664090898\n",
            "Len of Validation loss: 128, Average loss: 1.6977001917548478\n",
            "Epoch: 1461, Len of Training loss: 36, Average loss: 1.4366475409931607\n",
            "Len of Validation loss: 128, Average loss: 1.7028743186965585\n",
            "Epoch: 1462, Len of Training loss: 36, Average loss: 1.4027605603138606\n",
            "Len of Validation loss: 128, Average loss: 1.666483429260552\n",
            "Epoch: 1463, Len of Training loss: 36, Average loss: 1.396444825662507\n",
            "Len of Validation loss: 128, Average loss: 1.6952930274419487\n",
            "Epoch: 1464, Len of Training loss: 36, Average loss: 1.4220416694879532\n",
            "Len of Validation loss: 128, Average loss: 1.633250828133896\n",
            "Epoch: 1465, Len of Training loss: 36, Average loss: 1.431938777367274\n",
            "Len of Validation loss: 128, Average loss: 1.6835174397565424\n",
            "Epoch: 1466, Len of Training loss: 36, Average loss: 1.4139480193456013\n",
            "Len of Validation loss: 128, Average loss: 1.717693136073649\n",
            "Epoch: 1467, Len of Training loss: 36, Average loss: 1.4021401604016621\n",
            "Len of Validation loss: 128, Average loss: 1.6802598990034312\n",
            "Epoch: 1468, Len of Training loss: 36, Average loss: 1.4144500096638997\n",
            "Len of Validation loss: 128, Average loss: 1.6500335540622473\n",
            "Epoch: 1469, Len of Training loss: 36, Average loss: 1.4309692713949416\n",
            "Len of Validation loss: 128, Average loss: 1.7013668944127858\n",
            "Epoch: 1470, Len of Training loss: 36, Average loss: 1.411112003856235\n",
            "Len of Validation loss: 128, Average loss: 1.6766657321713865\n",
            "Epoch: 1471, Len of Training loss: 36, Average loss: 1.4323811067475214\n",
            "Len of Validation loss: 128, Average loss: 1.6982604330405593\n",
            "Epoch: 1472, Len of Training loss: 36, Average loss: 1.3953185710642073\n",
            "Len of Validation loss: 128, Average loss: 1.697517812717706\n",
            "Epoch: 1473, Len of Training loss: 36, Average loss: 1.4403859575589497\n",
            "Len of Validation loss: 128, Average loss: 1.6907104076817632\n",
            "Epoch: 1474, Len of Training loss: 36, Average loss: 1.427721358007855\n",
            "Len of Validation loss: 128, Average loss: 1.6773623214103281\n",
            "Epoch: 1475, Len of Training loss: 36, Average loss: 1.4311913417445288\n",
            "Len of Validation loss: 128, Average loss: 1.7048939620144665\n",
            "Epoch: 1476, Len of Training loss: 36, Average loss: 1.4745706419150035\n",
            "Len of Validation loss: 128, Average loss: 1.7387120528146625\n",
            "Epoch: 1477, Len of Training loss: 36, Average loss: 1.4626965920130413\n",
            "Len of Validation loss: 128, Average loss: 1.6738828076049685\n",
            "Epoch: 1478, Len of Training loss: 36, Average loss: 1.4235615597830877\n",
            "Len of Validation loss: 128, Average loss: 1.683963579358533\n",
            "Epoch: 1479, Len of Training loss: 36, Average loss: 1.400939146677653\n",
            "Len of Validation loss: 128, Average loss: 1.6562054972164333\n",
            "Epoch: 1480, Len of Training loss: 36, Average loss: 1.4128527955876455\n",
            "Len of Validation loss: 128, Average loss: 1.6968253031373024\n",
            "Epoch: 1481, Len of Training loss: 36, Average loss: 1.4104364216327667\n",
            "Len of Validation loss: 128, Average loss: 1.6371985825244337\n",
            "Epoch: 1482, Len of Training loss: 36, Average loss: 1.4194624688890245\n",
            "Len of Validation loss: 128, Average loss: 1.6487414613366127\n",
            "Epoch: 1483, Len of Training loss: 36, Average loss: 1.4050891333156161\n",
            "Len of Validation loss: 128, Average loss: 1.6469505105633289\n",
            "Epoch: 1484, Len of Training loss: 36, Average loss: 1.422460264629788\n",
            "Len of Validation loss: 128, Average loss: 1.738748126430437\n",
            "Epoch: 1485, Len of Training loss: 36, Average loss: 1.4666290647453732\n",
            "Len of Validation loss: 128, Average loss: 1.6644930178299546\n",
            "Epoch: 1486, Len of Training loss: 36, Average loss: 1.407537192106247\n",
            "Len of Validation loss: 128, Average loss: 1.687913668807596\n",
            "Epoch: 1487, Len of Training loss: 36, Average loss: 1.4349129531118605\n",
            "Len of Validation loss: 128, Average loss: 1.6864774338901043\n",
            "Epoch: 1488, Len of Training loss: 36, Average loss: 1.4225192815065384\n",
            "Len of Validation loss: 128, Average loss: 1.7074288032017648\n",
            "Epoch: 1489, Len of Training loss: 36, Average loss: 1.4366809493965573\n",
            "Len of Validation loss: 128, Average loss: 1.7300819186493754\n",
            "Epoch: 1490, Len of Training loss: 36, Average loss: 1.4172683854897816\n",
            "Len of Validation loss: 128, Average loss: 1.6756233307532966\n",
            "Epoch: 1491, Len of Training loss: 36, Average loss: 1.4234287407663133\n",
            "Len of Validation loss: 128, Average loss: 1.7002573620993644\n",
            "Epoch: 1492, Len of Training loss: 36, Average loss: 1.4510060085190668\n",
            "Len of Validation loss: 128, Average loss: 1.6946692189667374\n",
            "Epoch: 1493, Len of Training loss: 36, Average loss: 1.434162394868003\n",
            "Len of Validation loss: 128, Average loss: 1.6399080122355372\n",
            "Epoch: 1494, Len of Training loss: 36, Average loss: 1.4472899105813768\n",
            "Len of Validation loss: 128, Average loss: 1.7318317806348205\n",
            "Epoch: 1495, Len of Training loss: 36, Average loss: 1.4557606743441687\n",
            "Len of Validation loss: 128, Average loss: 1.6476922186557204\n",
            "Epoch: 1496, Len of Training loss: 36, Average loss: 1.401477677954568\n",
            "Len of Validation loss: 128, Average loss: 1.7360474208835512\n",
            "Epoch: 1497, Len of Training loss: 36, Average loss: 1.4700537224610646\n",
            "Len of Validation loss: 128, Average loss: 1.6937437343876809\n",
            "Epoch: 1498, Len of Training loss: 36, Average loss: 1.4542512463198767\n",
            "Len of Validation loss: 128, Average loss: 1.7145621206145734\n",
            "Epoch: 1499, Len of Training loss: 36, Average loss: 1.4214775131808386\n",
            "Len of Validation loss: 128, Average loss: 1.6654840880073607\n",
            "Epoch: 1500, Len of Training loss: 36, Average loss: 1.4025235573450725\n",
            "Len of Validation loss: 128, Average loss: 1.6534464927390218\n",
            "Epoch: 1501, Len of Training loss: 36, Average loss: 1.4161682658725314\n",
            "Len of Validation loss: 128, Average loss: 1.7017571930773556\n",
            "Epoch: 1502, Len of Training loss: 36, Average loss: 1.419964035352071\n",
            "Len of Validation loss: 128, Average loss: 1.7233951180242002\n",
            "Epoch: 1503, Len of Training loss: 36, Average loss: 1.4131170908610027\n",
            "Len of Validation loss: 128, Average loss: 1.6863660927629098\n",
            "Epoch: 1504, Len of Training loss: 36, Average loss: 1.3954306145509083\n",
            "Len of Validation loss: 128, Average loss: 1.63032213319093\n",
            "Epoch: 1505, Len of Training loss: 36, Average loss: 1.4329022864500682\n",
            "Len of Validation loss: 128, Average loss: 1.7525230250321329\n",
            "Epoch: 1506, Len of Training loss: 36, Average loss: 1.4436456792884402\n",
            "Len of Validation loss: 128, Average loss: 1.7926695537753403\n",
            "Epoch: 1507, Len of Training loss: 36, Average loss: 1.4274634785122342\n",
            "Len of Validation loss: 128, Average loss: 1.7003079489804804\n",
            "Epoch: 1508, Len of Training loss: 36, Average loss: 1.4142250882254706\n",
            "Len of Validation loss: 128, Average loss: 1.675400884123519\n",
            "Epoch: 1509, Len of Training loss: 36, Average loss: 1.404133419195811\n",
            "Len of Validation loss: 128, Average loss: 1.7285537228453904\n",
            "Epoch: 1510, Len of Training loss: 36, Average loss: 1.4112038181887732\n",
            "Len of Validation loss: 128, Average loss: 1.661155926529318\n",
            "Epoch: 1511, Len of Training loss: 36, Average loss: 1.411290854215622\n",
            "Len of Validation loss: 128, Average loss: 1.676302719861269\n",
            "Epoch: 1512, Len of Training loss: 36, Average loss: 1.4394950336880155\n",
            "Len of Validation loss: 128, Average loss: 1.713697831146419\n",
            "Epoch: 1513, Len of Training loss: 36, Average loss: 1.4296103484100766\n",
            "Len of Validation loss: 128, Average loss: 1.7133471546694636\n",
            "Epoch: 1514, Len of Training loss: 36, Average loss: 1.4409706857469347\n",
            "Len of Validation loss: 128, Average loss: 1.6793612204492092\n",
            "Epoch: 1515, Len of Training loss: 36, Average loss: 1.4190265039602916\n",
            "Len of Validation loss: 128, Average loss: 1.7214367690030485\n",
            "Epoch: 1516, Len of Training loss: 36, Average loss: 1.4064995911386278\n",
            "Len of Validation loss: 128, Average loss: 1.6704000104218721\n",
            "Epoch: 1517, Len of Training loss: 36, Average loss: 1.4191404779752095\n",
            "Len of Validation loss: 128, Average loss: 1.6866846345365047\n",
            "Epoch: 1518, Len of Training loss: 36, Average loss: 1.3987944175799687\n",
            "Len of Validation loss: 128, Average loss: 1.6447302275337279\n",
            "Epoch: 1519, Len of Training loss: 36, Average loss: 1.4148032301002078\n",
            "Len of Validation loss: 128, Average loss: 1.6582354111596942\n",
            "Epoch: 1520, Len of Training loss: 36, Average loss: 1.3918770882818434\n",
            "Len of Validation loss: 128, Average loss: 1.6760568551253527\n",
            "Epoch: 1521, Len of Training loss: 36, Average loss: 1.395429187350803\n",
            "Len of Validation loss: 128, Average loss: 1.6813472348731011\n",
            "Epoch: 1522, Len of Training loss: 36, Average loss: 1.4104690700769424\n",
            "Len of Validation loss: 128, Average loss: 1.64539950247854\n",
            "Epoch: 1523, Len of Training loss: 36, Average loss: 1.3916822655333414\n",
            "Len of Validation loss: 128, Average loss: 1.689046984538436\n",
            "Epoch: 1524, Len of Training loss: 36, Average loss: 1.4227806627750397\n",
            "Len of Validation loss: 128, Average loss: 1.7389326537959278\n",
            "Epoch: 1525, Len of Training loss: 36, Average loss: 1.4093369874689314\n",
            "Len of Validation loss: 128, Average loss: 1.7405532172415406\n",
            "Epoch: 1526, Len of Training loss: 36, Average loss: 1.3995595475037892\n",
            "Len of Validation loss: 128, Average loss: 1.635374692734331\n",
            "Epoch: 1527, Len of Training loss: 36, Average loss: 1.3727922870053186\n",
            "Len of Validation loss: 128, Average loss: 1.6428097276948392\n",
            "Epoch: 1528, Len of Training loss: 36, Average loss: 1.3852891971667607\n",
            "Len of Validation loss: 128, Average loss: 1.6599370207404718\n",
            "Epoch: 1529, Len of Training loss: 36, Average loss: 1.3852786189979978\n",
            "Len of Validation loss: 128, Average loss: 1.6340791138354689\n",
            "Epoch: 1530, Len of Training loss: 36, Average loss: 1.4256822102599673\n",
            "Len of Validation loss: 128, Average loss: 1.703527723904699\n",
            "Epoch: 1531, Len of Training loss: 36, Average loss: 1.4125696453783247\n",
            "Len of Validation loss: 128, Average loss: 1.674335083225742\n",
            "Epoch: 1532, Len of Training loss: 36, Average loss: 1.4137869626283646\n",
            "Len of Validation loss: 128, Average loss: 1.6629586416529492\n",
            "Epoch: 1533, Len of Training loss: 36, Average loss: 1.4026244580745697\n",
            "Len of Validation loss: 128, Average loss: 1.7203478168230504\n",
            "Epoch: 1534, Len of Training loss: 36, Average loss: 1.4189969135655298\n",
            "Len of Validation loss: 128, Average loss: 1.7002540004905313\n",
            "Epoch: 1535, Len of Training loss: 36, Average loss: 1.4359009828832414\n",
            "Len of Validation loss: 128, Average loss: 1.692819623509422\n",
            "Epoch: 1536, Len of Training loss: 36, Average loss: 1.4337676299942865\n",
            "Len of Validation loss: 128, Average loss: 1.6609324247110635\n",
            "Epoch: 1537, Len of Training loss: 36, Average loss: 1.4029676268498104\n",
            "Len of Validation loss: 128, Average loss: 1.680030879098922\n",
            "Epoch: 1538, Len of Training loss: 36, Average loss: 1.4026463230450947\n",
            "Len of Validation loss: 128, Average loss: 1.6431112438440323\n",
            "Epoch: 1539, Len of Training loss: 36, Average loss: 1.385388304789861\n",
            "Len of Validation loss: 128, Average loss: 1.7172071253880858\n",
            "Epoch: 1540, Len of Training loss: 36, Average loss: 1.420289569430881\n",
            "Len of Validation loss: 128, Average loss: 1.684998418437317\n",
            "Epoch: 1541, Len of Training loss: 36, Average loss: 1.4011000196139018\n",
            "Len of Validation loss: 128, Average loss: 1.64162040874362\n",
            "Epoch: 1542, Len of Training loss: 36, Average loss: 1.3900581929418776\n",
            "Len of Validation loss: 128, Average loss: 1.6784893774893135\n",
            "Epoch: 1543, Len of Training loss: 36, Average loss: 1.4035663803418477\n",
            "Len of Validation loss: 128, Average loss: 1.6662659377325326\n",
            "Epoch: 1544, Len of Training loss: 36, Average loss: 1.3993215925163693\n",
            "Len of Validation loss: 128, Average loss: 1.6998358466662467\n",
            "Epoch: 1545, Len of Training loss: 36, Average loss: 1.4062902761830225\n",
            "Len of Validation loss: 128, Average loss: 1.6613962713163346\n",
            "Epoch: 1546, Len of Training loss: 36, Average loss: 1.4021009471681383\n",
            "Len of Validation loss: 128, Average loss: 1.6877634264528751\n",
            "Epoch: 1547, Len of Training loss: 36, Average loss: 1.403553717666202\n",
            "Len of Validation loss: 128, Average loss: 1.7198574335779995\n",
            "Epoch: 1548, Len of Training loss: 36, Average loss: 1.3894659214549594\n",
            "Len of Validation loss: 128, Average loss: 1.6866139841731638\n",
            "Epoch: 1549, Len of Training loss: 36, Average loss: 1.3813288923766878\n",
            "Len of Validation loss: 128, Average loss: 1.642170260893181\n",
            "Epoch: 1550, Len of Training loss: 36, Average loss: 1.40298150645362\n",
            "Len of Validation loss: 128, Average loss: 1.693478346336633\n",
            "Epoch: 1551, Len of Training loss: 36, Average loss: 1.407640278339386\n",
            "Len of Validation loss: 128, Average loss: 1.6363115331623703\n",
            "Epoch: 1552, Len of Training loss: 36, Average loss: 1.3867655992507935\n",
            "Len of Validation loss: 128, Average loss: 1.7321685147471726\n",
            "Epoch: 1553, Len of Training loss: 36, Average loss: 1.4281399183803134\n",
            "Len of Validation loss: 128, Average loss: 1.7141421702690423\n",
            "Epoch: 1554, Len of Training loss: 36, Average loss: 1.3956877042849858\n",
            "Len of Validation loss: 128, Average loss: 1.6435618915129453\n",
            "Epoch: 1555, Len of Training loss: 36, Average loss: 1.4290473494264815\n",
            "Len of Validation loss: 128, Average loss: 1.7984503349289298\n",
            "Epoch: 1556, Len of Training loss: 36, Average loss: 1.4061009585857391\n",
            "Len of Validation loss: 128, Average loss: 1.6610795210581273\n",
            "Epoch: 1557, Len of Training loss: 36, Average loss: 1.385203515489896\n",
            "Len of Validation loss: 128, Average loss: 1.6502020470798016\n",
            "Epoch: 1558, Len of Training loss: 36, Average loss: 1.3800929486751556\n",
            "Len of Validation loss: 128, Average loss: 1.6452984751667827\n",
            "Epoch: 1559, Len of Training loss: 36, Average loss: 1.3853867318895128\n",
            "Len of Validation loss: 128, Average loss: 1.653585125459358\n",
            "Epoch: 1560, Len of Training loss: 36, Average loss: 1.4222705297999911\n",
            "Len of Validation loss: 128, Average loss: 1.6519297864288092\n",
            "Epoch: 1561, Len of Training loss: 36, Average loss: 1.4279168181949191\n",
            "Len of Validation loss: 128, Average loss: 1.6820194583851844\n",
            "Epoch: 1562, Len of Training loss: 36, Average loss: 1.3724496497048273\n",
            "Len of Validation loss: 128, Average loss: 1.624667125986889\n",
            "Epoch: 1563, Len of Training loss: 36, Average loss: 1.4106429815292358\n",
            "Len of Validation loss: 128, Average loss: 1.6378585922066122\n",
            "Epoch: 1564, Len of Training loss: 36, Average loss: 1.4001717699898615\n",
            "Len of Validation loss: 128, Average loss: 1.6176422031130642\n",
            "Epoch: 1565, Len of Training loss: 36, Average loss: 1.410784188244078\n",
            "Len of Validation loss: 128, Average loss: 1.6646659055259079\n",
            "Epoch: 1566, Len of Training loss: 36, Average loss: 1.3628353907002344\n",
            "Len of Validation loss: 128, Average loss: 1.6684324720408767\n",
            "Epoch: 1567, Len of Training loss: 36, Average loss: 1.3718611929151747\n",
            "Len of Validation loss: 128, Average loss: 1.6324194457847625\n",
            "Epoch: 1568, Len of Training loss: 36, Average loss: 1.3564147684309218\n",
            "Len of Validation loss: 128, Average loss: 1.6748773336876184\n",
            "Epoch: 1569, Len of Training loss: 36, Average loss: 1.4308324886692896\n",
            "Len of Validation loss: 128, Average loss: 1.6857447614893317\n",
            "Epoch: 1570, Len of Training loss: 36, Average loss: 1.3912038687202666\n",
            "Len of Validation loss: 128, Average loss: 1.6728684385307133\n",
            "Epoch: 1571, Len of Training loss: 36, Average loss: 1.4377040366331737\n",
            "Len of Validation loss: 128, Average loss: 1.750218250323087\n",
            "Epoch: 1572, Len of Training loss: 36, Average loss: 1.3936445944839053\n",
            "Len of Validation loss: 128, Average loss: 1.670345410471782\n",
            "Epoch: 1573, Len of Training loss: 36, Average loss: 1.3920389314492543\n",
            "Len of Validation loss: 128, Average loss: 1.6747225332073867\n",
            "Epoch: 1574, Len of Training loss: 36, Average loss: 1.4080339206589594\n",
            "Len of Validation loss: 128, Average loss: 1.6638022381812334\n",
            "Epoch: 1575, Len of Training loss: 36, Average loss: 1.3930283784866333\n",
            "Len of Validation loss: 128, Average loss: 1.6879816218279302\n",
            "Epoch: 1576, Len of Training loss: 36, Average loss: 1.3870558738708496\n",
            "Len of Validation loss: 128, Average loss: 1.66582749504596\n",
            "Epoch: 1577, Len of Training loss: 36, Average loss: 1.4006356961197324\n",
            "Len of Validation loss: 128, Average loss: 1.6492147552780807\n",
            "Epoch: 1578, Len of Training loss: 36, Average loss: 1.3784759607579973\n",
            "Len of Validation loss: 128, Average loss: 1.6760962549597025\n",
            "Epoch: 1579, Len of Training loss: 36, Average loss: 1.3742551041973963\n",
            "Len of Validation loss: 128, Average loss: 1.6434207467827946\n",
            "Epoch: 1580, Len of Training loss: 36, Average loss: 1.3782604336738586\n",
            "Len of Validation loss: 128, Average loss: 1.6925041035283357\n",
            "Epoch: 1581, Len of Training loss: 36, Average loss: 1.4051702386803098\n",
            "Len of Validation loss: 128, Average loss: 1.7249859091825783\n",
            "Epoch: 1582, Len of Training loss: 36, Average loss: 1.3760509855217404\n",
            "Len of Validation loss: 128, Average loss: 1.685652325162664\n",
            "Epoch: 1583, Len of Training loss: 36, Average loss: 1.3929186016321182\n",
            "Len of Validation loss: 128, Average loss: 1.6625863395165652\n",
            "Epoch: 1584, Len of Training loss: 36, Average loss: 1.3672875761985779\n",
            "Len of Validation loss: 128, Average loss: 1.6474067438393831\n",
            "Epoch: 1585, Len of Training loss: 36, Average loss: 1.3609789278772142\n",
            "Len of Validation loss: 128, Average loss: 1.6458991633262485\n",
            "Epoch: 1586, Len of Training loss: 36, Average loss: 1.3765641914473639\n",
            "Len of Validation loss: 128, Average loss: 1.6734965397045016\n",
            "Epoch: 1587, Len of Training loss: 36, Average loss: 1.362503093149927\n",
            "Len of Validation loss: 128, Average loss: 1.658401515102014\n",
            "Epoch: 1588, Len of Training loss: 36, Average loss: 1.3751692407661014\n",
            "Len of Validation loss: 128, Average loss: 1.6477906780783087\n",
            "Epoch: 1589, Len of Training loss: 36, Average loss: 1.3885040018293593\n",
            "Len of Validation loss: 128, Average loss: 1.716576374368742\n",
            "Epoch: 1590, Len of Training loss: 36, Average loss: 1.3781482461425993\n",
            "Len of Validation loss: 128, Average loss: 1.7120172972790897\n",
            "Epoch: 1591, Len of Training loss: 36, Average loss: 1.3689989745616913\n",
            "Len of Validation loss: 128, Average loss: 1.6595081705600023\n",
            "Epoch: 1592, Len of Training loss: 36, Average loss: 1.377256151702669\n",
            "Len of Validation loss: 128, Average loss: 1.6645944763440639\n",
            "Epoch: 1593, Len of Training loss: 36, Average loss: 1.3936780128214095\n",
            "Len of Validation loss: 128, Average loss: 1.6264485025312752\n",
            "Epoch: 1594, Len of Training loss: 36, Average loss: 1.4205742842621274\n",
            "Len of Validation loss: 128, Average loss: 1.6566315197851509\n",
            "Epoch: 1595, Len of Training loss: 36, Average loss: 1.370113925801383\n",
            "Len of Validation loss: 128, Average loss: 1.6532505557406694\n",
            "Epoch: 1596, Len of Training loss: 36, Average loss: 1.3700001736481984\n",
            "Len of Validation loss: 128, Average loss: 1.690261249896139\n",
            "Epoch: 1597, Len of Training loss: 36, Average loss: 1.3626306023862627\n",
            "Len of Validation loss: 128, Average loss: 1.6531421882100403\n",
            "Epoch: 1598, Len of Training loss: 36, Average loss: 1.3656418820222218\n",
            "Len of Validation loss: 128, Average loss: 1.6455681417137384\n",
            "Epoch: 1599, Len of Training loss: 36, Average loss: 1.3608263184626896\n",
            "Len of Validation loss: 128, Average loss: 1.6668166038580239\n",
            "Epoch: 1600, Len of Training loss: 36, Average loss: 1.3837579157617357\n",
            "Len of Validation loss: 128, Average loss: 1.649198904633522\n",
            "Epoch: 1601, Len of Training loss: 36, Average loss: 1.395046419567532\n",
            "Len of Validation loss: 128, Average loss: 1.666677788598463\n",
            "Epoch: 1602, Len of Training loss: 36, Average loss: 1.3643900917636023\n",
            "Len of Validation loss: 128, Average loss: 1.6652426396030933\n",
            "Epoch: 1603, Len of Training loss: 36, Average loss: 1.3714176813761394\n",
            "Len of Validation loss: 128, Average loss: 1.6579940693918616\n",
            "Epoch: 1604, Len of Training loss: 36, Average loss: 1.451249983575609\n",
            "Len of Validation loss: 128, Average loss: 1.6525379535742104\n",
            "Epoch: 1605, Len of Training loss: 36, Average loss: 1.4197499487135146\n",
            "Len of Validation loss: 128, Average loss: 1.6744995056651533\n",
            "Epoch: 1606, Len of Training loss: 36, Average loss: 1.413522995180554\n",
            "Len of Validation loss: 128, Average loss: 1.6328228455968201\n",
            "Epoch: 1607, Len of Training loss: 36, Average loss: 1.3729076186815898\n",
            "Len of Validation loss: 128, Average loss: 1.7315151682123542\n",
            "Epoch: 1608, Len of Training loss: 36, Average loss: 1.3730264438523188\n",
            "Len of Validation loss: 128, Average loss: 1.6598241922911257\n",
            "Epoch: 1609, Len of Training loss: 36, Average loss: 1.3574789332018957\n",
            "Len of Validation loss: 128, Average loss: 1.6467560157179832\n",
            "Epoch: 1610, Len of Training loss: 36, Average loss: 1.3547958119048014\n",
            "Len of Validation loss: 128, Average loss: 1.6650042885448784\n",
            "Epoch: 1611, Len of Training loss: 36, Average loss: 1.3961966534455617\n",
            "Len of Validation loss: 128, Average loss: 1.6397641182411462\n",
            "Epoch: 1612, Len of Training loss: 36, Average loss: 1.3567773087157144\n",
            "Len of Validation loss: 128, Average loss: 1.6740262925159186\n",
            "Epoch: 1613, Len of Training loss: 36, Average loss: 1.3606253614028294\n",
            "Len of Validation loss: 128, Average loss: 1.6522713876329362\n",
            "Epoch: 1614, Len of Training loss: 36, Average loss: 1.3552693095472124\n",
            "Len of Validation loss: 128, Average loss: 1.704708342673257\n",
            "Epoch: 1615, Len of Training loss: 36, Average loss: 1.3887138350142374\n",
            "Len of Validation loss: 128, Average loss: 1.6709951194934547\n",
            "Epoch: 1616, Len of Training loss: 36, Average loss: 1.4065683633089066\n",
            "Len of Validation loss: 128, Average loss: 1.6730382025707513\n",
            "Epoch: 1617, Len of Training loss: 36, Average loss: 1.372561600473192\n",
            "Len of Validation loss: 128, Average loss: 1.6609582416713238\n",
            "Epoch: 1618, Len of Training loss: 36, Average loss: 1.4146264510022268\n",
            "Len of Validation loss: 128, Average loss: 1.7454742202535272\n",
            "Epoch: 1619, Len of Training loss: 36, Average loss: 1.4121455020374722\n",
            "Len of Validation loss: 128, Average loss: 1.6963792310561985\n",
            "Epoch: 1620, Len of Training loss: 36, Average loss: 1.3654595067103703\n",
            "Len of Validation loss: 128, Average loss: 1.7004520362243056\n",
            "Epoch: 1621, Len of Training loss: 36, Average loss: 1.3712548381752439\n",
            "Len of Validation loss: 128, Average loss: 1.6614883833099157\n",
            "Epoch: 1622, Len of Training loss: 36, Average loss: 1.3743859496381547\n",
            "Len of Validation loss: 128, Average loss: 1.6775670300703496\n",
            "Epoch: 1623, Len of Training loss: 36, Average loss: 1.3641226043303807\n",
            "Len of Validation loss: 128, Average loss: 1.6362349321134388\n",
            "Epoch: 1624, Len of Training loss: 36, Average loss: 1.3779668344391718\n",
            "Len of Validation loss: 128, Average loss: 1.6776543271262199\n",
            "Epoch: 1625, Len of Training loss: 36, Average loss: 1.4311451746357813\n",
            "Len of Validation loss: 128, Average loss: 1.7495934781618416\n",
            "Epoch: 1626, Len of Training loss: 36, Average loss: 1.3764556696017582\n",
            "Len of Validation loss: 128, Average loss: 1.708943780278787\n",
            "Epoch: 1627, Len of Training loss: 36, Average loss: 1.3679226752784517\n",
            "Len of Validation loss: 128, Average loss: 1.656764764804393\n",
            "Epoch: 1628, Len of Training loss: 36, Average loss: 1.3475098974174924\n",
            "Len of Validation loss: 128, Average loss: 1.6364298951812088\n",
            "Epoch: 1629, Len of Training loss: 36, Average loss: 1.3687509298324585\n",
            "Len of Validation loss: 128, Average loss: 1.6529914380516857\n",
            "Epoch: 1630, Len of Training loss: 36, Average loss: 1.3639646222194035\n",
            "Len of Validation loss: 128, Average loss: 1.684840217931196\n",
            "Epoch: 1631, Len of Training loss: 36, Average loss: 1.351692580514484\n",
            "Len of Validation loss: 128, Average loss: 1.6967649144353345\n",
            "Epoch: 1632, Len of Training loss: 36, Average loss: 1.3646811081303492\n",
            "Len of Validation loss: 128, Average loss: 1.6683630121406168\n",
            "Epoch: 1633, Len of Training loss: 36, Average loss: 1.3460660593377218\n",
            "Len of Validation loss: 128, Average loss: 1.6462044434156269\n",
            "Epoch: 1634, Len of Training loss: 36, Average loss: 1.3689666191736858\n",
            "Len of Validation loss: 128, Average loss: 1.6452736700884998\n",
            "Epoch: 1635, Len of Training loss: 36, Average loss: 1.3441568281915452\n",
            "Len of Validation loss: 128, Average loss: 1.6663667932152748\n",
            "Epoch: 1636, Len of Training loss: 36, Average loss: 1.3580777512656317\n",
            "Len of Validation loss: 128, Average loss: 1.6843885781709105\n",
            "Epoch: 1637, Len of Training loss: 36, Average loss: 1.3688807818624709\n",
            "Len of Validation loss: 128, Average loss: 1.6786023036111146\n",
            "Epoch: 1638, Len of Training loss: 36, Average loss: 1.3601388898160722\n",
            "Len of Validation loss: 128, Average loss: 1.6998133959714323\n",
            "Epoch: 1639, Len of Training loss: 36, Average loss: 1.3836934053235583\n",
            "Len of Validation loss: 128, Average loss: 1.6493047322146595\n",
            "Epoch: 1640, Len of Training loss: 36, Average loss: 1.3328450603617563\n",
            "Len of Validation loss: 128, Average loss: 1.6273338277824223\n",
            "Epoch: 1641, Len of Training loss: 36, Average loss: 1.3813957009050581\n",
            "Len of Validation loss: 128, Average loss: 1.662599342991598\n",
            "Epoch: 1642, Len of Training loss: 36, Average loss: 1.3692866596910689\n",
            "Len of Validation loss: 128, Average loss: 1.6571758466307074\n",
            "Epoch: 1643, Len of Training loss: 36, Average loss: 1.3611992365784116\n",
            "Len of Validation loss: 128, Average loss: 1.6864359511528164\n",
            "Epoch: 1644, Len of Training loss: 36, Average loss: 1.3621849285231695\n",
            "Len of Validation loss: 128, Average loss: 1.6561178069096059\n",
            "Epoch: 1645, Len of Training loss: 36, Average loss: 1.380117505788803\n",
            "Len of Validation loss: 128, Average loss: 1.6356109362095594\n",
            "Epoch: 1646, Len of Training loss: 36, Average loss: 1.3599134385585785\n",
            "Len of Validation loss: 128, Average loss: 1.6919656470417976\n",
            "Epoch: 1647, Len of Training loss: 36, Average loss: 1.406095998154746\n",
            "Len of Validation loss: 128, Average loss: 1.6873162847477943\n",
            "Epoch: 1648, Len of Training loss: 36, Average loss: 1.3690462013085682\n",
            "Len of Validation loss: 128, Average loss: 1.7080030431970954\n",
            "Epoch: 1649, Len of Training loss: 36, Average loss: 1.3710192557838228\n",
            "Len of Validation loss: 128, Average loss: 1.6170833529904485\n",
            "Epoch: 1650, Len of Training loss: 36, Average loss: 1.3481246199872758\n",
            "Len of Validation loss: 128, Average loss: 1.684690029360354\n",
            "Epoch: 1651, Len of Training loss: 36, Average loss: 1.3710567851861317\n",
            "Len of Validation loss: 128, Average loss: 1.6197089629713446\n",
            "Epoch: 1652, Len of Training loss: 36, Average loss: 1.3627878725528717\n",
            "Len of Validation loss: 128, Average loss: 1.5948730495292693\n",
            "Epoch: 1653, Len of Training loss: 36, Average loss: 1.3896720459063847\n",
            "Len of Validation loss: 128, Average loss: 1.8342085788026452\n",
            "Epoch: 1654, Len of Training loss: 36, Average loss: 1.5286211437649198\n",
            "Len of Validation loss: 128, Average loss: 1.7566727113444358\n",
            "Epoch: 1655, Len of Training loss: 36, Average loss: 1.3898060752285852\n",
            "Len of Validation loss: 128, Average loss: 1.7423016470856965\n",
            "Epoch: 1656, Len of Training loss: 36, Average loss: 1.3560270369052887\n",
            "Len of Validation loss: 128, Average loss: 1.6602775305509567\n",
            "Epoch: 1657, Len of Training loss: 36, Average loss: 1.343610965543323\n",
            "Len of Validation loss: 128, Average loss: 1.6944899566005915\n",
            "Epoch: 1658, Len of Training loss: 36, Average loss: 1.396963679128223\n",
            "Len of Validation loss: 128, Average loss: 1.6699559187982231\n",
            "Epoch: 1659, Len of Training loss: 36, Average loss: 1.3474895705779393\n",
            "Len of Validation loss: 128, Average loss: 1.624742460437119\n",
            "Epoch: 1660, Len of Training loss: 36, Average loss: 1.3317100405693054\n",
            "Len of Validation loss: 128, Average loss: 1.6537784778047353\n",
            "Epoch: 1661, Len of Training loss: 36, Average loss: 1.3325490868753858\n",
            "Len of Validation loss: 128, Average loss: 1.6032746601849794\n",
            "Epoch: 1662, Len of Training loss: 36, Average loss: 1.3381034400728014\n",
            "Len of Validation loss: 128, Average loss: 1.705852230079472\n",
            "Epoch: 1663, Len of Training loss: 36, Average loss: 1.3808279501067267\n",
            "Len of Validation loss: 128, Average loss: 1.6421752674505115\n",
            "Epoch: 1664, Len of Training loss: 36, Average loss: 1.3425673047701518\n",
            "Len of Validation loss: 128, Average loss: 1.6574353375472128\n",
            "Epoch: 1665, Len of Training loss: 36, Average loss: 1.3371273941463895\n",
            "Len of Validation loss: 128, Average loss: 1.6385025999043137\n",
            "Epoch: 1666, Len of Training loss: 36, Average loss: 1.3590838031636343\n",
            "Len of Validation loss: 128, Average loss: 1.710218874271959\n",
            "Epoch: 1667, Len of Training loss: 36, Average loss: 1.3383197320832148\n",
            "Len of Validation loss: 128, Average loss: 1.6366573716513813\n",
            "Epoch: 1668, Len of Training loss: 36, Average loss: 1.3378417525026534\n",
            "Len of Validation loss: 128, Average loss: 1.7079209061339498\n",
            "Epoch: 1669, Len of Training loss: 36, Average loss: 1.423811415831248\n",
            "Len of Validation loss: 128, Average loss: 1.6706849043257535\n",
            "Epoch: 1670, Len of Training loss: 36, Average loss: 1.3592384739054575\n",
            "Len of Validation loss: 128, Average loss: 1.6418981708120555\n",
            "Epoch: 1671, Len of Training loss: 36, Average loss: 1.3604802091916401\n",
            "Len of Validation loss: 128, Average loss: 1.6860207845456898\n",
            "Epoch: 1672, Len of Training loss: 36, Average loss: 1.336563292476866\n",
            "Len of Validation loss: 128, Average loss: 1.6911576124839485\n",
            "Epoch: 1673, Len of Training loss: 36, Average loss: 1.3467766907480028\n",
            "Len of Validation loss: 128, Average loss: 1.6562405514996499\n",
            "Epoch: 1674, Len of Training loss: 36, Average loss: 1.3248243182897568\n",
            "Len of Validation loss: 128, Average loss: 1.6391458918806165\n",
            "Epoch: 1675, Len of Training loss: 36, Average loss: 1.3625064492225647\n",
            "Len of Validation loss: 128, Average loss: 1.6590858285780996\n",
            "Epoch: 1676, Len of Training loss: 36, Average loss: 1.3535600420501497\n",
            "Len of Validation loss: 128, Average loss: 1.757649214938283\n",
            "Epoch: 1677, Len of Training loss: 36, Average loss: 1.3403345594803493\n",
            "Len of Validation loss: 128, Average loss: 1.6904887629207224\n",
            "Epoch: 1678, Len of Training loss: 36, Average loss: 1.3422411117288802\n",
            "Len of Validation loss: 128, Average loss: 1.6433880554977804\n",
            "Epoch: 1679, Len of Training loss: 36, Average loss: 1.3836263087060716\n",
            "Len of Validation loss: 128, Average loss: 1.675366848707199\n",
            "Epoch: 1680, Len of Training loss: 36, Average loss: 1.3774187217156093\n",
            "Len of Validation loss: 128, Average loss: 1.716825888492167\n",
            "Epoch: 1681, Len of Training loss: 36, Average loss: 1.3725849390029907\n",
            "Len of Validation loss: 128, Average loss: 1.657747708261013\n",
            "Epoch: 1682, Len of Training loss: 36, Average loss: 1.3426459597216711\n",
            "Len of Validation loss: 128, Average loss: 1.6227664658799767\n",
            "Epoch: 1683, Len of Training loss: 36, Average loss: 1.3443232046233282\n",
            "Len of Validation loss: 128, Average loss: 1.65987544413656\n",
            "Epoch: 1684, Len of Training loss: 36, Average loss: 1.3535575833585527\n",
            "Len of Validation loss: 128, Average loss: 1.6232443898916245\n",
            "Epoch: 1685, Len of Training loss: 36, Average loss: 1.3321935103999243\n",
            "Len of Validation loss: 128, Average loss: 1.6602549343369901\n",
            "Epoch: 1686, Len of Training loss: 36, Average loss: 1.352684423327446\n",
            "Len of Validation loss: 128, Average loss: 1.669925668509677\n",
            "Epoch: 1687, Len of Training loss: 36, Average loss: 1.337650994459788\n",
            "Len of Validation loss: 128, Average loss: 1.6435614242218435\n",
            "Epoch: 1688, Len of Training loss: 36, Average loss: 1.3464206059773762\n",
            "Len of Validation loss: 128, Average loss: 1.636642434168607\n",
            "Epoch: 1689, Len of Training loss: 36, Average loss: 1.3222221351332135\n",
            "Len of Validation loss: 128, Average loss: 1.5887562653515488\n",
            "Epoch: 1690, Len of Training loss: 36, Average loss: 1.320681631565094\n",
            "Len of Validation loss: 128, Average loss: 1.6701727372128516\n",
            "Epoch: 1691, Len of Training loss: 36, Average loss: 1.3229377501540713\n",
            "Len of Validation loss: 128, Average loss: 1.6137160656508058\n",
            "Epoch: 1692, Len of Training loss: 36, Average loss: 1.34341974887583\n",
            "Len of Validation loss: 128, Average loss: 1.642719100927934\n",
            "Epoch: 1693, Len of Training loss: 36, Average loss: 1.3380203660991457\n",
            "Len of Validation loss: 128, Average loss: 1.6439327613916248\n",
            "Epoch: 1694, Len of Training loss: 36, Average loss: 1.3515421599149704\n",
            "Len of Validation loss: 128, Average loss: 1.6525317102205008\n",
            "Epoch: 1695, Len of Training loss: 36, Average loss: 1.3676349090205298\n",
            "Len of Validation loss: 128, Average loss: 1.6463628413621336\n",
            "Epoch: 1696, Len of Training loss: 36, Average loss: 1.3554856942759619\n",
            "Len of Validation loss: 128, Average loss: 1.7136149141006172\n",
            "Epoch: 1697, Len of Training loss: 36, Average loss: 1.353865181406339\n",
            "Len of Validation loss: 128, Average loss: 1.695166718447581\n",
            "Epoch: 1698, Len of Training loss: 36, Average loss: 1.33635317782561\n",
            "Len of Validation loss: 128, Average loss: 1.6271693755406886\n",
            "Epoch: 1699, Len of Training loss: 36, Average loss: 1.342974493900935\n",
            "Len of Validation loss: 128, Average loss: 1.6699107480235398\n",
            "Epoch: 1700, Len of Training loss: 36, Average loss: 1.3852514889505174\n",
            "Len of Validation loss: 128, Average loss: 1.6325881609227508\n",
            "Epoch: 1701, Len of Training loss: 36, Average loss: 1.3539813425805833\n",
            "Len of Validation loss: 128, Average loss: 1.6446511764079332\n",
            "Epoch: 1702, Len of Training loss: 36, Average loss: 1.3507291442818112\n",
            "Len of Validation loss: 128, Average loss: 1.6228044235613197\n",
            "Epoch: 1703, Len of Training loss: 36, Average loss: 1.3408175905545552\n",
            "Len of Validation loss: 128, Average loss: 1.6279520040843636\n",
            "Epoch: 1704, Len of Training loss: 36, Average loss: 1.350053209397528\n",
            "Len of Validation loss: 128, Average loss: 1.6696341219358146\n",
            "Epoch: 1705, Len of Training loss: 36, Average loss: 1.3380057215690613\n",
            "Len of Validation loss: 128, Average loss: 1.6292419261299074\n",
            "Epoch: 1706, Len of Training loss: 36, Average loss: 1.319103820456399\n",
            "Len of Validation loss: 128, Average loss: 1.6363746377173811\n",
            "Epoch: 1707, Len of Training loss: 36, Average loss: 1.3326478666729398\n",
            "Len of Validation loss: 128, Average loss: 1.6304663820192218\n",
            "Epoch: 1708, Len of Training loss: 36, Average loss: 1.3489892532428105\n",
            "Len of Validation loss: 128, Average loss: 1.757382362615317\n",
            "Epoch: 1709, Len of Training loss: 36, Average loss: 1.342431280348036\n",
            "Len of Validation loss: 128, Average loss: 1.6349829589016736\n",
            "Epoch: 1710, Len of Training loss: 36, Average loss: 1.340877377324634\n",
            "Len of Validation loss: 128, Average loss: 1.6320527102798223\n",
            "Epoch: 1711, Len of Training loss: 36, Average loss: 1.3202519019444783\n",
            "Len of Validation loss: 128, Average loss: 1.6441661997232586\n",
            "Epoch: 1712, Len of Training loss: 36, Average loss: 1.3493598865138159\n",
            "Len of Validation loss: 128, Average loss: 1.6303504996467382\n",
            "Epoch: 1713, Len of Training loss: 36, Average loss: 1.3464921712875366\n",
            "Len of Validation loss: 128, Average loss: 1.6846586444880813\n",
            "Epoch: 1714, Len of Training loss: 36, Average loss: 1.3433219608333375\n",
            "Len of Validation loss: 128, Average loss: 1.700783470645547\n",
            "Epoch: 1715, Len of Training loss: 36, Average loss: 1.366524381770028\n",
            "Len of Validation loss: 128, Average loss: 1.707128175534308\n",
            "Epoch: 1716, Len of Training loss: 36, Average loss: 1.3688096172279782\n",
            "Len of Validation loss: 128, Average loss: 1.6367789269424975\n",
            "Epoch: 1717, Len of Training loss: 36, Average loss: 1.336661116944419\n",
            "Len of Validation loss: 128, Average loss: 1.6652146796695888\n",
            "Epoch: 1718, Len of Training loss: 36, Average loss: 1.3149796028931935\n",
            "Len of Validation loss: 128, Average loss: 1.6322579635307193\n",
            "Epoch: 1719, Len of Training loss: 36, Average loss: 1.3760892550150554\n",
            "Len of Validation loss: 128, Average loss: 1.6770136780105531\n",
            "Epoch: 1720, Len of Training loss: 36, Average loss: 1.3739086753792233\n",
            "Len of Validation loss: 128, Average loss: 1.6294371087569743\n",
            "Epoch: 1721, Len of Training loss: 36, Average loss: 1.327154881424374\n",
            "Len of Validation loss: 128, Average loss: 1.6780845602042973\n",
            "Epoch: 1722, Len of Training loss: 36, Average loss: 1.3258044372002284\n",
            "Len of Validation loss: 128, Average loss: 1.648892905563116\n",
            "Epoch: 1723, Len of Training loss: 36, Average loss: 1.326755616399977\n",
            "Len of Validation loss: 128, Average loss: 1.624578357907012\n",
            "Epoch: 1724, Len of Training loss: 36, Average loss: 1.3337071455187268\n",
            "Len of Validation loss: 128, Average loss: 1.6463681724853814\n",
            "Epoch: 1725, Len of Training loss: 36, Average loss: 1.3272210442357593\n",
            "Len of Validation loss: 128, Average loss: 1.6375484927557409\n",
            "Epoch: 1726, Len of Training loss: 36, Average loss: 1.3408386872874365\n",
            "Len of Validation loss: 128, Average loss: 1.6861120339017361\n",
            "Epoch: 1727, Len of Training loss: 36, Average loss: 1.3516512744956546\n",
            "Len of Validation loss: 128, Average loss: 1.7905982218217105\n",
            "Epoch: 1728, Len of Training loss: 36, Average loss: 1.3562877542442746\n",
            "Len of Validation loss: 128, Average loss: 1.6277340080123395\n",
            "Epoch: 1729, Len of Training loss: 36, Average loss: 1.3449820379416149\n",
            "Len of Validation loss: 128, Average loss: 1.6896455525420606\n",
            "Epoch: 1730, Len of Training loss: 36, Average loss: 1.35049416952663\n",
            "Len of Validation loss: 128, Average loss: 1.6164495768025517\n",
            "Epoch: 1731, Len of Training loss: 36, Average loss: 1.3285620212554932\n",
            "Len of Validation loss: 128, Average loss: 1.6092849632259458\n",
            "Epoch: 1732, Len of Training loss: 36, Average loss: 1.3395166993141174\n",
            "Len of Validation loss: 128, Average loss: 1.655330556910485\n",
            "Epoch: 1733, Len of Training loss: 36, Average loss: 1.363594525390201\n",
            "Len of Validation loss: 128, Average loss: 1.6191274500451982\n",
            "Epoch: 1734, Len of Training loss: 36, Average loss: 1.3240768081612058\n",
            "Len of Validation loss: 128, Average loss: 1.6788040570681915\n",
            "Epoch: 1735, Len of Training loss: 36, Average loss: 1.3836121824052598\n",
            "Len of Validation loss: 128, Average loss: 1.644209279678762\n",
            "Epoch: 1736, Len of Training loss: 36, Average loss: 1.3360745675033994\n",
            "Len of Validation loss: 128, Average loss: 1.630721457535401\n",
            "Epoch: 1737, Len of Training loss: 36, Average loss: 1.3270155323876276\n",
            "Len of Validation loss: 128, Average loss: 1.6248273262754083\n",
            "Epoch: 1738, Len of Training loss: 36, Average loss: 1.3085414403014712\n",
            "Len of Validation loss: 128, Average loss: 1.6311552177648991\n",
            "Epoch: 1739, Len of Training loss: 36, Average loss: 1.308440410428577\n",
            "Len of Validation loss: 128, Average loss: 1.628444149158895\n",
            "Epoch: 1740, Len of Training loss: 36, Average loss: 1.3355286452505324\n",
            "Len of Validation loss: 128, Average loss: 1.6317094606347382\n",
            "Epoch: 1741, Len of Training loss: 36, Average loss: 1.3078840110037062\n",
            "Len of Validation loss: 128, Average loss: 1.6278837590944022\n",
            "Epoch: 1742, Len of Training loss: 36, Average loss: 1.3096189863151975\n",
            "Len of Validation loss: 128, Average loss: 1.6205980931408703\n",
            "Epoch: 1743, Len of Training loss: 36, Average loss: 1.3297251727845933\n",
            "Len of Validation loss: 128, Average loss: 1.6833478838670999\n",
            "Epoch: 1744, Len of Training loss: 36, Average loss: 1.3291527562671237\n",
            "Len of Validation loss: 128, Average loss: 1.6166596277616918\n",
            "Epoch: 1745, Len of Training loss: 36, Average loss: 1.3139372550778918\n",
            "Len of Validation loss: 128, Average loss: 1.6052291153464466\n",
            "Epoch: 1746, Len of Training loss: 36, Average loss: 1.3224172923300002\n",
            "Len of Validation loss: 128, Average loss: 1.6154442464467138\n",
            "Epoch: 1747, Len of Training loss: 36, Average loss: 1.2953899602095287\n",
            "Len of Validation loss: 128, Average loss: 1.6715399795211852\n",
            "Epoch: 1748, Len of Training loss: 36, Average loss: 1.3186864422427282\n",
            "Len of Validation loss: 128, Average loss: 1.6196887311525643\n",
            "Epoch: 1749, Len of Training loss: 36, Average loss: 1.3233211719327502\n",
            "Len of Validation loss: 128, Average loss: 1.6283632046543062\n",
            "Epoch: 1750, Len of Training loss: 36, Average loss: 1.3136972784996033\n",
            "Len of Validation loss: 128, Average loss: 1.6410856684669852\n",
            "Epoch: 1751, Len of Training loss: 36, Average loss: 1.3217158416906993\n",
            "Len of Validation loss: 128, Average loss: 1.6561474127229303\n",
            "Epoch: 1752, Len of Training loss: 36, Average loss: 1.306184748808543\n",
            "Len of Validation loss: 128, Average loss: 1.6417223094031215\n",
            "Epoch: 1753, Len of Training loss: 36, Average loss: 1.3019931730296876\n",
            "Len of Validation loss: 128, Average loss: 1.6463072337210178\n",
            "Epoch: 1754, Len of Training loss: 36, Average loss: 1.3047521975305345\n",
            "Len of Validation loss: 128, Average loss: 1.6201831921935081\n",
            "Epoch: 1755, Len of Training loss: 36, Average loss: 1.3213574439287186\n",
            "Len of Validation loss: 128, Average loss: 1.6648837560787797\n",
            "Epoch: 1756, Len of Training loss: 36, Average loss: 1.32129282090399\n",
            "Len of Validation loss: 128, Average loss: 1.6707616220228374\n",
            "Epoch: 1757, Len of Training loss: 36, Average loss: 1.2880375782648723\n",
            "Len of Validation loss: 128, Average loss: 1.6550250158179551\n",
            "Epoch: 1758, Len of Training loss: 36, Average loss: 1.3057536996073194\n",
            "Len of Validation loss: 128, Average loss: 1.6035002886783332\n",
            "Epoch: 1759, Len of Training loss: 36, Average loss: 1.3403823640611436\n",
            "Len of Validation loss: 128, Average loss: 1.6543125321622938\n",
            "Epoch: 1760, Len of Training loss: 36, Average loss: 1.3241510275337431\n",
            "Len of Validation loss: 128, Average loss: 1.6435375483706594\n",
            "Epoch: 1761, Len of Training loss: 36, Average loss: 1.3386644654803805\n",
            "Len of Validation loss: 128, Average loss: 1.6545997937209904\n",
            "Epoch: 1762, Len of Training loss: 36, Average loss: 1.341379741827647\n",
            "Len of Validation loss: 128, Average loss: 1.6545064139645547\n",
            "Epoch: 1763, Len of Training loss: 36, Average loss: 1.3005604330036376\n",
            "Len of Validation loss: 128, Average loss: 1.614536881679669\n",
            "Epoch: 1764, Len of Training loss: 36, Average loss: 1.290739842587047\n",
            "Len of Validation loss: 128, Average loss: 1.6096526295877993\n",
            "Epoch: 1765, Len of Training loss: 36, Average loss: 1.3308490183618333\n",
            "Len of Validation loss: 128, Average loss: 1.614783800439909\n",
            "Epoch: 1766, Len of Training loss: 36, Average loss: 1.3055924839443631\n",
            "Len of Validation loss: 128, Average loss: 1.6297789646778256\n",
            "Epoch: 1767, Len of Training loss: 36, Average loss: 1.3261696530712976\n",
            "Len of Validation loss: 128, Average loss: 1.6454500518739223\n",
            "Epoch: 1768, Len of Training loss: 36, Average loss: 1.2958057738012738\n",
            "Len of Validation loss: 128, Average loss: 1.6076866551302373\n",
            "Epoch: 1769, Len of Training loss: 36, Average loss: 1.299829633699523\n",
            "Len of Validation loss: 128, Average loss: 1.676841550739482\n",
            "Epoch: 1770, Len of Training loss: 36, Average loss: 1.3115901235077116\n",
            "Len of Validation loss: 128, Average loss: 1.6309513503219932\n",
            "Epoch: 1771, Len of Training loss: 36, Average loss: 1.3197047842873468\n",
            "Len of Validation loss: 128, Average loss: 1.6227417709305882\n",
            "Epoch: 1772, Len of Training loss: 36, Average loss: 1.3254816002315946\n",
            "Len of Validation loss: 128, Average loss: 1.6700531700626016\n",
            "Epoch: 1773, Len of Training loss: 36, Average loss: 1.3093844768073823\n",
            "Len of Validation loss: 128, Average loss: 1.621057775337249\n",
            "Epoch: 1774, Len of Training loss: 36, Average loss: 1.3186491396692064\n",
            "Len of Validation loss: 128, Average loss: 1.6294662524014711\n",
            "Epoch: 1775, Len of Training loss: 36, Average loss: 1.329890822370847\n",
            "Len of Validation loss: 128, Average loss: 1.6106404901947826\n",
            "Epoch: 1776, Len of Training loss: 36, Average loss: 1.3137964738739862\n",
            "Len of Validation loss: 128, Average loss: 1.5903780311346054\n",
            "Epoch: 1777, Len of Training loss: 36, Average loss: 1.3123315572738647\n",
            "Len of Validation loss: 128, Average loss: 1.6453883554786444\n",
            "Epoch: 1778, Len of Training loss: 36, Average loss: 1.3433994783295526\n",
            "Len of Validation loss: 128, Average loss: 1.6341258774045855\n",
            "Epoch: 1779, Len of Training loss: 36, Average loss: 1.3189422753122118\n",
            "Len of Validation loss: 128, Average loss: 1.6385523860808462\n",
            "Epoch: 1780, Len of Training loss: 36, Average loss: 1.306237174404992\n",
            "Len of Validation loss: 128, Average loss: 1.6128985229879618\n",
            "Epoch: 1781, Len of Training loss: 36, Average loss: 1.3100788195927937\n",
            "Len of Validation loss: 128, Average loss: 1.7072005439549685\n",
            "Epoch: 1782, Len of Training loss: 36, Average loss: 1.3354277080959744\n",
            "Len of Validation loss: 128, Average loss: 1.6184480804949999\n",
            "Epoch: 1783, Len of Training loss: 36, Average loss: 1.3304263022210863\n",
            "Len of Validation loss: 128, Average loss: 1.6065384454559535\n",
            "Epoch: 1784, Len of Training loss: 36, Average loss: 1.3075779378414154\n",
            "Len of Validation loss: 128, Average loss: 1.622488911729306\n",
            "Epoch: 1785, Len of Training loss: 36, Average loss: 1.2999308738443587\n",
            "Len of Validation loss: 128, Average loss: 1.6341484659351408\n",
            "Epoch: 1786, Len of Training loss: 36, Average loss: 1.2939832988712523\n",
            "Len of Validation loss: 128, Average loss: 1.6407298911362886\n",
            "Epoch: 1787, Len of Training loss: 36, Average loss: 1.3108423799276352\n",
            "Len of Validation loss: 128, Average loss: 1.5977127025835216\n",
            "Epoch: 1788, Len of Training loss: 36, Average loss: 1.3321080174711015\n",
            "Len of Validation loss: 128, Average loss: 1.6568166322540492\n",
            "Epoch: 1789, Len of Training loss: 36, Average loss: 1.3261386884583368\n",
            "Len of Validation loss: 128, Average loss: 1.6579013373702765\n",
            "Epoch: 1790, Len of Training loss: 36, Average loss: 1.3276251090897455\n",
            "Len of Validation loss: 128, Average loss: 1.625585034955293\n",
            "Epoch: 1791, Len of Training loss: 36, Average loss: 1.3183056347899966\n",
            "Len of Validation loss: 128, Average loss: 1.6298397788777947\n",
            "Epoch: 1792, Len of Training loss: 36, Average loss: 1.2948234511746302\n",
            "Len of Validation loss: 128, Average loss: 1.6350427102297544\n",
            "Epoch: 1793, Len of Training loss: 36, Average loss: 1.297436041964425\n",
            "Len of Validation loss: 128, Average loss: 1.6547793988138437\n",
            "Epoch: 1794, Len of Training loss: 36, Average loss: 1.3244975904623668\n",
            "Len of Validation loss: 128, Average loss: 1.6461621755734086\n",
            "Epoch: 1795, Len of Training loss: 36, Average loss: 1.3220678203635745\n",
            "Len of Validation loss: 128, Average loss: 1.652732164831832\n",
            "Epoch: 1796, Len of Training loss: 36, Average loss: 1.3254207322994869\n",
            "Len of Validation loss: 128, Average loss: 1.6098848783876747\n",
            "Epoch: 1797, Len of Training loss: 36, Average loss: 1.300237915582127\n",
            "Len of Validation loss: 128, Average loss: 1.61748424381949\n",
            "Epoch: 1798, Len of Training loss: 36, Average loss: 1.2972102413574855\n",
            "Len of Validation loss: 128, Average loss: 1.6575937964953482\n",
            "Epoch: 1799, Len of Training loss: 36, Average loss: 1.30279364850786\n",
            "Len of Validation loss: 128, Average loss: 1.6368405760731548\n",
            "Epoch: 1800, Len of Training loss: 36, Average loss: 1.2844628890355427\n",
            "Len of Validation loss: 128, Average loss: 1.6236891644075513\n",
            "Epoch: 1801, Len of Training loss: 36, Average loss: 1.2977454629209306\n",
            "Len of Validation loss: 128, Average loss: 1.6692487616091967\n",
            "Epoch: 1802, Len of Training loss: 36, Average loss: 1.296352156334453\n",
            "Len of Validation loss: 128, Average loss: 1.6203252766281366\n",
            "Epoch: 1803, Len of Training loss: 36, Average loss: 1.30957420832581\n",
            "Len of Validation loss: 128, Average loss: 1.659521794412285\n",
            "Epoch: 1804, Len of Training loss: 36, Average loss: 1.2934771163596048\n",
            "Len of Validation loss: 128, Average loss: 1.6442925294395536\n",
            "Epoch: 1805, Len of Training loss: 36, Average loss: 1.2965927206807666\n",
            "Len of Validation loss: 128, Average loss: 1.6381205620709807\n",
            "Epoch: 1806, Len of Training loss: 36, Average loss: 1.3329053752952151\n",
            "Len of Validation loss: 128, Average loss: 1.6643892945721745\n",
            "Epoch: 1807, Len of Training loss: 36, Average loss: 1.3376191092862024\n",
            "Len of Validation loss: 128, Average loss: 1.6426926301792264\n",
            "Epoch: 1808, Len of Training loss: 36, Average loss: 1.2999573697646458\n",
            "Len of Validation loss: 128, Average loss: 1.633803453296423\n",
            "Epoch: 1809, Len of Training loss: 36, Average loss: 1.2897151443693373\n",
            "Len of Validation loss: 128, Average loss: 1.6285206726752222\n",
            "Epoch: 1810, Len of Training loss: 36, Average loss: 1.339705033434762\n",
            "Len of Validation loss: 128, Average loss: 1.6656534001231194\n",
            "Epoch: 1811, Len of Training loss: 36, Average loss: 1.315359479851193\n",
            "Len of Validation loss: 128, Average loss: 1.658478874946013\n",
            "Epoch: 1812, Len of Training loss: 36, Average loss: 1.331026749478446\n",
            "Len of Validation loss: 128, Average loss: 1.685083211865276\n",
            "Epoch: 1813, Len of Training loss: 36, Average loss: 1.329417121079233\n",
            "Len of Validation loss: 128, Average loss: 1.665871772915125\n",
            "Epoch: 1814, Len of Training loss: 36, Average loss: 1.312590758005778\n",
            "Len of Validation loss: 128, Average loss: 1.6022111324127764\n",
            "Epoch: 1815, Len of Training loss: 36, Average loss: 1.2920145127508376\n",
            "Len of Validation loss: 128, Average loss: 1.6329707333352417\n",
            "Epoch: 1816, Len of Training loss: 36, Average loss: 1.2824279632833269\n",
            "Len of Validation loss: 128, Average loss: 1.608927265740931\n",
            "Epoch: 1817, Len of Training loss: 36, Average loss: 1.3056189368168514\n",
            "Len of Validation loss: 128, Average loss: 1.6329077845439315\n",
            "Epoch: 1818, Len of Training loss: 36, Average loss: 1.2867198040088017\n",
            "Len of Validation loss: 128, Average loss: 1.6291272302623838\n",
            "Epoch: 1819, Len of Training loss: 36, Average loss: 1.290930547648006\n",
            "Len of Validation loss: 128, Average loss: 1.6416306449100375\n",
            "Epoch: 1820, Len of Training loss: 36, Average loss: 1.3144016447994444\n",
            "Len of Validation loss: 128, Average loss: 1.649103095754981\n",
            "Epoch: 1821, Len of Training loss: 36, Average loss: 1.3109507345490985\n",
            "Len of Validation loss: 128, Average loss: 1.5813564625568688\n",
            "Epoch: 1822, Len of Training loss: 36, Average loss: 1.3104863862196605\n",
            "Len of Validation loss: 128, Average loss: 1.6440106108784676\n",
            "Epoch: 1823, Len of Training loss: 36, Average loss: 1.3032520330614514\n",
            "Len of Validation loss: 128, Average loss: 1.6295984194148332\n",
            "Epoch: 1824, Len of Training loss: 36, Average loss: 1.274444995654954\n",
            "Len of Validation loss: 128, Average loss: 1.6028676871210337\n",
            "Epoch: 1825, Len of Training loss: 36, Average loss: 1.2824609147177801\n",
            "Len of Validation loss: 128, Average loss: 1.6032429365441203\n",
            "Epoch: 1826, Len of Training loss: 36, Average loss: 1.2892575992478266\n",
            "Len of Validation loss: 128, Average loss: 1.613994061946869\n",
            "Epoch: 1827, Len of Training loss: 36, Average loss: 1.2869574228922527\n",
            "Len of Validation loss: 128, Average loss: 1.5927341471542604\n",
            "Epoch: 1828, Len of Training loss: 36, Average loss: 1.3164284858438704\n",
            "Len of Validation loss: 128, Average loss: 1.6641155576799065\n",
            "Epoch: 1829, Len of Training loss: 36, Average loss: 1.310375624232822\n",
            "Len of Validation loss: 128, Average loss: 1.6749754950869828\n",
            "Epoch: 1830, Len of Training loss: 36, Average loss: 1.299421038892534\n",
            "Len of Validation loss: 128, Average loss: 1.6281644525006413\n",
            "Epoch: 1831, Len of Training loss: 36, Average loss: 1.2981815106338925\n",
            "Len of Validation loss: 128, Average loss: 1.6544029582291842\n",
            "Epoch: 1832, Len of Training loss: 36, Average loss: 1.3442813240819507\n",
            "Len of Validation loss: 128, Average loss: 1.6068126207683235\n",
            "Epoch: 1833, Len of Training loss: 36, Average loss: 1.314189099603229\n",
            "Len of Validation loss: 128, Average loss: 1.6055852347053587\n",
            "Epoch: 1834, Len of Training loss: 36, Average loss: 1.2791142215331395\n",
            "Len of Validation loss: 128, Average loss: 1.6128851766698062\n",
            "Epoch: 1835, Len of Training loss: 36, Average loss: 1.283072559369935\n",
            "Len of Validation loss: 128, Average loss: 1.625919665209949\n",
            "Epoch: 1836, Len of Training loss: 36, Average loss: 1.2960922320683796\n",
            "Len of Validation loss: 128, Average loss: 1.5991116368677467\n",
            "Epoch: 1837, Len of Training loss: 36, Average loss: 1.275710243317816\n",
            "Len of Validation loss: 128, Average loss: 1.609402307542041\n",
            "Epoch: 1838, Len of Training loss: 36, Average loss: 1.2934379627307255\n",
            "Len of Validation loss: 128, Average loss: 1.6773914261721075\n",
            "Epoch: 1839, Len of Training loss: 36, Average loss: 1.3056988848580255\n",
            "Len of Validation loss: 128, Average loss: 1.6474520112387836\n",
            "Epoch: 1840, Len of Training loss: 36, Average loss: 1.2951670338710148\n",
            "Len of Validation loss: 128, Average loss: 1.604563700268045\n",
            "Epoch: 1841, Len of Training loss: 36, Average loss: 1.3131294929318957\n",
            "Len of Validation loss: 128, Average loss: 1.6490007473621517\n",
            "Epoch: 1842, Len of Training loss: 36, Average loss: 1.299238395359781\n",
            "Len of Validation loss: 128, Average loss: 1.6291488457936794\n",
            "Epoch: 1843, Len of Training loss: 36, Average loss: 1.2910113235314686\n",
            "Len of Validation loss: 128, Average loss: 1.5982380332425237\n",
            "Epoch: 1844, Len of Training loss: 36, Average loss: 1.31694861749808\n",
            "Len of Validation loss: 128, Average loss: 1.6483587848488241\n",
            "Epoch: 1845, Len of Training loss: 36, Average loss: 1.2857660452524822\n",
            "Len of Validation loss: 128, Average loss: 1.5907032547984272\n",
            "Epoch: 1846, Len of Training loss: 36, Average loss: 1.2786895086367924\n",
            "Len of Validation loss: 128, Average loss: 1.628767570713535\n",
            "Epoch: 1847, Len of Training loss: 36, Average loss: 1.2798181143071916\n",
            "Len of Validation loss: 128, Average loss: 1.6072972521651536\n",
            "Epoch: 1848, Len of Training loss: 36, Average loss: 1.272391367289755\n",
            "Len of Validation loss: 128, Average loss: 1.6173031183425337\n",
            "Epoch: 1849, Len of Training loss: 36, Average loss: 1.2997793174452252\n",
            "Len of Validation loss: 128, Average loss: 1.623037906596437\n",
            "Epoch: 1850, Len of Training loss: 36, Average loss: 1.3728857139746349\n",
            "Len of Validation loss: 128, Average loss: 1.64088573330082\n",
            "Epoch: 1851, Len of Training loss: 36, Average loss: 1.2822491808070078\n",
            "Len of Validation loss: 128, Average loss: 1.6545657629612833\n",
            "Epoch: 1852, Len of Training loss: 36, Average loss: 1.2959914323356416\n",
            "Len of Validation loss: 128, Average loss: 1.5851325113326311\n",
            "Epoch: 1853, Len of Training loss: 36, Average loss: 1.2967526879575517\n",
            "Len of Validation loss: 128, Average loss: 1.6414534386713058\n",
            "Epoch: 1854, Len of Training loss: 36, Average loss: 1.289482714401351\n",
            "Len of Validation loss: 128, Average loss: 1.6265443521551788\n",
            "Epoch: 1855, Len of Training loss: 36, Average loss: 1.280629888176918\n",
            "Len of Validation loss: 128, Average loss: 1.6425411098171026\n",
            "Epoch: 1856, Len of Training loss: 36, Average loss: 1.2839079863495297\n",
            "Len of Validation loss: 128, Average loss: 1.617562699597329\n",
            "Epoch: 1857, Len of Training loss: 36, Average loss: 1.2787811772690878\n",
            "Len of Validation loss: 128, Average loss: 1.5905457148328424\n",
            "Epoch: 1858, Len of Training loss: 36, Average loss: 1.2840423815780215\n",
            "Len of Validation loss: 128, Average loss: 1.5961704815272242\n",
            "Epoch: 1859, Len of Training loss: 36, Average loss: 1.307314587963952\n",
            "Len of Validation loss: 128, Average loss: 1.596647449536249\n",
            "Epoch: 1860, Len of Training loss: 36, Average loss: 1.2947715255949233\n",
            "Len of Validation loss: 128, Average loss: 1.6492534773424268\n",
            "Epoch: 1861, Len of Training loss: 36, Average loss: 1.352364758650462\n",
            "Len of Validation loss: 128, Average loss: 1.7189599985722452\n",
            "Epoch: 1862, Len of Training loss: 36, Average loss: 1.3289295964770846\n",
            "Len of Validation loss: 128, Average loss: 1.596677025547251\n",
            "Epoch: 1863, Len of Training loss: 36, Average loss: 1.3217978941069708\n",
            "Len of Validation loss: 128, Average loss: 1.6291088205762208\n",
            "Epoch: 1864, Len of Training loss: 36, Average loss: 1.2814193136162229\n",
            "Len of Validation loss: 128, Average loss: 1.6079958253540099\n",
            "Epoch: 1865, Len of Training loss: 36, Average loss: 1.277775055832333\n",
            "Len of Validation loss: 128, Average loss: 1.6526694963686168\n",
            "Epoch: 1866, Len of Training loss: 36, Average loss: 1.2796364161703322\n",
            "Len of Validation loss: 128, Average loss: 1.6258291713893414\n",
            "Epoch: 1867, Len of Training loss: 36, Average loss: 1.3081681281328201\n",
            "Len of Validation loss: 128, Average loss: 1.5948079666122794\n",
            "Epoch: 1868, Len of Training loss: 36, Average loss: 1.2698245677683089\n",
            "Len of Validation loss: 128, Average loss: 1.632901327451691\n",
            "Epoch: 1869, Len of Training loss: 36, Average loss: 1.2769896636406581\n",
            "Len of Validation loss: 128, Average loss: 1.624095474369824\n",
            "Epoch: 1870, Len of Training loss: 36, Average loss: 1.2960181501176622\n",
            "Len of Validation loss: 128, Average loss: 1.5848852035123855\n",
            "Epoch: 1871, Len of Training loss: 36, Average loss: 1.273634907272127\n",
            "Len of Validation loss: 128, Average loss: 1.6372358431108296\n",
            "Epoch: 1872, Len of Training loss: 36, Average loss: 1.2576513174507353\n",
            "Len of Validation loss: 128, Average loss: 1.5751279008109123\n",
            "Epoch: 1873, Len of Training loss: 36, Average loss: 1.2672730485598247\n",
            "Len of Validation loss: 128, Average loss: 1.626198377693072\n",
            "Epoch: 1874, Len of Training loss: 36, Average loss: 1.2878484610054228\n",
            "Len of Validation loss: 128, Average loss: 1.600564741063863\n",
            "Epoch: 1875, Len of Training loss: 36, Average loss: 1.2842161920335557\n",
            "Len of Validation loss: 128, Average loss: 1.6359931570477784\n",
            "Epoch: 1876, Len of Training loss: 36, Average loss: 1.258976987666554\n",
            "Len of Validation loss: 128, Average loss: 1.6286233197897673\n",
            "Epoch: 1877, Len of Training loss: 36, Average loss: 1.2913057059049606\n",
            "Len of Validation loss: 128, Average loss: 1.6504456088878214\n",
            "Epoch: 1878, Len of Training loss: 36, Average loss: 1.2703386826647654\n",
            "Len of Validation loss: 128, Average loss: 1.5977126671932638\n",
            "Epoch: 1879, Len of Training loss: 36, Average loss: 1.2863977352778118\n",
            "Len of Validation loss: 128, Average loss: 1.6526685252320021\n",
            "Epoch: 1880, Len of Training loss: 36, Average loss: 1.266311428613133\n",
            "Len of Validation loss: 128, Average loss: 1.6081702860537916\n",
            "Epoch: 1881, Len of Training loss: 36, Average loss: 1.259219679567549\n",
            "Len of Validation loss: 128, Average loss: 1.594851033645682\n",
            "Epoch: 1882, Len of Training loss: 36, Average loss: 1.294681145085229\n",
            "Len of Validation loss: 128, Average loss: 1.6487486632540822\n",
            "Epoch: 1883, Len of Training loss: 36, Average loss: 1.3473748962084453\n",
            "Len of Validation loss: 128, Average loss: 1.6281972583383322\n",
            "Epoch: 1884, Len of Training loss: 36, Average loss: 1.283681399292416\n",
            "Len of Validation loss: 128, Average loss: 1.6583744913805276\n",
            "Epoch: 1885, Len of Training loss: 36, Average loss: 1.2769958012633853\n",
            "Len of Validation loss: 128, Average loss: 1.5980268404819071\n",
            "Epoch: 1886, Len of Training loss: 36, Average loss: 1.2798439082172182\n",
            "Len of Validation loss: 128, Average loss: 1.664081022143364\n",
            "Epoch: 1887, Len of Training loss: 36, Average loss: 1.2738473779625363\n",
            "Len of Validation loss: 128, Average loss: 1.618910663179122\n",
            "Epoch: 1888, Len of Training loss: 36, Average loss: 1.250305293334855\n",
            "Len of Validation loss: 128, Average loss: 1.574279738822952\n",
            "Epoch: 1889, Len of Training loss: 36, Average loss: 1.275304224756029\n",
            "Len of Validation loss: 128, Average loss: 1.6827852632850409\n",
            "Epoch: 1890, Len of Training loss: 36, Average loss: 1.2752696010801527\n",
            "Len of Validation loss: 128, Average loss: 1.6110481824725866\n",
            "Epoch: 1891, Len of Training loss: 36, Average loss: 1.2856457730134327\n",
            "Len of Validation loss: 128, Average loss: 1.6895605868194252\n",
            "Epoch: 1892, Len of Training loss: 36, Average loss: 1.2774514373805788\n",
            "Len of Validation loss: 128, Average loss: 1.5997390553820878\n",
            "Epoch: 1893, Len of Training loss: 36, Average loss: 1.282867858807246\n",
            "Len of Validation loss: 128, Average loss: 1.6705730322282761\n",
            "Epoch: 1894, Len of Training loss: 36, Average loss: 1.2852392908599641\n",
            "Len of Validation loss: 128, Average loss: 1.6387122480664402\n",
            "Epoch: 1895, Len of Training loss: 36, Average loss: 1.2765640020370483\n",
            "Len of Validation loss: 128, Average loss: 1.59476981125772\n",
            "Epoch: 1896, Len of Training loss: 36, Average loss: 1.2870210972097185\n",
            "Len of Validation loss: 128, Average loss: 1.7155504242982715\n",
            "Epoch: 1897, Len of Training loss: 36, Average loss: 1.3113518572515912\n",
            "Len of Validation loss: 128, Average loss: 1.6143322933930904\n",
            "Epoch: 1898, Len of Training loss: 36, Average loss: 1.268817553917567\n",
            "Len of Validation loss: 128, Average loss: 1.6267844177782536\n",
            "Epoch: 1899, Len of Training loss: 36, Average loss: 1.2633367975552876\n",
            "Len of Validation loss: 128, Average loss: 1.6061881030909717\n",
            "Epoch: 1900, Len of Training loss: 36, Average loss: 1.2597832630077999\n",
            "Len of Validation loss: 128, Average loss: 1.5949686474632472\n",
            "Epoch: 1901, Len of Training loss: 36, Average loss: 1.2824725326564577\n",
            "Len of Validation loss: 128, Average loss: 1.6163070499897003\n",
            "Epoch: 1902, Len of Training loss: 36, Average loss: 1.2501063495874405\n",
            "Len of Validation loss: 128, Average loss: 1.609749752562493\n",
            "Epoch: 1903, Len of Training loss: 36, Average loss: 1.2664366629388597\n",
            "Len of Validation loss: 128, Average loss: 1.6312317289412022\n",
            "Epoch: 1904, Len of Training loss: 36, Average loss: 1.2860973940955267\n",
            "Len of Validation loss: 128, Average loss: 1.653323468985036\n",
            "Epoch: 1905, Len of Training loss: 36, Average loss: 1.2715213944514592\n",
            "Len of Validation loss: 128, Average loss: 1.6953609334304929\n",
            "Epoch: 1906, Len of Training loss: 36, Average loss: 1.2861559324794345\n",
            "Len of Validation loss: 128, Average loss: 1.7237163232639432\n",
            "Epoch: 1907, Len of Training loss: 36, Average loss: 1.3056117875708475\n",
            "Len of Validation loss: 128, Average loss: 1.632913127541542\n",
            "Epoch: 1908, Len of Training loss: 36, Average loss: 1.2705797768301434\n",
            "Len of Validation loss: 128, Average loss: 1.6197021342813969\n",
            "Epoch: 1909, Len of Training loss: 36, Average loss: 1.2669173545307584\n",
            "Len of Validation loss: 128, Average loss: 1.626218392048031\n",
            "Epoch: 1910, Len of Training loss: 36, Average loss: 1.3047226170698802\n",
            "Len of Validation loss: 128, Average loss: 1.6674972618930042\n",
            "Epoch: 1911, Len of Training loss: 36, Average loss: 1.2932500176959567\n",
            "Len of Validation loss: 128, Average loss: 1.5914536819327623\n",
            "Epoch: 1912, Len of Training loss: 36, Average loss: 1.2536032994588215\n",
            "Len of Validation loss: 128, Average loss: 1.6237949938513339\n",
            "Epoch: 1913, Len of Training loss: 36, Average loss: 1.2697025289138157\n",
            "Len of Validation loss: 128, Average loss: 1.6414323612116277\n",
            "Epoch: 1914, Len of Training loss: 36, Average loss: 1.2715902924537659\n",
            "Len of Validation loss: 128, Average loss: 1.6283178061712533\n",
            "Epoch: 1915, Len of Training loss: 36, Average loss: 1.2579365869363148\n",
            "Len of Validation loss: 128, Average loss: 1.6059625323396176\n",
            "Epoch: 1916, Len of Training loss: 36, Average loss: 1.2940886633263693\n",
            "Len of Validation loss: 128, Average loss: 1.6622040192596614\n",
            "Epoch: 1917, Len of Training loss: 36, Average loss: 1.284416503376431\n",
            "Len of Validation loss: 128, Average loss: 1.614666435867548\n",
            "Epoch: 1918, Len of Training loss: 36, Average loss: 1.2899871071179707\n",
            "Len of Validation loss: 128, Average loss: 1.6471994202584028\n",
            "Epoch: 1919, Len of Training loss: 36, Average loss: 1.2729675306214228\n",
            "Len of Validation loss: 128, Average loss: 1.5909248359967023\n",
            "Epoch: 1920, Len of Training loss: 36, Average loss: 1.2703605194886525\n",
            "Len of Validation loss: 128, Average loss: 1.5898704114370048\n",
            "Epoch: 1921, Len of Training loss: 36, Average loss: 1.2738736139403448\n",
            "Len of Validation loss: 128, Average loss: 1.6279480257071555\n",
            "Epoch: 1922, Len of Training loss: 36, Average loss: 1.2676003624995549\n",
            "Len of Validation loss: 128, Average loss: 1.613296531373635\n",
            "Epoch: 1923, Len of Training loss: 36, Average loss: 1.2740863263607025\n",
            "Len of Validation loss: 128, Average loss: 1.5804366688244045\n",
            "Epoch: 1924, Len of Training loss: 36, Average loss: 1.2693487852811813\n",
            "Len of Validation loss: 128, Average loss: 1.5999770690687\n",
            "Epoch: 1925, Len of Training loss: 36, Average loss: 1.2753699355655246\n",
            "Len of Validation loss: 128, Average loss: 1.6731814905069768\n",
            "Epoch: 1926, Len of Training loss: 36, Average loss: 1.2576868020825915\n",
            "Len of Validation loss: 128, Average loss: 1.5927329184487462\n",
            "Epoch: 1927, Len of Training loss: 36, Average loss: 1.2556794418228998\n",
            "Len of Validation loss: 128, Average loss: 1.6110976920463145\n",
            "Epoch: 1928, Len of Training loss: 36, Average loss: 1.265113161669837\n",
            "Len of Validation loss: 128, Average loss: 1.6114315511658788\n",
            "Epoch: 1929, Len of Training loss: 36, Average loss: 1.2763436436653137\n",
            "Len of Validation loss: 128, Average loss: 1.6065396975027397\n",
            "Epoch: 1930, Len of Training loss: 36, Average loss: 1.2649127675427332\n",
            "Len of Validation loss: 128, Average loss: 1.6065360547509044\n",
            "Epoch: 1931, Len of Training loss: 36, Average loss: 1.2523974461687937\n",
            "Len of Validation loss: 128, Average loss: 1.6141665081959218\n",
            "Epoch: 1932, Len of Training loss: 36, Average loss: 1.266412552860048\n",
            "Len of Validation loss: 128, Average loss: 1.6227082540281117\n",
            "Epoch: 1933, Len of Training loss: 36, Average loss: 1.252877222167121\n",
            "Len of Validation loss: 128, Average loss: 1.6082434081472456\n",
            "Epoch: 1934, Len of Training loss: 36, Average loss: 1.3006298028760486\n",
            "Len of Validation loss: 128, Average loss: 1.638997482135892\n",
            "Epoch: 1935, Len of Training loss: 36, Average loss: 1.2918710278140173\n",
            "Len of Validation loss: 128, Average loss: 1.579601702047512\n",
            "Epoch: 1936, Len of Training loss: 36, Average loss: 1.2883855601151784\n",
            "Len of Validation loss: 128, Average loss: 1.6028065783903003\n",
            "Epoch: 1937, Len of Training loss: 36, Average loss: 1.267232487599055\n",
            "Len of Validation loss: 128, Average loss: 1.584572005784139\n",
            "Epoch: 1938, Len of Training loss: 36, Average loss: 1.25405556956927\n",
            "Len of Validation loss: 128, Average loss: 1.6265057798009366\n",
            "Epoch: 1939, Len of Training loss: 36, Average loss: 1.2531720995903015\n",
            "Len of Validation loss: 128, Average loss: 1.6261634968686849\n",
            "Epoch: 1940, Len of Training loss: 36, Average loss: 1.2419063283337488\n",
            "Len of Validation loss: 128, Average loss: 1.6468648831360042\n",
            "Epoch: 1941, Len of Training loss: 36, Average loss: 1.3039824747376971\n",
            "Len of Validation loss: 128, Average loss: 1.584905100055039\n",
            "Epoch: 1942, Len of Training loss: 36, Average loss: 1.27958834833569\n",
            "Len of Validation loss: 128, Average loss: 1.6039443514309824\n",
            "Epoch: 1943, Len of Training loss: 36, Average loss: 1.2749293280972376\n",
            "Len of Validation loss: 128, Average loss: 1.6567780817858875\n",
            "Epoch: 1944, Len of Training loss: 36, Average loss: 1.2725682689083948\n",
            "Len of Validation loss: 128, Average loss: 1.6014110031537712\n",
            "Epoch: 1945, Len of Training loss: 36, Average loss: 1.2766221281554964\n",
            "Len of Validation loss: 128, Average loss: 1.5920902530197054\n",
            "Epoch: 1946, Len of Training loss: 36, Average loss: 1.2705890304512448\n",
            "Len of Validation loss: 128, Average loss: 1.5992124089971185\n",
            "Epoch: 1947, Len of Training loss: 36, Average loss: 1.2689468529489305\n",
            "Len of Validation loss: 128, Average loss: 1.6119271670468152\n",
            "Epoch: 1948, Len of Training loss: 36, Average loss: 1.2585593544774585\n",
            "Len of Validation loss: 128, Average loss: 1.588431652635336\n",
            "Epoch: 1949, Len of Training loss: 36, Average loss: 1.2762354794475768\n",
            "Len of Validation loss: 128, Average loss: 1.6000493199098855\n",
            "Epoch: 1950, Len of Training loss: 36, Average loss: 1.2809521754582722\n",
            "Len of Validation loss: 128, Average loss: 1.6519467942416668\n",
            "Epoch: 1951, Len of Training loss: 36, Average loss: 1.296598141392072\n",
            "Len of Validation loss: 128, Average loss: 1.6390091378707439\n",
            "Epoch: 1952, Len of Training loss: 36, Average loss: 1.2712947693136003\n",
            "Len of Validation loss: 128, Average loss: 1.6147149314638227\n",
            "Epoch: 1953, Len of Training loss: 36, Average loss: 1.258465439081192\n",
            "Len of Validation loss: 128, Average loss: 1.6259667158592492\n",
            "Epoch: 1954, Len of Training loss: 36, Average loss: 1.260840783516566\n",
            "Len of Validation loss: 128, Average loss: 1.5982769739348441\n",
            "Epoch: 1955, Len of Training loss: 36, Average loss: 1.2664630793862872\n",
            "Len of Validation loss: 128, Average loss: 1.6074142432771623\n",
            "Epoch: 1956, Len of Training loss: 36, Average loss: 1.2758225815163717\n",
            "Len of Validation loss: 128, Average loss: 1.5756903318688273\n",
            "Epoch: 1957, Len of Training loss: 36, Average loss: 1.246643152501848\n",
            "Len of Validation loss: 128, Average loss: 1.6344262927304953\n",
            "Epoch: 1958, Len of Training loss: 36, Average loss: 1.2669689920213487\n",
            "Len of Validation loss: 128, Average loss: 1.5710364654660225\n",
            "Epoch: 1959, Len of Training loss: 36, Average loss: 1.2413356370396085\n",
            "Len of Validation loss: 128, Average loss: 1.5543590392917395\n",
            "Epoch: 1960, Len of Training loss: 36, Average loss: 1.2388678971264098\n",
            "Len of Validation loss: 128, Average loss: 1.5884758783504367\n",
            "Epoch: 1961, Len of Training loss: 36, Average loss: 1.2250369687875111\n",
            "Len of Validation loss: 128, Average loss: 1.619082291610539\n",
            "Epoch: 1962, Len of Training loss: 36, Average loss: 1.2561056812604268\n",
            "Len of Validation loss: 128, Average loss: 1.5940233836881816\n",
            "Epoch: 1963, Len of Training loss: 36, Average loss: 1.2452553742461734\n",
            "Len of Validation loss: 128, Average loss: 1.6122283264994621\n",
            "Epoch: 1964, Len of Training loss: 36, Average loss: 1.2394306891494327\n",
            "Len of Validation loss: 128, Average loss: 1.5729278451763093\n",
            "Epoch: 1965, Len of Training loss: 36, Average loss: 1.2534820785125096\n",
            "Len of Validation loss: 128, Average loss: 1.6689967019483447\n",
            "Epoch: 1966, Len of Training loss: 36, Average loss: 1.269736647605896\n",
            "Len of Validation loss: 128, Average loss: 1.577896828064695\n",
            "Epoch: 1967, Len of Training loss: 36, Average loss: 1.2398242354393005\n",
            "Len of Validation loss: 128, Average loss: 1.6226863591000438\n",
            "Epoch: 1968, Len of Training loss: 36, Average loss: 1.2476900782850053\n",
            "Len of Validation loss: 128, Average loss: 1.5820224690251052\n",
            "Epoch: 1969, Len of Training loss: 36, Average loss: 1.252191851536433\n",
            "Len of Validation loss: 128, Average loss: 1.6043054212350398\n",
            "Epoch: 1970, Len of Training loss: 36, Average loss: 1.228339057829645\n",
            "Len of Validation loss: 128, Average loss: 1.5910764151485637\n",
            "Epoch: 1971, Len of Training loss: 36, Average loss: 1.2708706160386403\n",
            "Len of Validation loss: 128, Average loss: 1.634873094735667\n",
            "Epoch: 1972, Len of Training loss: 36, Average loss: 1.256339540084203\n",
            "Len of Validation loss: 128, Average loss: 1.5758475319016725\n",
            "Epoch: 1973, Len of Training loss: 36, Average loss: 1.2630352063311472\n",
            "Len of Validation loss: 128, Average loss: 1.6469163466244936\n",
            "Epoch: 1974, Len of Training loss: 36, Average loss: 1.2768967350323994\n",
            "Len of Validation loss: 128, Average loss: 1.6367857465520501\n",
            "Epoch: 1975, Len of Training loss: 36, Average loss: 1.2500203599532445\n",
            "Len of Validation loss: 128, Average loss: 1.6015400616452098\n",
            "Epoch: 1976, Len of Training loss: 36, Average loss: 1.260186140735944\n",
            "Len of Validation loss: 128, Average loss: 1.6126458472572267\n",
            "Epoch: 1977, Len of Training loss: 36, Average loss: 1.2728092074394226\n",
            "Len of Validation loss: 128, Average loss: 1.604389801621437\n",
            "Epoch: 1978, Len of Training loss: 36, Average loss: 1.3054205079873402\n",
            "Len of Validation loss: 128, Average loss: 1.8026378569193184\n",
            "Epoch: 1979, Len of Training loss: 36, Average loss: 1.2980297952890396\n",
            "Len of Validation loss: 128, Average loss: 1.6315468351822346\n",
            "Epoch: 1980, Len of Training loss: 36, Average loss: 1.2562412884500291\n",
            "Len of Validation loss: 128, Average loss: 1.600610125809908\n",
            "Epoch: 1981, Len of Training loss: 36, Average loss: 1.249416212240855\n",
            "Len of Validation loss: 128, Average loss: 1.690917870029807\n",
            "Epoch: 1982, Len of Training loss: 36, Average loss: 1.2884332305855222\n",
            "Len of Validation loss: 128, Average loss: 1.5970890754833817\n",
            "Epoch: 1983, Len of Training loss: 36, Average loss: 1.2614969958861668\n",
            "Len of Validation loss: 128, Average loss: 1.6717272067908198\n",
            "Epoch: 1984, Len of Training loss: 36, Average loss: 1.2993538445896573\n",
            "Len of Validation loss: 128, Average loss: 1.6295940808486193\n",
            "Epoch: 1985, Len of Training loss: 36, Average loss: 1.2951049473550584\n",
            "Len of Validation loss: 128, Average loss: 1.6359899123199284\n",
            "Epoch: 1986, Len of Training loss: 36, Average loss: 1.2969934973451827\n",
            "Len of Validation loss: 128, Average loss: 1.612954752286896\n",
            "Epoch: 1987, Len of Training loss: 36, Average loss: 1.2462763223383162\n",
            "Len of Validation loss: 128, Average loss: 1.5776404398493469\n",
            "Epoch: 1988, Len of Training loss: 36, Average loss: 1.2407979567845662\n",
            "Len of Validation loss: 128, Average loss: 1.6388322974089533\n",
            "Epoch: 1989, Len of Training loss: 36, Average loss: 1.2354347573386297\n",
            "Len of Validation loss: 128, Average loss: 1.5909162557218224\n",
            "Epoch: 1990, Len of Training loss: 36, Average loss: 1.2333257562584348\n",
            "Len of Validation loss: 128, Average loss: 1.6008700928650796\n",
            "Epoch: 1991, Len of Training loss: 36, Average loss: 1.2353809707694583\n",
            "Len of Validation loss: 128, Average loss: 1.5575832643080503\n",
            "Epoch: 1992, Len of Training loss: 36, Average loss: 1.2328455696503322\n",
            "Len of Validation loss: 128, Average loss: 1.6101382030174136\n",
            "Epoch: 1993, Len of Training loss: 36, Average loss: 1.2390580690569348\n",
            "Len of Validation loss: 128, Average loss: 1.5608552587218583\n",
            "Epoch: 1994, Len of Training loss: 36, Average loss: 1.2177453653679953\n",
            "Len of Validation loss: 128, Average loss: 1.6253394407685846\n",
            "Epoch: 1995, Len of Training loss: 36, Average loss: 1.2460176481140985\n",
            "Len of Validation loss: 128, Average loss: 1.6044042685534805\n",
            "Epoch: 1996, Len of Training loss: 36, Average loss: 1.2330056991842058\n",
            "Len of Validation loss: 128, Average loss: 1.606959733646363\n",
            "Epoch: 1997, Len of Training loss: 36, Average loss: 1.233143664068646\n",
            "Len of Validation loss: 128, Average loss: 1.6122612941544503\n",
            "Epoch: 1998, Len of Training loss: 36, Average loss: 1.2295438365803824\n",
            "Len of Validation loss: 128, Average loss: 1.6000612645875663\n",
            "Epoch: 1999, Len of Training loss: 36, Average loss: 1.278291682402293\n",
            "Len of Validation loss: 128, Average loss: 1.61267623398453\n",
            "Epoch: 2000, Len of Training loss: 36, Average loss: 1.235154476430681\n",
            "Len of Validation loss: 128, Average loss: 1.589610319584608\n",
            "Epoch: 2001, Len of Training loss: 36, Average loss: 1.2351493289073308\n",
            "Len of Validation loss: 128, Average loss: 1.6786362575367093\n",
            "Epoch: 2002, Len of Training loss: 36, Average loss: 1.2536828186776903\n",
            "Len of Validation loss: 128, Average loss: 1.583977330243215\n",
            "Epoch: 2003, Len of Training loss: 36, Average loss: 1.2419946905639436\n",
            "Len of Validation loss: 128, Average loss: 1.6164277682546526\n",
            "Epoch: 2004, Len of Training loss: 36, Average loss: 1.2505296700530582\n",
            "Len of Validation loss: 128, Average loss: 1.628673923900351\n",
            "Epoch: 2005, Len of Training loss: 36, Average loss: 1.2472674051920574\n",
            "Len of Validation loss: 128, Average loss: 1.627875777427107\n",
            "Epoch: 2006, Len of Training loss: 36, Average loss: 1.2378214332792494\n",
            "Len of Validation loss: 128, Average loss: 1.5717211819719523\n",
            "Epoch: 2007, Len of Training loss: 36, Average loss: 1.2441648426983092\n",
            "Len of Validation loss: 128, Average loss: 1.593715984839946\n",
            "Epoch: 2008, Len of Training loss: 36, Average loss: 1.2291758573717542\n",
            "Len of Validation loss: 128, Average loss: 1.6073986084666103\n",
            "Epoch: 2009, Len of Training loss: 36, Average loss: 1.244128230545256\n",
            "Len of Validation loss: 128, Average loss: 1.655105959624052\n",
            "Epoch: 2010, Len of Training loss: 36, Average loss: 1.2953333109617233\n",
            "Len of Validation loss: 128, Average loss: 1.6600721408613026\n",
            "Epoch: 2011, Len of Training loss: 36, Average loss: 1.262382146384981\n",
            "Len of Validation loss: 128, Average loss: 1.5715933186002076\n",
            "Epoch: 2012, Len of Training loss: 36, Average loss: 1.258076204193963\n",
            "Len of Validation loss: 128, Average loss: 1.5930387426633388\n",
            "Epoch: 2013, Len of Training loss: 36, Average loss: 1.2234336286783218\n",
            "Len of Validation loss: 128, Average loss: 1.5825360701419413\n",
            "Epoch: 2014, Len of Training loss: 36, Average loss: 1.2343615028593276\n",
            "Len of Validation loss: 128, Average loss: 1.6093825218267739\n",
            "Epoch: 2015, Len of Training loss: 36, Average loss: 1.2548588348759546\n",
            "Len of Validation loss: 128, Average loss: 1.6147507154382765\n",
            "Epoch: 2016, Len of Training loss: 36, Average loss: 1.2483389410707686\n",
            "Len of Validation loss: 128, Average loss: 1.5867150973062962\n",
            "Epoch: 2017, Len of Training loss: 36, Average loss: 1.2347507890727785\n",
            "Len of Validation loss: 128, Average loss: 1.5793888096231967\n",
            "Epoch: 2018, Len of Training loss: 36, Average loss: 1.2511077291435666\n",
            "Len of Validation loss: 128, Average loss: 1.6691743796691298\n",
            "Epoch: 2019, Len of Training loss: 36, Average loss: 1.2738084064589605\n",
            "Len of Validation loss: 128, Average loss: 1.618441208731383\n",
            "Epoch: 2020, Len of Training loss: 36, Average loss: 1.272498173846139\n",
            "Len of Validation loss: 128, Average loss: 1.610373051604256\n",
            "Epoch: 2021, Len of Training loss: 36, Average loss: 1.2576886895630095\n",
            "Len of Validation loss: 128, Average loss: 1.5907767626922578\n",
            "Epoch: 2022, Len of Training loss: 36, Average loss: 1.2321280356910493\n",
            "Len of Validation loss: 128, Average loss: 1.6123923370614648\n",
            "Epoch: 2023, Len of Training loss: 36, Average loss: 1.2575764043463602\n",
            "Len of Validation loss: 128, Average loss: 1.6495152674615383\n",
            "Epoch: 2024, Len of Training loss: 36, Average loss: 1.2616145329342947\n",
            "Len of Validation loss: 128, Average loss: 1.5817406014539301\n",
            "Epoch: 2025, Len of Training loss: 36, Average loss: 1.25075982676612\n",
            "Len of Validation loss: 128, Average loss: 1.6408570669591427\n",
            "Epoch: 2026, Len of Training loss: 36, Average loss: 1.2742110788822174\n",
            "Len of Validation loss: 128, Average loss: 1.6167355589568615\n",
            "Epoch: 2027, Len of Training loss: 36, Average loss: 1.2290324767430623\n",
            "Len of Validation loss: 128, Average loss: 1.607318952679634\n",
            "Epoch: 2028, Len of Training loss: 36, Average loss: 1.2280166844526927\n",
            "Len of Validation loss: 128, Average loss: 1.58270322997123\n",
            "Epoch: 2029, Len of Training loss: 36, Average loss: 1.2507830146286223\n",
            "Len of Validation loss: 128, Average loss: 1.632064059143886\n",
            "Epoch: 2030, Len of Training loss: 36, Average loss: 1.2358879115846422\n",
            "Len of Validation loss: 128, Average loss: 1.612557508284226\n",
            "Epoch: 2031, Len of Training loss: 36, Average loss: 1.2297224319643445\n",
            "Len of Validation loss: 128, Average loss: 1.6421173699200153\n",
            "Epoch: 2032, Len of Training loss: 36, Average loss: 1.2353397144211664\n",
            "Len of Validation loss: 128, Average loss: 1.603593604406342\n",
            "Epoch: 2033, Len of Training loss: 36, Average loss: 1.2245149678654141\n",
            "Len of Validation loss: 128, Average loss: 1.586852343287319\n",
            "Epoch: 2034, Len of Training loss: 36, Average loss: 1.235133558511734\n",
            "Len of Validation loss: 128, Average loss: 1.5937325763516128\n",
            "Epoch: 2035, Len of Training loss: 36, Average loss: 1.23114579419295\n",
            "Len of Validation loss: 128, Average loss: 1.5659189713187516\n",
            "Epoch: 2036, Len of Training loss: 36, Average loss: 1.214641683631473\n",
            "Len of Validation loss: 128, Average loss: 1.603390203556046\n",
            "Epoch: 2037, Len of Training loss: 36, Average loss: 1.2338398396968842\n",
            "Len of Validation loss: 128, Average loss: 1.6302949350792915\n",
            "Epoch: 2038, Len of Training loss: 36, Average loss: 1.2420593582921557\n",
            "Len of Validation loss: 128, Average loss: 1.5836213149596006\n",
            "Epoch: 2039, Len of Training loss: 36, Average loss: 1.2354173329141405\n",
            "Len of Validation loss: 128, Average loss: 1.6022480570245534\n",
            "Epoch: 2040, Len of Training loss: 36, Average loss: 1.248179121149911\n",
            "Len of Validation loss: 128, Average loss: 1.6765243767295033\n",
            "Epoch: 2041, Len of Training loss: 36, Average loss: 1.2711709837118785\n",
            "Len of Validation loss: 128, Average loss: 1.6143964887596667\n",
            "Epoch: 2042, Len of Training loss: 36, Average loss: 1.2437478188011382\n",
            "Len of Validation loss: 128, Average loss: 1.5841699780430645\n",
            "Epoch: 2043, Len of Training loss: 36, Average loss: 1.2368606113725238\n",
            "Len of Validation loss: 128, Average loss: 1.5923758253920823\n",
            "Epoch: 2044, Len of Training loss: 36, Average loss: 1.2442362424400117\n",
            "Len of Validation loss: 128, Average loss: 1.5991090855095536\n",
            "Epoch: 2045, Len of Training loss: 36, Average loss: 1.234254338675075\n",
            "Len of Validation loss: 128, Average loss: 1.560453369282186\n",
            "Epoch: 2046, Len of Training loss: 36, Average loss: 1.25381528503365\n",
            "Len of Validation loss: 128, Average loss: 1.6075524894986302\n",
            "Epoch: 2047, Len of Training loss: 36, Average loss: 1.2367756714423497\n",
            "Len of Validation loss: 128, Average loss: 1.62518255924806\n",
            "Epoch: 2048, Len of Training loss: 36, Average loss: 1.2316941900385752\n",
            "Len of Validation loss: 128, Average loss: 1.56253170222044\n",
            "Epoch: 2049, Len of Training loss: 36, Average loss: 1.2274922513299518\n",
            "Len of Validation loss: 128, Average loss: 1.6343782329931855\n",
            "Epoch: 2050, Len of Training loss: 36, Average loss: 1.2215331461694505\n",
            "Len of Validation loss: 128, Average loss: 1.5895488623064011\n",
            "Epoch: 2051, Len of Training loss: 36, Average loss: 1.2217222336265776\n",
            "Len of Validation loss: 128, Average loss: 1.5913465186022222\n",
            "Epoch: 2052, Len of Training loss: 36, Average loss: 1.2677389548884497\n",
            "Len of Validation loss: 128, Average loss: 1.6288563662674278\n",
            "Epoch: 2053, Len of Training loss: 36, Average loss: 1.3105558587445154\n",
            "Len of Validation loss: 128, Average loss: 1.632151428842917\n",
            "Epoch: 2054, Len of Training loss: 36, Average loss: 1.255814070502917\n",
            "Len of Validation loss: 128, Average loss: 1.6836110996082425\n",
            "Epoch: 2055, Len of Training loss: 36, Average loss: 1.2684502452611923\n",
            "Len of Validation loss: 128, Average loss: 1.611079464200884\n",
            "Epoch: 2056, Len of Training loss: 36, Average loss: 1.223748341202736\n",
            "Len of Validation loss: 128, Average loss: 1.617461730260402\n",
            "Epoch: 2057, Len of Training loss: 36, Average loss: 1.2259099632501602\n",
            "Len of Validation loss: 128, Average loss: 1.6115437801927328\n",
            "Epoch: 2058, Len of Training loss: 36, Average loss: 1.251035996609264\n",
            "Len of Validation loss: 128, Average loss: 1.629209581995383\n",
            "Epoch: 2059, Len of Training loss: 36, Average loss: 1.2540068411164813\n",
            "Len of Validation loss: 128, Average loss: 1.5961394379846752\n",
            "Epoch: 2060, Len of Training loss: 36, Average loss: 1.2462143550316493\n",
            "Len of Validation loss: 128, Average loss: 1.6086999003309757\n",
            "Epoch: 2061, Len of Training loss: 36, Average loss: 1.2337496446238623\n",
            "Len of Validation loss: 128, Average loss: 1.5630488025490195\n",
            "Epoch: 2062, Len of Training loss: 36, Average loss: 1.2475220163663228\n",
            "Len of Validation loss: 128, Average loss: 1.6112962793558836\n",
            "Epoch: 2063, Len of Training loss: 36, Average loss: 1.245956164267328\n",
            "Len of Validation loss: 128, Average loss: 1.6066653614398092\n",
            "Epoch: 2064, Len of Training loss: 36, Average loss: 1.2190980232424207\n",
            "Len of Validation loss: 128, Average loss: 1.6341051887720823\n",
            "Epoch: 2065, Len of Training loss: 36, Average loss: 1.2310995360215504\n",
            "Len of Validation loss: 128, Average loss: 1.6467868783511221\n",
            "Epoch: 2066, Len of Training loss: 36, Average loss: 1.2853946603006787\n",
            "Len of Validation loss: 128, Average loss: 1.5922855488024652\n",
            "Epoch: 2067, Len of Training loss: 36, Average loss: 1.2592251582278147\n",
            "Len of Validation loss: 128, Average loss: 1.6542110154405236\n",
            "Epoch: 2068, Len of Training loss: 36, Average loss: 1.2848804742097855\n",
            "Len of Validation loss: 128, Average loss: 1.6312197563238442\n",
            "Epoch: 2069, Len of Training loss: 36, Average loss: 1.2703816692034404\n",
            "Len of Validation loss: 128, Average loss: 1.6218352797441185\n",
            "Epoch: 2070, Len of Training loss: 36, Average loss: 1.242772940132353\n",
            "Len of Validation loss: 128, Average loss: 1.5554484077729285\n",
            "Epoch: 2071, Len of Training loss: 36, Average loss: 1.2534529351525836\n",
            "Len of Validation loss: 128, Average loss: 1.5995835340581834\n",
            "Epoch: 2072, Len of Training loss: 36, Average loss: 1.2427522639433544\n",
            "Len of Validation loss: 128, Average loss: 1.5841717889998108\n",
            "Epoch: 2073, Len of Training loss: 36, Average loss: 1.217807380689515\n",
            "Len of Validation loss: 128, Average loss: 1.5759098583366722\n",
            "Epoch: 2074, Len of Training loss: 36, Average loss: 1.2154161764515772\n",
            "Len of Validation loss: 128, Average loss: 1.5882626292295754\n",
            "Epoch: 2075, Len of Training loss: 36, Average loss: 1.2351229323281183\n",
            "Len of Validation loss: 128, Average loss: 1.5996883099433035\n",
            "Epoch: 2076, Len of Training loss: 36, Average loss: 1.2351432293653488\n",
            "Len of Validation loss: 128, Average loss: 1.6002561557106674\n",
            "Epoch: 2077, Len of Training loss: 36, Average loss: 1.2165145062737994\n",
            "Len of Validation loss: 128, Average loss: 1.5992865862790495\n",
            "Epoch: 2078, Len of Training loss: 36, Average loss: 1.2086835304896038\n",
            "Len of Validation loss: 128, Average loss: 1.5788785370532423\n",
            "Epoch: 2079, Len of Training loss: 36, Average loss: 1.2207616766293843\n",
            "Len of Validation loss: 128, Average loss: 1.583913613576442\n",
            "Epoch: 2080, Len of Training loss: 36, Average loss: 1.2226936651600733\n",
            "Len of Validation loss: 128, Average loss: 1.6351095859427005\n",
            "Epoch: 2081, Len of Training loss: 36, Average loss: 1.2153024044301775\n",
            "Len of Validation loss: 128, Average loss: 1.5750381252728403\n",
            "Epoch: 2082, Len of Training loss: 36, Average loss: 1.2142167174153857\n",
            "Len of Validation loss: 128, Average loss: 1.588133754907176\n",
            "Epoch: 2083, Len of Training loss: 36, Average loss: 1.2132468620936077\n",
            "Len of Validation loss: 128, Average loss: 1.6236863881349564\n",
            "Epoch: 2084, Len of Training loss: 36, Average loss: 1.2099278022845585\n",
            "Len of Validation loss: 128, Average loss: 1.5981854575220495\n",
            "Epoch: 2085, Len of Training loss: 36, Average loss: 1.2209970901409786\n",
            "Len of Validation loss: 128, Average loss: 1.5779415271244943\n",
            "Epoch: 2086, Len of Training loss: 36, Average loss: 1.2193268338839214\n",
            "Len of Validation loss: 128, Average loss: 1.6179845340084285\n",
            "Epoch: 2087, Len of Training loss: 36, Average loss: 1.2361188696490393\n",
            "Len of Validation loss: 128, Average loss: 1.6183214171323925\n",
            "Epoch: 2088, Len of Training loss: 36, Average loss: 1.2247558120224211\n",
            "Len of Validation loss: 128, Average loss: 1.5918338587507606\n",
            "Epoch: 2089, Len of Training loss: 36, Average loss: 1.2242332928710513\n",
            "Len of Validation loss: 128, Average loss: 1.5892709151376039\n",
            "Epoch: 2090, Len of Training loss: 36, Average loss: 1.2241586695114772\n",
            "Len of Validation loss: 128, Average loss: 1.5656289022881538\n",
            "Epoch: 2091, Len of Training loss: 36, Average loss: 1.2242476277881198\n",
            "Len of Validation loss: 128, Average loss: 1.5935864690691233\n",
            "Epoch: 2092, Len of Training loss: 36, Average loss: 1.2399703330463834\n",
            "Len of Validation loss: 128, Average loss: 1.6247385188471526\n",
            "Epoch: 2093, Len of Training loss: 36, Average loss: 1.2415969752603107\n",
            "Len of Validation loss: 128, Average loss: 1.5964092968497425\n",
            "Epoch: 2094, Len of Training loss: 36, Average loss: 1.2060360345575545\n",
            "Len of Validation loss: 128, Average loss: 1.6023274001199752\n",
            "Epoch: 2095, Len of Training loss: 36, Average loss: 1.2264392839537726\n",
            "Len of Validation loss: 128, Average loss: 1.5778310091700405\n",
            "Epoch: 2096, Len of Training loss: 36, Average loss: 1.2148426638709173\n",
            "Len of Validation loss: 128, Average loss: 1.6012743371538818\n",
            "Epoch: 2097, Len of Training loss: 36, Average loss: 1.2322230786085129\n",
            "Len of Validation loss: 128, Average loss: 1.590596332680434\n",
            "Epoch: 2098, Len of Training loss: 36, Average loss: 1.2095486505164041\n",
            "Len of Validation loss: 128, Average loss: 1.6032783309929073\n",
            "Epoch: 2099, Len of Training loss: 36, Average loss: 1.224994237224261\n",
            "Len of Validation loss: 128, Average loss: 1.5967420982196927\n",
            "Epoch: 2100, Len of Training loss: 36, Average loss: 1.2312822590271633\n",
            "Len of Validation loss: 128, Average loss: 1.6083973064087331\n",
            "Epoch: 2101, Len of Training loss: 36, Average loss: 1.2110681881507237\n",
            "Len of Validation loss: 128, Average loss: 1.6394027266651392\n",
            "Epoch: 2102, Len of Training loss: 36, Average loss: 1.2301218410332997\n",
            "Len of Validation loss: 128, Average loss: 1.5783556480892003\n",
            "Epoch: 2103, Len of Training loss: 36, Average loss: 1.2275312940279643\n",
            "Len of Validation loss: 128, Average loss: 1.58502624835819\n",
            "Epoch: 2104, Len of Training loss: 36, Average loss: 1.21685248778926\n",
            "Len of Validation loss: 128, Average loss: 1.5840749577619135\n",
            "Epoch: 2105, Len of Training loss: 36, Average loss: 1.2409648166762457\n",
            "Len of Validation loss: 128, Average loss: 1.5931328169535846\n",
            "Epoch: 2106, Len of Training loss: 36, Average loss: 1.2296688540114298\n",
            "Len of Validation loss: 128, Average loss: 1.601901166839525\n",
            "Epoch: 2107, Len of Training loss: 36, Average loss: 1.2160503930515714\n",
            "Len of Validation loss: 128, Average loss: 1.5874219951219857\n",
            "Epoch: 2108, Len of Training loss: 36, Average loss: 1.218234149946107\n",
            "Len of Validation loss: 128, Average loss: 1.5781576274894178\n",
            "Epoch: 2109, Len of Training loss: 36, Average loss: 1.2048994368977017\n",
            "Len of Validation loss: 128, Average loss: 1.5543209598399699\n",
            "Epoch: 2110, Len of Training loss: 36, Average loss: 1.1995244953367445\n",
            "Len of Validation loss: 128, Average loss: 1.5809810685459524\n",
            "Epoch: 2111, Len of Training loss: 36, Average loss: 1.2051499055491552\n",
            "Len of Validation loss: 128, Average loss: 1.5730938944034278\n",
            "Epoch: 2112, Len of Training loss: 36, Average loss: 1.1993160562382803\n",
            "Len of Validation loss: 128, Average loss: 1.5960405482910573\n",
            "Epoch: 2113, Len of Training loss: 36, Average loss: 1.2024364819129307\n",
            "Len of Validation loss: 128, Average loss: 1.6366818596143275\n",
            "Epoch: 2114, Len of Training loss: 36, Average loss: 1.2977329062090979\n",
            "Len of Validation loss: 128, Average loss: 1.58647080976516\n",
            "Epoch: 2115, Len of Training loss: 36, Average loss: 1.278085975183381\n",
            "Len of Validation loss: 128, Average loss: 1.6394779479596764\n",
            "Epoch: 2116, Len of Training loss: 36, Average loss: 1.257326387696796\n",
            "Len of Validation loss: 128, Average loss: 1.6058451626449823\n",
            "Epoch: 2117, Len of Training loss: 36, Average loss: 1.2193274978134367\n",
            "Len of Validation loss: 128, Average loss: 1.5778972504194826\n",
            "Epoch: 2118, Len of Training loss: 36, Average loss: 1.2007429897785187\n",
            "Len of Validation loss: 128, Average loss: 1.5689761766698211\n",
            "Epoch: 2119, Len of Training loss: 36, Average loss: 1.205627014239629\n",
            "Len of Validation loss: 128, Average loss: 1.555572162149474\n",
            "Epoch: 2120, Len of Training loss: 36, Average loss: 1.2239948411782582\n",
            "Len of Validation loss: 128, Average loss: 1.5892012156546116\n",
            "Epoch: 2121, Len of Training loss: 36, Average loss: 1.2218147565921147\n",
            "Len of Validation loss: 128, Average loss: 1.5892958019394428\n",
            "Epoch: 2122, Len of Training loss: 36, Average loss: 1.1992739389340084\n",
            "Len of Validation loss: 128, Average loss: 1.5893767988309264\n",
            "Epoch: 2123, Len of Training loss: 36, Average loss: 1.214069248901473\n",
            "Len of Validation loss: 128, Average loss: 1.6163665754720569\n",
            "Epoch: 2124, Len of Training loss: 36, Average loss: 1.2117829488383398\n",
            "Len of Validation loss: 128, Average loss: 1.5569964016322047\n",
            "Epoch: 2125, Len of Training loss: 36, Average loss: 1.1971024142371283\n",
            "Len of Validation loss: 128, Average loss: 1.5804983770940453\n",
            "Epoch: 2126, Len of Training loss: 36, Average loss: 1.2083075675699446\n",
            "Len of Validation loss: 128, Average loss: 1.5871704565361142\n",
            "Epoch: 2127, Len of Training loss: 36, Average loss: 1.223549743493398\n",
            "Len of Validation loss: 128, Average loss: 1.6059653300326318\n",
            "Epoch: 2128, Len of Training loss: 36, Average loss: 1.214239279429118\n",
            "Len of Validation loss: 128, Average loss: 1.5645796864992008\n",
            "Epoch: 2129, Len of Training loss: 36, Average loss: 1.2490894049406052\n",
            "Len of Validation loss: 128, Average loss: 1.6565515252295882\n",
            "Epoch: 2130, Len of Training loss: 36, Average loss: 1.2111343757973776\n",
            "Len of Validation loss: 128, Average loss: 1.5945704728364944\n",
            "Epoch: 2131, Len of Training loss: 36, Average loss: 1.2369712144136429\n",
            "Len of Validation loss: 128, Average loss: 1.6101032851729542\n",
            "Epoch: 2132, Len of Training loss: 36, Average loss: 1.2545627024438646\n",
            "Len of Validation loss: 128, Average loss: 1.5880090112332255\n",
            "Epoch: 2133, Len of Training loss: 36, Average loss: 1.2039957361088858\n",
            "Len of Validation loss: 128, Average loss: 1.5551528823561966\n",
            "Epoch: 2134, Len of Training loss: 36, Average loss: 1.2103585965103574\n",
            "Len of Validation loss: 128, Average loss: 1.5711938650347292\n",
            "Epoch: 2135, Len of Training loss: 36, Average loss: 1.203586800230874\n",
            "Len of Validation loss: 128, Average loss: 1.594736892497167\n",
            "Epoch: 2136, Len of Training loss: 36, Average loss: 1.2096915592749913\n",
            "Len of Validation loss: 128, Average loss: 1.5913581440690905\n",
            "Epoch: 2137, Len of Training loss: 36, Average loss: 1.202584233548906\n",
            "Len of Validation loss: 128, Average loss: 1.5808970460202545\n",
            "Epoch: 2138, Len of Training loss: 36, Average loss: 1.2102100219991472\n",
            "Len of Validation loss: 128, Average loss: 1.5873924624174833\n",
            "Epoch: 2139, Len of Training loss: 36, Average loss: 1.1930642161104414\n",
            "Len of Validation loss: 128, Average loss: 1.526695372769609\n",
            "Epoch: 2140, Len of Training loss: 36, Average loss: 1.2049295587672129\n",
            "Len of Validation loss: 128, Average loss: 1.562438384629786\n",
            "Epoch: 2141, Len of Training loss: 36, Average loss: 1.2120950255129073\n",
            "Len of Validation loss: 128, Average loss: 1.5585442276205868\n",
            "Epoch: 2142, Len of Training loss: 36, Average loss: 1.2206341276566188\n",
            "Len of Validation loss: 128, Average loss: 1.6193177998065948\n",
            "Epoch: 2143, Len of Training loss: 36, Average loss: 1.2058834930260975\n",
            "Len of Validation loss: 128, Average loss: 1.5578535601962358\n",
            "Epoch: 2144, Len of Training loss: 36, Average loss: 1.1936607344283\n",
            "Len of Validation loss: 128, Average loss: 1.6084456597454846\n",
            "Epoch: 2145, Len of Training loss: 36, Average loss: 1.198604057232539\n",
            "Len of Validation loss: 128, Average loss: 1.588484926847741\n",
            "Epoch: 2146, Len of Training loss: 36, Average loss: 1.2559400548537571\n",
            "Len of Validation loss: 128, Average loss: 1.5844420855864882\n",
            "Epoch: 2147, Len of Training loss: 36, Average loss: 1.2162177032894559\n",
            "Len of Validation loss: 128, Average loss: 1.5855257641524076\n",
            "Epoch: 2148, Len of Training loss: 36, Average loss: 1.2086194372839398\n",
            "Len of Validation loss: 128, Average loss: 1.619799591600895\n",
            "Epoch: 2149, Len of Training loss: 36, Average loss: 1.2141030497021146\n",
            "Len of Validation loss: 128, Average loss: 1.5740561615675688\n",
            "Epoch: 2150, Len of Training loss: 36, Average loss: 1.2149154014057584\n",
            "Len of Validation loss: 128, Average loss: 1.6146337708923966\n",
            "Epoch: 2151, Len of Training loss: 36, Average loss: 1.2060836404561996\n",
            "Len of Validation loss: 128, Average loss: 1.5910331988707185\n",
            "Epoch: 2152, Len of Training loss: 36, Average loss: 1.210982660452525\n",
            "Len of Validation loss: 128, Average loss: 1.5812028360087425\n",
            "Epoch: 2153, Len of Training loss: 36, Average loss: 1.2105198121733136\n",
            "Len of Validation loss: 128, Average loss: 1.6129727691877633\n",
            "Epoch: 2154, Len of Training loss: 36, Average loss: 1.2369459238317277\n",
            "Len of Validation loss: 128, Average loss: 1.6000038697384298\n",
            "Epoch: 2155, Len of Training loss: 36, Average loss: 1.2276726629998949\n",
            "Len of Validation loss: 128, Average loss: 1.5895000302698463\n",
            "Epoch: 2156, Len of Training loss: 36, Average loss: 1.1904471069574356\n",
            "Len of Validation loss: 128, Average loss: 1.5522285304032266\n",
            "Epoch: 2157, Len of Training loss: 36, Average loss: 1.1835130684905582\n",
            "Len of Validation loss: 128, Average loss: 1.5990559810306877\n",
            "Epoch: 2158, Len of Training loss: 36, Average loss: 1.1999610397550795\n",
            "Len of Validation loss: 128, Average loss: 1.6441711192019284\n",
            "Epoch: 2159, Len of Training loss: 36, Average loss: 1.2402369992600546\n",
            "Len of Validation loss: 128, Average loss: 1.56443661917001\n",
            "Epoch: 2160, Len of Training loss: 36, Average loss: 1.227182490958108\n",
            "Len of Validation loss: 128, Average loss: 1.597095871111378\n",
            "Epoch: 2161, Len of Training loss: 36, Average loss: 1.2166788859499826\n",
            "Len of Validation loss: 128, Average loss: 1.5619054874405265\n",
            "Epoch: 2162, Len of Training loss: 36, Average loss: 1.2041056553522747\n",
            "Len of Validation loss: 128, Average loss: 1.5548019735142589\n",
            "Epoch: 2163, Len of Training loss: 36, Average loss: 1.2089955177572038\n",
            "Len of Validation loss: 128, Average loss: 1.609230680624023\n",
            "Epoch: 2164, Len of Training loss: 36, Average loss: 1.218203021420373\n",
            "Len of Validation loss: 128, Average loss: 1.5786011167801917\n",
            "Epoch: 2165, Len of Training loss: 36, Average loss: 1.2312107417318556\n",
            "Len of Validation loss: 128, Average loss: 1.5742926374077797\n",
            "Epoch: 2166, Len of Training loss: 36, Average loss: 1.2092538442876604\n",
            "Len of Validation loss: 128, Average loss: 1.578065833542496\n",
            "Epoch: 2167, Len of Training loss: 36, Average loss: 1.2037063522471323\n",
            "Len of Validation loss: 128, Average loss: 1.5859452665317804\n",
            "Epoch: 2168, Len of Training loss: 36, Average loss: 1.2233999768892925\n",
            "Len of Validation loss: 128, Average loss: 1.5819699226412922\n",
            "Epoch: 2169, Len of Training loss: 36, Average loss: 1.1986528121762805\n",
            "Len of Validation loss: 128, Average loss: 1.5876907228957862\n",
            "Epoch: 2170, Len of Training loss: 36, Average loss: 1.2098258601294622\n",
            "Len of Validation loss: 128, Average loss: 1.6075296425260603\n",
            "Epoch: 2171, Len of Training loss: 36, Average loss: 1.2126907871829138\n",
            "Len of Validation loss: 128, Average loss: 1.599590570665896\n",
            "Epoch: 2172, Len of Training loss: 36, Average loss: 1.2400706940227084\n",
            "Len of Validation loss: 128, Average loss: 1.5948692890815437\n",
            "Epoch: 2173, Len of Training loss: 36, Average loss: 1.2132045080264409\n",
            "Len of Validation loss: 128, Average loss: 1.5982172244694084\n",
            "Epoch: 2174, Len of Training loss: 36, Average loss: 1.205467876460817\n",
            "Len of Validation loss: 128, Average loss: 1.5949918555561453\n",
            "Epoch: 2175, Len of Training loss: 36, Average loss: 1.2047691941261292\n",
            "Len of Validation loss: 128, Average loss: 1.5914432075805962\n",
            "Epoch: 2176, Len of Training loss: 36, Average loss: 1.2045789857705433\n",
            "Len of Validation loss: 128, Average loss: 1.603852204279974\n",
            "Epoch: 2177, Len of Training loss: 36, Average loss: 1.2096420493390825\n",
            "Len of Validation loss: 128, Average loss: 1.6382028230000287\n",
            "Epoch: 2178, Len of Training loss: 36, Average loss: 1.1973197311162949\n",
            "Len of Validation loss: 128, Average loss: 1.5982826855033636\n",
            "Epoch: 2179, Len of Training loss: 36, Average loss: 1.1932919687694974\n",
            "Len of Validation loss: 128, Average loss: 1.5648413447197527\n",
            "Epoch: 2180, Len of Training loss: 36, Average loss: 1.2048999187019136\n",
            "Len of Validation loss: 128, Average loss: 1.6417068891460076\n",
            "Epoch: 2181, Len of Training loss: 36, Average loss: 1.1987722085581884\n",
            "Len of Validation loss: 128, Average loss: 1.5664051547646523\n",
            "Epoch: 2182, Len of Training loss: 36, Average loss: 1.2048944168620639\n",
            "Len of Validation loss: 128, Average loss: 1.5980883166193962\n",
            "Epoch: 2183, Len of Training loss: 36, Average loss: 1.2096233748727374\n",
            "Len of Validation loss: 128, Average loss: 1.5830576161388308\n",
            "Epoch: 2184, Len of Training loss: 36, Average loss: 1.1866135713126924\n",
            "Len of Validation loss: 128, Average loss: 1.5849805511534214\n",
            "Epoch: 2185, Len of Training loss: 36, Average loss: 1.1939001033703487\n",
            "Len of Validation loss: 128, Average loss: 1.5620874201413244\n",
            "Epoch: 2186, Len of Training loss: 36, Average loss: 1.213965008656184\n",
            "Len of Validation loss: 128, Average loss: 1.5994096845388412\n",
            "Epoch: 2187, Len of Training loss: 36, Average loss: 1.2104036046399012\n",
            "Len of Validation loss: 128, Average loss: 1.560219339793548\n",
            "Epoch: 2188, Len of Training loss: 36, Average loss: 1.1922557834121916\n",
            "Len of Validation loss: 128, Average loss: 1.569590677274391\n",
            "Epoch: 2189, Len of Training loss: 36, Average loss: 1.1897196455134287\n",
            "Len of Validation loss: 128, Average loss: 1.569788014050573\n",
            "Epoch: 2190, Len of Training loss: 36, Average loss: 1.1981361607710521\n",
            "Len of Validation loss: 128, Average loss: 1.5614092191681266\n",
            "Epoch: 2191, Len of Training loss: 36, Average loss: 1.2225500411457486\n",
            "Len of Validation loss: 128, Average loss: 1.56066135619767\n",
            "Epoch: 2192, Len of Training loss: 36, Average loss: 1.224454316827986\n",
            "Len of Validation loss: 128, Average loss: 1.6099385633133352\n",
            "Epoch: 2193, Len of Training loss: 36, Average loss: 1.2111662245459027\n",
            "Len of Validation loss: 128, Average loss: 1.6285226175095886\n",
            "Epoch: 2194, Len of Training loss: 36, Average loss: 1.2340885665681627\n",
            "Len of Validation loss: 128, Average loss: 1.6393021133262664\n",
            "Epoch: 2195, Len of Training loss: 36, Average loss: 1.219674888584349\n",
            "Len of Validation loss: 128, Average loss: 1.5833586929365993\n",
            "Epoch: 2196, Len of Training loss: 36, Average loss: 1.2347118904193242\n",
            "Len of Validation loss: 128, Average loss: 1.5807198290713131\n",
            "Epoch: 2197, Len of Training loss: 36, Average loss: 1.2129880040884018\n",
            "Len of Validation loss: 128, Average loss: 1.5369822662323713\n",
            "Epoch: 2198, Len of Training loss: 36, Average loss: 1.2027553502056334\n",
            "Len of Validation loss: 128, Average loss: 1.5838710784446448\n",
            "Epoch: 2199, Len of Training loss: 36, Average loss: 1.1843305842743979\n",
            "Len of Validation loss: 128, Average loss: 1.5668017980642617\n",
            "Epoch: 2200, Len of Training loss: 36, Average loss: 1.2104082687033548\n",
            "Len of Validation loss: 128, Average loss: 1.6167103087063879\n",
            "Epoch: 2201, Len of Training loss: 36, Average loss: 1.2180857840511534\n",
            "Len of Validation loss: 128, Average loss: 1.5655303676612675\n",
            "Epoch: 2202, Len of Training loss: 36, Average loss: 1.1901941200097401\n",
            "Len of Validation loss: 128, Average loss: 1.562709063058719\n",
            "Epoch: 2203, Len of Training loss: 36, Average loss: 1.204484224319458\n",
            "Len of Validation loss: 128, Average loss: 1.6087384088896215\n",
            "Epoch: 2204, Len of Training loss: 36, Average loss: 1.2039392789204915\n",
            "Len of Validation loss: 128, Average loss: 1.5703184932935983\n",
            "Epoch: 2205, Len of Training loss: 36, Average loss: 1.2194168070952098\n",
            "Len of Validation loss: 128, Average loss: 1.6074352571740746\n",
            "Epoch: 2206, Len of Training loss: 36, Average loss: 1.2166246672471364\n",
            "Len of Validation loss: 128, Average loss: 1.5795820015482605\n",
            "Epoch: 2207, Len of Training loss: 36, Average loss: 1.1985635194513533\n",
            "Len of Validation loss: 128, Average loss: 1.5437021513935179\n",
            "Epoch: 2208, Len of Training loss: 36, Average loss: 1.2101749330759048\n",
            "Len of Validation loss: 128, Average loss: 1.5844066207064316\n",
            "Epoch: 2209, Len of Training loss: 36, Average loss: 1.1932075834936566\n",
            "Len of Validation loss: 128, Average loss: 1.5965287208091468\n",
            "Epoch: 2210, Len of Training loss: 36, Average loss: 1.2227764295207129\n",
            "Len of Validation loss: 128, Average loss: 1.6319864981342107\n",
            "Epoch: 2211, Len of Training loss: 36, Average loss: 1.2317432479725943\n",
            "Len of Validation loss: 128, Average loss: 1.6316628642380238\n",
            "Epoch: 2212, Len of Training loss: 36, Average loss: 1.2278474271297455\n",
            "Len of Validation loss: 128, Average loss: 1.6269459219183773\n",
            "Epoch: 2213, Len of Training loss: 36, Average loss: 1.2073948681354523\n",
            "Len of Validation loss: 128, Average loss: 1.5317191602662206\n",
            "Epoch: 2214, Len of Training loss: 36, Average loss: 1.2089973621898227\n",
            "Len of Validation loss: 128, Average loss: 1.5988466644193977\n",
            "Epoch: 2215, Len of Training loss: 36, Average loss: 1.2065816554758284\n",
            "Len of Validation loss: 128, Average loss: 1.5809871854726225\n",
            "Epoch: 2216, Len of Training loss: 36, Average loss: 1.1918790820572112\n",
            "Len of Validation loss: 128, Average loss: 1.55530573008582\n",
            "Epoch: 2217, Len of Training loss: 36, Average loss: 1.218930658366945\n",
            "Len of Validation loss: 128, Average loss: 1.569731176365167\n",
            "Epoch: 2218, Len of Training loss: 36, Average loss: 1.197958724366294\n",
            "Len of Validation loss: 128, Average loss: 1.5812003402970731\n",
            "Epoch: 2219, Len of Training loss: 36, Average loss: 1.1992472542656794\n",
            "Len of Validation loss: 128, Average loss: 1.6140906536020339\n",
            "Epoch: 2220, Len of Training loss: 36, Average loss: 1.2515241321590211\n",
            "Len of Validation loss: 128, Average loss: 1.6944880306255072\n",
            "Epoch: 2221, Len of Training loss: 36, Average loss: 1.2794843349191878\n",
            "Len of Validation loss: 128, Average loss: 1.614369843620807\n",
            "Epoch: 2222, Len of Training loss: 36, Average loss: 1.2019548217455547\n",
            "Len of Validation loss: 128, Average loss: 1.5335816014558077\n",
            "Epoch: 2223, Len of Training loss: 36, Average loss: 1.1932989209890366\n",
            "Len of Validation loss: 128, Average loss: 1.5782436709851027\n",
            "Epoch: 2224, Len of Training loss: 36, Average loss: 1.1864419662290149\n",
            "Len of Validation loss: 128, Average loss: 1.5904744043946266\n",
            "Epoch: 2225, Len of Training loss: 36, Average loss: 1.1924260606368382\n",
            "Len of Validation loss: 128, Average loss: 1.5632102561648935\n",
            "Epoch: 2226, Len of Training loss: 36, Average loss: 1.1976492702960968\n",
            "Len of Validation loss: 128, Average loss: 1.5582724804989994\n",
            "Epoch: 2227, Len of Training loss: 36, Average loss: 1.188232711619801\n",
            "Len of Validation loss: 128, Average loss: 1.5771948515903205\n",
            "Epoch: 2228, Len of Training loss: 36, Average loss: 1.1945806857612398\n",
            "Len of Validation loss: 128, Average loss: 1.6325179890263826\n",
            "Epoch: 2229, Len of Training loss: 36, Average loss: 1.1914871103233762\n",
            "Len of Validation loss: 128, Average loss: 1.5644359304569662\n",
            "Epoch: 2230, Len of Training loss: 36, Average loss: 1.1900484098328485\n",
            "Len of Validation loss: 128, Average loss: 1.5890662977471948\n",
            "Epoch: 2231, Len of Training loss: 36, Average loss: 1.2196280674801931\n",
            "Len of Validation loss: 128, Average loss: 1.5985926161520183\n",
            "Epoch: 2232, Len of Training loss: 36, Average loss: 1.192110988828871\n",
            "Len of Validation loss: 128, Average loss: 1.5836504688486457\n",
            "Epoch: 2233, Len of Training loss: 36, Average loss: 1.2091591623094347\n",
            "Len of Validation loss: 128, Average loss: 1.5757699515670538\n",
            "Epoch: 2234, Len of Training loss: 36, Average loss: 1.2596047288841672\n",
            "Len of Validation loss: 128, Average loss: 1.5671205358812585\n",
            "Epoch: 2235, Len of Training loss: 36, Average loss: 1.2029427372746997\n",
            "Len of Validation loss: 128, Average loss: 1.5636827391572297\n",
            "Epoch: 2236, Len of Training loss: 36, Average loss: 1.1773938437302907\n",
            "Len of Validation loss: 128, Average loss: 1.5662391639780253\n",
            "Epoch: 2237, Len of Training loss: 36, Average loss: 1.1965871817535825\n",
            "Len of Validation loss: 128, Average loss: 1.5736460846383125\n",
            "Epoch: 2238, Len of Training loss: 36, Average loss: 1.19703319006496\n",
            "Len of Validation loss: 128, Average loss: 1.5709134032949805\n",
            "Epoch: 2239, Len of Training loss: 36, Average loss: 1.1973549988534715\n",
            "Len of Validation loss: 128, Average loss: 1.5597972853574902\n",
            "Epoch: 2240, Len of Training loss: 36, Average loss: 1.1873018311129675\n",
            "Len of Validation loss: 128, Average loss: 1.550161928171292\n",
            "Epoch: 2241, Len of Training loss: 36, Average loss: 1.1862400869528453\n",
            "Len of Validation loss: 128, Average loss: 1.5821846569888294\n",
            "Epoch: 2242, Len of Training loss: 36, Average loss: 1.1727182898256514\n",
            "Len of Validation loss: 128, Average loss: 1.5872865605633706\n",
            "Epoch: 2243, Len of Training loss: 36, Average loss: 1.1676221903827455\n",
            "Len of Validation loss: 128, Average loss: 1.57904372503981\n",
            "Epoch: 2244, Len of Training loss: 36, Average loss: 1.1787989454136953\n",
            "Len of Validation loss: 128, Average loss: 1.5835003305692226\n",
            "Epoch: 2245, Len of Training loss: 36, Average loss: 1.1960372593667772\n",
            "Len of Validation loss: 128, Average loss: 1.5661780743394047\n",
            "Epoch: 2246, Len of Training loss: 36, Average loss: 1.18995068470637\n",
            "Len of Validation loss: 128, Average loss: 1.5731035554781556\n",
            "Epoch: 2247, Len of Training loss: 36, Average loss: 1.2042562829123602\n",
            "Len of Validation loss: 128, Average loss: 1.6005628635175526\n",
            "Epoch: 2248, Len of Training loss: 36, Average loss: 1.1930617011255689\n",
            "Len of Validation loss: 128, Average loss: 1.5772786582820117\n",
            "Epoch: 2249, Len of Training loss: 36, Average loss: 1.1952717800935109\n",
            "Len of Validation loss: 128, Average loss: 1.5495685283094645\n",
            "Epoch: 2250, Len of Training loss: 36, Average loss: 1.2177312622467678\n",
            "Len of Validation loss: 128, Average loss: 1.583362122066319\n",
            "Epoch: 2251, Len of Training loss: 36, Average loss: 1.2016321967045467\n",
            "Len of Validation loss: 128, Average loss: 1.5694860480725765\n",
            "Epoch: 2252, Len of Training loss: 36, Average loss: 1.1769357836908765\n",
            "Len of Validation loss: 128, Average loss: 1.5963488805573434\n",
            "Epoch: 2253, Len of Training loss: 36, Average loss: 1.190257238017188\n",
            "Len of Validation loss: 128, Average loss: 1.6143223433755338\n",
            "Epoch: 2254, Len of Training loss: 36, Average loss: 1.2283473345968459\n",
            "Len of Validation loss: 128, Average loss: 1.5908787483349442\n",
            "Epoch: 2255, Len of Training loss: 36, Average loss: 1.2262599700027041\n",
            "Len of Validation loss: 128, Average loss: 1.5975777208805084\n",
            "Epoch: 2256, Len of Training loss: 36, Average loss: 1.1773245731989543\n",
            "Len of Validation loss: 128, Average loss: 1.5891683958470821\n",
            "Epoch: 2257, Len of Training loss: 36, Average loss: 1.1834477368328307\n",
            "Len of Validation loss: 128, Average loss: 1.5705690197646618\n",
            "Epoch: 2258, Len of Training loss: 36, Average loss: 1.1960493508312438\n",
            "Len of Validation loss: 128, Average loss: 1.606830045580864\n",
            "Epoch: 2259, Len of Training loss: 36, Average loss: 1.1770733992258708\n",
            "Len of Validation loss: 128, Average loss: 1.5718701413134113\n",
            "Epoch: 2260, Len of Training loss: 36, Average loss: 1.1933082971307967\n",
            "Len of Validation loss: 128, Average loss: 1.5697968103922904\n",
            "Epoch: 2261, Len of Training loss: 36, Average loss: 1.2067978845702276\n",
            "Len of Validation loss: 128, Average loss: 1.5839869866613299\n",
            "Epoch: 2262, Len of Training loss: 36, Average loss: 1.1999719821744494\n",
            "Len of Validation loss: 128, Average loss: 1.597403499763459\n",
            "Epoch: 2263, Len of Training loss: 36, Average loss: 1.1807422604825761\n",
            "Len of Validation loss: 128, Average loss: 1.5787650186102837\n",
            "Epoch: 2264, Len of Training loss: 36, Average loss: 1.2050316482782364\n",
            "Len of Validation loss: 128, Average loss: 1.6081286643166095\n",
            "Epoch: 2265, Len of Training loss: 36, Average loss: 1.174691958559884\n",
            "Len of Validation loss: 128, Average loss: 1.578008385375142\n",
            "Epoch: 2266, Len of Training loss: 36, Average loss: 1.1831396950615778\n",
            "Len of Validation loss: 128, Average loss: 1.5646294625476003\n",
            "Epoch: 2267, Len of Training loss: 36, Average loss: 1.1983668290906482\n",
            "Len of Validation loss: 128, Average loss: 1.549653112422675\n",
            "Epoch: 2268, Len of Training loss: 36, Average loss: 1.173281204369333\n",
            "Len of Validation loss: 128, Average loss: 1.5901722668204457\n",
            "Epoch: 2269, Len of Training loss: 36, Average loss: 1.1889749500486586\n",
            "Len of Validation loss: 128, Average loss: 1.5928412484936416\n",
            "Epoch: 2270, Len of Training loss: 36, Average loss: 1.184577465057373\n",
            "Len of Validation loss: 128, Average loss: 1.5753419802058488\n",
            "Epoch: 2271, Len of Training loss: 36, Average loss: 1.1707510501146317\n",
            "Len of Validation loss: 128, Average loss: 1.5425963890156709\n",
            "Epoch: 2272, Len of Training loss: 36, Average loss: 1.167587669359313\n",
            "Len of Validation loss: 128, Average loss: 1.5617529072333127\n",
            "Epoch: 2273, Len of Training loss: 36, Average loss: 1.166534647345543\n",
            "Len of Validation loss: 128, Average loss: 1.564004399231635\n",
            "Epoch: 2274, Len of Training loss: 36, Average loss: 1.2002610961596172\n",
            "Len of Validation loss: 128, Average loss: 1.581544353859499\n",
            "Epoch: 2275, Len of Training loss: 36, Average loss: 1.1857122017277613\n",
            "Len of Validation loss: 128, Average loss: 1.5947018761653453\n",
            "Epoch: 2276, Len of Training loss: 36, Average loss: 1.209595787856314\n",
            "Len of Validation loss: 128, Average loss: 1.6218495753128082\n",
            "Epoch: 2277, Len of Training loss: 36, Average loss: 1.185294336742825\n",
            "Len of Validation loss: 128, Average loss: 1.5916983345523477\n",
            "Epoch: 2278, Len of Training loss: 36, Average loss: 1.206282330883874\n",
            "Len of Validation loss: 128, Average loss: 1.562332937028259\n",
            "Epoch: 2279, Len of Training loss: 36, Average loss: 1.1950735482904646\n",
            "Len of Validation loss: 128, Average loss: 1.6119786121416837\n",
            "Epoch: 2280, Len of Training loss: 36, Average loss: 1.186890896823671\n",
            "Len of Validation loss: 128, Average loss: 1.5806333189830184\n",
            "Epoch: 2281, Len of Training loss: 36, Average loss: 1.1859415339099035\n",
            "Len of Validation loss: 128, Average loss: 1.5876397294923663\n",
            "Epoch: 2282, Len of Training loss: 36, Average loss: 1.1922921025090747\n",
            "Len of Validation loss: 128, Average loss: 1.5861742594279349\n",
            "Epoch: 2283, Len of Training loss: 36, Average loss: 1.1871310240692563\n",
            "Len of Validation loss: 128, Average loss: 1.5473901950754225\n",
            "Epoch: 2284, Len of Training loss: 36, Average loss: 1.176982303460439\n",
            "Len of Validation loss: 128, Average loss: 1.555190576473251\n",
            "Epoch: 2285, Len of Training loss: 36, Average loss: 1.1831880741649203\n",
            "Len of Validation loss: 128, Average loss: 1.5644695220980793\n",
            "Epoch: 2286, Len of Training loss: 36, Average loss: 1.194798658291499\n",
            "Len of Validation loss: 128, Average loss: 1.5715866568498313\n",
            "Epoch: 2287, Len of Training loss: 36, Average loss: 1.2094756066799164\n",
            "Len of Validation loss: 128, Average loss: 1.5819390094839036\n",
            "Epoch: 2288, Len of Training loss: 36, Average loss: 1.18710239066018\n",
            "Len of Validation loss: 128, Average loss: 1.58241683174856\n",
            "Epoch: 2289, Len of Training loss: 36, Average loss: 1.164703420466847\n",
            "Len of Validation loss: 128, Average loss: 1.5450003066798672\n",
            "Epoch: 2290, Len of Training loss: 36, Average loss: 1.169518169429567\n",
            "Len of Validation loss: 128, Average loss: 1.5696881283074617\n",
            "Epoch: 2291, Len of Training loss: 36, Average loss: 1.207845292157597\n",
            "Len of Validation loss: 128, Average loss: 1.5569406198337674\n",
            "Epoch: 2292, Len of Training loss: 36, Average loss: 1.2277617769108877\n",
            "Len of Validation loss: 128, Average loss: 1.573986246716231\n",
            "Epoch: 2293, Len of Training loss: 36, Average loss: 1.1899239702357187\n",
            "Len of Validation loss: 128, Average loss: 1.55884425714612\n",
            "Epoch: 2294, Len of Training loss: 36, Average loss: 1.1755234830909305\n",
            "Len of Validation loss: 128, Average loss: 1.584982855245471\n",
            "Epoch: 2295, Len of Training loss: 36, Average loss: 1.1849313096867666\n",
            "Len of Validation loss: 128, Average loss: 1.5516180160921067\n",
            "Epoch: 2296, Len of Training loss: 36, Average loss: 1.2081309656302135\n",
            "Len of Validation loss: 128, Average loss: 1.5664466412272304\n",
            "Epoch: 2297, Len of Training loss: 36, Average loss: 1.174243892232577\n",
            "Len of Validation loss: 128, Average loss: 1.5506298274267465\n",
            "Epoch: 2298, Len of Training loss: 36, Average loss: 1.173624841703309\n",
            "Len of Validation loss: 128, Average loss: 1.5370308861602098\n",
            "Epoch: 2299, Len of Training loss: 36, Average loss: 1.1922157125340567\n",
            "Len of Validation loss: 128, Average loss: 1.6239623865112662\n",
            "Epoch: 2300, Len of Training loss: 36, Average loss: 1.2055475016434987\n",
            "Len of Validation loss: 128, Average loss: 1.5512458765879273\n",
            "Epoch: 2301, Len of Training loss: 36, Average loss: 1.1847613503535588\n",
            "Len of Validation loss: 128, Average loss: 1.5606444273144007\n",
            "Epoch: 2302, Len of Training loss: 36, Average loss: 1.188430713282691\n",
            "Len of Validation loss: 128, Average loss: 1.5472377599216998\n",
            "Epoch: 2303, Len of Training loss: 36, Average loss: 1.199754332502683\n",
            "Len of Validation loss: 128, Average loss: 1.56334736244753\n",
            "Epoch: 2304, Len of Training loss: 36, Average loss: 1.1661560071839228\n",
            "Len of Validation loss: 128, Average loss: 1.5820152398664504\n",
            "Epoch: 2305, Len of Training loss: 36, Average loss: 1.1755395382642746\n",
            "Len of Validation loss: 128, Average loss: 1.563216476002708\n",
            "Epoch: 2306, Len of Training loss: 36, Average loss: 1.171567709909545\n",
            "Len of Validation loss: 128, Average loss: 1.5758279506117105\n",
            "Epoch: 2307, Len of Training loss: 36, Average loss: 1.1888174696101084\n",
            "Len of Validation loss: 128, Average loss: 1.5498107834719121\n",
            "Epoch: 2308, Len of Training loss: 36, Average loss: 1.1577070107062657\n",
            "Len of Validation loss: 128, Average loss: 1.5925594095606357\n",
            "Epoch: 2309, Len of Training loss: 36, Average loss: 1.1848356508546405\n",
            "Len of Validation loss: 128, Average loss: 1.5730377056170255\n",
            "Epoch: 2310, Len of Training loss: 36, Average loss: 1.17868396308687\n",
            "Len of Validation loss: 128, Average loss: 1.5871350176166743\n",
            "Epoch: 2311, Len of Training loss: 36, Average loss: 1.1784154971440632\n",
            "Len of Validation loss: 128, Average loss: 1.5752326995134354\n",
            "Epoch: 2312, Len of Training loss: 36, Average loss: 1.1724954264031515\n",
            "Len of Validation loss: 128, Average loss: 1.5650804832112044\n",
            "Epoch: 2313, Len of Training loss: 36, Average loss: 1.169920351770189\n",
            "Len of Validation loss: 128, Average loss: 1.5766371784266084\n",
            "Epoch: 2314, Len of Training loss: 36, Average loss: 1.1817626141839557\n",
            "Len of Validation loss: 128, Average loss: 1.5794914653524756\n",
            "Epoch: 2315, Len of Training loss: 36, Average loss: 1.1821571009026632\n",
            "Len of Validation loss: 128, Average loss: 1.5536111232358962\n",
            "Epoch: 2316, Len of Training loss: 36, Average loss: 1.1765778793228998\n",
            "Len of Validation loss: 128, Average loss: 1.5807331826072186\n",
            "Epoch: 2317, Len of Training loss: 36, Average loss: 1.1904236757093005\n",
            "Len of Validation loss: 128, Average loss: 1.563405700493604\n",
            "Epoch: 2318, Len of Training loss: 36, Average loss: 1.1471581409374874\n",
            "Len of Validation loss: 128, Average loss: 1.5729814250953496\n",
            "Epoch: 2319, Len of Training loss: 36, Average loss: 1.1596288002199597\n",
            "Len of Validation loss: 128, Average loss: 1.5671046674251556\n",
            "Epoch: 2320, Len of Training loss: 36, Average loss: 1.188790026638243\n",
            "Len of Validation loss: 128, Average loss: 1.591706252656877\n",
            "Epoch: 2321, Len of Training loss: 36, Average loss: 1.1981879820426304\n",
            "Len of Validation loss: 128, Average loss: 1.5629191792104393\n",
            "Epoch: 2322, Len of Training loss: 36, Average loss: 1.1730142335096996\n",
            "Len of Validation loss: 128, Average loss: 1.5920638921670616\n",
            "Epoch: 2323, Len of Training loss: 36, Average loss: 1.212018198437161\n",
            "Len of Validation loss: 128, Average loss: 1.558467041933909\n",
            "Epoch: 2324, Len of Training loss: 36, Average loss: 1.1960224707921345\n",
            "Len of Validation loss: 128, Average loss: 1.571044858545065\n",
            "Epoch: 2325, Len of Training loss: 36, Average loss: 1.2180177536275651\n",
            "Len of Validation loss: 128, Average loss: 1.5492601811420172\n",
            "Epoch: 2326, Len of Training loss: 36, Average loss: 1.1647589935196772\n",
            "Len of Validation loss: 128, Average loss: 1.5477759707719088\n",
            "Epoch: 2327, Len of Training loss: 36, Average loss: 1.1760775629017088\n",
            "Len of Validation loss: 128, Average loss: 1.5525205796584487\n",
            "Epoch: 2328, Len of Training loss: 36, Average loss: 1.1846309436692133\n",
            "Len of Validation loss: 128, Average loss: 1.5986330963205546\n",
            "Epoch: 2329, Len of Training loss: 36, Average loss: 1.1918666511774063\n",
            "Len of Validation loss: 128, Average loss: 1.5676710361149162\n",
            "Epoch: 2330, Len of Training loss: 36, Average loss: 1.1808942490153842\n",
            "Len of Validation loss: 128, Average loss: 1.5737489906605333\n",
            "Epoch: 2331, Len of Training loss: 36, Average loss: 1.1849067790640726\n",
            "Len of Validation loss: 128, Average loss: 1.5590802521910518\n",
            "Epoch: 2332, Len of Training loss: 36, Average loss: 1.2162999047173395\n",
            "Len of Validation loss: 128, Average loss: 1.5679284553043544\n",
            "Epoch: 2333, Len of Training loss: 36, Average loss: 1.2127671755022473\n",
            "Len of Validation loss: 128, Average loss: 1.5682382211089134\n",
            "Epoch: 2334, Len of Training loss: 36, Average loss: 1.1998765187131033\n",
            "Len of Validation loss: 128, Average loss: 1.5722679717000574\n",
            "Epoch: 2335, Len of Training loss: 36, Average loss: 1.1708780179421108\n",
            "Len of Validation loss: 128, Average loss: 1.5731222650501877\n",
            "Epoch: 2336, Len of Training loss: 36, Average loss: 1.1770763678683176\n",
            "Len of Validation loss: 128, Average loss: 1.5685430623125285\n",
            "Epoch: 2337, Len of Training loss: 36, Average loss: 1.1899720811181598\n",
            "Len of Validation loss: 128, Average loss: 1.5556809633271769\n",
            "Epoch: 2338, Len of Training loss: 36, Average loss: 1.17035896744993\n",
            "Len of Validation loss: 128, Average loss: 1.5849717452656478\n",
            "Epoch: 2339, Len of Training loss: 36, Average loss: 1.169695806172159\n",
            "Len of Validation loss: 128, Average loss: 1.5382315926253796\n",
            "Epoch: 2340, Len of Training loss: 36, Average loss: 1.1548490325609844\n",
            "Len of Validation loss: 128, Average loss: 1.594053825829178\n",
            "Epoch: 2341, Len of Training loss: 36, Average loss: 1.1733453588353262\n",
            "Len of Validation loss: 128, Average loss: 1.5658635494764894\n",
            "Epoch: 2342, Len of Training loss: 36, Average loss: 1.1920824878745608\n",
            "Len of Validation loss: 128, Average loss: 1.5476468726992607\n",
            "Epoch: 2343, Len of Training loss: 36, Average loss: 1.2029461728201971\n",
            "Len of Validation loss: 128, Average loss: 1.6197006369475275\n",
            "Epoch: 2344, Len of Training loss: 36, Average loss: 1.1795566843615637\n",
            "Len of Validation loss: 128, Average loss: 1.571737326681614\n",
            "Epoch: 2345, Len of Training loss: 36, Average loss: 1.1613054937786527\n",
            "Len of Validation loss: 128, Average loss: 1.5439363464247435\n",
            "Epoch: 2346, Len of Training loss: 36, Average loss: 1.154623183939192\n",
            "Len of Validation loss: 128, Average loss: 1.5627312143333256\n",
            "Epoch: 2347, Len of Training loss: 36, Average loss: 1.1896003882090251\n",
            "Len of Validation loss: 128, Average loss: 1.616526139434427\n",
            "Epoch: 2348, Len of Training loss: 36, Average loss: 1.2100416190094418\n",
            "Len of Validation loss: 128, Average loss: 1.5377927338704467\n",
            "Epoch: 2349, Len of Training loss: 36, Average loss: 1.1709315147664812\n",
            "Len of Validation loss: 128, Average loss: 1.5275573001708835\n",
            "Epoch: 2350, Len of Training loss: 36, Average loss: 1.1695129755470488\n",
            "Len of Validation loss: 128, Average loss: 1.557887259637937\n",
            "Epoch: 2351, Len of Training loss: 36, Average loss: 1.166984733608034\n",
            "Len of Validation loss: 128, Average loss: 1.538789896061644\n",
            "Epoch: 2352, Len of Training loss: 36, Average loss: 1.1824733449353113\n",
            "Len of Validation loss: 128, Average loss: 1.543847964843735\n",
            "Epoch: 2353, Len of Training loss: 36, Average loss: 1.1699785954422421\n",
            "Len of Validation loss: 128, Average loss: 1.5583443907089531\n",
            "Epoch: 2354, Len of Training loss: 36, Average loss: 1.1600445244047377\n",
            "Len of Validation loss: 128, Average loss: 1.5419204285135493\n",
            "Epoch: 2355, Len of Training loss: 36, Average loss: 1.1602833833959367\n",
            "Len of Validation loss: 128, Average loss: 1.5484773011412472\n",
            "Epoch: 2356, Len of Training loss: 36, Average loss: 1.1669899572928746\n",
            "Len of Validation loss: 128, Average loss: 1.5539978716988117\n",
            "Epoch: 2357, Len of Training loss: 36, Average loss: 1.1782544553279877\n",
            "Len of Validation loss: 128, Average loss: 1.5540209715254605\n",
            "Epoch: 2358, Len of Training loss: 36, Average loss: 1.1854624781343672\n",
            "Len of Validation loss: 128, Average loss: 1.54825475695543\n",
            "Epoch: 2359, Len of Training loss: 36, Average loss: 1.1717861195405324\n",
            "Len of Validation loss: 128, Average loss: 1.561448210850358\n",
            "Epoch: 2360, Len of Training loss: 36, Average loss: 1.1738807327217526\n",
            "Len of Validation loss: 128, Average loss: 1.5620329235680401\n",
            "Epoch: 2361, Len of Training loss: 36, Average loss: 1.1596514913770888\n",
            "Len of Validation loss: 128, Average loss: 1.5303862949367613\n",
            "Epoch: 2362, Len of Training loss: 36, Average loss: 1.1640211128526263\n",
            "Len of Validation loss: 128, Average loss: 1.5748855504207313\n",
            "Epoch: 2363, Len of Training loss: 36, Average loss: 1.1703802330626383\n",
            "Len of Validation loss: 128, Average loss: 1.550411397125572\n",
            "Epoch: 2364, Len of Training loss: 36, Average loss: 1.172416627407074\n",
            "Len of Validation loss: 128, Average loss: 1.5451746184844524\n",
            "Epoch: 2365, Len of Training loss: 36, Average loss: 1.1624995867411296\n",
            "Len of Validation loss: 128, Average loss: 1.5526651537511498\n",
            "Epoch: 2366, Len of Training loss: 36, Average loss: 1.1511348618401422\n",
            "Len of Validation loss: 128, Average loss: 1.6006852977443486\n",
            "Epoch: 2367, Len of Training loss: 36, Average loss: 1.1945625709162817\n",
            "Len of Validation loss: 128, Average loss: 1.54991669440642\n",
            "Epoch: 2368, Len of Training loss: 36, Average loss: 1.1651456985208724\n",
            "Len of Validation loss: 128, Average loss: 1.5578077263198793\n",
            "Epoch: 2369, Len of Training loss: 36, Average loss: 1.1537910252809525\n",
            "Len of Validation loss: 128, Average loss: 1.5941637742798775\n",
            "Epoch: 2370, Len of Training loss: 36, Average loss: 1.2060869303014543\n",
            "Len of Validation loss: 128, Average loss: 1.5689584934152663\n",
            "Epoch: 2371, Len of Training loss: 36, Average loss: 1.1592462261517842\n",
            "Len of Validation loss: 128, Average loss: 1.5667977512348443\n",
            "Epoch: 2372, Len of Training loss: 36, Average loss: 1.158757496211264\n",
            "Len of Validation loss: 128, Average loss: 1.5660312727559358\n",
            "Epoch: 2373, Len of Training loss: 36, Average loss: 1.161966363588969\n",
            "Len of Validation loss: 128, Average loss: 1.5616571011487395\n",
            "Epoch: 2374, Len of Training loss: 36, Average loss: 1.1531493531333075\n",
            "Len of Validation loss: 128, Average loss: 1.52796993823722\n",
            "Epoch: 2375, Len of Training loss: 36, Average loss: 1.1686982529030905\n",
            "Len of Validation loss: 128, Average loss: 1.5942848965059966\n",
            "Epoch: 2376, Len of Training loss: 36, Average loss: 1.1921801169713337\n",
            "Len of Validation loss: 128, Average loss: 1.5664444528520107\n",
            "Epoch: 2377, Len of Training loss: 36, Average loss: 1.1791113118330638\n",
            "Len of Validation loss: 128, Average loss: 1.572350969305262\n",
            "Epoch: 2378, Len of Training loss: 36, Average loss: 1.174623938070403\n",
            "Len of Validation loss: 128, Average loss: 1.547276285244152\n",
            "Epoch: 2379, Len of Training loss: 36, Average loss: 1.196026537153456\n",
            "Len of Validation loss: 128, Average loss: 1.5688291350379586\n",
            "Epoch: 2380, Len of Training loss: 36, Average loss: 1.164503049519327\n",
            "Len of Validation loss: 128, Average loss: 1.536825101589784\n",
            "Epoch: 2381, Len of Training loss: 36, Average loss: 1.1617072853777144\n",
            "Len of Validation loss: 128, Average loss: 1.538433437468484\n",
            "Epoch: 2382, Len of Training loss: 36, Average loss: 1.1675140261650085\n",
            "Len of Validation loss: 128, Average loss: 1.5524606830440462\n",
            "Epoch: 2383, Len of Training loss: 36, Average loss: 1.1634206937419043\n",
            "Len of Validation loss: 128, Average loss: 1.5667693198192865\n",
            "Epoch: 2384, Len of Training loss: 36, Average loss: 1.1610215587748423\n",
            "Len of Validation loss: 128, Average loss: 1.5630546587053686\n",
            "Epoch: 2385, Len of Training loss: 36, Average loss: 1.147357036670049\n",
            "Len of Validation loss: 128, Average loss: 1.5297141037881374\n",
            "Epoch: 2386, Len of Training loss: 36, Average loss: 1.159724987215466\n",
            "Len of Validation loss: 128, Average loss: 1.5601156884804368\n",
            "Epoch: 2387, Len of Training loss: 36, Average loss: 1.1667929159270392\n",
            "Len of Validation loss: 128, Average loss: 1.5265820529311895\n",
            "Epoch: 2388, Len of Training loss: 36, Average loss: 1.1771667169200049\n",
            "Len of Validation loss: 128, Average loss: 1.57096201996319\n",
            "Epoch: 2389, Len of Training loss: 36, Average loss: 1.1501154717471864\n",
            "Len of Validation loss: 128, Average loss: 1.5545261728111655\n",
            "Epoch: 2390, Len of Training loss: 36, Average loss: 1.1496590243445501\n",
            "Len of Validation loss: 128, Average loss: 1.5573732575867325\n",
            "Epoch: 2391, Len of Training loss: 36, Average loss: 1.1615613765186734\n",
            "Len of Validation loss: 128, Average loss: 1.54952537920326\n",
            "Epoch: 2392, Len of Training loss: 36, Average loss: 1.1695545613765717\n",
            "Len of Validation loss: 128, Average loss: 1.5548044245224446\n",
            "Epoch: 2393, Len of Training loss: 36, Average loss: 1.1526119593116972\n",
            "Len of Validation loss: 128, Average loss: 1.5816342479083687\n",
            "Epoch: 2394, Len of Training loss: 36, Average loss: 1.1567409021986856\n",
            "Len of Validation loss: 128, Average loss: 1.548008868470788\n",
            "Epoch: 2395, Len of Training loss: 36, Average loss: 1.1521242376830843\n",
            "Len of Validation loss: 128, Average loss: 1.5970985577441752\n",
            "Epoch: 2396, Len of Training loss: 36, Average loss: 1.1606416039996676\n",
            "Len of Validation loss: 128, Average loss: 1.5823256615549326\n",
            "Epoch: 2397, Len of Training loss: 36, Average loss: 1.2169479827086132\n",
            "Len of Validation loss: 128, Average loss: 1.6138327373191714\n",
            "Epoch: 2398, Len of Training loss: 36, Average loss: 1.5923130280441709\n",
            "Len of Validation loss: 128, Average loss: 2.3562961206771433\n",
            "Epoch: 2399, Len of Training loss: 36, Average loss: 1.9203532073232863\n",
            "Len of Validation loss: 128, Average loss: 1.8000043688807636\n",
            "Epoch: 2400, Len of Training loss: 36, Average loss: 1.4358239952060912\n",
            "Len of Validation loss: 128, Average loss: 1.6098253696691245\n",
            "Epoch: 2401, Len of Training loss: 36, Average loss: 1.2765015827284918\n",
            "Len of Validation loss: 128, Average loss: 1.5850143560674042\n",
            "Epoch: 2402, Len of Training loss: 36, Average loss: 1.2236143516169653\n",
            "Len of Validation loss: 128, Average loss: 1.5687803863547742\n",
            "Epoch: 2403, Len of Training loss: 36, Average loss: 1.1824648562404845\n",
            "Len of Validation loss: 128, Average loss: 1.5570209030993283\n",
            "Epoch: 2404, Len of Training loss: 36, Average loss: 1.1780601690212886\n",
            "Len of Validation loss: 128, Average loss: 1.6025806972756982\n",
            "Epoch: 2405, Len of Training loss: 36, Average loss: 1.1564953923225403\n",
            "Len of Validation loss: 128, Average loss: 1.5208547353977337\n",
            "Epoch: 2406, Len of Training loss: 36, Average loss: 1.1656273868348863\n",
            "Len of Validation loss: 128, Average loss: 1.5136750624515116\n",
            "Epoch: 2407, Len of Training loss: 36, Average loss: 1.1556448373529646\n",
            "Len of Validation loss: 128, Average loss: 1.5309869055636227\n",
            "Epoch: 2408, Len of Training loss: 36, Average loss: 1.1497603572077222\n",
            "Len of Validation loss: 128, Average loss: 1.5480265519581735\n",
            "Epoch: 2409, Len of Training loss: 36, Average loss: 1.140238149298562\n",
            "Len of Validation loss: 128, Average loss: 1.5492947234306484\n",
            "Epoch: 2410, Len of Training loss: 36, Average loss: 1.1559759378433228\n",
            "Len of Validation loss: 128, Average loss: 1.5382207606453449\n",
            "Epoch: 2411, Len of Training loss: 36, Average loss: 1.150895216398769\n",
            "Len of Validation loss: 128, Average loss: 1.5845147555228323\n",
            "Epoch: 2412, Len of Training loss: 36, Average loss: 1.1666186849276226\n",
            "Len of Validation loss: 128, Average loss: 1.5371180174406618\n",
            "Epoch: 2413, Len of Training loss: 36, Average loss: 1.1483284085988998\n",
            "Len of Validation loss: 128, Average loss: 1.538241344038397\n",
            "Epoch: 2414, Len of Training loss: 36, Average loss: 1.1420134471522436\n",
            "Len of Validation loss: 128, Average loss: 1.5141923965420574\n",
            "Epoch: 2415, Len of Training loss: 36, Average loss: 1.1453555822372437\n",
            "Len of Validation loss: 128, Average loss: 1.5647145367693156\n",
            "Epoch: 2416, Len of Training loss: 36, Average loss: 1.1480760590897665\n",
            "Len of Validation loss: 128, Average loss: 1.5456319735385478\n",
            "Epoch: 2417, Len of Training loss: 36, Average loss: 1.1568953924708896\n",
            "Len of Validation loss: 128, Average loss: 1.553595582023263\n",
            "Epoch: 2418, Len of Training loss: 36, Average loss: 1.1409451646937265\n",
            "Len of Validation loss: 128, Average loss: 1.5897962474264205\n",
            "Epoch: 2419, Len of Training loss: 36, Average loss: 1.147251723541154\n",
            "Len of Validation loss: 128, Average loss: 1.5720648220740259\n",
            "Epoch: 2420, Len of Training loss: 36, Average loss: 1.1463988439904318\n",
            "Len of Validation loss: 128, Average loss: 1.5559778327587992\n",
            "Epoch: 2421, Len of Training loss: 36, Average loss: 1.1322688923941717\n",
            "Len of Validation loss: 128, Average loss: 1.5276402533054352\n",
            "Epoch: 2422, Len of Training loss: 36, Average loss: 1.1465804229180019\n",
            "Len of Validation loss: 128, Average loss: 1.550354702398181\n",
            "Epoch: 2423, Len of Training loss: 36, Average loss: 1.1452522923549016\n",
            "Len of Validation loss: 128, Average loss: 1.5715701975859702\n",
            "Epoch: 2424, Len of Training loss: 36, Average loss: 1.1431246714459524\n",
            "Len of Validation loss: 128, Average loss: 1.586066479794681\n",
            "Epoch: 2425, Len of Training loss: 36, Average loss: 1.1566480199495952\n",
            "Len of Validation loss: 128, Average loss: 1.541542093968019\n",
            "Epoch: 2426, Len of Training loss: 36, Average loss: 1.1657663666539722\n",
            "Len of Validation loss: 128, Average loss: 1.5803396995179355\n",
            "Epoch: 2427, Len of Training loss: 36, Average loss: 1.1936053418450885\n",
            "Len of Validation loss: 128, Average loss: 1.5271624973975122\n",
            "Epoch: 2428, Len of Training loss: 36, Average loss: 1.1728042662143707\n",
            "Len of Validation loss: 128, Average loss: 1.554955297615379\n",
            "Epoch: 2429, Len of Training loss: 36, Average loss: 1.1551297191116545\n",
            "Len of Validation loss: 128, Average loss: 1.5521798732224852\n",
            "Epoch: 2430, Len of Training loss: 36, Average loss: 1.1404616004890866\n",
            "Len of Validation loss: 128, Average loss: 1.5579959927126765\n",
            "Epoch: 2431, Len of Training loss: 36, Average loss: 1.1603552997112274\n",
            "Len of Validation loss: 128, Average loss: 1.5606187998782843\n",
            "Epoch: 2432, Len of Training loss: 36, Average loss: 1.1639789905813005\n",
            "Len of Validation loss: 128, Average loss: 1.576915688579902\n",
            "Epoch: 2433, Len of Training loss: 36, Average loss: 1.1414339161581464\n",
            "Len of Validation loss: 128, Average loss: 1.6020627575926483\n",
            "Epoch: 2434, Len of Training loss: 36, Average loss: 1.149431382616361\n",
            "Len of Validation loss: 128, Average loss: 1.5616775890812278\n",
            "Epoch: 2435, Len of Training loss: 36, Average loss: 1.1405000752872891\n",
            "Len of Validation loss: 128, Average loss: 1.5377198366913944\n",
            "Epoch: 2436, Len of Training loss: 36, Average loss: 1.136922225356102\n",
            "Len of Validation loss: 128, Average loss: 1.5436385362409055\n",
            "Epoch: 2437, Len of Training loss: 36, Average loss: 1.175944608118799\n",
            "Len of Validation loss: 128, Average loss: 1.5538363058585674\n",
            "Epoch: 2438, Len of Training loss: 36, Average loss: 1.1802526778644986\n",
            "Len of Validation loss: 128, Average loss: 1.5752831068821251\n",
            "Epoch: 2439, Len of Training loss: 36, Average loss: 1.1559250437551074\n",
            "Len of Validation loss: 128, Average loss: 1.5598666807636619\n",
            "Epoch: 2440, Len of Training loss: 36, Average loss: 1.1667579015096028\n",
            "Len of Validation loss: 128, Average loss: 1.5736426564399153\n",
            "Epoch: 2441, Len of Training loss: 36, Average loss: 1.1504378765821457\n",
            "Len of Validation loss: 128, Average loss: 1.548840350471437\n",
            "Epoch: 2442, Len of Training loss: 36, Average loss: 1.139003210597568\n",
            "Len of Validation loss: 128, Average loss: 1.5563546773046255\n",
            "Epoch: 2443, Len of Training loss: 36, Average loss: 1.1475550995932684\n",
            "Len of Validation loss: 128, Average loss: 1.5553362511564046\n",
            "Epoch: 2444, Len of Training loss: 36, Average loss: 1.1544510142670736\n",
            "Len of Validation loss: 128, Average loss: 1.5742650295142084\n",
            "Epoch: 2445, Len of Training loss: 36, Average loss: 1.1411011152797275\n",
            "Len of Validation loss: 128, Average loss: 1.5822208288591355\n",
            "Epoch: 2446, Len of Training loss: 36, Average loss: 1.1398494127723906\n",
            "Len of Validation loss: 128, Average loss: 1.5578613975085318\n",
            "Epoch: 2447, Len of Training loss: 36, Average loss: 1.1537979924016528\n",
            "Len of Validation loss: 128, Average loss: 1.5676478233654052\n",
            "Epoch: 2448, Len of Training loss: 36, Average loss: 1.1511029998461406\n",
            "Len of Validation loss: 128, Average loss: 1.5726609241683036\n",
            "Epoch: 2449, Len of Training loss: 36, Average loss: 1.1642068690723844\n",
            "Len of Validation loss: 128, Average loss: 1.5257880457211286\n",
            "Epoch: 2450, Len of Training loss: 36, Average loss: 1.140171883834733\n",
            "Len of Validation loss: 128, Average loss: 1.5643384563736618\n",
            "Epoch: 2451, Len of Training loss: 36, Average loss: 1.1310416393809848\n",
            "Len of Validation loss: 128, Average loss: 1.5578214346896857\n",
            "Epoch: 2452, Len of Training loss: 36, Average loss: 1.1519344978862338\n",
            "Len of Validation loss: 128, Average loss: 1.5568328481167555\n",
            "Epoch: 2453, Len of Training loss: 36, Average loss: 1.1704665770133336\n",
            "Len of Validation loss: 128, Average loss: 1.5652337343199179\n",
            "Epoch: 2454, Len of Training loss: 36, Average loss: 1.1402683522966173\n",
            "Len of Validation loss: 128, Average loss: 1.5746455758344382\n",
            "Epoch: 2455, Len of Training loss: 36, Average loss: 1.1498443533976872\n",
            "Len of Validation loss: 128, Average loss: 1.553180756047368\n",
            "Epoch: 2456, Len of Training loss: 36, Average loss: 1.1470073494646285\n",
            "Len of Validation loss: 128, Average loss: 1.563896140549332\n",
            "Epoch: 2457, Len of Training loss: 36, Average loss: 1.1411467509137259\n",
            "Len of Validation loss: 128, Average loss: 1.5403982447460294\n",
            "Epoch: 2458, Len of Training loss: 36, Average loss: 1.1904844972822402\n",
            "Len of Validation loss: 128, Average loss: 1.532291506184265\n",
            "Epoch: 2459, Len of Training loss: 36, Average loss: 1.1510868238078222\n",
            "Len of Validation loss: 128, Average loss: 1.5513573624193668\n",
            "Epoch: 2460, Len of Training loss: 36, Average loss: 1.1503082911173503\n",
            "Len of Validation loss: 128, Average loss: 1.5861660458613187\n",
            "Epoch: 2461, Len of Training loss: 36, Average loss: 1.1632548454735014\n",
            "Len of Validation loss: 128, Average loss: 1.5928891068324447\n",
            "Epoch: 2462, Len of Training loss: 36, Average loss: 1.169070960746871\n",
            "Len of Validation loss: 128, Average loss: 1.6034838655032218\n",
            "Epoch: 2463, Len of Training loss: 36, Average loss: 1.1827769213252597\n",
            "Len of Validation loss: 128, Average loss: 1.5742950686253607\n",
            "Epoch: 2464, Len of Training loss: 36, Average loss: 1.1580818047126133\n",
            "Len of Validation loss: 128, Average loss: 1.5419615283608437\n",
            "Epoch: 2465, Len of Training loss: 36, Average loss: 1.149408706360393\n",
            "Len of Validation loss: 128, Average loss: 1.581657834816724\n",
            "Epoch: 2466, Len of Training loss: 36, Average loss: 1.1915961156288783\n",
            "Len of Validation loss: 128, Average loss: 1.7092701983638108\n",
            "Epoch: 2467, Len of Training loss: 36, Average loss: 1.2508219844765134\n",
            "Len of Validation loss: 128, Average loss: 1.558357025962323\n",
            "Epoch: 2468, Len of Training loss: 36, Average loss: 1.17473456925816\n",
            "Len of Validation loss: 128, Average loss: 1.5797660022508353\n",
            "Epoch: 2469, Len of Training loss: 36, Average loss: 1.1393950515323215\n",
            "Len of Validation loss: 128, Average loss: 1.5370756210759282\n",
            "Epoch: 2470, Len of Training loss: 36, Average loss: 1.141262388891644\n",
            "Len of Validation loss: 128, Average loss: 1.5395278451032937\n",
            "Epoch: 2471, Len of Training loss: 36, Average loss: 1.14627096719212\n",
            "Len of Validation loss: 128, Average loss: 1.5997733459807932\n",
            "Epoch: 2472, Len of Training loss: 36, Average loss: 1.163127741879887\n",
            "Len of Validation loss: 128, Average loss: 1.5643752519972622\n",
            "Epoch: 2473, Len of Training loss: 36, Average loss: 1.154344720972909\n",
            "Len of Validation loss: 128, Average loss: 1.5119676247704774\n",
            "Epoch: 2474, Len of Training loss: 36, Average loss: 1.1337439003917906\n",
            "Len of Validation loss: 128, Average loss: 1.5322357239201665\n",
            "Epoch: 2475, Len of Training loss: 36, Average loss: 1.125233416755994\n",
            "Len of Validation loss: 128, Average loss: 1.5515267653390765\n",
            "Epoch: 2476, Len of Training loss: 36, Average loss: 1.1230396694607205\n",
            "Len of Validation loss: 128, Average loss: 1.5753823006525636\n",
            "Epoch: 2477, Len of Training loss: 36, Average loss: 1.1298436290687985\n",
            "Len of Validation loss: 128, Average loss: 1.543041710043326\n",
            "Epoch: 2478, Len of Training loss: 36, Average loss: 1.1238481154044468\n",
            "Len of Validation loss: 128, Average loss: 1.5257995361462235\n",
            "Epoch: 2479, Len of Training loss: 36, Average loss: 1.127104287346204\n",
            "Len of Validation loss: 128, Average loss: 1.5414239617530257\n",
            "Epoch: 2480, Len of Training loss: 36, Average loss: 1.1418433917893305\n",
            "Len of Validation loss: 128, Average loss: 1.5371591590810567\n",
            "Epoch: 2481, Len of Training loss: 36, Average loss: 1.1425988607936435\n",
            "Len of Validation loss: 128, Average loss: 1.5324949807254598\n",
            "Epoch: 2482, Len of Training loss: 36, Average loss: 1.128927864962154\n",
            "Len of Validation loss: 128, Average loss: 1.5881141680292785\n",
            "Epoch: 2483, Len of Training loss: 36, Average loss: 1.1484469225009282\n",
            "Len of Validation loss: 128, Average loss: 1.5626605725847185\n",
            "Epoch: 2484, Len of Training loss: 36, Average loss: 1.1366653541723888\n",
            "Len of Validation loss: 128, Average loss: 1.5275165075436234\n",
            "Epoch: 2485, Len of Training loss: 36, Average loss: 1.1330594304535124\n",
            "Len of Validation loss: 128, Average loss: 1.5487249908037484\n",
            "Epoch: 2486, Len of Training loss: 36, Average loss: 1.1627524826261733\n",
            "Len of Validation loss: 128, Average loss: 1.5596887711435556\n",
            "Epoch: 2487, Len of Training loss: 36, Average loss: 1.176039598054356\n",
            "Len of Validation loss: 128, Average loss: 1.559895149897784\n",
            "Epoch: 2488, Len of Training loss: 36, Average loss: 1.1900440123346117\n",
            "Len of Validation loss: 128, Average loss: 1.5779347978532314\n",
            "Epoch: 2489, Len of Training loss: 36, Average loss: 1.1536755462487538\n",
            "Len of Validation loss: 128, Average loss: 1.5463590333238244\n",
            "Epoch: 2490, Len of Training loss: 36, Average loss: 1.1572200920846727\n",
            "Len of Validation loss: 128, Average loss: 1.549245411530137\n",
            "Epoch: 2491, Len of Training loss: 36, Average loss: 1.1448004378212824\n",
            "Len of Validation loss: 128, Average loss: 1.560391451464966\n",
            "Epoch: 2492, Len of Training loss: 36, Average loss: 1.161642536520958\n",
            "Len of Validation loss: 128, Average loss: 1.5834050446283072\n",
            "Epoch: 2493, Len of Training loss: 36, Average loss: 1.218863781955507\n",
            "Len of Validation loss: 128, Average loss: 1.561924290144816\n",
            "Epoch: 2494, Len of Training loss: 36, Average loss: 1.1844075636731253\n",
            "Len of Validation loss: 128, Average loss: 1.544733751565218\n",
            "Epoch: 2495, Len of Training loss: 36, Average loss: 1.1319105277458827\n",
            "Len of Validation loss: 128, Average loss: 1.549372207839042\n",
            "Epoch: 2496, Len of Training loss: 36, Average loss: 1.1577546894550323\n",
            "Len of Validation loss: 128, Average loss: 1.5677825356833637\n",
            "Epoch: 2497, Len of Training loss: 36, Average loss: 1.1494248774316576\n",
            "Len of Validation loss: 128, Average loss: 1.6247855143155903\n",
            "Epoch: 2498, Len of Training loss: 36, Average loss: 1.1610090053743787\n",
            "Len of Validation loss: 128, Average loss: 1.544646841706708\n",
            "Epoch: 2499, Len of Training loss: 36, Average loss: 1.1557764874564276\n",
            "Len of Validation loss: 128, Average loss: 1.511049940949306\n",
            "Epoch: 2500, Len of Training loss: 36, Average loss: 1.1418204721477296\n",
            "Len of Validation loss: 128, Average loss: 1.5673322300426662\n",
            "Epoch: 2501, Len of Training loss: 36, Average loss: 1.1771527247296438\n",
            "Len of Validation loss: 128, Average loss: 1.6470389943569899\n",
            "Epoch: 2502, Len of Training loss: 36, Average loss: 1.1965232458379533\n",
            "Len of Validation loss: 128, Average loss: 1.554795709438622\n",
            "Epoch: 2503, Len of Training loss: 36, Average loss: 1.17408600780699\n",
            "Len of Validation loss: 128, Average loss: 1.5737483631819487\n",
            "Epoch: 2504, Len of Training loss: 36, Average loss: 1.149430286553171\n",
            "Len of Validation loss: 128, Average loss: 1.5237981148529798\n",
            "Epoch: 2505, Len of Training loss: 36, Average loss: 1.1254380030764475\n",
            "Len of Validation loss: 128, Average loss: 1.558342435862869\n",
            "Epoch: 2506, Len of Training loss: 36, Average loss: 1.1278474181890488\n",
            "Len of Validation loss: 128, Average loss: 1.5739277247339487\n",
            "Epoch: 2507, Len of Training loss: 36, Average loss: 1.1240910010205374\n",
            "Len of Validation loss: 128, Average loss: 1.5633498970419168\n",
            "Epoch: 2508, Len of Training loss: 36, Average loss: 1.1406329505973392\n",
            "Len of Validation loss: 128, Average loss: 1.6111596398986876\n",
            "Epoch: 2509, Len of Training loss: 36, Average loss: 1.1910365091429815\n",
            "Len of Validation loss: 128, Average loss: 1.5421877424232662\n",
            "Epoch: 2510, Len of Training loss: 36, Average loss: 1.1625099612606897\n",
            "Len of Validation loss: 128, Average loss: 1.5657011815346777\n",
            "Epoch: 2511, Len of Training loss: 36, Average loss: 1.1711081912120183\n",
            "Len of Validation loss: 128, Average loss: 1.542247532401234\n",
            "Epoch: 2512, Len of Training loss: 36, Average loss: 1.1800859438048468\n",
            "Len of Validation loss: 128, Average loss: 1.5249744083266705\n",
            "Epoch: 2513, Len of Training loss: 36, Average loss: 1.1510615299145381\n",
            "Len of Validation loss: 128, Average loss: 1.557245618198067\n",
            "Epoch: 2514, Len of Training loss: 36, Average loss: 1.1217028945684433\n",
            "Len of Validation loss: 128, Average loss: 1.5104473440442234\n",
            "Epoch: 2515, Len of Training loss: 36, Average loss: 1.1228041218386755\n",
            "Len of Validation loss: 128, Average loss: 1.5364916711114347\n",
            "Epoch: 2516, Len of Training loss: 36, Average loss: 1.1429165601730347\n",
            "Len of Validation loss: 128, Average loss: 1.5588799968827516\n",
            "Epoch: 2517, Len of Training loss: 36, Average loss: 1.1329049815734227\n",
            "Len of Validation loss: 128, Average loss: 1.5502851977944374\n",
            "Epoch: 2518, Len of Training loss: 36, Average loss: 1.1523687806394365\n",
            "Len of Validation loss: 128, Average loss: 1.5576300292741507\n",
            "Epoch: 2519, Len of Training loss: 36, Average loss: 1.1549491087595622\n",
            "Len of Validation loss: 128, Average loss: 1.5877558591309935\n",
            "Epoch: 2520, Len of Training loss: 36, Average loss: 1.1453562080860138\n",
            "Len of Validation loss: 128, Average loss: 1.5676579892169684\n",
            "Epoch: 2521, Len of Training loss: 36, Average loss: 1.188574395245976\n",
            "Len of Validation loss: 128, Average loss: 1.545959566021338\n",
            "Epoch: 2522, Len of Training loss: 36, Average loss: 1.170851354797681\n",
            "Len of Validation loss: 128, Average loss: 1.5400249734520912\n",
            "Epoch: 2523, Len of Training loss: 36, Average loss: 1.1599070115221872\n",
            "Len of Validation loss: 128, Average loss: 1.5714444175828248\n",
            "Epoch: 2524, Len of Training loss: 36, Average loss: 1.1336627850929897\n",
            "Len of Validation loss: 128, Average loss: 1.5521249338053167\n",
            "Epoch: 2525, Len of Training loss: 36, Average loss: 1.12656493153837\n",
            "Len of Validation loss: 128, Average loss: 1.583191571990028\n",
            "Epoch: 2526, Len of Training loss: 36, Average loss: 1.1401754568020503\n",
            "Len of Validation loss: 128, Average loss: 1.545890320558101\n",
            "Epoch: 2527, Len of Training loss: 36, Average loss: 1.1274312155114279\n",
            "Len of Validation loss: 128, Average loss: 1.558721557026729\n",
            "Epoch: 2528, Len of Training loss: 36, Average loss: 1.146715717183219\n",
            "Len of Validation loss: 128, Average loss: 1.5553368963301182\n",
            "Epoch: 2529, Len of Training loss: 36, Average loss: 1.1344621578852336\n",
            "Len of Validation loss: 128, Average loss: 1.5823641882743686\n",
            "Epoch: 2530, Len of Training loss: 36, Average loss: 1.131652706199222\n",
            "Len of Validation loss: 128, Average loss: 1.5639866839628667\n",
            "Epoch: 2531, Len of Training loss: 36, Average loss: 1.1382919798294704\n",
            "Len of Validation loss: 128, Average loss: 1.515580684877932\n",
            "Epoch: 2532, Len of Training loss: 36, Average loss: 1.1340823488103018\n",
            "Len of Validation loss: 128, Average loss: 1.5811157166026533\n",
            "Epoch: 2533, Len of Training loss: 36, Average loss: 1.1915895856089063\n",
            "Len of Validation loss: 128, Average loss: 1.630395881831646\n",
            "Epoch: 2534, Len of Training loss: 36, Average loss: 1.1665883428520627\n",
            "Len of Validation loss: 128, Average loss: 1.5270462732296437\n",
            "Epoch: 2535, Len of Training loss: 36, Average loss: 1.145522854394383\n",
            "Len of Validation loss: 128, Average loss: 1.5724732757080346\n",
            "Epoch: 2536, Len of Training loss: 36, Average loss: 1.1342999587456386\n",
            "Len of Validation loss: 128, Average loss: 1.5433613313362002\n",
            "Epoch: 2537, Len of Training loss: 36, Average loss: 1.1215515666537814\n",
            "Len of Validation loss: 128, Average loss: 1.511149034369737\n",
            "Epoch: 2538, Len of Training loss: 36, Average loss: 1.1191154685285356\n",
            "Len of Validation loss: 128, Average loss: 1.4975384389981627\n",
            "Epoch: 2539, Len of Training loss: 36, Average loss: 1.14278178413709\n",
            "Len of Validation loss: 128, Average loss: 1.568176720989868\n",
            "Epoch: 2540, Len of Training loss: 36, Average loss: 1.1561088644795947\n",
            "Len of Validation loss: 128, Average loss: 1.5671027137432247\n",
            "Epoch: 2541, Len of Training loss: 36, Average loss: 1.1595942725737889\n",
            "Len of Validation loss: 128, Average loss: 1.5620998172089458\n",
            "Epoch: 2542, Len of Training loss: 36, Average loss: 1.145526463786761\n",
            "Len of Validation loss: 128, Average loss: 1.5374636566266418\n",
            "Epoch: 2543, Len of Training loss: 36, Average loss: 1.1417115645276175\n",
            "Len of Validation loss: 128, Average loss: 1.5442643915303051\n",
            "Epoch: 2544, Len of Training loss: 36, Average loss: 1.158019436730279\n",
            "Len of Validation loss: 128, Average loss: 1.545533494092524\n",
            "Epoch: 2545, Len of Training loss: 36, Average loss: 1.1288502133554883\n",
            "Len of Validation loss: 128, Average loss: 1.5194348802324384\n",
            "Epoch: 2546, Len of Training loss: 36, Average loss: 1.1251816368765302\n",
            "Len of Validation loss: 128, Average loss: 1.523629005998373\n",
            "Epoch: 2547, Len of Training loss: 36, Average loss: 1.1015405886703067\n",
            "Len of Validation loss: 128, Average loss: 1.5088508452754468\n",
            "Epoch: 2548, Len of Training loss: 36, Average loss: 1.134223085310724\n",
            "Len of Validation loss: 128, Average loss: 1.5643452503718436\n",
            "Epoch: 2549, Len of Training loss: 36, Average loss: 1.1190433750549953\n",
            "Len of Validation loss: 128, Average loss: 1.5520257921889424\n",
            "Epoch: 2550, Len of Training loss: 36, Average loss: 1.1220337599515915\n",
            "Len of Validation loss: 128, Average loss: 1.5358701043296605\n",
            "Epoch: 2551, Len of Training loss: 36, Average loss: 1.1143608109818564\n",
            "Len of Validation loss: 128, Average loss: 1.5339355324395\n",
            "Epoch: 2552, Len of Training loss: 36, Average loss: 1.1187833729717467\n",
            "Len of Validation loss: 128, Average loss: 1.510637705679983\n",
            "Epoch: 2553, Len of Training loss: 36, Average loss: 1.1349160969257355\n",
            "Len of Validation loss: 128, Average loss: 1.5343321848195046\n",
            "Epoch: 2554, Len of Training loss: 36, Average loss: 1.1427677339977689\n",
            "Len of Validation loss: 128, Average loss: 1.5567582319490612\n",
            "Epoch: 2555, Len of Training loss: 36, Average loss: 1.1720849954419665\n",
            "Len of Validation loss: 128, Average loss: 1.5618226507212967\n",
            "Epoch: 2556, Len of Training loss: 36, Average loss: 1.1379811151160135\n",
            "Len of Validation loss: 128, Average loss: 1.549319172743708\n",
            "Epoch: 2557, Len of Training loss: 36, Average loss: 1.1334427727593317\n",
            "Len of Validation loss: 128, Average loss: 1.5014884294942021\n",
            "Epoch: 2558, Len of Training loss: 36, Average loss: 1.144199851486418\n",
            "Len of Validation loss: 128, Average loss: 1.5413841905537993\n",
            "Epoch: 2559, Len of Training loss: 36, Average loss: 1.1543188691139221\n",
            "Len of Validation loss: 128, Average loss: 1.537834199378267\n",
            "Epoch: 2560, Len of Training loss: 36, Average loss: 1.1412963668505351\n",
            "Len of Validation loss: 128, Average loss: 1.5499916127882898\n",
            "Epoch: 2561, Len of Training loss: 36, Average loss: 1.2232017699215147\n",
            "Len of Validation loss: 128, Average loss: 1.6247934957500547\n",
            "Epoch: 2562, Len of Training loss: 36, Average loss: 1.2369356171952353\n",
            "Len of Validation loss: 128, Average loss: 1.5801314762793481\n",
            "Epoch: 2563, Len of Training loss: 36, Average loss: 1.1635727418793573\n",
            "Len of Validation loss: 128, Average loss: 1.575193575117737\n",
            "Epoch: 2564, Len of Training loss: 36, Average loss: 1.1536759833494823\n",
            "Len of Validation loss: 128, Average loss: 1.544203328434378\n",
            "Epoch: 2565, Len of Training loss: 36, Average loss: 1.139541380935245\n",
            "Len of Validation loss: 128, Average loss: 1.5217467944603413\n",
            "Epoch: 2566, Len of Training loss: 36, Average loss: 1.1280532164706125\n",
            "Len of Validation loss: 128, Average loss: 1.5341220395639539\n",
            "Epoch: 2567, Len of Training loss: 36, Average loss: 1.129549001653989\n",
            "Len of Validation loss: 128, Average loss: 1.5690099322237074\n",
            "Epoch: 2568, Len of Training loss: 36, Average loss: 1.1230701588922076\n",
            "Len of Validation loss: 128, Average loss: 1.5477600754238665\n",
            "Epoch: 2569, Len of Training loss: 36, Average loss: 1.1623316788011127\n",
            "Len of Validation loss: 128, Average loss: 1.5547135081142187\n",
            "Epoch: 2570, Len of Training loss: 36, Average loss: 1.1503076569901571\n",
            "Len of Validation loss: 128, Average loss: 1.525569832418114\n",
            "Epoch: 2571, Len of Training loss: 36, Average loss: 1.134521711203787\n",
            "Len of Validation loss: 128, Average loss: 1.556644800119102\n",
            "Epoch: 2572, Len of Training loss: 36, Average loss: 1.147823507587115\n",
            "Len of Validation loss: 128, Average loss: 1.5762316642794758\n",
            "Epoch: 2573, Len of Training loss: 36, Average loss: 1.1503040707773633\n",
            "Len of Validation loss: 128, Average loss: 1.5809929282404482\n",
            "Epoch: 2574, Len of Training loss: 36, Average loss: 1.158626036511527\n",
            "Len of Validation loss: 128, Average loss: 1.5761753842234612\n",
            "Epoch: 2575, Len of Training loss: 36, Average loss: 1.1185253262519836\n",
            "Len of Validation loss: 128, Average loss: 1.536314403405413\n",
            "Epoch: 2576, Len of Training loss: 36, Average loss: 1.1208035813437567\n",
            "Len of Validation loss: 128, Average loss: 1.5466426184866577\n",
            "Epoch: 2577, Len of Training loss: 36, Average loss: 1.1136729054980807\n",
            "Len of Validation loss: 128, Average loss: 1.564625887433067\n",
            "Epoch: 2578, Len of Training loss: 36, Average loss: 1.117900116576089\n",
            "Len of Validation loss: 128, Average loss: 1.520886630518362\n",
            "Epoch: 2579, Len of Training loss: 36, Average loss: 1.1163912763198216\n",
            "Len of Validation loss: 128, Average loss: 1.5327757180202752\n",
            "Epoch: 2580, Len of Training loss: 36, Average loss: 1.1387218866083357\n",
            "Len of Validation loss: 128, Average loss: 1.5301893898285925\n",
            "Epoch: 2581, Len of Training loss: 36, Average loss: 1.1208105799224641\n",
            "Len of Validation loss: 128, Average loss: 1.5389955891296268\n",
            "Epoch: 2582, Len of Training loss: 36, Average loss: 1.1135854870080948\n",
            "Len of Validation loss: 128, Average loss: 1.5557580303866416\n",
            "Epoch: 2583, Len of Training loss: 36, Average loss: 1.1148947295215395\n",
            "Len of Validation loss: 128, Average loss: 1.568431131541729\n",
            "Epoch: 2584, Len of Training loss: 36, Average loss: 1.1476215024789174\n",
            "Len of Validation loss: 128, Average loss: 1.5420441660098732\n",
            "Epoch: 2585, Len of Training loss: 36, Average loss: 1.1458548837237887\n",
            "Len of Validation loss: 128, Average loss: 1.5507934582419693\n",
            "Epoch: 2586, Len of Training loss: 36, Average loss: 1.1309789932436414\n",
            "Len of Validation loss: 128, Average loss: 1.5363126744050533\n",
            "Epoch: 2587, Len of Training loss: 36, Average loss: 1.1378668662574556\n",
            "Len of Validation loss: 128, Average loss: 1.5749176384415478\n",
            "Epoch: 2588, Len of Training loss: 36, Average loss: 1.1362578637070126\n",
            "Len of Validation loss: 128, Average loss: 1.5270312293432653\n",
            "Epoch: 2589, Len of Training loss: 36, Average loss: 1.1437597589360342\n",
            "Len of Validation loss: 128, Average loss: 1.5258874285500497\n",
            "Epoch: 2590, Len of Training loss: 36, Average loss: 1.1342020117574267\n",
            "Len of Validation loss: 128, Average loss: 1.5791893545538187\n",
            "Epoch: 2591, Len of Training loss: 36, Average loss: 1.145624621046914\n",
            "Len of Validation loss: 128, Average loss: 1.5641572454478592\n",
            "Epoch: 2592, Len of Training loss: 36, Average loss: 1.1696738749742508\n",
            "Len of Validation loss: 128, Average loss: 1.5800445119384676\n",
            "Epoch: 2593, Len of Training loss: 36, Average loss: 1.1489736702707078\n",
            "Len of Validation loss: 128, Average loss: 1.5583356223069131\n",
            "Epoch: 2594, Len of Training loss: 36, Average loss: 1.1317508816719055\n",
            "Len of Validation loss: 128, Average loss: 1.5625025688204914\n",
            "Epoch: 2595, Len of Training loss: 36, Average loss: 1.1275920619567235\n",
            "Len of Validation loss: 128, Average loss: 1.5097485415171832\n",
            "Epoch: 2596, Len of Training loss: 36, Average loss: 1.1159718847937055\n",
            "Len of Validation loss: 128, Average loss: 1.539724434260279\n",
            "Epoch: 2597, Len of Training loss: 36, Average loss: 1.108172497815556\n",
            "Len of Validation loss: 128, Average loss: 1.5339548732154071\n",
            "Epoch: 2598, Len of Training loss: 36, Average loss: 1.1119262526432674\n",
            "Len of Validation loss: 128, Average loss: 1.5224269225727767\n",
            "Epoch: 2599, Len of Training loss: 36, Average loss: 1.1158796432945464\n",
            "Len of Validation loss: 128, Average loss: 1.534871588461101\n",
            "Epoch: 2600, Len of Training loss: 36, Average loss: 1.1263119263781443\n",
            "Len of Validation loss: 128, Average loss: 1.568244735011831\n",
            "Epoch: 2601, Len of Training loss: 36, Average loss: 1.1473621312114928\n",
            "Len of Validation loss: 128, Average loss: 1.5568931750021875\n",
            "Epoch: 2602, Len of Training loss: 36, Average loss: 1.1288839545514848\n",
            "Len of Validation loss: 128, Average loss: 1.5244802120141685\n",
            "Epoch: 2603, Len of Training loss: 36, Average loss: 1.1186528205871582\n",
            "Len of Validation loss: 128, Average loss: 1.525383687345311\n",
            "Epoch: 2604, Len of Training loss: 36, Average loss: 1.125549531645245\n",
            "Len of Validation loss: 128, Average loss: 1.5650177095085382\n",
            "Epoch: 2605, Len of Training loss: 36, Average loss: 1.11899899939696\n",
            "Len of Validation loss: 128, Average loss: 1.5470579119864851\n",
            "Epoch: 2606, Len of Training loss: 36, Average loss: 1.1214926011032529\n",
            "Len of Validation loss: 128, Average loss: 1.548984101973474\n",
            "Epoch: 2607, Len of Training loss: 36, Average loss: 1.1288119786315494\n",
            "Len of Validation loss: 128, Average loss: 1.5188654435332865\n",
            "Epoch: 2608, Len of Training loss: 36, Average loss: 1.110489199558894\n",
            "Len of Validation loss: 128, Average loss: 1.5440913503989577\n",
            "Epoch: 2609, Len of Training loss: 36, Average loss: 1.1217033962408702\n",
            "Len of Validation loss: 128, Average loss: 1.5677677090279758\n",
            "Epoch: 2610, Len of Training loss: 36, Average loss: 1.138734628756841\n",
            "Len of Validation loss: 128, Average loss: 1.5933359451591969\n",
            "Epoch: 2611, Len of Training loss: 36, Average loss: 1.2256012972858217\n",
            "Len of Validation loss: 128, Average loss: 1.5639145649038255\n",
            "Epoch: 2612, Len of Training loss: 36, Average loss: 1.1387561360994976\n",
            "Len of Validation loss: 128, Average loss: 1.5438057705760002\n",
            "Epoch: 2613, Len of Training loss: 36, Average loss: 1.1535667147901323\n",
            "Len of Validation loss: 128, Average loss: 1.6945065688341856\n",
            "Epoch: 2614, Len of Training loss: 36, Average loss: 1.148953730861346\n",
            "Len of Validation loss: 128, Average loss: 1.5724341885652393\n",
            "Epoch: 2615, Len of Training loss: 36, Average loss: 1.1288140101565256\n",
            "Len of Validation loss: 128, Average loss: 1.5145899162162095\n",
            "Epoch: 2616, Len of Training loss: 36, Average loss: 1.1283798764149349\n",
            "Len of Validation loss: 128, Average loss: 1.5193253606557846\n",
            "Epoch: 2617, Len of Training loss: 36, Average loss: 1.1113448242346446\n",
            "Len of Validation loss: 128, Average loss: 1.5670318230986595\n",
            "Epoch: 2618, Len of Training loss: 36, Average loss: 1.127698947985967\n",
            "Len of Validation loss: 128, Average loss: 1.5525997383520007\n",
            "Epoch: 2619, Len of Training loss: 36, Average loss: 1.1324412574370701\n",
            "Len of Validation loss: 128, Average loss: 1.5339344029780477\n",
            "Epoch: 2620, Len of Training loss: 36, Average loss: 1.1155474781990051\n",
            "Len of Validation loss: 128, Average loss: 1.5297933244146407\n",
            "Epoch: 2621, Len of Training loss: 36, Average loss: 1.1607922911643982\n",
            "Len of Validation loss: 128, Average loss: 1.5946615389548242\n",
            "Epoch: 2622, Len of Training loss: 36, Average loss: 1.168483727508121\n",
            "Len of Validation loss: 128, Average loss: 1.5300642903894186\n",
            "Epoch: 2623, Len of Training loss: 36, Average loss: 1.137080020374722\n",
            "Len of Validation loss: 128, Average loss: 1.5375186442397535\n",
            "Epoch: 2624, Len of Training loss: 36, Average loss: 1.117611762550142\n",
            "Len of Validation loss: 128, Average loss: 1.5279544591903687\n",
            "Epoch: 2625, Len of Training loss: 36, Average loss: 1.1082997338639364\n",
            "Len of Validation loss: 128, Average loss: 1.5261760046705604\n",
            "Epoch: 2626, Len of Training loss: 36, Average loss: 1.1035192244582706\n",
            "Len of Validation loss: 128, Average loss: 1.5101729040034115\n",
            "Epoch: 2627, Len of Training loss: 36, Average loss: 1.1176771935489442\n",
            "Len of Validation loss: 128, Average loss: 1.533250859938562\n",
            "Epoch: 2628, Len of Training loss: 36, Average loss: 1.1278302404615614\n",
            "Len of Validation loss: 128, Average loss: 1.5477950093336403\n",
            "Epoch: 2629, Len of Training loss: 36, Average loss: 1.1273928764793608\n",
            "Len of Validation loss: 128, Average loss: 1.5105254442896694\n",
            "Epoch: 2630, Len of Training loss: 36, Average loss: 1.152317883239852\n",
            "Len of Validation loss: 128, Average loss: 1.5653548517730087\n",
            "Epoch: 2631, Len of Training loss: 36, Average loss: 1.1521687623527315\n",
            "Len of Validation loss: 128, Average loss: 1.5394332834985107\n",
            "Epoch: 2632, Len of Training loss: 36, Average loss: 1.1272726009289424\n",
            "Len of Validation loss: 128, Average loss: 1.5570198334753513\n",
            "Epoch: 2633, Len of Training loss: 36, Average loss: 1.132153140174018\n",
            "Len of Validation loss: 128, Average loss: 1.5460178116336465\n",
            "Epoch: 2634, Len of Training loss: 36, Average loss: 1.1301836851570342\n",
            "Len of Validation loss: 128, Average loss: 1.5107919992879033\n",
            "Epoch: 2635, Len of Training loss: 36, Average loss: 1.1517374730772443\n",
            "Len of Validation loss: 128, Average loss: 1.5325403253082186\n",
            "Epoch: 2636, Len of Training loss: 36, Average loss: 1.1328061140245862\n",
            "Len of Validation loss: 128, Average loss: 1.5182775401044637\n",
            "Epoch: 2637, Len of Training loss: 36, Average loss: 1.1390674660603206\n",
            "Len of Validation loss: 128, Average loss: 1.546200955286622\n",
            "Epoch: 2638, Len of Training loss: 36, Average loss: 1.134105619457033\n",
            "Len of Validation loss: 128, Average loss: 1.577905016951263\n",
            "Epoch: 2639, Len of Training loss: 36, Average loss: 1.1517337527539995\n",
            "Len of Validation loss: 128, Average loss: 1.5535568543709815\n",
            "Epoch: 2640, Len of Training loss: 36, Average loss: 1.1270651188161638\n",
            "Len of Validation loss: 128, Average loss: 1.5185765277128667\n",
            "Epoch: 2641, Len of Training loss: 36, Average loss: 1.1036094824473064\n",
            "Len of Validation loss: 128, Average loss: 1.4980643989983946\n",
            "Epoch: 2642, Len of Training loss: 36, Average loss: 1.0906654447317123\n",
            "Len of Validation loss: 128, Average loss: 1.5199230059515685\n",
            "Epoch: 2643, Len of Training loss: 36, Average loss: 1.107444127400716\n",
            "Len of Validation loss: 128, Average loss: 1.5131453275680542\n",
            "Epoch: 2644, Len of Training loss: 36, Average loss: 1.1066503988371954\n",
            "Len of Validation loss: 128, Average loss: 1.543709079734981\n",
            "Epoch: 2645, Len of Training loss: 36, Average loss: 1.1356094164980783\n",
            "Len of Validation loss: 128, Average loss: 1.5345480074174702\n",
            "Epoch: 2646, Len of Training loss: 36, Average loss: 1.1332802226146061\n",
            "Len of Validation loss: 128, Average loss: 1.5409845737740397\n",
            "Epoch: 2647, Len of Training loss: 36, Average loss: 1.1241434266169865\n",
            "Len of Validation loss: 128, Average loss: 1.5386575260199606\n",
            "Epoch: 2648, Len of Training loss: 36, Average loss: 1.1236426300472684\n",
            "Len of Validation loss: 128, Average loss: 1.5350280874408782\n",
            "Epoch: 2649, Len of Training loss: 36, Average loss: 1.1110235055287678\n",
            "Len of Validation loss: 128, Average loss: 1.5386587337125093\n",
            "Epoch: 2650, Len of Training loss: 36, Average loss: 1.1117369135220845\n",
            "Len of Validation loss: 128, Average loss: 1.5523094469681382\n",
            "Epoch: 2651, Len of Training loss: 36, Average loss: 1.1181809438599482\n",
            "Len of Validation loss: 128, Average loss: 1.5208302845712751\n",
            "Epoch: 2652, Len of Training loss: 36, Average loss: 1.1132493019104004\n",
            "Len of Validation loss: 128, Average loss: 1.558572090929374\n",
            "Epoch: 2653, Len of Training loss: 36, Average loss: 1.1457904544141557\n",
            "Len of Validation loss: 128, Average loss: 1.5129739851690829\n",
            "Epoch: 2654, Len of Training loss: 36, Average loss: 1.1250804083214865\n",
            "Len of Validation loss: 128, Average loss: 1.584238362731412\n",
            "Epoch: 2655, Len of Training loss: 36, Average loss: 1.1496157579951816\n",
            "Len of Validation loss: 128, Average loss: 1.529806235106662\n",
            "Epoch: 2656, Len of Training loss: 36, Average loss: 1.1226491232713063\n",
            "Len of Validation loss: 128, Average loss: 1.5676530697382987\n",
            "Epoch: 2657, Len of Training loss: 36, Average loss: 1.1073829183975856\n",
            "Len of Validation loss: 128, Average loss: 1.5759216824080795\n",
            "Epoch: 2658, Len of Training loss: 36, Average loss: 1.1023800290293164\n",
            "Len of Validation loss: 128, Average loss: 1.539162817876786\n",
            "Epoch: 2659, Len of Training loss: 36, Average loss: 1.1124469091494877\n",
            "Len of Validation loss: 128, Average loss: 1.5179681633599102\n",
            "Epoch: 2660, Len of Training loss: 36, Average loss: 1.0916082859039307\n",
            "Len of Validation loss: 128, Average loss: 1.5283553914632648\n",
            "Epoch: 2661, Len of Training loss: 36, Average loss: 1.1013697104321585\n",
            "Len of Validation loss: 128, Average loss: 1.5130917311180383\n",
            "Epoch: 2662, Len of Training loss: 36, Average loss: 1.1362369308869045\n",
            "Len of Validation loss: 128, Average loss: 1.567649407312274\n",
            "Epoch: 2663, Len of Training loss: 36, Average loss: 1.1877647638320923\n",
            "Len of Validation loss: 128, Average loss: 1.5327502880245447\n",
            "Epoch: 2664, Len of Training loss: 36, Average loss: 1.1376331796248753\n",
            "Len of Validation loss: 128, Average loss: 1.5317589789628983\n",
            "Epoch: 2665, Len of Training loss: 36, Average loss: 1.105331497059928\n",
            "Len of Validation loss: 128, Average loss: 1.5411250574979931\n",
            "Epoch: 2666, Len of Training loss: 36, Average loss: 1.112594539920489\n",
            "Len of Validation loss: 128, Average loss: 1.5227837425190955\n",
            "Epoch: 2667, Len of Training loss: 36, Average loss: 1.1314522723356883\n",
            "Len of Validation loss: 128, Average loss: 1.5676323208026588\n",
            "Epoch: 2668, Len of Training loss: 36, Average loss: 1.1383439368671842\n",
            "Len of Validation loss: 128, Average loss: 1.5478708534501493\n",
            "Epoch: 2669, Len of Training loss: 36, Average loss: 1.1073981374502182\n",
            "Len of Validation loss: 128, Average loss: 1.540283583337441\n",
            "Epoch: 2670, Len of Training loss: 36, Average loss: 1.1148400985532336\n",
            "Len of Validation loss: 128, Average loss: 1.5649931752122939\n",
            "Epoch: 2671, Len of Training loss: 36, Average loss: 1.108035135600302\n",
            "Len of Validation loss: 128, Average loss: 1.5450596006121486\n",
            "Epoch: 2672, Len of Training loss: 36, Average loss: 1.1151785602172215\n",
            "Len of Validation loss: 128, Average loss: 1.5387682032305747\n",
            "Epoch: 2673, Len of Training loss: 36, Average loss: 1.1089855763647292\n",
            "Len of Validation loss: 128, Average loss: 1.5131791112944484\n",
            "Epoch: 2674, Len of Training loss: 36, Average loss: 1.1199861781464682\n",
            "Len of Validation loss: 128, Average loss: 1.5168090397492051\n",
            "Epoch: 2675, Len of Training loss: 36, Average loss: 1.1332346714205213\n",
            "Len of Validation loss: 128, Average loss: 1.5037989048287272\n",
            "Epoch: 2676, Len of Training loss: 36, Average loss: 1.1280781560473971\n",
            "Len of Validation loss: 128, Average loss: 1.5040090919937938\n",
            "Epoch: 2677, Len of Training loss: 36, Average loss: 1.1249436603652105\n",
            "Len of Validation loss: 128, Average loss: 1.5430360841564834\n",
            "Epoch: 2678, Len of Training loss: 36, Average loss: 1.1110428505473666\n",
            "Len of Validation loss: 128, Average loss: 1.5314340493641794\n",
            "Epoch: 2679, Len of Training loss: 36, Average loss: 1.1145862489938736\n",
            "Len of Validation loss: 128, Average loss: 1.5429368226323277\n",
            "Epoch: 2680, Len of Training loss: 36, Average loss: 1.098723774154981\n",
            "Len of Validation loss: 128, Average loss: 1.4915741803124547\n",
            "Epoch: 2681, Len of Training loss: 36, Average loss: 1.1177236421240702\n",
            "Len of Validation loss: 128, Average loss: 1.5164002801757306\n",
            "Epoch: 2682, Len of Training loss: 36, Average loss: 1.1208179791768391\n",
            "Len of Validation loss: 128, Average loss: 1.5080437276046723\n",
            "Epoch: 2683, Len of Training loss: 36, Average loss: 1.1256071047650442\n",
            "Len of Validation loss: 128, Average loss: 1.5015188697725534\n",
            "Epoch: 2684, Len of Training loss: 36, Average loss: 1.1416021204657025\n",
            "Len of Validation loss: 128, Average loss: 1.5433528516441584\n",
            "Epoch: 2685, Len of Training loss: 36, Average loss: 1.1523769895235698\n",
            "Len of Validation loss: 128, Average loss: 1.6475211104843765\n",
            "Epoch: 2686, Len of Training loss: 36, Average loss: 1.1178078419632382\n",
            "Len of Validation loss: 128, Average loss: 1.525178099051118\n",
            "Epoch: 2687, Len of Training loss: 36, Average loss: 1.1238360669877794\n",
            "Len of Validation loss: 128, Average loss: 1.529218706768006\n",
            "Epoch: 2688, Len of Training loss: 36, Average loss: 1.1130385945240657\n",
            "Len of Validation loss: 128, Average loss: 1.5342505432199687\n",
            "Epoch: 2689, Len of Training loss: 36, Average loss: 1.0992620918485854\n",
            "Len of Validation loss: 128, Average loss: 1.5094963270239532\n",
            "Epoch: 2690, Len of Training loss: 36, Average loss: 1.1000037325753107\n",
            "Len of Validation loss: 128, Average loss: 1.5270873666740954\n",
            "Epoch: 2691, Len of Training loss: 36, Average loss: 1.1228182365496953\n",
            "Len of Validation loss: 128, Average loss: 1.5330494849476963\n",
            "Epoch: 2692, Len of Training loss: 36, Average loss: 1.1175798674424489\n",
            "Len of Validation loss: 128, Average loss: 1.5810409630648792\n",
            "Epoch: 2693, Len of Training loss: 36, Average loss: 1.1218160291512806\n",
            "Len of Validation loss: 128, Average loss: 1.5833959616720676\n",
            "Epoch: 2694, Len of Training loss: 36, Average loss: 1.1116124540567398\n",
            "Len of Validation loss: 128, Average loss: 1.550913684652187\n",
            "Epoch: 2695, Len of Training loss: 36, Average loss: 1.1119639873504639\n",
            "Len of Validation loss: 128, Average loss: 1.5160118720959872\n",
            "Epoch: 2696, Len of Training loss: 36, Average loss: 1.1455089582337274\n",
            "Len of Validation loss: 128, Average loss: 1.5868645943701267\n",
            "Epoch: 2697, Len of Training loss: 36, Average loss: 1.161181577377849\n",
            "Len of Validation loss: 128, Average loss: 1.5520300099160522\n",
            "Epoch: 2698, Len of Training loss: 36, Average loss: 1.1184401330020692\n",
            "Len of Validation loss: 128, Average loss: 1.5863253104034811\n",
            "Epoch: 2699, Len of Training loss: 36, Average loss: 1.105524480342865\n",
            "Len of Validation loss: 128, Average loss: 1.5105248782783747\n",
            "Epoch: 2700, Len of Training loss: 36, Average loss: 1.106938287615776\n",
            "Len of Validation loss: 128, Average loss: 1.5443653222173452\n",
            "Epoch: 2701, Len of Training loss: 36, Average loss: 1.1059085329373677\n",
            "Len of Validation loss: 128, Average loss: 1.5185502599924803\n",
            "Epoch: 2702, Len of Training loss: 36, Average loss: 1.100435862938563\n",
            "Len of Validation loss: 128, Average loss: 1.5803470278624445\n",
            "Epoch: 2703, Len of Training loss: 36, Average loss: 1.1024478657378092\n",
            "Len of Validation loss: 128, Average loss: 1.5378086131531745\n",
            "Epoch: 2704, Len of Training loss: 36, Average loss: 1.1307058152225282\n",
            "Len of Validation loss: 128, Average loss: 1.5611181929707527\n",
            "Epoch: 2705, Len of Training loss: 36, Average loss: 1.1106003837452993\n",
            "Len of Validation loss: 128, Average loss: 1.5276733234059066\n",
            "Epoch: 2706, Len of Training loss: 36, Average loss: 1.1152014599906073\n",
            "Len of Validation loss: 128, Average loss: 1.550371966790408\n",
            "Epoch: 2707, Len of Training loss: 36, Average loss: 1.0994635025660198\n",
            "Len of Validation loss: 128, Average loss: 1.5337095481809229\n",
            "Epoch: 2708, Len of Training loss: 36, Average loss: 1.0940395676427417\n",
            "Len of Validation loss: 128, Average loss: 1.5494615761563182\n",
            "Epoch: 2709, Len of Training loss: 36, Average loss: 1.1160644839207332\n",
            "Len of Validation loss: 128, Average loss: 1.5383452991954982\n",
            "Epoch: 2710, Len of Training loss: 36, Average loss: 1.1448124200105667\n",
            "Len of Validation loss: 128, Average loss: 1.5561590641736984\n",
            "Epoch: 2711, Len of Training loss: 36, Average loss: 1.135960802435875\n",
            "Len of Validation loss: 128, Average loss: 1.539642900112085\n",
            "Epoch: 2712, Len of Training loss: 36, Average loss: 1.134528711438179\n",
            "Len of Validation loss: 128, Average loss: 1.5504609120544046\n",
            "Epoch: 2713, Len of Training loss: 36, Average loss: 1.1097890436649323\n",
            "Len of Validation loss: 128, Average loss: 1.5880868583917618\n",
            "Epoch: 2714, Len of Training loss: 36, Average loss: 1.103232165177663\n",
            "Len of Validation loss: 128, Average loss: 1.5226686538662761\n",
            "Epoch: 2715, Len of Training loss: 36, Average loss: 1.1074768751859665\n",
            "Len of Validation loss: 128, Average loss: 1.5349680273793638\n",
            "Epoch: 2716, Len of Training loss: 36, Average loss: 1.1160426023933623\n",
            "Len of Validation loss: 128, Average loss: 1.5330211254768074\n",
            "Epoch: 2717, Len of Training loss: 36, Average loss: 1.1077019886838064\n",
            "Len of Validation loss: 128, Average loss: 1.5045680913608521\n",
            "Epoch: 2718, Len of Training loss: 36, Average loss: 1.1132042159636815\n",
            "Len of Validation loss: 128, Average loss: 1.5573190785944462\n",
            "Epoch: 2719, Len of Training loss: 36, Average loss: 1.1047366278039084\n",
            "Len of Validation loss: 128, Average loss: 1.5240173870697618\n",
            "Epoch: 2720, Len of Training loss: 36, Average loss: 1.1325317025184631\n",
            "Len of Validation loss: 128, Average loss: 1.531749497866258\n",
            "Epoch: 2721, Len of Training loss: 36, Average loss: 1.1168705175320308\n",
            "Len of Validation loss: 128, Average loss: 1.5394623668398708\n",
            "Epoch: 2722, Len of Training loss: 36, Average loss: 1.1018443405628204\n",
            "Len of Validation loss: 128, Average loss: 1.5312424548901618\n",
            "Epoch: 2723, Len of Training loss: 36, Average loss: 1.1369520492023892\n",
            "Len of Validation loss: 128, Average loss: 1.5763026610948145\n",
            "Epoch: 2724, Len of Training loss: 36, Average loss: 1.1011917177173827\n",
            "Len of Validation loss: 128, Average loss: 1.5169554103631526\n",
            "Epoch: 2725, Len of Training loss: 36, Average loss: 1.0837975094715755\n",
            "Len of Validation loss: 128, Average loss: 1.4882039886433631\n",
            "Epoch: 2726, Len of Training loss: 36, Average loss: 1.1291262590222888\n",
            "Len of Validation loss: 128, Average loss: 1.508646978996694\n",
            "Epoch: 2727, Len of Training loss: 36, Average loss: 1.1205524106820424\n",
            "Len of Validation loss: 128, Average loss: 1.5427128991577774\n",
            "Epoch: 2728, Len of Training loss: 36, Average loss: 1.1027659293678072\n",
            "Len of Validation loss: 128, Average loss: 1.5243357446743175\n",
            "Epoch: 2729, Len of Training loss: 36, Average loss: 1.1004677149984572\n",
            "Len of Validation loss: 128, Average loss: 1.517672955058515\n",
            "Epoch: 2730, Len of Training loss: 36, Average loss: 1.121797263622284\n",
            "Len of Validation loss: 128, Average loss: 1.5134939807467163\n",
            "Epoch: 2731, Len of Training loss: 36, Average loss: 1.1292989055315654\n",
            "Len of Validation loss: 128, Average loss: 1.4995822992641479\n",
            "Epoch: 2732, Len of Training loss: 36, Average loss: 1.126650897992982\n",
            "Len of Validation loss: 128, Average loss: 1.5559073807671666\n",
            "Epoch: 2733, Len of Training loss: 36, Average loss: 1.1126939952373505\n",
            "Len of Validation loss: 128, Average loss: 1.5323168733157218\n",
            "Epoch: 2734, Len of Training loss: 36, Average loss: 1.0978443655702803\n",
            "Len of Validation loss: 128, Average loss: 1.494210405740887\n",
            "Epoch: 2735, Len of Training loss: 36, Average loss: 1.0925167832109663\n",
            "Len of Validation loss: 128, Average loss: 1.513593565672636\n",
            "Epoch: 2736, Len of Training loss: 36, Average loss: 1.135945998960071\n",
            "Len of Validation loss: 128, Average loss: 1.555181382689625\n",
            "Epoch: 2737, Len of Training loss: 36, Average loss: 1.1135488748550415\n",
            "Len of Validation loss: 128, Average loss: 1.5195337228942662\n",
            "Epoch: 2738, Len of Training loss: 36, Average loss: 1.0856581429640453\n",
            "Len of Validation loss: 128, Average loss: 1.538157273316756\n",
            "Epoch: 2739, Len of Training loss: 36, Average loss: 1.099580681986279\n",
            "Len of Validation loss: 128, Average loss: 1.5242820971179754\n",
            "Epoch: 2740, Len of Training loss: 36, Average loss: 1.102178058690495\n",
            "Len of Validation loss: 128, Average loss: 1.5395111364778131\n",
            "Epoch: 2741, Len of Training loss: 36, Average loss: 1.1121120966143079\n",
            "Len of Validation loss: 128, Average loss: 1.5518594696186483\n",
            "Epoch: 2742, Len of Training loss: 36, Average loss: 1.104222199983067\n",
            "Len of Validation loss: 128, Average loss: 1.5316226538270712\n",
            "Epoch: 2743, Len of Training loss: 36, Average loss: 1.0972833981116612\n",
            "Len of Validation loss: 128, Average loss: 1.5321966140763834\n",
            "Epoch: 2744, Len of Training loss: 36, Average loss: 1.1126386324564617\n",
            "Len of Validation loss: 128, Average loss: 1.5971950604580343\n",
            "Epoch: 2745, Len of Training loss: 36, Average loss: 1.098048523068428\n",
            "Len of Validation loss: 128, Average loss: 1.5310634670313448\n",
            "Epoch: 2746, Len of Training loss: 36, Average loss: 1.0977166659302182\n",
            "Len of Validation loss: 128, Average loss: 1.5553730549290776\n",
            "Epoch: 2747, Len of Training loss: 36, Average loss: 1.0929773598909378\n",
            "Len of Validation loss: 128, Average loss: 1.5396611264441162\n",
            "Epoch: 2748, Len of Training loss: 36, Average loss: 1.1162601759036381\n",
            "Len of Validation loss: 128, Average loss: 1.5184423660393804\n",
            "Epoch: 2749, Len of Training loss: 36, Average loss: 1.101170755094952\n",
            "Len of Validation loss: 128, Average loss: 1.5054669280070812\n",
            "Epoch: 2750, Len of Training loss: 36, Average loss: 1.1187781261073217\n",
            "Len of Validation loss: 128, Average loss: 1.5515469259116799\n",
            "Epoch: 2751, Len of Training loss: 36, Average loss: 1.096113567550977\n",
            "Len of Validation loss: 128, Average loss: 1.5452159328851849\n",
            "Epoch: 2752, Len of Training loss: 36, Average loss: 1.1010357091824214\n",
            "Len of Validation loss: 128, Average loss: 1.531215806491673\n",
            "Epoch: 2753, Len of Training loss: 36, Average loss: 1.0937625335322485\n",
            "Len of Validation loss: 128, Average loss: 1.501182680716738\n",
            "Epoch: 2754, Len of Training loss: 36, Average loss: 1.09521936542458\n",
            "Len of Validation loss: 128, Average loss: 1.500519399298355\n",
            "Epoch: 2755, Len of Training loss: 36, Average loss: 1.1046693738963869\n",
            "Len of Validation loss: 128, Average loss: 1.5444878521375358\n",
            "Epoch: 2756, Len of Training loss: 36, Average loss: 1.10585038529502\n",
            "Len of Validation loss: 128, Average loss: 1.591698945267126\n",
            "Epoch: 2757, Len of Training loss: 36, Average loss: 1.117623766263326\n",
            "Len of Validation loss: 128, Average loss: 1.5333862868137658\n",
            "Epoch: 2758, Len of Training loss: 36, Average loss: 1.1207622521453433\n",
            "Len of Validation loss: 128, Average loss: 1.5409140542615205\n",
            "Epoch: 2759, Len of Training loss: 36, Average loss: 1.119961569706599\n",
            "Len of Validation loss: 128, Average loss: 1.5470859829802066\n",
            "Epoch: 2760, Len of Training loss: 36, Average loss: 1.0970325387186475\n",
            "Len of Validation loss: 128, Average loss: 1.5199594453442842\n",
            "Epoch: 2761, Len of Training loss: 36, Average loss: 1.09726187090079\n",
            "Len of Validation loss: 128, Average loss: 1.5195341610815376\n",
            "Epoch: 2762, Len of Training loss: 36, Average loss: 1.088462041483985\n",
            "Len of Validation loss: 128, Average loss: 1.5066257233265787\n",
            "Epoch: 2763, Len of Training loss: 36, Average loss: 1.0871464610099792\n",
            "Len of Validation loss: 128, Average loss: 1.4828029084019363\n",
            "Epoch: 2764, Len of Training loss: 36, Average loss: 1.1095787949032254\n",
            "Len of Validation loss: 128, Average loss: 1.5351928207091987\n",
            "Epoch: 2765, Len of Training loss: 36, Average loss: 1.1166672772831387\n",
            "Len of Validation loss: 128, Average loss: 1.5525001366622746\n",
            "Epoch: 2766, Len of Training loss: 36, Average loss: 1.1105401780870225\n",
            "Len of Validation loss: 128, Average loss: 1.5401794698555022\n",
            "Epoch: 2767, Len of Training loss: 36, Average loss: 1.1070678151316113\n",
            "Len of Validation loss: 128, Average loss: 1.5315364331472665\n",
            "Epoch: 2768, Len of Training loss: 36, Average loss: 1.0993071678611968\n",
            "Len of Validation loss: 128, Average loss: 1.5436196115333587\n",
            "Epoch: 2769, Len of Training loss: 36, Average loss: 1.1267708589633305\n",
            "Len of Validation loss: 128, Average loss: 1.5336683762725443\n",
            "Epoch: 2770, Len of Training loss: 36, Average loss: 1.0989712592628267\n",
            "Len of Validation loss: 128, Average loss: 1.5281652316916734\n",
            "Epoch: 2771, Len of Training loss: 36, Average loss: 1.101316761639383\n",
            "Len of Validation loss: 128, Average loss: 1.5165089131332934\n",
            "Epoch: 2772, Len of Training loss: 36, Average loss: 1.10909670094649\n",
            "Len of Validation loss: 128, Average loss: 1.5334708564914763\n",
            "Epoch: 2773, Len of Training loss: 36, Average loss: 1.1198512895239725\n",
            "Len of Validation loss: 128, Average loss: 1.5185724524781108\n",
            "Epoch: 2774, Len of Training loss: 36, Average loss: 1.1036515401469336\n",
            "Len of Validation loss: 128, Average loss: 1.518711180659011\n",
            "Epoch: 2775, Len of Training loss: 36, Average loss: 1.1056569351090326\n",
            "Len of Validation loss: 128, Average loss: 1.4965243530459702\n",
            "Epoch: 2776, Len of Training loss: 36, Average loss: 1.1181782533725102\n",
            "Len of Validation loss: 128, Average loss: 1.5177109765354544\n",
            "Epoch: 2777, Len of Training loss: 36, Average loss: 1.106150723165936\n",
            "Len of Validation loss: 128, Average loss: 1.5229012398049235\n",
            "Epoch: 2778, Len of Training loss: 36, Average loss: 1.1138048784600363\n",
            "Len of Validation loss: 128, Average loss: 1.5036167474463582\n",
            "Epoch: 2779, Len of Training loss: 36, Average loss: 1.1246367841959\n",
            "Len of Validation loss: 128, Average loss: 1.5360758679453284\n",
            "Epoch: 2780, Len of Training loss: 36, Average loss: 1.0906734532780118\n",
            "Len of Validation loss: 128, Average loss: 1.5203364314511418\n",
            "Epoch: 2781, Len of Training loss: 36, Average loss: 1.087232349647416\n",
            "Len of Validation loss: 128, Average loss: 1.5506988188717514\n",
            "Epoch: 2782, Len of Training loss: 36, Average loss: 1.0840459134843614\n",
            "Len of Validation loss: 128, Average loss: 1.5716265705414116\n",
            "Epoch: 2783, Len of Training loss: 36, Average loss: 1.0774920764896605\n",
            "Len of Validation loss: 128, Average loss: 1.5426064843777567\n",
            "Epoch: 2784, Len of Training loss: 36, Average loss: 1.095318951540523\n",
            "Len of Validation loss: 128, Average loss: 1.5542907607741654\n",
            "Epoch: 2785, Len of Training loss: 36, Average loss: 1.1031953824890985\n",
            "Len of Validation loss: 128, Average loss: 1.5183870431501418\n",
            "Epoch: 2786, Len of Training loss: 36, Average loss: 1.090823608967993\n",
            "Len of Validation loss: 128, Average loss: 1.519866045564413\n",
            "Epoch: 2787, Len of Training loss: 36, Average loss: 1.0911574297481113\n",
            "Len of Validation loss: 128, Average loss: 1.5118951598415151\n",
            "Epoch: 2788, Len of Training loss: 36, Average loss: 1.097162009941207\n",
            "Len of Validation loss: 128, Average loss: 1.570280902320519\n",
            "Epoch: 2789, Len of Training loss: 36, Average loss: 1.0967592597007751\n",
            "Len of Validation loss: 128, Average loss: 1.5791604823898524\n",
            "Epoch: 2790, Len of Training loss: 36, Average loss: 1.0994664761755202\n",
            "Len of Validation loss: 128, Average loss: 1.5179816072341055\n",
            "Epoch: 2791, Len of Training loss: 36, Average loss: 1.0804246366024017\n",
            "Len of Validation loss: 128, Average loss: 1.5460460991598666\n",
            "Epoch: 2792, Len of Training loss: 36, Average loss: 1.1260291089614232\n",
            "Len of Validation loss: 128, Average loss: 1.5271500668022782\n",
            "Epoch: 2793, Len of Training loss: 36, Average loss: 1.1003108239836163\n",
            "Len of Validation loss: 128, Average loss: 1.569789275759831\n",
            "Epoch: 2794, Len of Training loss: 36, Average loss: 1.1078595866759617\n",
            "Len of Validation loss: 128, Average loss: 1.5214425288140774\n",
            "Epoch: 2795, Len of Training loss: 36, Average loss: 1.1143017361561458\n",
            "Len of Validation loss: 128, Average loss: 1.543087259400636\n",
            "Epoch: 2796, Len of Training loss: 36, Average loss: 1.1083896921740637\n",
            "Len of Validation loss: 128, Average loss: 1.536481450079009\n",
            "Epoch: 2797, Len of Training loss: 36, Average loss: 1.0898008147875469\n",
            "Len of Validation loss: 128, Average loss: 1.5135990204289556\n",
            "Epoch: 2798, Len of Training loss: 36, Average loss: 1.1008299953407712\n",
            "Len of Validation loss: 128, Average loss: 1.5454375876579434\n",
            "Epoch: 2799, Len of Training loss: 36, Average loss: 1.0911065174473658\n",
            "Len of Validation loss: 128, Average loss: 1.5152756839524955\n",
            "Epoch: 2800, Len of Training loss: 36, Average loss: 1.0958058089017868\n",
            "Len of Validation loss: 128, Average loss: 1.5243402868509293\n",
            "Epoch: 2801, Len of Training loss: 36, Average loss: 1.1353752828306622\n",
            "Len of Validation loss: 128, Average loss: 1.5809518380556256\n",
            "Epoch: 2802, Len of Training loss: 36, Average loss: 1.1278871132267847\n",
            "Len of Validation loss: 128, Average loss: 1.524774620309472\n",
            "Epoch: 2803, Len of Training loss: 36, Average loss: 1.0912258343564138\n",
            "Len of Validation loss: 128, Average loss: 1.543018712196499\n",
            "Epoch: 2804, Len of Training loss: 36, Average loss: 1.0890226993295882\n",
            "Len of Validation loss: 128, Average loss: 1.5539072304964066\n",
            "Epoch: 2805, Len of Training loss: 36, Average loss: 1.0974841051631503\n",
            "Len of Validation loss: 128, Average loss: 1.5386050706729293\n",
            "Epoch: 2806, Len of Training loss: 36, Average loss: 1.1080330163240433\n",
            "Len of Validation loss: 128, Average loss: 1.5891166334040463\n",
            "Epoch: 2807, Len of Training loss: 36, Average loss: 1.1179975767930348\n",
            "Len of Validation loss: 128, Average loss: 1.5388584474567324\n",
            "Epoch: 2808, Len of Training loss: 36, Average loss: 1.0990861025121477\n",
            "Len of Validation loss: 128, Average loss: 1.5203856653533876\n",
            "Epoch: 2809, Len of Training loss: 36, Average loss: 1.0984088099665112\n",
            "Len of Validation loss: 128, Average loss: 1.5238083528820425\n",
            "Epoch: 2810, Len of Training loss: 36, Average loss: 1.1165507949060864\n",
            "Len of Validation loss: 128, Average loss: 1.5188325478229672\n",
            "Epoch: 2811, Len of Training loss: 36, Average loss: 1.0899465928475063\n",
            "Len of Validation loss: 128, Average loss: 1.5236789979971945\n",
            "Epoch: 2812, Len of Training loss: 36, Average loss: 1.0966504166523616\n",
            "Len of Validation loss: 128, Average loss: 1.4980231341905892\n",
            "Epoch: 2813, Len of Training loss: 36, Average loss: 1.0778502772251766\n",
            "Len of Validation loss: 128, Average loss: 1.5232518662232906\n",
            "Epoch: 2814, Len of Training loss: 36, Average loss: 1.086939083205329\n",
            "Len of Validation loss: 128, Average loss: 1.5609165574423969\n",
            "Epoch: 2815, Len of Training loss: 36, Average loss: 1.0971961104207568\n",
            "Len of Validation loss: 128, Average loss: 1.5620134645141661\n",
            "Epoch: 2816, Len of Training loss: 36, Average loss: 1.0937307559781604\n",
            "Len of Validation loss: 128, Average loss: 1.5045463857240975\n",
            "Epoch: 2817, Len of Training loss: 36, Average loss: 1.083211104075114\n",
            "Len of Validation loss: 128, Average loss: 1.5168844398576766\n",
            "Epoch: 2818, Len of Training loss: 36, Average loss: 1.0803105466895633\n",
            "Len of Validation loss: 128, Average loss: 1.540336742065847\n",
            "Epoch: 2819, Len of Training loss: 36, Average loss: 1.0944972783327103\n",
            "Len of Validation loss: 128, Average loss: 1.5212620568927377\n",
            "Epoch: 2820, Len of Training loss: 36, Average loss: 1.073126956820488\n",
            "Len of Validation loss: 128, Average loss: 1.5130175754893571\n",
            "Epoch: 2821, Len of Training loss: 36, Average loss: 1.0840356018808153\n",
            "Len of Validation loss: 128, Average loss: 1.521531512727961\n",
            "Epoch: 2822, Len of Training loss: 36, Average loss: 1.1052546948194504\n",
            "Len of Validation loss: 128, Average loss: 1.527305492432788\n",
            "Epoch: 2823, Len of Training loss: 36, Average loss: 1.1366024944517348\n",
            "Len of Validation loss: 128, Average loss: 1.5569538895506412\n",
            "Epoch: 2824, Len of Training loss: 36, Average loss: 1.1095883415804968\n",
            "Len of Validation loss: 128, Average loss: 1.5230715167708695\n",
            "Epoch: 2825, Len of Training loss: 36, Average loss: 1.0958840565549002\n",
            "Len of Validation loss: 128, Average loss: 1.4916045570280403\n",
            "Epoch: 2826, Len of Training loss: 36, Average loss: 1.089441983236207\n",
            "Len of Validation loss: 128, Average loss: 1.5463891790714115\n",
            "Epoch: 2827, Len of Training loss: 36, Average loss: 1.1256330642435286\n",
            "Len of Validation loss: 128, Average loss: 1.5503575867041945\n",
            "Epoch: 2828, Len of Training loss: 36, Average loss: 1.1132378346390195\n",
            "Len of Validation loss: 128, Average loss: 1.5539304518606514\n",
            "Epoch: 2829, Len of Training loss: 36, Average loss: 1.1273269438081317\n",
            "Len of Validation loss: 128, Average loss: 1.5131907383911312\n",
            "Epoch: 2830, Len of Training loss: 36, Average loss: 1.087580563293563\n",
            "Len of Validation loss: 128, Average loss: 1.5040968772955239\n",
            "Epoch: 2831, Len of Training loss: 36, Average loss: 1.0658085760143068\n",
            "Len of Validation loss: 128, Average loss: 1.4958178494125605\n",
            "Epoch: 2832, Len of Training loss: 36, Average loss: 1.0714577817254596\n",
            "Len of Validation loss: 128, Average loss: 1.5058064493350685\n",
            "Epoch: 2833, Len of Training loss: 36, Average loss: 1.1003897835810978\n",
            "Len of Validation loss: 128, Average loss: 1.5155195745173842\n",
            "Epoch: 2834, Len of Training loss: 36, Average loss: 1.0798850788010492\n",
            "Len of Validation loss: 128, Average loss: 1.5583920353092253\n",
            "Epoch: 2835, Len of Training loss: 36, Average loss: 1.0807035830285814\n",
            "Len of Validation loss: 128, Average loss: 1.5277106096036732\n",
            "Epoch: 2836, Len of Training loss: 36, Average loss: 1.0877638624774084\n",
            "Len of Validation loss: 128, Average loss: 1.5221078221220523\n",
            "Epoch: 2837, Len of Training loss: 36, Average loss: 1.086134781440099\n",
            "Len of Validation loss: 128, Average loss: 1.534247658913955\n",
            "Epoch: 2838, Len of Training loss: 36, Average loss: 1.0852525515688791\n",
            "Len of Validation loss: 128, Average loss: 1.5312355116475374\n",
            "Epoch: 2839, Len of Training loss: 36, Average loss: 1.0777533517943487\n",
            "Len of Validation loss: 128, Average loss: 1.5333838986698538\n",
            "Epoch: 2840, Len of Training loss: 36, Average loss: 1.0967248794105318\n",
            "Len of Validation loss: 128, Average loss: 1.550682567525655\n",
            "Epoch: 2841, Len of Training loss: 36, Average loss: 1.1143549879391987\n",
            "Len of Validation loss: 128, Average loss: 1.516479530138895\n",
            "Epoch: 2842, Len of Training loss: 36, Average loss: 1.0952072789271672\n",
            "Len of Validation loss: 128, Average loss: 1.552912301151082\n",
            "Epoch: 2843, Len of Training loss: 36, Average loss: 1.098349016573694\n",
            "Len of Validation loss: 128, Average loss: 1.5178724117577076\n",
            "Epoch: 2844, Len of Training loss: 36, Average loss: 1.0908225542969174\n",
            "Len of Validation loss: 128, Average loss: 1.5209655943326652\n",
            "Epoch: 2845, Len of Training loss: 36, Average loss: 1.0907234748204548\n",
            "Len of Validation loss: 128, Average loss: 1.5260704054962844\n",
            "Epoch: 2846, Len of Training loss: 36, Average loss: 1.0759131958087285\n",
            "Len of Validation loss: 128, Average loss: 1.5235858568921685\n",
            "Epoch: 2847, Len of Training loss: 36, Average loss: 1.0857636150386598\n",
            "Len of Validation loss: 128, Average loss: 1.5200288123451173\n",
            "Epoch: 2848, Len of Training loss: 36, Average loss: 1.0927152501212225\n",
            "Len of Validation loss: 128, Average loss: 1.5811173804104328\n",
            "Epoch: 2849, Len of Training loss: 36, Average loss: 1.1091262234581842\n",
            "Len of Validation loss: 128, Average loss: 1.560493492987007\n",
            "Epoch: 2850, Len of Training loss: 36, Average loss: 1.07973988685343\n",
            "Len of Validation loss: 128, Average loss: 1.5267121959477663\n",
            "Epoch: 2851, Len of Training loss: 36, Average loss: 1.1022769576973386\n",
            "Len of Validation loss: 128, Average loss: 1.5213830166030675\n",
            "Epoch: 2852, Len of Training loss: 36, Average loss: 1.1367948883109622\n",
            "Len of Validation loss: 128, Average loss: 1.5647811237722635\n",
            "Epoch: 2853, Len of Training loss: 36, Average loss: 1.1257018248240154\n",
            "Len of Validation loss: 128, Average loss: 1.5561972277937457\n",
            "Epoch: 2854, Len of Training loss: 36, Average loss: 1.0944314002990723\n",
            "Len of Validation loss: 128, Average loss: 1.5344070591963828\n",
            "Epoch: 2855, Len of Training loss: 36, Average loss: 1.1126142541567485\n",
            "Len of Validation loss: 128, Average loss: 1.54717315454036\n",
            "Epoch: 2856, Len of Training loss: 36, Average loss: 1.0812010169029236\n",
            "Len of Validation loss: 128, Average loss: 1.4722082279622555\n",
            "Epoch: 2857, Len of Training loss: 36, Average loss: 1.0992083615726895\n",
            "Len of Validation loss: 128, Average loss: 1.5044268446508795\n",
            "Epoch: 2858, Len of Training loss: 36, Average loss: 1.0822238243288465\n",
            "Len of Validation loss: 128, Average loss: 1.5404333877377212\n",
            "Epoch: 2859, Len of Training loss: 36, Average loss: 1.1113638149367437\n",
            "Len of Validation loss: 128, Average loss: 1.524376627523452\n",
            "Epoch: 2860, Len of Training loss: 36, Average loss: 1.1140480356083975\n",
            "Len of Validation loss: 128, Average loss: 1.595717703923583\n",
            "Epoch: 2861, Len of Training loss: 36, Average loss: 1.1127359982993867\n",
            "Len of Validation loss: 128, Average loss: 1.5351965930312872\n",
            "Epoch: 2862, Len of Training loss: 36, Average loss: 1.0946251352628071\n",
            "Len of Validation loss: 128, Average loss: 1.5194946741685271\n",
            "Epoch: 2863, Len of Training loss: 36, Average loss: 1.0793309658765793\n",
            "Len of Validation loss: 128, Average loss: 1.5282183426897973\n",
            "Epoch: 2864, Len of Training loss: 36, Average loss: 1.1271668424208958\n",
            "Len of Validation loss: 128, Average loss: 1.6657536767888814\n",
            "Epoch: 2865, Len of Training loss: 36, Average loss: 1.1176459756162431\n",
            "Len of Validation loss: 128, Average loss: 1.5479466991964728\n",
            "Epoch: 2866, Len of Training loss: 36, Average loss: 1.0934893207417593\n",
            "Len of Validation loss: 128, Average loss: 1.5317292618565261\n",
            "Epoch: 2867, Len of Training loss: 36, Average loss: 1.090927807821168\n",
            "Len of Validation loss: 128, Average loss: 1.531244844198227\n",
            "Epoch: 2868, Len of Training loss: 36, Average loss: 1.0850560118754704\n",
            "Len of Validation loss: 128, Average loss: 1.5354340721387416\n",
            "Epoch: 2869, Len of Training loss: 36, Average loss: 1.0833724521928363\n",
            "Len of Validation loss: 128, Average loss: 1.5054234429262578\n",
            "Epoch: 2870, Len of Training loss: 36, Average loss: 1.0789298431740866\n",
            "Len of Validation loss: 128, Average loss: 1.5414811817463487\n",
            "Epoch: 2871, Len of Training loss: 36, Average loss: 1.0715729577673807\n",
            "Len of Validation loss: 128, Average loss: 1.545479234540835\n",
            "Epoch: 2872, Len of Training loss: 36, Average loss: 1.0943236433797412\n",
            "Len of Validation loss: 128, Average loss: 1.5628816401585937\n",
            "Epoch: 2873, Len of Training loss: 36, Average loss: 1.0930525759855907\n",
            "Len of Validation loss: 128, Average loss: 1.6174719370901585\n",
            "Epoch: 2874, Len of Training loss: 36, Average loss: 1.1021052300930023\n",
            "Len of Validation loss: 128, Average loss: 1.5130205473396927\n",
            "Epoch: 2875, Len of Training loss: 36, Average loss: 1.0788956665330462\n",
            "Len of Validation loss: 128, Average loss: 1.5481971257831901\n",
            "Epoch: 2876, Len of Training loss: 36, Average loss: 1.0942656348148982\n",
            "Len of Validation loss: 128, Average loss: 1.4974481048993766\n",
            "Epoch: 2877, Len of Training loss: 36, Average loss: 1.0816870265536838\n",
            "Len of Validation loss: 128, Average loss: 1.518785652704537\n",
            "Epoch: 2878, Len of Training loss: 36, Average loss: 1.0871265414688323\n",
            "Len of Validation loss: 128, Average loss: 1.5473568455781788\n",
            "Epoch: 2879, Len of Training loss: 36, Average loss: 1.0941889997985628\n",
            "Len of Validation loss: 128, Average loss: 1.5308415298350155\n",
            "Epoch: 2880, Len of Training loss: 36, Average loss: 1.0862013730737898\n",
            "Len of Validation loss: 128, Average loss: 1.5142205816227943\n",
            "Epoch: 2881, Len of Training loss: 36, Average loss: 1.0947096827957365\n",
            "Len of Validation loss: 128, Average loss: 1.521222202340141\n",
            "Epoch: 2882, Len of Training loss: 36, Average loss: 1.077095843023724\n",
            "Len of Validation loss: 128, Average loss: 1.5545463869348168\n",
            "Epoch: 2883, Len of Training loss: 36, Average loss: 1.0938582784599729\n",
            "Len of Validation loss: 128, Average loss: 1.5776623620186\n",
            "Epoch: 2884, Len of Training loss: 36, Average loss: 1.1041628287898169\n",
            "Len of Validation loss: 128, Average loss: 1.5297037190757692\n",
            "Epoch: 2885, Len of Training loss: 36, Average loss: 1.119715455505583\n",
            "Len of Validation loss: 128, Average loss: 1.5535904439166188\n",
            "Epoch: 2886, Len of Training loss: 36, Average loss: 1.1139149533377752\n",
            "Len of Validation loss: 128, Average loss: 1.522921648574993\n",
            "Epoch: 2887, Len of Training loss: 36, Average loss: 1.0835976633760664\n",
            "Len of Validation loss: 128, Average loss: 1.5326362689957023\n",
            "Epoch: 2888, Len of Training loss: 36, Average loss: 1.0858127888705995\n",
            "Len of Validation loss: 128, Average loss: 1.5019135179463774\n",
            "Epoch: 2889, Len of Training loss: 36, Average loss: 1.0862576613823574\n",
            "Len of Validation loss: 128, Average loss: 1.5165773439221084\n",
            "Epoch: 2890, Len of Training loss: 36, Average loss: 1.085780648721589\n",
            "Len of Validation loss: 128, Average loss: 1.533287157304585\n",
            "Epoch: 2891, Len of Training loss: 36, Average loss: 1.0818121085564296\n",
            "Len of Validation loss: 128, Average loss: 1.5137845224235207\n",
            "Epoch: 2892, Len of Training loss: 36, Average loss: 1.0731358329455059\n",
            "Len of Validation loss: 128, Average loss: 1.5038951367605478\n",
            "Epoch: 2893, Len of Training loss: 36, Average loss: 1.094748963912328\n",
            "Len of Validation loss: 128, Average loss: 1.5286496467888355\n",
            "Epoch: 2894, Len of Training loss: 36, Average loss: 1.0901270591550403\n",
            "Len of Validation loss: 128, Average loss: 1.5496590931434184\n",
            "Epoch: 2895, Len of Training loss: 36, Average loss: 1.076208621263504\n",
            "Len of Validation loss: 128, Average loss: 1.520026810001582\n",
            "Epoch: 2896, Len of Training loss: 36, Average loss: 1.072330367234018\n",
            "Len of Validation loss: 128, Average loss: 1.5080533537548035\n",
            "Epoch: 2897, Len of Training loss: 36, Average loss: 1.0788819922341242\n",
            "Len of Validation loss: 128, Average loss: 1.536289731040597\n",
            "Epoch: 2898, Len of Training loss: 36, Average loss: 1.085743087861273\n",
            "Len of Validation loss: 128, Average loss: 1.504153202753514\n",
            "Epoch: 2899, Len of Training loss: 36, Average loss: 1.0947499672571819\n",
            "Len of Validation loss: 128, Average loss: 1.5246207134332508\n",
            "Epoch: 2900, Len of Training loss: 36, Average loss: 1.1012073573138978\n",
            "Len of Validation loss: 128, Average loss: 1.5157915183808655\n",
            "Epoch: 2901, Len of Training loss: 36, Average loss: 1.1101770864592657\n",
            "Len of Validation loss: 128, Average loss: 1.5412501357495785\n",
            "Epoch: 2902, Len of Training loss: 36, Average loss: 1.1152187420262232\n",
            "Len of Validation loss: 128, Average loss: 1.5252909855917096\n",
            "Epoch: 2903, Len of Training loss: 36, Average loss: 1.0985932466056612\n",
            "Len of Validation loss: 128, Average loss: 1.5688389400020242\n",
            "Epoch: 2904, Len of Training loss: 36, Average loss: 1.147091461552514\n",
            "Len of Validation loss: 128, Average loss: 1.5451927515678108\n",
            "Epoch: 2905, Len of Training loss: 36, Average loss: 1.0944279316398833\n",
            "Len of Validation loss: 128, Average loss: 1.5125446578022093\n",
            "Epoch: 2906, Len of Training loss: 36, Average loss: 1.0924171192778482\n",
            "Len of Validation loss: 128, Average loss: 1.4688398111611605\n",
            "Epoch: 2907, Len of Training loss: 36, Average loss: 1.0735994676748912\n",
            "Len of Validation loss: 128, Average loss: 1.521778208669275\n",
            "Epoch: 2908, Len of Training loss: 36, Average loss: 1.0670917679866154\n",
            "Len of Validation loss: 128, Average loss: 1.5309004278387874\n",
            "Epoch: 2909, Len of Training loss: 36, Average loss: 1.0611536221371756\n",
            "Len of Validation loss: 128, Average loss: 1.5240753516554832\n",
            "Epoch: 2910, Len of Training loss: 36, Average loss: 1.0878406465053558\n",
            "Len of Validation loss: 128, Average loss: 1.5015517114661634\n",
            "Epoch: 2911, Len of Training loss: 36, Average loss: 1.076331646906005\n",
            "Len of Validation loss: 128, Average loss: 1.5396329462528229\n",
            "Epoch: 2912, Len of Training loss: 36, Average loss: 1.0818375382158492\n",
            "Len of Validation loss: 128, Average loss: 1.539144458482042\n",
            "Epoch: 2913, Len of Training loss: 36, Average loss: 1.0651451150576274\n",
            "Len of Validation loss: 128, Average loss: 1.5159602998755872\n",
            "Epoch: 2914, Len of Training loss: 36, Average loss: 1.0743682616286807\n",
            "Len of Validation loss: 128, Average loss: 1.5109308805549517\n",
            "Epoch: 2915, Len of Training loss: 36, Average loss: 1.0631131496694353\n",
            "Len of Validation loss: 128, Average loss: 1.5199078782461584\n",
            "Epoch: 2916, Len of Training loss: 36, Average loss: 1.0496918625301785\n",
            "Len of Validation loss: 128, Average loss: 1.4906503083184361\n",
            "Epoch: 2917, Len of Training loss: 36, Average loss: 1.0597702612479527\n",
            "Len of Validation loss: 128, Average loss: 1.552294221939519\n",
            "Epoch: 2918, Len of Training loss: 36, Average loss: 1.0789674801958933\n",
            "Len of Validation loss: 128, Average loss: 1.5182799666654319\n",
            "Epoch: 2919, Len of Training loss: 36, Average loss: 1.0730383561717138\n",
            "Len of Validation loss: 128, Average loss: 1.533473871415481\n",
            "Epoch: 2920, Len of Training loss: 36, Average loss: 1.0778867138756647\n",
            "Len of Validation loss: 128, Average loss: 1.508285297313705\n",
            "Epoch: 2921, Len of Training loss: 36, Average loss: 1.0973277042309444\n",
            "Len of Validation loss: 128, Average loss: 1.5419162032194436\n",
            "Epoch: 2922, Len of Training loss: 36, Average loss: 1.0938143564595117\n",
            "Len of Validation loss: 128, Average loss: 1.5353717997204512\n",
            "Epoch: 2923, Len of Training loss: 36, Average loss: 1.0942058000299666\n",
            "Len of Validation loss: 128, Average loss: 1.5376812083413824\n",
            "Epoch: 2924, Len of Training loss: 36, Average loss: 1.067839867538876\n",
            "Len of Validation loss: 128, Average loss: 1.508391904644668\n",
            "Epoch: 2925, Len of Training loss: 36, Average loss: 1.0922475821442075\n",
            "Len of Validation loss: 128, Average loss: 1.5444775966461748\n",
            "Epoch: 2926, Len of Training loss: 36, Average loss: 1.0839047415388956\n",
            "Len of Validation loss: 128, Average loss: 1.5250017563812435\n",
            "Epoch: 2927, Len of Training loss: 36, Average loss: 1.0843402908907995\n",
            "Len of Validation loss: 128, Average loss: 1.542469665640965\n",
            "Epoch: 2928, Len of Training loss: 36, Average loss: 1.07820077571604\n",
            "Len of Validation loss: 128, Average loss: 1.545246950816363\n",
            "Epoch: 2929, Len of Training loss: 36, Average loss: 1.0999615093072255\n",
            "Len of Validation loss: 128, Average loss: 1.5288827160838991\n",
            "Epoch: 2930, Len of Training loss: 36, Average loss: 1.1058382540941238\n",
            "Len of Validation loss: 128, Average loss: 1.5276353298686445\n",
            "Epoch: 2931, Len of Training loss: 36, Average loss: 1.0986730804045994\n",
            "Len of Validation loss: 128, Average loss: 1.5344989183358848\n",
            "Epoch: 2932, Len of Training loss: 36, Average loss: 1.0705250336064234\n",
            "Len of Validation loss: 128, Average loss: 1.5185579140670598\n",
            "Epoch: 2933, Len of Training loss: 36, Average loss: 1.0553446693552866\n",
            "Len of Validation loss: 128, Average loss: 1.5154887621756643\n",
            "Epoch: 2934, Len of Training loss: 36, Average loss: 1.0704132831758923\n",
            "Len of Validation loss: 128, Average loss: 1.5512994409073144\n",
            "Epoch: 2935, Len of Training loss: 36, Average loss: 1.0861510684092839\n",
            "Len of Validation loss: 128, Average loss: 1.552023206371814\n",
            "Epoch: 2936, Len of Training loss: 36, Average loss: 1.0940188831753201\n",
            "Len of Validation loss: 128, Average loss: 1.5413154743146151\n",
            "Epoch: 2937, Len of Training loss: 36, Average loss: 1.0737433781226475\n",
            "Len of Validation loss: 128, Average loss: 1.5365596564952284\n",
            "Epoch: 2938, Len of Training loss: 36, Average loss: 1.074953673614396\n",
            "Len of Validation loss: 128, Average loss: 1.5452390303835273\n",
            "Epoch: 2939, Len of Training loss: 36, Average loss: 1.0768473032448027\n",
            "Len of Validation loss: 128, Average loss: 1.5575629526283592\n",
            "Epoch: 2940, Len of Training loss: 36, Average loss: 1.0828459395302668\n",
            "Len of Validation loss: 128, Average loss: 1.5269945834297687\n",
            "Epoch: 2941, Len of Training loss: 36, Average loss: 1.0669085847006903\n",
            "Len of Validation loss: 128, Average loss: 1.5271837890613824\n",
            "Epoch: 2942, Len of Training loss: 36, Average loss: 1.0872268329064052\n",
            "Len of Validation loss: 128, Average loss: 1.5713818133808672\n",
            "Epoch: 2943, Len of Training loss: 36, Average loss: 1.1019849247402616\n",
            "Len of Validation loss: 128, Average loss: 1.5915522798895836\n",
            "Epoch: 2944, Len of Training loss: 36, Average loss: 1.096827702389823\n",
            "Len of Validation loss: 128, Average loss: 1.575565387494862\n",
            "Epoch: 2945, Len of Training loss: 36, Average loss: 1.0941296931770113\n",
            "Len of Validation loss: 128, Average loss: 1.5251060037408024\n",
            "Epoch: 2946, Len of Training loss: 36, Average loss: 1.0808347447050943\n",
            "Len of Validation loss: 128, Average loss: 1.5189255471341312\n",
            "Epoch: 2947, Len of Training loss: 36, Average loss: 1.074673280119896\n",
            "Len of Validation loss: 128, Average loss: 1.5419600161258131\n",
            "Epoch: 2948, Len of Training loss: 36, Average loss: 1.0958499378628201\n",
            "Len of Validation loss: 128, Average loss: 1.5552532218862325\n",
            "Epoch: 2949, Len of Training loss: 36, Average loss: 1.0886884646283255\n",
            "Len of Validation loss: 128, Average loss: 1.4938256780151278\n",
            "Epoch: 2950, Len of Training loss: 36, Average loss: 1.079028132889006\n",
            "Len of Validation loss: 128, Average loss: 1.5250685648061335\n",
            "Epoch: 2951, Len of Training loss: 36, Average loss: 1.0745213197337256\n",
            "Len of Validation loss: 128, Average loss: 1.5311361000640318\n",
            "Epoch: 2952, Len of Training loss: 36, Average loss: 1.0689785099691815\n",
            "Len of Validation loss: 128, Average loss: 1.5212931734276935\n",
            "Epoch: 2953, Len of Training loss: 36, Average loss: 1.0653900967703924\n",
            "Len of Validation loss: 128, Average loss: 1.4930447372607887\n",
            "Epoch: 2954, Len of Training loss: 36, Average loss: 1.0532313519053988\n",
            "Len of Validation loss: 128, Average loss: 1.5317828585393727\n",
            "Epoch: 2955, Len of Training loss: 36, Average loss: 1.0725088500314288\n",
            "Len of Validation loss: 128, Average loss: 1.5202745515853167\n",
            "Epoch: 2956, Len of Training loss: 36, Average loss: 1.0704817208978865\n",
            "Len of Validation loss: 128, Average loss: 1.5155263389460742\n",
            "Epoch: 2957, Len of Training loss: 36, Average loss: 1.0910785529348586\n",
            "Len of Validation loss: 128, Average loss: 1.5089784138835967\n",
            "Epoch: 2958, Len of Training loss: 36, Average loss: 1.0691539496183395\n",
            "Len of Validation loss: 128, Average loss: 1.517147206934169\n",
            "Epoch: 2959, Len of Training loss: 36, Average loss: 1.0626974602540333\n",
            "Len of Validation loss: 128, Average loss: 1.5202295938506722\n",
            "Epoch: 2960, Len of Training loss: 36, Average loss: 1.084043754471673\n",
            "Len of Validation loss: 128, Average loss: 1.522392955608666\n",
            "Epoch: 2961, Len of Training loss: 36, Average loss: 1.0594622178210154\n",
            "Len of Validation loss: 128, Average loss: 1.4874879084527493\n",
            "Epoch: 2962, Len of Training loss: 36, Average loss: 1.087895029120975\n",
            "Len of Validation loss: 128, Average loss: 1.5110664770472795\n",
            "Epoch: 2963, Len of Training loss: 36, Average loss: 1.0694472657309637\n",
            "Len of Validation loss: 128, Average loss: 1.520123608643189\n",
            "Epoch: 2964, Len of Training loss: 36, Average loss: 1.0685282978746626\n",
            "Len of Validation loss: 128, Average loss: 1.5957162778358907\n",
            "Epoch: 2965, Len of Training loss: 36, Average loss: 1.1444050189521577\n",
            "Len of Validation loss: 128, Average loss: 1.552139089209959\n",
            "Epoch: 2966, Len of Training loss: 36, Average loss: 1.0914900418784883\n",
            "Len of Validation loss: 128, Average loss: 1.5646066807676107\n",
            "Epoch: 2967, Len of Training loss: 36, Average loss: 1.0610783365037706\n",
            "Len of Validation loss: 128, Average loss: 1.5247972004581243\n",
            "Epoch: 2968, Len of Training loss: 36, Average loss: 1.0747540079885058\n",
            "Len of Validation loss: 128, Average loss: 1.5385379015933722\n",
            "Epoch: 2969, Len of Training loss: 36, Average loss: 1.079454157087538\n",
            "Len of Validation loss: 128, Average loss: 1.5644985977560282\n",
            "Epoch: 2970, Len of Training loss: 36, Average loss: 1.0775861077838473\n",
            "Len of Validation loss: 128, Average loss: 1.506905956659466\n",
            "Epoch: 2971, Len of Training loss: 36, Average loss: 1.078886502318912\n",
            "Len of Validation loss: 128, Average loss: 1.5173504622653127\n",
            "Epoch: 2972, Len of Training loss: 36, Average loss: 1.0644421726465225\n",
            "Len of Validation loss: 128, Average loss: 1.5510983967687935\n",
            "Epoch: 2973, Len of Training loss: 36, Average loss: 1.0670802195866902\n",
            "Len of Validation loss: 128, Average loss: 1.492855193791911\n",
            "Epoch: 2974, Len of Training loss: 36, Average loss: 1.0591239498721228\n",
            "Len of Validation loss: 128, Average loss: 1.5059174373745918\n",
            "Epoch: 2975, Len of Training loss: 36, Average loss: 1.0674585286113951\n",
            "Len of Validation loss: 128, Average loss: 1.5212816852144897\n",
            "Epoch: 2976, Len of Training loss: 36, Average loss: 1.0621613611777623\n",
            "Len of Validation loss: 128, Average loss: 1.509399930248037\n",
            "Epoch: 2977, Len of Training loss: 36, Average loss: 1.0769776122437582\n",
            "Len of Validation loss: 128, Average loss: 1.5411505762021989\n",
            "Epoch: 2978, Len of Training loss: 36, Average loss: 1.0842289676268895\n",
            "Len of Validation loss: 128, Average loss: 1.529063947731629\n",
            "Epoch: 2979, Len of Training loss: 36, Average loss: 1.0625861369901233\n",
            "Len of Validation loss: 128, Average loss: 1.5094836445059627\n",
            "Epoch: 2980, Len of Training loss: 36, Average loss: 1.0686623719003465\n",
            "Len of Validation loss: 128, Average loss: 1.5308343884535134\n",
            "Epoch: 2981, Len of Training loss: 36, Average loss: 1.0696319590012233\n",
            "Len of Validation loss: 128, Average loss: 1.538269826443866\n",
            "Epoch: 2982, Len of Training loss: 36, Average loss: 1.076143816113472\n",
            "Len of Validation loss: 128, Average loss: 1.544309169985354\n",
            "Epoch: 2983, Len of Training loss: 36, Average loss: 1.0728502107991114\n",
            "Len of Validation loss: 128, Average loss: 1.5053942597005516\n",
            "Epoch: 2984, Len of Training loss: 36, Average loss: 1.0685210327307384\n",
            "Len of Validation loss: 128, Average loss: 1.5408190658781677\n",
            "Epoch: 2985, Len of Training loss: 36, Average loss: 1.0732180790768728\n",
            "Len of Validation loss: 128, Average loss: 1.5375973598565906\n",
            "Epoch: 2986, Len of Training loss: 36, Average loss: 1.0688664780722723\n",
            "Len of Validation loss: 128, Average loss: 1.5633258095476776\n",
            "Epoch: 2987, Len of Training loss: 36, Average loss: 1.0786306063334148\n",
            "Len of Validation loss: 128, Average loss: 1.5218279999680817\n",
            "Epoch: 2988, Len of Training loss: 36, Average loss: 1.069693414701356\n",
            "Len of Validation loss: 128, Average loss: 1.5063439134974033\n",
            "Epoch: 2989, Len of Training loss: 36, Average loss: 1.0663153015904956\n",
            "Len of Validation loss: 128, Average loss: 1.4878206311259419\n",
            "Epoch: 2990, Len of Training loss: 36, Average loss: 1.0729176617330975\n",
            "Len of Validation loss: 128, Average loss: 1.4864662133622915\n",
            "Epoch: 2991, Len of Training loss: 36, Average loss: 1.0682440648476283\n",
            "Len of Validation loss: 128, Average loss: 1.5696555874310434\n",
            "Epoch: 2992, Len of Training loss: 36, Average loss: 1.0755025810665555\n",
            "Len of Validation loss: 128, Average loss: 1.5780318558681756\n",
            "Epoch: 2993, Len of Training loss: 36, Average loss: 1.0729465468062296\n",
            "Len of Validation loss: 128, Average loss: 1.5536760117392987\n",
            "Epoch: 2994, Len of Training loss: 36, Average loss: 1.0843408703804016\n",
            "Len of Validation loss: 128, Average loss: 1.5936178790871054\n",
            "Epoch: 2995, Len of Training loss: 36, Average loss: 1.0921716309256024\n",
            "Len of Validation loss: 128, Average loss: 1.526146564166993\n",
            "Epoch: 2996, Len of Training loss: 36, Average loss: 1.0836819724904165\n",
            "Len of Validation loss: 128, Average loss: 1.5236854401882738\n",
            "Epoch: 2997, Len of Training loss: 36, Average loss: 1.0863850861787796\n",
            "Len of Validation loss: 128, Average loss: 1.5216608075425029\n",
            "Epoch: 2998, Len of Training loss: 36, Average loss: 1.0725861589113872\n",
            "Len of Validation loss: 128, Average loss: 1.5149741168133914\n",
            "Epoch: 2999, Len of Training loss: 36, Average loss: 1.0523760749234095\n",
            "Len of Validation loss: 128, Average loss: 1.509265347616747\n"
          ]
        }
      ],
      "source": [
        "# Metrics recorder per epoch.\n",
        "train_losses = []\n",
        "\n",
        "valid_losses = []\n",
        "valid_losses_corrected = []\n",
        "\n",
        "# Training loop.\n",
        "model.train()\n",
        "for epoch in range(3000):\n",
        "    # Train.\n",
        "    train_epoch_losses = train(train_loader)\n",
        "    print(f\"Epoch: {epoch}, Len of Training loss: {len(train_epoch_losses)}, Average loss: {float(np.sum(train_epoch_losses))/len(train_epoch_losses)}\")\n",
        "    train_losses.append(np.mean(train_epoch_losses))\n",
        "\n",
        "    valid_epoch_losses= evaluate(valid_loader)\n",
        "    print(f\"Len of Validation loss: {len(valid_epoch_losses)}, Average loss: {float(np.sum(valid_epoch_losses))/len(valid_epoch_losses)}\")\n",
        "    valid_losses.append(np.mean(valid_epoch_losses))\n",
        "    if epoch==500:\n",
        "      torch.save(model,\"model_500.pth\")\n",
        "    if epoch==1000:\n",
        "      torch.save(model,\"model_1000.pth\")\n",
        "    if epoch==1500:\n",
        "      torch.save(model,\"model_1500.pth\")\n",
        "    if epoch==2500:\n",
        "      torch.save(model,\"model_2500.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "n7mjcXV3coC4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "fmZa5ypccoC5",
        "outputId": "9c69ee7e-5fdd-4ac1-a1c7-7b6f6ac695cc"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 500\n",
        "\n",
        "plt.plot(np.linspace(1, num_epochs, num_epochs).astype(int), train_losses[:500], label=\"Training loss\")\n",
        "plt.plot(np.linspace(1, num_epochs, num_epochs).astype(int), valid_losses[:500], label=\"Testing loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "MZ4PODuucoC6",
        "outputId": "39439df3-3349-4bef-9673-a5857f645120"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 1600\n",
        "\n",
        "plt.plot(np.linspace(1, num_epochs, num_epochs).astype(int), train_losses[:1600], label=\"Training loss\")\n",
        "plt.plot(np.linspace(1, num_epochs, num_epochs).astype(int), valid_losses[:1600], label=\"Testing loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "OmlQIUD1PPOn",
        "outputId": "d908180d-9cc4-4f24-f4d8-10f9e4f62826"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 3000\n",
        "\n",
        "plt.plot(np.linspace(1, num_epochs, num_epochs).astype(int), train_losses[:3000], label=\"Training loss\")\n",
        "plt.plot(np.linspace(1, num_epochs, num_epochs).astype(int), valid_losses[:3000], label=\"Testing loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "iYfRwBEScoC7"
      },
      "outputs": [],
      "source": [
        "torch.save(model,\"GAT_3k.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uy1xuS5ZcoC8",
        "outputId": "56b1fe8c-3abd-4189-9793-9bc9ae4a88ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MetaNet(\n",
              "  (input): MetaLayer(\n",
              "    edge_model=EdgeModel(\n",
              "    (edge_mlp): Sequential(\n",
              "      (0): Linear(in_features=29, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "  ),\n",
              "    node_model=NodeModel(\n",
              "    (node_mlp_1): Sequential(\n",
              "      (0): Linear(in_features=141, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "    (node_mlp_2): Sequential(\n",
              "      (0): Linear(in_features=141, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "  ),\n",
              "    global_model=None\n",
              "  )\n",
              "  (output): MetaLayer(\n",
              "    edge_model=EdgeModel(\n",
              "    (edge_mlp): Sequential(\n",
              "      (0): Linear(in_features=384, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "  ),\n",
              "    node_model=NodeModel(\n",
              "    (node_mlp_1): Sequential(\n",
              "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "    (node_mlp_2): Sequential(\n",
              "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
              "    )\n",
              "  ),\n",
              "    global_model=None\n",
              "  )\n",
              "  (attention): MultiheadAttention(\n",
              "    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "modelv2 = torch.load(\"GAT_3k.pth\")\n",
        "modelv2.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3JS5k_jcoC8",
        "outputId": "59c29326-ef8d-4b15-a8f3-91a170523ecd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Len of Validation loss: 384, Average loss: 1.2066738811069324\n"
          ]
        }
      ],
      "source": [
        "#evaluate the model on the test set\n",
        "test_epoch_losses= evaluate(test_loader)\n",
        "print(f\"Len of Validation loss: {len(test_epoch_losses)}, Average loss: {float(np.sum(test_epoch_losses))/len(test_epoch_losses)}\")\n",
        "valid_losses.append(np.mean(test_epoch_losses))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20xjqvi0coC9",
        "outputId": "9d3b0cdd-54b4-4bce-c819-6a5e561299df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(128,)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(np.array(test_epoch_losses)).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYnsCHSpcoC-",
        "outputId": "cdc1184f-3480-4e85-b42e-d034830343e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.2066738811069324\n",
            "1.0166825899169798\n",
            "1.0984870873646775\n"
          ]
        }
      ],
      "source": [
        "#calculate the mean squared error for the test set\n",
        "print(np.mean(test_epoch_losses))\n",
        "#calculate the mean absolute error for the test set\n",
        "print(np.mean(np.sqrt(test_epoch_losses)))\n",
        "#calculate the root mean squared error for the test set\n",
        "print(np.sqrt(np.mean(test_epoch_losses)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2aUkX4uMe7x",
        "outputId": "bd51ff15-2199-4a4e-bc90-7962f6cfe8f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.6939736510830614\n",
            "1.2066738811069324\n",
            "1.6413328885643708\n"
          ]
        }
      ],
      "source": [
        "squared_loss = [i**2 for i in test_epoch_losses]\n",
        "print(np.mean(squared_loss))\n",
        "#calculate the mean absolute error for the test set\n",
        "print(np.mean(np.sqrt(squared_loss)))\n",
        "#calculate the root mean squared error for the test set\n",
        "print(np.sqrt(np.mean(squared_loss)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'test_loader' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Asus\\Documents\\ml\\PIL\\gnn_pil_versions\\pil_gnnv4_attention.ipynb Cell 45\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv4_attention.ipynb#Y105sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m np\u001b[39m.\u001b[39mset_printoptions(suppress\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv4_attention.ipynb#Y105sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m test_loader:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv4_attention.ipynb#Y105sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m#create a 3d plot of the access points which has node_type = 0 and the stations which has node_type=1 \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv4_attention.ipynb#Y105sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m#and the edges between them\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv4_attention.ipynb#Y105sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     fig \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39mfigure()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv4_attention.ipynb#Y105sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     ax \u001b[39m=\u001b[39m fig\u001b[39m.\u001b[39madd_subplot(\u001b[39m111\u001b[39m, projection\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m3d\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'test_loader' is not defined"
          ]
        }
      ],
      "source": [
        "np.set_printoptions(suppress=True)\n",
        "for data in test_loader:\n",
        "    #create a 3d plot of the access points which has node_type = 0 and the stations which has node_type=1 \n",
        "    #and the edges between them\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    x = data.x[:,0].cpu().detach().numpy()\n",
        "    y = data.x[:,1].cpu().detach().numpy()\n",
        "    z = data.x[:,2].cpu().detach().numpy()\n",
        "    node_type = data.x[:,10].cpu().detach().numpy()\n",
        "    ax.scatter(x[node_type==0], y[node_type==0], z[node_type==0], c='r', marker='o')\n",
        "    ax.scatter(x[node_type==1], y[node_type==1], z[node_type==1], c='b', marker='o')\n",
        "    for i in range(len(data.edge_index[0])):\n",
        "        if data.x[ data.edge_index[0][i],10] == 0 and data.x[ data.edge_index[1][i],10] == 1:\n",
        "            x_values = [data.x[ data.edge_index[0][i],0],data.x[ data.edge_index[1][i],0]]\n",
        "            y_values = [data.x[ data.edge_index[0][i],1],data.x[ data.edge_index[1][i],1]]\n",
        "            z_values = [data.x[ data.edge_index[0][i],2],data.x[ data.edge_index[1][i],2]]\n",
        "            ax.plot(x_values, y_values, z_values)\n",
        "    plt.show()\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "_3CHUFo5coC_"
      },
      "outputs": [],
      "source": [
        "# #evaluate model and predict on the test set\n",
        "# np.set_printoptions(suppress=True)\n",
        "# #dont print tensor in scientific notation\n",
        "# torch.set_printoptions(sci_mode=False)\n",
        "# modelv2.eval()\n",
        "# for data in test_loader:\n",
        "#     # print(data.shape)\n",
        "#     out = modelv2(data.to(device))\n",
        "#     # #print the predicted values and the actual values side by side for comparison\n",
        "\n",
        "#     print(out)\n",
        "#     print(data.y)\n",
        "#     break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku68BCZicoDA",
        "outputId": "77816810-2981-4950-f0a6-d5b44a6049ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE: 1.6629234308900376\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Set the model in evaluation mode\n",
        "modelv2.eval()\n",
        "\n",
        "# Initialize variables\n",
        "mse = 0.0\n",
        "total_samples = 0\n",
        "\n",
        "# Iterate over the test loader batches\n",
        "for data in test_loader:\n",
        "    # Pass the data through the model\n",
        "    out = modelv2(data.to(device))\n",
        "\n",
        "    # Calculate squared differences\n",
        "    squared_diff = torch.pow(out.view(-1) - data.y.view(-1), 2)\n",
        "    # print(torch.cat((out.view(-1,1),data.y.view(-1,1)),1))\n",
        "\n",
        "    # Accumulate the squared differences\n",
        "    mse += torch.sum(squared_diff).item()\n",
        "\n",
        "    # Update the total number of samples\n",
        "    total_samples += len(data.y)\n",
        "\n",
        "# Calculate the mean squared error\n",
        "mse /= total_samples\n",
        "\n",
        "# Calculate the root mean squared error\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(f\"RMSE: {rmse}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "n4rY-r30coDB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SecW8YxzcoDC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRlb8Zs-coDC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxJbwLD4coDD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJkPXM-fcoDD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4koFooORcoDD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfGznCq1coDD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oo2nLp2WcoDH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
