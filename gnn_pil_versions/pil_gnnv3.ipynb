{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-06T10:08:11.655513Z",
          "iopub.status.busy": "2023-07-06T10:08:11.655094Z",
          "iopub.status.idle": "2023-07-06T10:10:02.592058Z",
          "shell.execute_reply": "2023-07-06T10:10:02.590896Z",
          "shell.execute_reply.started": "2023-07-06T10:08:11.655481Z"
        },
        "id": "8vHlenqqcoB8",
        "outputId": "6f88cbb1-441d-4172-fb21-cb45ffc57d2b",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu117\n",
            "Collecting torch==1.13.1+cu117\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torch-1.13.1%2Bcu117-cp310-cp310-linux_x86_64.whl (1801.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m795.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.14.1+cu117\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torchvision-0.14.1%2Bcu117-cp310-cp310-linux_x86_64.whl (24.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.3/24.3 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.13.1\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torchaudio-0.13.1%2Bcu117-cp310-cp310-linux_x86_64.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1+cu117) (4.6.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1+cu117) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1+cu117) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1+cu117) (8.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (3.4)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.15.2+cu118\n",
            "    Uninstalling torchvision-0.15.2+cu118:\n",
            "      Successfully uninstalled torchvision-0.15.2+cu118\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.0.2+cu118\n",
            "    Uninstalling torchaudio-2.0.2+cu118:\n",
            "      Successfully uninstalled torchaudio-2.0.2+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.13.1+cu117 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.13.1+cu117 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.13.1+cu117 torchaudio-0.13.1+cu117 torchvision-0.14.1+cu117\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-06T10:10:02.596163Z",
          "iopub.status.busy": "2023-07-06T10:10:02.595858Z",
          "iopub.status.idle": "2023-07-06T10:10:17.089867Z",
          "shell.execute_reply": "2023-07-06T10:10:17.088572Z",
          "shell.execute_reply.started": "2023-07-06T10:10:02.596135Z"
        },
        "id": "iZgyOuL7coCG",
        "outputId": "8f3817cc-81f2-47f4-f33c-e29569536b44",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchdata==0.5.1\n",
            "  Downloading torchdata-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchtext==0.14.1\n",
            "  Downloading torchtext-0.14.1-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.5.1) (1.26.16)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata==0.5.1) (2.27.1)\n",
            "Collecting portalocker>=2.0.0 (from torchdata==0.5.1)\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.5.1) (1.13.1+cu117)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.14.1) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.14.1) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->torchdata==0.5.1) (4.6.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.5.1) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.5.1) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.5.1) (3.4)\n",
            "Installing collected packages: portalocker, torchtext, torchdata\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.15.2\n",
            "    Uninstalling torchtext-0.15.2:\n",
            "      Successfully uninstalled torchtext-0.15.2\n",
            "  Attempting uninstall: torchdata\n",
            "    Found existing installation: torchdata 0.6.1\n",
            "    Uninstalling torchdata-0.6.1:\n",
            "      Successfully uninstalled torchdata-0.6.1\n",
            "Successfully installed portalocker-2.7.0 torchdata-0.5.1 torchtext-0.14.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torchdata==0.5.1 torchtext==0.14.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-06T10:10:17.097502Z",
          "iopub.status.busy": "2023-07-06T10:10:17.095299Z",
          "iopub.status.idle": "2023-07-06T10:10:31.268254Z",
          "shell.execute_reply": "2023-07-06T10:10:31.267128Z",
          "shell.execute_reply.started": "2023-07-06T10:10:17.097463Z"
        },
        "id": "fgIfr5w2coCI",
        "outputId": "441a64ca-edf7-43ef-c1a2-0695b1c66c14",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-1.13.1+cu117.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu117/torch_scatter-2.1.1%2Bpt113cu117-cp310-cp310-linux_x86_64.whl (10.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.1+pt113cu117\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.13.1+cu117.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-06T10:10:31.271935Z",
          "iopub.status.busy": "2023-07-06T10:10:31.271551Z",
          "iopub.status.idle": "2023-07-06T10:10:56.721495Z",
          "shell.execute_reply": "2023-07-06T10:10:56.720245Z",
          "shell.execute_reply.started": "2023-07-06T10:10:31.271896Z"
        },
        "id": "JgVl6k43coCK",
        "outputId": "c62b9db7-7902-43cd-fb2e-f7e3e6ef4aed",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/661.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch_geometric\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910459 sha256=883774aa71b94c44af7855016d39229d06a2683df6f213a072739300ceade1b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "Successfully built torch_geometric\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T10:10:56.724219Z",
          "iopub.status.busy": "2023-07-06T10:10:56.723782Z",
          "iopub.status.idle": "2023-07-06T10:10:59.243881Z",
          "shell.execute_reply": "2023-07-06T10:10:59.242716Z",
          "shell.execute_reply.started": "2023-07-06T10:10:56.724166Z"
        },
        "id": "yN1FByVgcoCL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GCNConv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv,GATv2Conv\n",
        "from torch_scatter import scatter_mean\n",
        "from torch_geometric.data import InMemoryDataset, download_url, extract_zip\n",
        "from torch_geometric.nn import MetaLayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "7UCLVQJjrfGZ",
        "outputId": "d89fd7e6-47e7-46d8-9aab-6a7d24cc06ca"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-06T10:20:24.423030Z",
          "iopub.status.busy": "2023-07-06T10:20:24.422651Z",
          "iopub.status.idle": "2023-07-06T10:20:24.430269Z",
          "shell.execute_reply": "2023-07-06T10:20:24.429163Z",
          "shell.execute_reply.started": "2023-07-06T10:20:24.423000Z"
        },
        "id": "0Pzb-vSNcoCV",
        "outputId": "91898bc9-1b7c-4ea0-d399-3183fe214031",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available()  else \"cpu\" )\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Asus\\Documents\\ml\\PIL\\gnn_pil_versions\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"../required_format.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T10:13:51.804937Z",
          "iopub.status.busy": "2023-07-06T10:13:51.804545Z",
          "iopub.status.idle": "2023-07-06T10:13:51.811195Z",
          "shell.execute_reply": "2023-07-06T10:13:51.810142Z",
          "shell.execute_reply.started": "2023-07-06T10:13:51.804907Z"
        },
        "id": "_Pj8h3HecoCW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def get_int_map(dep):\n",
        "    dep = df.loc[df[\"deployment\"]==dep]\n",
        "    dep = dep.reset_index(drop=True)\n",
        "    return dep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "k3RVPftNcoCX"
      },
      "outputs": [],
      "source": [
        "# t1 = get_int_map(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "execution": {
          "iopub.execute_input": "2023-07-06T10:11:00.454755Z",
          "iopub.status.busy": "2023-07-06T10:11:00.454402Z",
          "iopub.status.idle": "2023-07-06T10:11:00.497714Z",
          "shell.execute_reply": "2023-07-06T10:11:00.496795Z",
          "shell.execute_reply.started": "2023-07-06T10:11:00.454723Z"
        },
        "id": "K26GfkcfcoCa",
        "outputId": "c60e7622-2fb4-4dfd-c32f-f8d99d92dfaf",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>primary_channel</th>\n",
              "      <th>min_channel_allowed</th>\n",
              "      <th>max_channel_allowed</th>\n",
              "      <th>rssi</th>\n",
              "      <th>node_type</th>\n",
              "      <th>sinr</th>\n",
              "      <th>air_time_mean</th>\n",
              "      <th>deployment</th>\n",
              "      <th>channel_bonding_model</th>\n",
              "      <th>throughput</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-79.65</td>\n",
              "      <td>-93.86</td>\n",
              "      <td>-109.01</td>\n",
              "      <td>-78.68</td>\n",
              "      <td>-85.3</td>\n",
              "      <td>-98.24</td>\n",
              "      <td>-108.79</td>\n",
              "      <td>-97.21</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>-55.42</td>\n",
              "      <td>0</td>\n",
              "      <td>35.68</td>\n",
              "      <td>25.15</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>104.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-79.65</td>\n",
              "      <td>-93.86</td>\n",
              "      <td>-109.01</td>\n",
              "      <td>-78.68</td>\n",
              "      <td>-85.3</td>\n",
              "      <td>-98.24</td>\n",
              "      <td>-108.79</td>\n",
              "      <td>-97.21</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>-62.31</td>\n",
              "      <td>1</td>\n",
              "      <td>26.99</td>\n",
              "      <td>25.15</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>7.68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-79.65</td>\n",
              "      <td>-93.86</td>\n",
              "      <td>-109.01</td>\n",
              "      <td>-78.68</td>\n",
              "      <td>-85.3</td>\n",
              "      <td>-98.24</td>\n",
              "      <td>-108.79</td>\n",
              "      <td>-97.21</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>-55.42</td>\n",
              "      <td>1</td>\n",
              "      <td>35.68</td>\n",
              "      <td>25.15</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>11.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-79.65</td>\n",
              "      <td>-93.86</td>\n",
              "      <td>-109.01</td>\n",
              "      <td>-78.68</td>\n",
              "      <td>-85.3</td>\n",
              "      <td>-98.24</td>\n",
              "      <td>-108.79</td>\n",
              "      <td>-97.21</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>-58.23</td>\n",
              "      <td>1</td>\n",
              "      <td>33.42</td>\n",
              "      <td>25.15</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>14.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-79.65</td>\n",
              "      <td>-93.86</td>\n",
              "      <td>-109.01</td>\n",
              "      <td>-78.68</td>\n",
              "      <td>-85.3</td>\n",
              "      <td>-98.24</td>\n",
              "      <td>-108.79</td>\n",
              "      <td>-97.21</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>-66.64</td>\n",
              "      <td>1</td>\n",
              "      <td>26.83</td>\n",
              "      <td>25.15</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>11.95</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 27 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0    0      1      2       3      4     5      6       7      8  \\\n",
              "0           0  0.0 -79.65 -93.86 -109.01 -78.68 -85.3 -98.24 -108.79 -97.21   \n",
              "1           1  0.0 -79.65 -93.86 -109.01 -78.68 -85.3 -98.24 -108.79 -97.21   \n",
              "2           2  0.0 -79.65 -93.86 -109.01 -78.68 -85.3 -98.24 -108.79 -97.21   \n",
              "3           3  0.0 -79.65 -93.86 -109.01 -78.68 -85.3 -98.24 -108.79 -97.21   \n",
              "4           4  0.0 -79.65 -93.86 -109.01 -78.68 -85.3 -98.24 -108.79 -97.21   \n",
              "\n",
              "   ...  primary_channel  min_channel_allowed  max_channel_allowed   rssi  \\\n",
              "0  ...                0                    0                    3 -55.42   \n",
              "1  ...                0                    0                    3 -62.31   \n",
              "2  ...                0                    0                    3 -55.42   \n",
              "3  ...                0                    0                    3 -58.23   \n",
              "4  ...                0                    0                    3 -66.64   \n",
              "\n",
              "   node_type   sinr  air_time_mean  deployment  channel_bonding_model  \\\n",
              "0          0  35.68          25.15         0.0                      4   \n",
              "1          1  26.99          25.15         0.0                      4   \n",
              "2          1  35.68          25.15         0.0                      4   \n",
              "3          1  33.42          25.15         0.0                      4   \n",
              "4          1  26.83          25.15         0.0                      4   \n",
              "\n",
              "   throughput  \n",
              "0      104.96  \n",
              "1        7.68  \n",
              "2       11.09  \n",
              "3       14.51  \n",
              "4       11.95  \n",
              "\n",
              "[5 rows x 27 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ndx5diS21Fh3",
        "outputId": "d3b5029a-6250-47d9-888a-8c924d9fa942"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "306424"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T10:28:27.630716Z",
          "iopub.status.busy": "2023-07-06T10:28:27.630364Z",
          "iopub.status.idle": "2023-07-06T10:28:27.648692Z",
          "shell.execute_reply": "2023-07-06T10:28:27.647731Z",
          "shell.execute_reply.started": "2023-07-06T10:28:27.630687Z"
        },
        "id": "-bXLwx5ycoCb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Creating individual graphs\n",
        "# This assumes all APs and STAs are connected to each other\n",
        "def create_graph(split,split_y,deployment):\n",
        "    dep = get_int_map(deployment)\n",
        "    dep_y = dep[\"throughput\"]\n",
        "    dep_x = dep[['0', '1', '2', '3', '4', '5', '6', '7','8', '9', '10', '11', 'wlan_code_index', 'x(m)', 'y(m)','z(m)',\n",
        "            'primary_channel', 'min_channel_allowed', 'max_channel_allowed', 'rssi', 'node_type',\n",
        "            'sinr', 'air_time_mean', 'deployment',\"channel_bonding_model\"]]\n",
        "    #print(dep_x)\n",
        "    dep_reset = dep.reset_index(drop=True)\n",
        "    ap_index = {}\n",
        "    out = dep_reset[dep_reset[\"node_type\"] == 0]\n",
        "    for i in range(len(out)):\n",
        "        ap_index[out.index[i]] = i\n",
        "    #print(ap_index)\n",
        "    node_features = dep_x.iloc[:,12:].values\n",
        "    #edge_features = dep.iloc[:,:12].values - here each node has been given an edge feature\n",
        "    # need to give each edge an edge feature\n",
        "    node_targets = dep_y.values\n",
        "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
        "    print(node_features.shape)\n",
        "    #edge_features = torch.tensor(edge_features, dtype=torch.float)\n",
        "    node_targets = torch.tensor(node_targets, dtype=torch.float)\n",
        "    # Add edges here for each deployment\n",
        "    edges = []\n",
        "    edge_features = []\n",
        "    edge_index = []\n",
        "    for i in range(len(dep)):\n",
        "        for j in range(len(dep)):\n",
        "            if (i != j and (dep[\"node_type\"].iloc[i] == 0 and dep[\"node_type\"].iloc[j] == 0)) or (i !=j and (dep[\"node_type\"].iloc[i] == 1 and dep[\"node_type\"].iloc[j] == 0)):\n",
        "                edges.append([i,j])\n",
        "    #print(edges)\n",
        "    edges2=edges\n",
        "    edges = torch.tensor(edges, dtype=torch.float)\n",
        "    #print(\"Edges: \", edges, edges.shape)\n",
        "    # edge_index = torch.tensor(edges, dtype=torch.long)\n",
        "    edge_index = torch.tensor(edges,dtype=torch.long)\n",
        "    edge_index = edge_index.t().contiguous()\n",
        "    #print(edges.detach(), edges.shape)\n",
        "    print(edges.shape[0])\n",
        "    for i in range(edges.shape[0]):\n",
        "        # print(dep.iloc[int(edges[i][0]), ap_index[int(edges[i][1])]])\n",
        "        i_pos = np.asarray([dep.at[edges2[i][0],\"x(m)\"],dep.at[edges2[i][0],\"y(m)\"],dep.at[edges2[i][0],\"z(m)\"]])\n",
        "        j_pos = np.asarray([dep.at[edges2[i][1],\"x(m)\"],dep.at[edges2[i][1],\"y(m)\"],dep.at[edges2[i][1],\"z(m)\"]])\n",
        "        distance = np.linalg.norm(i_pos - j_pos)\n",
        "        edge_type = 0\n",
        "        if int(edges2[i][0]) in ap_index.keys():\n",
        "          edge_type = 0\n",
        "        else:\n",
        "          edge_type = 1\n",
        "        edge_features.append([edge_type,dep.at[edges2[i][0],\"rssi\"],dep.iloc[int(edges[i][0]), ap_index[int(edges[i][1])]]])\n",
        "    edge_features = torch.tensor(edge_features, dtype=torch.float)\n",
        "    graph = {\n",
        "        \"edges\": edges,\n",
        "        \"edge_index\": edge_index,\n",
        "        \"node_features\": node_features,\n",
        "        \"edge_features\": edge_features,\n",
        "        \"node_targets\": node_targets\n",
        "    }\n",
        "    return graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available()  else \"cpu\" )\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_graph(split,split_y,deployment, device):\n",
        "    dep = get_int_map(deployment)\n",
        "    dep_y = dep[\"throughput\"]\n",
        "    dep_x = dep[['0', '1', '2', '3', '4', '5', '6', '7','8', '9', '10', '11', 'wlan_code_index', 'x(m)', 'y(m)','z(m)',\n",
        "            'primary_channel', 'min_channel_allowed', 'max_channel_allowed', 'rssi', 'node_type',\n",
        "            'sinr', 'air_time_mean', 'deployment',\"channel_bonding_model\"]]\n",
        "    dep_reset = dep.reset_index(drop=True)\n",
        "    ap_index = {}\n",
        "    out = dep_reset[dep_reset[\"node_type\"] == 0]\n",
        "    for i in range(len(out)):\n",
        "        ap_index[out.index[i]] = i\n",
        "    node_features = dep_x.iloc[:,12:].values\n",
        "    node_targets = dep_y.values\n",
        "    node_features = torch.tensor(node_features, dtype=torch.float, device=device)\n",
        "    node_targets = torch.tensor(node_targets, dtype=torch.float, device=device)\n",
        "    edges = []\n",
        "    edge_features = []\n",
        "    edge_index = []\n",
        "    for i in range(len(dep)):\n",
        "        for j in range(len(dep)):\n",
        "            if (i != j and (dep[\"node_type\"].iloc[i] == 0 and dep[\"node_type\"].iloc[j] == 0)) or (i !=j and (dep[\"node_type\"].iloc[i] == 1 and dep[\"node_type\"].iloc[j] == 0)):\n",
        "                edges.append([i,j])\n",
        "    edges2=edges\n",
        "    edges = torch.tensor(edges, dtype=torch.float, device=device)\n",
        "    edge_index = torch.tensor(edges,dtype=torch.long, device=device)\n",
        "    edge_index = edge_index.t().contiguous()\n",
        "    print(node_features.shape)\n",
        "    print(edges.shape[0])\n",
        "    for i in range(edges.shape[0]):\n",
        "        i_pos = np.asarray([dep.at[edges2[i][0],\"x(m)\"],dep.at[edges2[i][0],\"y(m)\"],dep.at[edges2[i][0],\"z(m)\"]])\n",
        "        j_pos = np.asarray([dep.at[edges2[i][1],\"x(m)\"],dep.at[edges2[i][1],\"y(m)\"],dep.at[edges2[i][1],\"z(m)\"]])\n",
        "        distance = np.linalg.norm(i_pos - j_pos)\n",
        "        edge_type = 0\n",
        "        if int(edges2[i][0]) in ap_index.keys():\n",
        "          edge_type = 0\n",
        "        else:\n",
        "          edge_type = 1\n",
        "        edge_features.append([edge_type,dep.at[edges2[i][0],\"rssi\"],dep.iloc[int(edges[i][0]), ap_index[int(edges[i][1])]]])\n",
        "    edge_features = torch.tensor(edge_features, dtype=torch.float, device=device)\n",
        "    graph = {\n",
        "        \"edges\": edges,\n",
        "        \"edge_index\": edge_index,\n",
        "        \"node_features\": node_features,\n",
        "        \"edge_features\": edge_features,\n",
        "        \"node_targets\": node_targets\n",
        "    }\n",
        "    return graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-06T10:19:03.496718Z",
          "iopub.status.busy": "2023-07-06T10:19:03.496147Z",
          "iopub.status.idle": "2023-07-06T10:19:05.076427Z",
          "shell.execute_reply": "2023-07-06T10:19:05.075388Z",
          "shell.execute_reply.started": "2023-07-06T10:19:03.496690Z"
        },
        "id": "LFkij5z5coCd",
        "outputId": "9edf5ebd-410c-4d6a-d2c0-25dd57d4f189",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([170, 13])\n",
            "2028\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'edges': tensor([[  0.,  11.],\n",
              "         [  0.,  27.],\n",
              "         [  0.,  44.],\n",
              "         ...,\n",
              "         [169., 128.],\n",
              "         [169., 139.],\n",
              "         [169., 156.]], device='cuda:0'),\n",
              " 'edge_index': tensor([[  0,   0,   0,  ..., 169, 169, 169],\n",
              "         [ 11,  27,  44,  ..., 128, 139, 156]], device='cuda:0'),\n",
              " 'node_features': tensor([[ 0.0000,  7.5000,  8.3333,  ..., 25.1500,  0.0000,  4.0000],\n",
              "         [ 0.0000, 12.0627,  4.6918,  ..., 25.1500,  0.0000,  4.0000],\n",
              "         [ 0.0000,  8.2712,  4.8383,  ..., 25.1500,  0.0000,  4.0000],\n",
              "         ...,\n",
              "         [11.0000, 56.2321, 45.8161,  ..., 25.7725,  0.0000,  4.0000],\n",
              "         [11.0000, 50.4659, 43.8516,  ..., 25.7725,  0.0000,  4.0000],\n",
              "         [11.0000, 46.8457, 36.8127,  ..., 25.7725,  0.0000,  4.0000]],\n",
              "        device='cuda:0'),\n",
              " 'edge_features': tensor([[   0.0000,  -55.4200,    0.0000],\n",
              "         [   0.0000,  -55.4200,  -79.6500],\n",
              "         [   0.0000,  -55.4200,  -93.8600],\n",
              "         ...,\n",
              "         [   1.0000,  -64.6900, -108.8900],\n",
              "         [   1.0000,  -64.6900,  -96.9100],\n",
              "         [   1.0000,  -64.6900,  -76.9500]], device='cuda:0'),\n",
              " 'node_targets': tensor([104.9600,   7.6800,  11.0900,  14.5100,  11.9500,  10.2400,  11.0900,\n",
              "           9.3900,   3.4100,  13.6500,  11.9500,  52.0500,   5.9700,   0.8500,\n",
              "           5.1200,   0.8500,   5.9700,   5.1200,   7.6800,   5.1200,   0.0000,\n",
              "           7.6800,   0.0000,   0.8500,   3.4100,   1.7100,   1.7100,  40.1100,\n",
              "           0.0000,   3.4100,   1.7100,   1.7100,   7.6800,   1.7100,   2.5600,\n",
              "           1.7100,   2.5600,   0.0000,   1.7100,   3.4100,   1.7100,   3.4100,\n",
              "           3.4100,   3.4100, 125.4400,  10.2400,  16.2100,  11.0900,  14.5100,\n",
              "           5.1200,   2.5600,   7.6800,  10.2400,  12.8000,   7.6800,  15.3600,\n",
              "          11.9500,  34.1300,   0.0000,   5.9700,   0.8500,   3.4100,   0.8500,\n",
              "           2.5600,   2.5600,   8.5300,   4.2700,   0.8500,   0.0000,   4.2700,\n",
              "          18.7700,   0.0000,   2.5600,   1.7100,   2.5600,   2.5600,   0.0000,\n",
              "           4.2700,   0.8500,   1.7100,   0.0000,   2.5600,  44.3700,   2.5600,\n",
              "           2.5600,   2.5600,   3.4100,   5.9700,   0.0000,   1.7100,  11.0900,\n",
              "           0.0000,   0.0000,  10.2400,   0.8500,   0.8500,   2.5600,  27.1700,\n",
              "           1.3300,   0.0000,   0.6700,   0.6700,   0.5100,   3.0400,   0.7600,\n",
              "           0.0000,   0.0000,   2.5600,   0.0000,   4.2700,   5.1200,   2.2800,\n",
              "           5.9700,  90.4500,   5.9700,   9.3900,   5.9700,   8.5300,  11.9500,\n",
              "           2.5600,   6.8300,   8.5300,   5.9700,   2.5600,   1.7100,   6.8300,\n",
              "           6.8300,   6.8300,  74.2400,   3.4100,  14.5100,   6.8300,   1.7100,\n",
              "           3.4100,  14.5100,   8.5300,   5.9700,   6.8300,   8.5300,  32.2800,\n",
              "           0.8500,   1.7100,   2.5600,   0.7100,   0.8500,   0.8500,   0.8500,\n",
              "           3.4100,   1.7100,   4.2700,   5.1200,   4.2700,   1.7100,   0.8500,\n",
              "           1.7100,   0.8500,  78.5100,   2.5600,   0.0000,   1.7100,  10.2400,\n",
              "           3.4100,   8.5300,  12.8000,   1.7100,   8.5300,  17.0700,   6.8300,\n",
              "           5.1200,   0.0000], device='cuda:0')}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "create_graph(0,0,0,device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T10:19:05.079352Z",
          "iopub.status.busy": "2023-07-06T10:19:05.078366Z",
          "iopub.status.idle": "2023-07-06T10:19:05.085230Z",
          "shell.execute_reply": "2023-07-06T10:19:05.083975Z",
          "shell.execute_reply.started": "2023-07-06T10:19:05.079300Z"
        },
        "id": "ZO3ljwu7coCe",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def create_geometric_graph(graph):\n",
        "    data = Data(\n",
        "        # Input graph.\n",
        "        x=graph[\"node_features\"],\n",
        "        #pos=pos,\n",
        "        edge_index=graph[\"edge_index\"],\n",
        "        edge_attr=graph[\"edge_features\"],\n",
        "        # Output node targets.\n",
        "        y=graph[\"node_targets\"],\n",
        "        num_nodes = len(graph[\"node_features\"])\n",
        "\n",
        "    )\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-06T10:19:05.087282Z",
          "iopub.status.busy": "2023-07-06T10:19:05.086872Z",
          "iopub.status.idle": "2023-07-06T10:19:06.732846Z",
          "shell.execute_reply": "2023-07-06T10:19:06.731610Z",
          "shell.execute_reply.started": "2023-07-06T10:19:05.087249Z"
        },
        "id": "Kr7yzgfocoCf",
        "outputId": "299b94e3-2a50-4b4d-848c-6b7caf62d077",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([170, 13])\n",
            "2028\n",
            "Data(x=[170, 13], edge_index=[2, 2028], edge_attr=[2028, 3], y=[170], num_nodes=170)\n"
          ]
        }
      ],
      "source": [
        "print(create_geometric_graph(create_graph(0,0,0,device)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_scatter import scatter_mean\n",
        "from torch_geometric.nn import MetaLayer\n",
        "\n",
        "class EdgeModel(torch.nn.Module):\n",
        "    def __init__(self, n_node_features, n_edge_features, hiddens, n_targets):\n",
        "        super().__init__()\n",
        "        self.edge_mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(2 * n_node_features + n_edge_features, hiddens),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hiddens, n_targets),\n",
        "        )\n",
        "\n",
        "    def forward(self, src, dest, edge_attr, u=None, batch=None):\n",
        "        #print(\"In edge model\")\n",
        "        #print(src, src.shape)\n",
        "        #print(dest, dest.shape)\n",
        "        #print(edge_attr, edge_attr.shape)\n",
        "        out = torch.cat([src, dest, edge_attr], 1)\n",
        "        out = self.edge_mlp(out)\n",
        "        #print(\"Exit edge model\")\n",
        "        return out\n",
        "\n",
        "\n",
        "class NodeModel(torch.nn.Module):\n",
        "    def __init__(self, n_node_features, hiddens, n_targets):\n",
        "        super(NodeModel, self).__init__()\n",
        "        self.node_mlp_1 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(n_node_features + hiddens, hiddens),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hiddens, hiddens),\n",
        "        )\n",
        "        self.node_mlp_2 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(n_node_features + hiddens, hiddens),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hiddens, n_targets),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
        "        #print(\"In node model\")\n",
        "        row, col = edge_index\n",
        "        out = torch.cat([x[col], edge_attr], dim=1)\n",
        "        out = self.node_mlp_1(out)\n",
        "        out = scatter_mean(out, row, dim=0, dim_size=x.size(0))\n",
        "        out = torch.cat([x, out], dim=1)\n",
        "        out = self.node_mlp_2(out)\n",
        "        #print(\"Exit node model\")\n",
        "        return out\n",
        "\n",
        "\n",
        "class MetaNet(torch.nn.Module):\n",
        "    def __init__(self, n_node_features, n_edge_features, num_hidden):\n",
        "        super(MetaNet, self).__init__()\n",
        "\n",
        "        # Input Layer\n",
        "        self.input = MetaLayer(\n",
        "            edge_model=EdgeModel(\n",
        "                n_node_features=n_node_features, n_edge_features=n_edge_features,\n",
        "                hiddens=num_hidden, n_targets=num_hidden),\n",
        "            node_model=NodeModel(n_node_features=n_node_features, hiddens=num_hidden, n_targets=num_hidden)\n",
        "            )\n",
        "\n",
        "        # Output Layer\n",
        "        self.output = MetaLayer(\n",
        "            edge_model=EdgeModel(\n",
        "                n_node_features=num_hidden, n_edge_features=num_hidden,\n",
        "                hiddens=num_hidden, n_targets=num_hidden),\n",
        "            node_model=NodeModel(n_node_features=num_hidden, hiddens=num_hidden, n_targets=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_attr, y = data.x, data.edge_index, data.edge_attr, data.y\n",
        "        #print(\"In meta model\")\n",
        "        x, edge_attr, _ = self.input(x, edge_index, edge_attr)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x, edge_attr, _ = self.output(x, edge_index, edge_attr)\n",
        "        #x = F.dropout(x, p=0.5, training=self.training)\n",
        "        #print(\"Exit meta model\")\n",
        "        return x\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, num_input, num_hidden):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.name = \"Net\"\n",
        "\n",
        "        # Input GCN layer.\n",
        "        self.conv1 = GCNConv(num_input, num_hidden)\n",
        "        self.conv2 = GCNConv(num_hidden, num_hidden)\n",
        "        self.conv3 = GCNConv(num_hidden, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, y = data.x, data.edge_index, data.y\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = F.dropout(x, training=self.training)\n",
        "\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        return x\n",
        "\n",
        "class AttentionNet(torch.nn.Module):\n",
        "    def __init__(self, num_input, num_hidden):\n",
        "        super(AttentionNet, self).__init__()\n",
        "\n",
        "        self.name = \"AttentionNet\"\n",
        "\n",
        "        # Input GCN layer.\n",
        "        self.conv1 = GATv2Conv(num_input, num_hidden,heads=2,concat=False)\n",
        "        self.conv2 = GATv2Conv(num_hidden, num_hidden,heads=2,concat=False)\n",
        "        self.conv3 = GATv2Conv(num_hidden, 1,num_heads=2,concat=False)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, y = data.x, data.edge_index, data.y\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = F.dropout(x, training=self.training)\n",
        "\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# mods start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_scatter import scatter_mean\n",
        "from torch_geometric.nn import MetaLayer\n",
        "\n",
        "class EdgeModel(torch.nn.Module):\n",
        "    def __init__(self, n_node_features, n_edge_features, hiddens, n_targets):\n",
        "        super().__init__()\n",
        "        self.edge_mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(2 * n_node_features + n_edge_features, hiddens),\n",
        "            torch.nn.LeakyReLU(0.2),\n",
        "            torch.nn.Linear(hiddens, n_targets),\n",
        "        )\n",
        "\n",
        "    def forward(self, src, dest, edge_attr, u=None, batch=None):\n",
        "        #print(\"In edge model\")\n",
        "        #print(src, src.shape)\n",
        "        #print(dest, dest.shape)\n",
        "        #print(edge_attr, edge_attr.shape)\n",
        "        out = torch.cat([src, dest, edge_attr], 1)\n",
        "        out = self.edge_mlp(out)\n",
        "        #print(\"Exit edge model\")\n",
        "        return out\n",
        "\n",
        "\n",
        "class NodeModel(torch.nn.Module):\n",
        "    def __init__(self, n_node_features, hiddens, n_targets):\n",
        "        super(NodeModel, self).__init__()\n",
        "        self.node_mlp_1 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(n_node_features + hiddens, hiddens),\n",
        "            torch.nn.LeakyReLU(0.2),\n",
        "            torch.nn.Linear(hiddens, hiddens),\n",
        "        )\n",
        "        self.node_mlp_2 = torch.nn.Sequential(\n",
        "            torch.nn.Linear(n_node_features + hiddens, hiddens),\n",
        "            torch.nn.LeakyReLU(0.2),\n",
        "            torch.nn.Linear(hiddens, n_targets),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
        "        #print(\"In node model\")\n",
        "        row, col = edge_index\n",
        "        out = torch.cat([x[col], edge_attr], dim=1)\n",
        "        out = self.node_mlp_1(out)\n",
        "        out = scatter_mean(out, row, dim=0, dim_size=x.size(0))\n",
        "        out = torch.cat([x, out], dim=1)\n",
        "        out = self.node_mlp_2(out)\n",
        "        #print(\"Exit node model\")\n",
        "        return out\n",
        "    \n",
        "class MetaNet(torch.nn.Module):\n",
        "    def __init__(self, n_node_features, n_edge_features, num_hidden):\n",
        "        super(MetaNet, self).__init__()\n",
        "\n",
        "        # Input Layer\n",
        "        self.input = MetaLayer(\n",
        "            edge_model=EdgeModel(\n",
        "                n_node_features=n_node_features, n_edge_features=n_edge_features,\n",
        "                hiddens=num_hidden, n_targets=num_hidden),\n",
        "            node_model=NodeModel(n_node_features=n_node_features, hiddens=num_hidden, n_targets=num_hidden)\n",
        "            )\n",
        "\n",
        "        # Output Layer\n",
        "        self.output = MetaLayer(\n",
        "            edge_model=EdgeModel(\n",
        "                n_node_features=num_hidden, n_edge_features=num_hidden,\n",
        "                hiddens=num_hidden, n_targets=num_hidden),\n",
        "            node_model=NodeModel(n_node_features=num_hidden, hiddens=num_hidden, n_targets=1)\n",
        "        )\n",
        "\n",
        "        # Attention Mechanism\n",
        "        self.attention = torch.nn.MultiheadAttention(embed_dim=num_hidden, num_heads=2, dropout=0.2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_attr, y = data.x, data.edge_index, data.edge_attr, data.y\n",
        "        x, edge_attr, _ = self.input(x, edge_index, edge_attr)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "\n",
        "        # Attention Mechanism\n",
        "        x = x.unsqueeze(0)\n",
        "        x, _ = self.attention(x, x, x)\n",
        "        x = x.squeeze(0)\n",
        "\n",
        "        x, edge_attr, _ = self.output(x, edge_index, edge_attr)\n",
        "        #x = F.dropout(x, p=0.5, training=self.training)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# mods end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T10:20:13.786261Z",
          "iopub.status.busy": "2023-07-06T10:20:13.785196Z",
          "iopub.status.idle": "2023-07-06T10:20:13.791675Z",
          "shell.execute_reply": "2023-07-06T10:20:13.790368Z",
          "shell.execute_reply.started": "2023-07-06T10:20:13.786213Z"
        },
        "id": "bPRRP8jlcoCh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "num_node_features = 13\n",
        "num_edge_features = 3\n",
        "num_hidden = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T10:20:32.677829Z",
          "iopub.status.busy": "2023-07-06T10:20:32.677457Z",
          "iopub.status.idle": "2023-07-06T10:20:34.309011Z",
          "shell.execute_reply": "2023-07-06T10:20:34.308022Z",
          "shell.execute_reply.started": "2023-07-06T10:20:32.677800Z"
        },
        "id": "u_oMx_LLcoCh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model  = MetaNet(num_node_features, num_edge_features, num_hidden).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2B_GYSk3nZ4",
        "outputId": "ba18e809-f5ec-430b-8cf4-e9d3a4b98dcf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MetaNet(\n",
              "  (input): MetaLayer(\n",
              "    edge_model=EdgeModel(\n",
              "    (edge_mlp): Sequential(\n",
              "      (0): Linear(in_features=29, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.2)\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "  ),\n",
              "    node_model=NodeModel(\n",
              "    (node_mlp_1): Sequential(\n",
              "      (0): Linear(in_features=141, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.2)\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "    (node_mlp_2): Sequential(\n",
              "      (0): Linear(in_features=141, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.2)\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "  ),\n",
              "    global_model=None\n",
              "  )\n",
              "  (output): MetaLayer(\n",
              "    edge_model=EdgeModel(\n",
              "    (edge_mlp): Sequential(\n",
              "      (0): Linear(in_features=384, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.2)\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "  ),\n",
              "    node_model=NodeModel(\n",
              "    (node_mlp_1): Sequential(\n",
              "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.2)\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "    (node_mlp_2): Sequential(\n",
              "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.2)\n",
              "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
              "    )\n",
              "  ),\n",
              "    global_model=None\n",
              "  )\n",
              "  (attention): MultiheadAttention(\n",
              "    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T10:20:46.697896Z",
          "iopub.status.busy": "2023-07-06T10:20:46.697337Z",
          "iopub.status.idle": "2023-07-06T10:20:46.703235Z",
          "shell.execute_reply": "2023-07-06T10:20:46.702092Z",
          "shell.execute_reply.started": "2023-07-06T10:20:46.697865Z"
        },
        "id": "tyIe36JGcoCh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(lr=1e-3,params=model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T10:20:46.966933Z",
          "iopub.status.busy": "2023-07-06T10:20:46.966338Z",
          "iopub.status.idle": "2023-07-06T10:20:46.975716Z",
          "shell.execute_reply": "2023-07-06T10:20:46.974673Z",
          "shell.execute_reply.started": "2023-07-06T10:20:46.966903Z"
        },
        "id": "kOdrIldDcoCi",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def train(dataset):\n",
        "    # Monitor training.\n",
        "    losses = []\n",
        "\n",
        "    # Put model in training mode!\n",
        "    model.train()\n",
        "    i=0\n",
        "    for i, batch in enumerate(dataset):\n",
        "        #print(\"misaa\")\n",
        "        # Training step.\n",
        "\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch)\n",
        "        loss = torch.sqrt(F.mse_loss(out.squeeze(), batch.y.squeeze()))\n",
        "        #print(f\"Training oss for {i}: {loss}\")\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Monitoring\n",
        "        losses.append(loss.item())\n",
        "        if(i == 1151): break\n",
        "    # Return training metrics.\n",
        "    return losses\n",
        "\n",
        "\n",
        "def evaluate(dataset):\n",
        "    # Monitor evaluation.\n",
        "    losses = []\n",
        "\n",
        "    # Validation (1)\n",
        "    model.eval()\n",
        "    i = 0\n",
        "    for i, batch in enumerate(dataset):\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        # Calculate validation losses.\n",
        "        out = model(batch)\n",
        "        loss = torch.sqrt(F.mse_loss(out.squeeze(), batch.y.squeeze()))\n",
        "\n",
        "        # Metric logging.\n",
        "        losses.append(loss.item())\n",
        "        if(i == 383): break\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T10:20:51.967254Z",
          "iopub.status.busy": "2023-07-06T10:20:51.966651Z",
          "iopub.status.idle": "2023-07-06T10:20:51.972521Z",
          "shell.execute_reply": "2023-07-06T10:20:51.971278Z",
          "shell.execute_reply.started": "2023-07-06T10:20:51.967213Z"
        },
        "id": "wxbz_k7dcoCp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-06T10:26:27.926786Z",
          "iopub.status.busy": "2023-07-06T10:26:27.926425Z",
          "iopub.status.idle": "2023-07-06T10:26:27.948100Z",
          "shell.execute_reply": "2023-07-06T10:26:27.947207Z",
          "shell.execute_reply.started": "2023-07-06T10:26:27.926757Z"
        },
        "id": "y-oS27FycoCp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# With train, validation and test data.\n",
        "import random\n",
        "import os\n",
        "from torch_geometric.data import InMemoryDataset, download_url, extract_zip\n",
        "# divide into training and testing points\n",
        "class CustomDataset(InMemoryDataset):\n",
        "    def __init__(self, split=\"train\", transform=None):\n",
        "        self.data = pd.read_csv(\"../required_format.csv\")\n",
        "        self.split = split\n",
        "        super(CustomDataset, self).__init__( split, transform)\n",
        "        #self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "        #self.data = pd.read_csv(\"deployment_with_int_map.csv\")\n",
        "        #self.data, self.slices = pd.read_csv(\"deployment_with_int_map.csv\")\n",
        "\n",
        "        # print(\"In init\")\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        # print(\"In raw_file_names\")\n",
        "        return [\"../required_format.csv\"]\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        # print(\"In processed_file_names\")\n",
        "        li = ['data_train_' + str(i) + '.pt' for i in range(1152)]+ ['data_valid_' + str(j) + '.pt' for j in range(1152, 1536)] + ['data_test_' + str(k) + '.pt' for k in range(1536, 1920)]\n",
        "        #print(li)\n",
        "        return ['data_train_' + str(i) + '.pt' for i in range(1152)]+ ['data_valid_' + str(j) + '.pt' for j in range(1152,1536)] + ['data_test_' + str(k) + '.pt' for k in range(384)]\n",
        "\n",
        "    def _download(self):\n",
        "        '''\n",
        "        print(\"In download\")\n",
        "        path = download_url(self.url, self.raw_dir)\n",
        "        extract_zip(path, self.raw_dir)\n",
        "        # The zip file is removed\n",
        "        os.unlink(path)\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "    def process(self):\n",
        "        print(\"In process\")\n",
        "        #df = pd.read_csv(self.raw_paths[0])\n",
        "        X = self.data[['0', '1', '2', '3', '4', '5', '6', '7','8', '9', '10', '11', 'wlan_code_index', 'x(m)', 'y(m)','z(m)',\n",
        "            'primary_channel', 'min_channel_allowed', 'max_channel_allowed', 'node_type','rssi',\n",
        "            'sinr', 'air_time_mean','channel_bonding_model','deployment']]\n",
        "        y = self.data.loc[:, [\"throughput\", \"deployment\"]]\n",
        "#         X_train = X.iloc[:183854, :]\n",
        "#         X_valid = X.iloc[183854:245139, :]\n",
        "#         X_test = X.iloc[245139:,:]\n",
        "#         print(X_test.columns)\n",
        "#         y_train = y.iloc[:183854, :]\n",
        "#         y_valid = y.iloc[183854:245139, :]\n",
        "#         y_test = y.iloc[245139:,:]\n",
        "        graphs = []\n",
        "        # print(\"Here\")\n",
        "        l = [i for i in range(1920)]\n",
        "        self.l_train = random.sample(l, 1152)\n",
        "        l = [x for x in l if x not in self.l_train]\n",
        "        self.l_valid = random.sample(l, 384)\n",
        "        l = [x for x in l if x not in self.l_valid]\n",
        "        self.l_test = l\n",
        "        count = 0\n",
        "        if(self.split == \"train\"):\n",
        "\n",
        "            for i in self.l_train:\n",
        "\n",
        "                #X_train = torch.tensor(X_train.to_numpy(), dtype=torch.float)\n",
        "                #y_train = torch.tensor(y_train.to_numpy(), dtype=torch.float)\n",
        "                graph = create_graph(X, y, i,device)\n",
        "\n",
        "                graph = create_geometric_graph(graph)\n",
        "                graphs.append(graph)\n",
        "\n",
        "                torch.save(graph, os.path.join(self.processed_dir, f'data_train_{count}.pt'))\n",
        "                count += 1\n",
        "        elif(self.split == \"valid\"):\n",
        "            for i in self.l_valid:\n",
        "                #X_test = torch.tensor(X_test.to_numpy(), dtype=torch.float)\n",
        "                #y_test = torch.tensor(y_test.to_numpy(), dtype=torch.float)\n",
        "                graph = create_graph(X, y, i,device)\n",
        "                graph = create_geometric_graph(graph)\n",
        "                graphs.append(graph)\n",
        "\n",
        "                torch.save(graph, os.path.join(self.processed_dir, f'data_valid_{count}.pt'))\n",
        "                count += 1\n",
        "        else:\n",
        "            for i in self.l_test:\n",
        "                #X_test = torch.tensor(X_test.to_numpy(), dtype=torch.float)\n",
        "                #y_test = torch.tensor(y_test.to_numpy(), dtype=torch.float)\n",
        "                graph = create_graph(X, y, i,device)\n",
        "                graph = create_geometric_graph(graph)\n",
        "                graphs.append(graph)\n",
        "\n",
        "                torch.save(graph, os.path.join(self.processed_dir, f'data_test_{count}.pt'))\n",
        "                count += 1\n",
        "        #return graphs[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        if(self.split == \"train\"):\n",
        "            #return len(self.processed_file_names[0])\n",
        "            return 1152\n",
        "        elif self.split == \"valid\":\n",
        "            return 384\n",
        "        else:\n",
        "            return 384\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #print(\"Part: \", self.processed_file_names[1])\n",
        "\n",
        "        if(self.split == \"train\"):\n",
        "            data = torch.load(os.path.join(self.processed_dir, f'data_train_{idx}.pt'))\n",
        "        elif(self.split == \"valid\"):\n",
        "            data = torch.load(os.path.join(self.processed_dir, f'data_valid_{idx}.pt'))\n",
        "        elif (self.split==\"test\"):\n",
        "            data = torch.load(os.path.join(self.processed_dir, f'data_test_{idx}.pt'))\n",
        "        return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\Asus\\\\Documents\\\\ml\\\\PIL\\\\gnn_pil_versions'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[WinError 3] The system cannot find the path specified: 'train'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Asus\\Documents\\ml\\PIL\\gnn_pil_versions\\pil_gnnv3.ipynb Cell 32\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#Y106sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# delete train test and valid directories\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#Y106sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mshutil\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#Y106sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m shutil\u001b[39m.\u001b[39;49mrmtree(\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#Y106sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m shutil\u001b[39m.\u001b[39mrmtree(\u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#Y106sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m shutil\u001b[39m.\u001b[39mrmtree(\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\torch-gpu\\lib\\shutil.py:740\u001b[0m, in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[0;32m    738\u001b[0m     \u001b[39m# can't continue even if onerror hook returns\u001b[39;00m\n\u001b[0;32m    739\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 740\u001b[0m \u001b[39mreturn\u001b[39;00m _rmtree_unsafe(path, onerror)\n",
            "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\torch-gpu\\lib\\shutil.py:599\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    597\u001b[0m         entries \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(scandir_it)\n\u001b[0;32m    598\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m--> 599\u001b[0m     onerror(os\u001b[39m.\u001b[39;49mscandir, path, sys\u001b[39m.\u001b[39;49mexc_info())\n\u001b[0;32m    600\u001b[0m     entries \u001b[39m=\u001b[39m []\n\u001b[0;32m    601\u001b[0m \u001b[39mfor\u001b[39;00m entry \u001b[39min\u001b[39;00m entries:\n",
            "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\torch-gpu\\lib\\shutil.py:596\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_rmtree_unsafe\u001b[39m(path, onerror):\n\u001b[0;32m    595\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 596\u001b[0m         \u001b[39mwith\u001b[39;00m os\u001b[39m.\u001b[39;49mscandir(path) \u001b[39mas\u001b[39;00m scandir_it:\n\u001b[0;32m    597\u001b[0m             entries \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(scandir_it)\n\u001b[0;32m    598\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'train'"
          ]
        }
      ],
      "source": [
        "# delete train test and valid directories\n",
        "import shutil\n",
        "shutil.rmtree(\"train\")\n",
        "shutil.rmtree(\"valid\")\n",
        "shutil.rmtree(\"test\")\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-06T10:28:36.317595Z",
          "iopub.status.busy": "2023-07-06T10:28:36.315907Z"
        },
        "id": "dXdnYhegcoCq",
        "outputId": "a4cbe747-097a-4108-96ac-53e7c455792b",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In process\n",
            "torch.Size([173, 13])\n",
            "1720\n",
            "torch.Size([178, 13])\n",
            "2124\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([150, 13])\n",
            "1490\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([196, 13])\n",
            "2340\n",
            "torch.Size([145, 13])\n",
            "1440\n",
            "torch.Size([192, 13])\n",
            "2292\n",
            "torch.Size([184, 13])\n",
            "2196\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([135, 13])\n",
            "1072\n",
            "torch.Size([167, 13])\n",
            "1660\n",
            "torch.Size([190, 13])\n",
            "2268\n",
            "torch.Size([172, 13])\n",
            "2052\n",
            "torch.Size([160, 13])\n",
            "1590\n",
            "torch.Size([202, 13])\n",
            "2412\n",
            "torch.Size([125, 13])\n",
            "992\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([171, 13])\n",
            "1700\n",
            "torch.Size([117, 13])\n",
            "928\n",
            "torch.Size([197, 13])\n",
            "2352\n",
            "torch.Size([212, 13])\n",
            "2532\n",
            "torch.Size([162, 13])\n",
            "1610\n",
            "torch.Size([194, 13])\n",
            "2316\n",
            "torch.Size([204, 13])\n",
            "2436\n",
            "torch.Size([134, 13])\n",
            "1064\n",
            "torch.Size([149, 13])\n",
            "1480\n",
            "torch.Size([142, 13])\n",
            "1410\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([130, 13])\n",
            "1032\n",
            "torch.Size([186, 13])\n",
            "2220\n",
            "torch.Size([176, 13])\n",
            "1750\n",
            "torch.Size([177, 13])\n",
            "2112\n",
            "torch.Size([161, 13])\n",
            "1600\n",
            "torch.Size([200, 13])\n",
            "2388\n",
            "torch.Size([195, 13])\n",
            "2328\n",
            "torch.Size([191, 13])\n",
            "2280\n",
            "torch.Size([187, 13])\n",
            "2232\n",
            "torch.Size([124, 13])\n",
            "984\n",
            "torch.Size([169, 13])\n",
            "2016\n",
            "torch.Size([205, 13])\n",
            "2448\n",
            "torch.Size([183, 13])\n",
            "1820\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Asus\\Documents\\ml\\PIL\\gnn_pil_versions\\pil_gnnv3.ipynb Cell 33\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dataset_train \u001b[39m=\u001b[39m CustomDataset( split\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dataset_valid \u001b[39m=\u001b[39m CustomDataset( split\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dataset_test \u001b[39m=\u001b[39m CustomDataset( split\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[1;32mc:\\Users\\Asus\\Documents\\ml\\PIL\\gnn_pil_versions\\pil_gnnv3.ipynb Cell 33\u001b[0m in \u001b[0;36mCustomDataset.__init__\u001b[1;34m(self, split, transform)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X40sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39m../required_format.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X40sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit \u001b[39m=\u001b[39m split\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X40sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39msuper\u001b[39;49m(CustomDataset, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m( split, transform)\n",
            "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:57\u001b[0m, in \u001b[0;36mInMemoryDataset.__init__\u001b[1;34m(self, root, transform, pre_transform, pre_filter, log)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     50\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     51\u001b[0m     root: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m     log: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     56\u001b[0m ):\n\u001b[1;32m---> 57\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(root, transform, pre_transform, pre_filter, log)\n\u001b[0;32m     58\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     59\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslices \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch_geometric\\data\\dataset.py:97\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[1;34m(self, root, transform, pre_transform, pre_filter, log)\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download()\n\u001b[0;32m     96\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_process:\n\u001b[1;32m---> 97\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process()\n",
            "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch_geometric\\data\\dataset.py:230\u001b[0m, in \u001b[0;36mDataset._process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mProcessing...\u001b[39m\u001b[39m'\u001b[39m, file\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39mstderr)\n\u001b[0;32m    229\u001b[0m makedirs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessed_dir)\n\u001b[1;32m--> 230\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess()\n\u001b[0;32m    232\u001b[0m path \u001b[39m=\u001b[39m osp\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessed_dir, \u001b[39m'\u001b[39m\u001b[39mpre_transform.pt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    233\u001b[0m torch\u001b[39m.\u001b[39msave(_repr(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_transform), path)\n",
            "\u001b[1;32mc:\\Users\\Asus\\Documents\\ml\\PIL\\gnn_pil_versions\\pil_gnnv3.ipynb Cell 33\u001b[0m in \u001b[0;36mCustomDataset.process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X40sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39mif\u001b[39;00m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X40sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_train:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X40sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X40sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m         \u001b[39m#X_train = torch.tensor(X_train.to_numpy(), dtype=torch.float)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X40sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m         \u001b[39m#y_train = torch.tensor(y_train.to_numpy(), dtype=torch.float)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X40sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m         graph \u001b[39m=\u001b[39m create_graph(X, y, i,device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X40sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m         graph \u001b[39m=\u001b[39m create_geometric_graph(graph)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X40sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m         graphs\u001b[39m.\u001b[39mappend(graph)\n",
            "\u001b[1;32mc:\\Users\\Asus\\Documents\\ml\\PIL\\gnn_pil_versions\\pil_gnnv3.ipynb Cell 33\u001b[0m in \u001b[0;36mcreate_graph\u001b[1;34m(split, split_y, deployment, device)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X40sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(dep)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X40sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(dep)):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X40sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         \u001b[39mif\u001b[39;00m (i \u001b[39m!=\u001b[39m j \u001b[39mand\u001b[39;00m (dep[\u001b[39m\"\u001b[39m\u001b[39mnode_type\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39miloc[i] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m dep[\u001b[39m\"\u001b[39m\u001b[39mnode_type\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39miloc[j] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)) \u001b[39mor\u001b[39;00m (i \u001b[39m!=\u001b[39mj \u001b[39mand\u001b[39;00m (dep[\u001b[39m\"\u001b[39m\u001b[39mnode_type\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39miloc[i] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m dep[\u001b[39m\"\u001b[39;49m\u001b[39mnode_type\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39miloc[j] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X40sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m             edges\u001b[39m.\u001b[39mappend([i,j])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X40sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m edges2\u001b[39m=\u001b[39medges\n",
            "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\pandas\\core\\frame.py:3466\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3464\u001b[0m check_deprecated_indexers(key)\n\u001b[0;32m   3465\u001b[0m key \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39mitem_from_zerodim(key)\n\u001b[1;32m-> 3466\u001b[0m key \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(key, \u001b[39mself\u001b[39m)\n\u001b[0;32m   3468\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3469\u001b[0m     \u001b[39m# is_iterator to exclude generator e.g. test_getitem_listlike\u001b[39;00m\n\u001b[0;32m   3470\u001b[0m     \u001b[39m# shortcut if the key is in columns\u001b[39;00m\n\u001b[0;32m   3471\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique \u001b[39mand\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "dataset_train = CustomDataset( split='train')\n",
        "dataset_valid = CustomDataset( split='valid')\n",
        "dataset_test = CustomDataset( split='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "jrn8HVXecoCr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import shuffle\n",
        "from torch_geometric.data import DataLoader\n",
        "train_loader = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(dataset_valid, batch_size=3, shuffle=True)\n",
        "test_loader = DataLoader(dataset_test, batch_size=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lip8xSKacoCr",
        "outputId": "41ac8408-7810-490e-d096-e84ecebe4594",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataBatch(x=[474, 13], edge_index=[2, 4842], edge_attr=[4842, 3], y=[474], num_nodes=474, batch=[474], ptr=[4])\n"
          ]
        }
      ],
      "source": [
        "for batch in valid_loader:\n",
        "    print(batch)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeVD_pAqcoCr",
        "outputId": "d9d4d007-fa56-40c2-f632-36797c49c145",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Len of Training loss: 36, Average loss: 10.402308212386238\n",
            "Len of Validation loss: 128, Average loss: 7.455494046211243\n",
            "Epoch: 1, Len of Training loss: 36, Average loss: 8.873201568921408\n",
            "Len of Validation loss: 128, Average loss: 7.292831236496568\n",
            "Epoch: 2, Len of Training loss: 36, Average loss: 8.665252645810446\n",
            "Len of Validation loss: 128, Average loss: 7.446019722148776\n",
            "Epoch: 3, Len of Training loss: 36, Average loss: 8.413640724288094\n",
            "Len of Validation loss: 128, Average loss: 7.06231501698494\n",
            "Epoch: 4, Len of Training loss: 36, Average loss: 8.709049079153273\n",
            "Len of Validation loss: 128, Average loss: 6.943327439948916\n",
            "Epoch: 5, Len of Training loss: 36, Average loss: 8.196843001577589\n",
            "Len of Validation loss: 128, Average loss: 7.344432560727\n",
            "Epoch: 6, Len of Training loss: 36, Average loss: 7.933055718739827\n",
            "Len of Validation loss: 128, Average loss: 6.255120639689267\n",
            "Epoch: 7, Len of Training loss: 36, Average loss: 7.487523847156101\n",
            "Len of Validation loss: 128, Average loss: 6.3075099885463715\n",
            "Epoch: 8, Len of Training loss: 36, Average loss: 7.285996245013343\n",
            "Len of Validation loss: 128, Average loss: 6.191566780209541\n",
            "Epoch: 9, Len of Training loss: 36, Average loss: 7.021205597453648\n",
            "Len of Validation loss: 128, Average loss: 5.602986415848136\n",
            "Epoch: 10, Len of Training loss: 36, Average loss: 6.891246981090969\n",
            "Len of Validation loss: 128, Average loss: 5.939055566675961\n",
            "Epoch: 11, Len of Training loss: 36, Average loss: 6.642402741644117\n",
            "Len of Validation loss: 128, Average loss: 5.135223845485598\n",
            "Epoch: 12, Len of Training loss: 36, Average loss: 6.739770240253872\n",
            "Len of Validation loss: 128, Average loss: 6.210650205612183\n",
            "Epoch: 13, Len of Training loss: 36, Average loss: 7.044994791348775\n",
            "Len of Validation loss: 128, Average loss: 5.342883390374482\n",
            "Epoch: 14, Len of Training loss: 36, Average loss: 6.439735452334086\n",
            "Len of Validation loss: 128, Average loss: 6.060212981887162\n",
            "Epoch: 15, Len of Training loss: 36, Average loss: 7.216847194565667\n",
            "Len of Validation loss: 128, Average loss: 5.397846622392535\n",
            "Epoch: 16, Len of Training loss: 36, Average loss: 6.204186095131768\n",
            "Len of Validation loss: 128, Average loss: 5.044211120344698\n",
            "Epoch: 17, Len of Training loss: 36, Average loss: 6.030889272689819\n",
            "Len of Validation loss: 128, Average loss: 5.661192431114614\n",
            "Epoch: 18, Len of Training loss: 36, Average loss: 6.625727746221754\n",
            "Len of Validation loss: 128, Average loss: 5.572027174755931\n",
            "Epoch: 19, Len of Training loss: 36, Average loss: 6.914050658543904\n",
            "Len of Validation loss: 128, Average loss: 12.677787896245718\n",
            "Epoch: 20, Len of Training loss: 36, Average loss: 8.279667814572653\n",
            "Len of Validation loss: 128, Average loss: 5.5411838330328465\n",
            "Epoch: 21, Len of Training loss: 36, Average loss: 6.965204318364461\n",
            "Len of Validation loss: 128, Average loss: 5.371871831826866\n",
            "Epoch: 22, Len of Training loss: 36, Average loss: 6.271860427326626\n",
            "Len of Validation loss: 128, Average loss: 5.312696410808712\n",
            "Epoch: 23, Len of Training loss: 36, Average loss: 6.060478395885891\n",
            "Len of Validation loss: 128, Average loss: 6.091376933734864\n",
            "Epoch: 24, Len of Training loss: 36, Average loss: 6.047813600964016\n",
            "Len of Validation loss: 128, Average loss: 4.713222546502948\n",
            "Epoch: 25, Len of Training loss: 36, Average loss: 5.637164460288154\n",
            "Len of Validation loss: 128, Average loss: 4.382609678432345\n",
            "Epoch: 26, Len of Training loss: 36, Average loss: 5.539912939071655\n",
            "Len of Validation loss: 128, Average loss: 4.9383668401278555\n",
            "Epoch: 27, Len of Training loss: 36, Average loss: 5.3868550062179565\n",
            "Len of Validation loss: 128, Average loss: 4.453251539729536\n",
            "Epoch: 28, Len of Training loss: 36, Average loss: 5.1161091857486305\n",
            "Len of Validation loss: 128, Average loss: 4.149862756021321\n",
            "Epoch: 29, Len of Training loss: 36, Average loss: 4.689464502864414\n",
            "Len of Validation loss: 128, Average loss: 4.08366722939536\n",
            "Epoch: 30, Len of Training loss: 36, Average loss: 4.389517724514008\n",
            "Len of Validation loss: 128, Average loss: 4.529633803293109\n",
            "Epoch: 31, Len of Training loss: 36, Average loss: 4.727877040704091\n",
            "Len of Validation loss: 128, Average loss: 3.665359442587942\n",
            "Epoch: 32, Len of Training loss: 36, Average loss: 4.871673981348674\n",
            "Len of Validation loss: 128, Average loss: 3.788763399235904\n",
            "Epoch: 33, Len of Training loss: 36, Average loss: 4.378693170017666\n",
            "Len of Validation loss: 128, Average loss: 3.229752846993506\n",
            "Epoch: 34, Len of Training loss: 36, Average loss: 3.9715867903497486\n",
            "Len of Validation loss: 128, Average loss: 3.3448521574027836\n",
            "Epoch: 35, Len of Training loss: 36, Average loss: 3.5600208044052124\n",
            "Len of Validation loss: 128, Average loss: 3.6653956877999008\n",
            "Epoch: 36, Len of Training loss: 36, Average loss: 5.115215394232008\n",
            "Len of Validation loss: 128, Average loss: 5.688055066391826\n",
            "Epoch: 37, Len of Training loss: 36, Average loss: 4.871223615275489\n",
            "Len of Validation loss: 128, Average loss: 3.220234236679971\n",
            "Epoch: 38, Len of Training loss: 36, Average loss: 3.936532457669576\n",
            "Len of Validation loss: 128, Average loss: 4.057128614746034\n",
            "Epoch: 39, Len of Training loss: 36, Average loss: 3.9783018363846674\n",
            "Len of Validation loss: 128, Average loss: 3.0345116066746414\n",
            "Epoch: 40, Len of Training loss: 36, Average loss: 3.475755194822947\n",
            "Len of Validation loss: 128, Average loss: 3.734012817032635\n",
            "Epoch: 41, Len of Training loss: 36, Average loss: 3.4297585686047873\n",
            "Len of Validation loss: 128, Average loss: 2.8880159412510693\n",
            "Epoch: 42, Len of Training loss: 36, Average loss: 3.069116923544142\n",
            "Len of Validation loss: 128, Average loss: 2.5810997714288533\n",
            "Epoch: 43, Len of Training loss: 36, Average loss: 2.906208058198293\n",
            "Len of Validation loss: 128, Average loss: 2.861521535553038\n",
            "Epoch: 44, Len of Training loss: 36, Average loss: 2.9154760572645397\n",
            "Len of Validation loss: 128, Average loss: 2.453501719981432\n",
            "Epoch: 45, Len of Training loss: 36, Average loss: 4.2344732483228045\n",
            "Len of Validation loss: 128, Average loss: 4.460060466080904\n",
            "Epoch: 46, Len of Training loss: 36, Average loss: 4.865117914146847\n",
            "Len of Validation loss: 128, Average loss: 8.515914339572191\n",
            "Epoch: 47, Len of Training loss: 36, Average loss: 5.284238153033787\n",
            "Len of Validation loss: 128, Average loss: 2.826507853809744\n",
            "Epoch: 48, Len of Training loss: 36, Average loss: 3.1559711363580494\n",
            "Len of Validation loss: 128, Average loss: 2.518190802074969\n",
            "Epoch: 49, Len of Training loss: 36, Average loss: 3.131753941377004\n",
            "Len of Validation loss: 128, Average loss: 3.167450888082385\n",
            "Epoch: 50, Len of Training loss: 36, Average loss: 3.4805745283762612\n",
            "Len of Validation loss: 128, Average loss: 2.4703502226620913\n",
            "Epoch: 51, Len of Training loss: 36, Average loss: 3.1993938353326588\n",
            "Len of Validation loss: 128, Average loss: 3.0827528457157314\n",
            "Epoch: 52, Len of Training loss: 36, Average loss: 3.089574999279446\n",
            "Len of Validation loss: 128, Average loss: 2.483086669817567\n",
            "Epoch: 53, Len of Training loss: 36, Average loss: 3.138924870226118\n",
            "Len of Validation loss: 128, Average loss: 2.3693974395282567\n",
            "Epoch: 54, Len of Training loss: 36, Average loss: 2.9495903286668987\n",
            "Len of Validation loss: 128, Average loss: 2.728954811580479\n",
            "Epoch: 55, Len of Training loss: 36, Average loss: 2.6671099265416465\n",
            "Len of Validation loss: 128, Average loss: 2.311954627279192\n",
            "Epoch: 56, Len of Training loss: 36, Average loss: 2.605699214670393\n",
            "Len of Validation loss: 128, Average loss: 2.2332676430232823\n",
            "Epoch: 57, Len of Training loss: 36, Average loss: 2.6512008772956\n",
            "Len of Validation loss: 128, Average loss: 2.712679853197187\n",
            "Epoch: 58, Len of Training loss: 36, Average loss: 2.7719771133528814\n",
            "Len of Validation loss: 128, Average loss: 2.473504675552249\n",
            "Epoch: 59, Len of Training loss: 36, Average loss: 2.5985843936602273\n",
            "Len of Validation loss: 128, Average loss: 2.241407208610326\n",
            "Epoch: 60, Len of Training loss: 36, Average loss: 2.529329518477122\n",
            "Len of Validation loss: 128, Average loss: 2.320784480776638\n",
            "Epoch: 61, Len of Training loss: 36, Average loss: 2.7221510211626687\n",
            "Len of Validation loss: 128, Average loss: 2.3927093069069088\n",
            "Epoch: 62, Len of Training loss: 36, Average loss: 2.7899004055394068\n",
            "Len of Validation loss: 128, Average loss: 2.7842858154326677\n",
            "Epoch: 63, Len of Training loss: 36, Average loss: 2.614920053217146\n",
            "Len of Validation loss: 128, Average loss: 2.2491044434718788\n",
            "Epoch: 64, Len of Training loss: 36, Average loss: 2.5933593412240348\n",
            "Len of Validation loss: 128, Average loss: 2.190891874022782\n",
            "Epoch: 65, Len of Training loss: 36, Average loss: 2.5327218406730228\n",
            "Len of Validation loss: 128, Average loss: 2.1222906410694122\n",
            "Epoch: 66, Len of Training loss: 36, Average loss: 2.57745698094368\n",
            "Len of Validation loss: 128, Average loss: 2.1713618435896933\n",
            "Epoch: 67, Len of Training loss: 36, Average loss: 2.440112461646398\n",
            "Len of Validation loss: 128, Average loss: 2.0446558985859156\n",
            "Epoch: 68, Len of Training loss: 36, Average loss: 2.787712958123949\n",
            "Len of Validation loss: 128, Average loss: 2.0837097261101007\n",
            "Epoch: 69, Len of Training loss: 36, Average loss: 2.5754984849029117\n",
            "Len of Validation loss: 128, Average loss: 2.2821922628208995\n",
            "Epoch: 70, Len of Training loss: 36, Average loss: 2.5370351572831473\n",
            "Len of Validation loss: 128, Average loss: 2.1332654682919383\n",
            "Epoch: 71, Len of Training loss: 36, Average loss: 2.529165463315116\n",
            "Len of Validation loss: 128, Average loss: 2.3031771504320204\n",
            "Epoch: 72, Len of Training loss: 36, Average loss: 2.708328197399775\n",
            "Len of Validation loss: 128, Average loss: 2.412981405388564\n",
            "Epoch: 73, Len of Training loss: 36, Average loss: 2.728577666812473\n",
            "Len of Validation loss: 128, Average loss: 2.1745147444307804\n",
            "Epoch: 74, Len of Training loss: 36, Average loss: 2.507672746976217\n",
            "Len of Validation loss: 128, Average loss: 1.993177684955299\n",
            "Epoch: 75, Len of Training loss: 36, Average loss: 2.437428504228592\n",
            "Len of Validation loss: 128, Average loss: 1.9497209922410548\n",
            "Epoch: 76, Len of Training loss: 36, Average loss: 2.447045041455163\n",
            "Len of Validation loss: 128, Average loss: 2.1386739583685994\n",
            "Epoch: 77, Len of Training loss: 36, Average loss: 2.4190854529539743\n",
            "Len of Validation loss: 128, Average loss: 2.534714404027909\n",
            "Epoch: 78, Len of Training loss: 36, Average loss: 2.586204184426202\n",
            "Len of Validation loss: 128, Average loss: 2.256396634504199\n",
            "Epoch: 79, Len of Training loss: 36, Average loss: 2.435119516319699\n",
            "Len of Validation loss: 128, Average loss: 2.054586747661233\n",
            "Epoch: 80, Len of Training loss: 36, Average loss: 2.3326912191179066\n",
            "Len of Validation loss: 128, Average loss: 2.2438009059987962\n",
            "Epoch: 81, Len of Training loss: 36, Average loss: 2.573736243777805\n",
            "Len of Validation loss: 128, Average loss: 2.187304782215506\n",
            "Epoch: 82, Len of Training loss: 36, Average loss: 2.4903995427820416\n",
            "Len of Validation loss: 128, Average loss: 2.722336368635297\n",
            "Epoch: 83, Len of Training loss: 36, Average loss: 2.4610230359766216\n",
            "Len of Validation loss: 128, Average loss: 2.086550203152001\n",
            "Epoch: 84, Len of Training loss: 36, Average loss: 2.556310339106454\n",
            "Len of Validation loss: 128, Average loss: 2.131848830031231\n",
            "Epoch: 85, Len of Training loss: 36, Average loss: 4.686838325526979\n",
            "Len of Validation loss: 128, Average loss: 53.11537592858076\n",
            "Epoch: 86, Len of Training loss: 36, Average loss: 21.55470148722331\n",
            "Len of Validation loss: 128, Average loss: 6.8873600251972675\n",
            "Epoch: 87, Len of Training loss: 36, Average loss: 7.067331671714783\n",
            "Len of Validation loss: 128, Average loss: 5.0873449724167585\n",
            "Epoch: 88, Len of Training loss: 36, Average loss: 6.2067498498492775\n",
            "Len of Validation loss: 128, Average loss: 4.652694365940988\n",
            "Epoch: 89, Len of Training loss: 36, Average loss: 5.121507571803199\n",
            "Len of Validation loss: 128, Average loss: 3.60529313236475\n",
            "Epoch: 90, Len of Training loss: 36, Average loss: 3.4224859211179943\n",
            "Len of Validation loss: 128, Average loss: 2.604464122094214\n",
            "Epoch: 91, Len of Training loss: 36, Average loss: 3.0999127758873835\n",
            "Len of Validation loss: 128, Average loss: 2.5607283180579543\n",
            "Epoch: 92, Len of Training loss: 36, Average loss: 3.015984833240509\n",
            "Len of Validation loss: 128, Average loss: 2.867168365046382\n",
            "Epoch: 93, Len of Training loss: 36, Average loss: 2.846301347017288\n",
            "Len of Validation loss: 128, Average loss: 3.834606099873781\n",
            "Epoch: 94, Len of Training loss: 36, Average loss: 2.9260273509555392\n",
            "Len of Validation loss: 128, Average loss: 2.2696034745313227\n",
            "Epoch: 95, Len of Training loss: 36, Average loss: 3.042770004934735\n",
            "Len of Validation loss: 128, Average loss: 2.9364895676262677\n",
            "Epoch: 96, Len of Training loss: 36, Average loss: 2.7250927488009133\n",
            "Len of Validation loss: 128, Average loss: 2.2831098050810397\n",
            "Epoch: 97, Len of Training loss: 36, Average loss: 5.572913395033942\n",
            "Len of Validation loss: 128, Average loss: 3.6073186341673136\n",
            "Epoch: 98, Len of Training loss: 36, Average loss: 3.3202991286913552\n",
            "Len of Validation loss: 128, Average loss: 2.4032039679586887\n",
            "Epoch: 99, Len of Training loss: 36, Average loss: 2.6879403789838157\n",
            "Len of Validation loss: 128, Average loss: 2.5127046988345683\n",
            "Epoch: 100, Len of Training loss: 36, Average loss: 2.682261268297831\n",
            "Len of Validation loss: 128, Average loss: 2.356581963598728\n",
            "Epoch: 101, Len of Training loss: 36, Average loss: 2.5830018222332\n",
            "Len of Validation loss: 128, Average loss: 2.2417945587076247\n",
            "Epoch: 102, Len of Training loss: 36, Average loss: 2.8420904080073037\n",
            "Len of Validation loss: 128, Average loss: 2.3971732202917337\n",
            "Epoch: 103, Len of Training loss: 36, Average loss: 2.773769815762838\n",
            "Len of Validation loss: 128, Average loss: 2.2319359299726784\n",
            "Epoch: 104, Len of Training loss: 36, Average loss: 2.535612245400747\n",
            "Len of Validation loss: 128, Average loss: 2.251319552306086\n",
            "Epoch: 105, Len of Training loss: 36, Average loss: 2.4641314148902893\n",
            "Len of Validation loss: 128, Average loss: 2.230727787129581\n",
            "Epoch: 106, Len of Training loss: 36, Average loss: 2.5881416963206396\n",
            "Len of Validation loss: 128, Average loss: 2.210210701916367\n",
            "Epoch: 107, Len of Training loss: 36, Average loss: 2.505960848596361\n",
            "Len of Validation loss: 128, Average loss: 2.272140915505588\n",
            "Epoch: 108, Len of Training loss: 36, Average loss: 2.90771891673406\n",
            "Len of Validation loss: 128, Average loss: 2.1658579711802304\n",
            "Epoch: 109, Len of Training loss: 36, Average loss: 2.6205059157477484\n",
            "Len of Validation loss: 128, Average loss: 2.4826944349333644\n",
            "Epoch: 110, Len of Training loss: 36, Average loss: 2.5850704378551908\n",
            "Len of Validation loss: 128, Average loss: 2.2526704017072916\n",
            "Epoch: 111, Len of Training loss: 36, Average loss: 2.4560573498408\n",
            "Len of Validation loss: 128, Average loss: 2.3332580565474927\n",
            "Epoch: 112, Len of Training loss: 36, Average loss: 2.5891220735179052\n",
            "Len of Validation loss: 128, Average loss: 2.285042597912252\n",
            "Epoch: 113, Len of Training loss: 36, Average loss: 2.6247059835327997\n",
            "Len of Validation loss: 128, Average loss: 2.1330332863144577\n",
            "Epoch: 114, Len of Training loss: 36, Average loss: 2.6207414203219943\n",
            "Len of Validation loss: 128, Average loss: 2.094649069942534\n",
            "Epoch: 115, Len of Training loss: 36, Average loss: 2.5434466203053794\n",
            "Len of Validation loss: 128, Average loss: 2.097687116358429\n",
            "Epoch: 116, Len of Training loss: 36, Average loss: 2.4067553844716816\n",
            "Len of Validation loss: 128, Average loss: 2.1427549170330167\n",
            "Epoch: 117, Len of Training loss: 36, Average loss: 2.431814127498203\n",
            "Len of Validation loss: 128, Average loss: 2.070932189002633\n",
            "Epoch: 118, Len of Training loss: 36, Average loss: 2.4070865511894226\n",
            "Len of Validation loss: 128, Average loss: 2.8634672490879893\n",
            "Epoch: 119, Len of Training loss: 36, Average loss: 2.6233086155520544\n",
            "Len of Validation loss: 128, Average loss: 2.1107662431895733\n",
            "Epoch: 120, Len of Training loss: 36, Average loss: 2.534508297840754\n",
            "Len of Validation loss: 128, Average loss: 2.1350742890499532\n",
            "Epoch: 121, Len of Training loss: 36, Average loss: 2.536845578087701\n",
            "Len of Validation loss: 128, Average loss: 2.3498573997057974\n",
            "Epoch: 122, Len of Training loss: 36, Average loss: 2.7480071319474115\n",
            "Len of Validation loss: 128, Average loss: 2.4150431640446186\n",
            "Epoch: 123, Len of Training loss: 36, Average loss: 2.498733639717102\n",
            "Len of Validation loss: 128, Average loss: 2.1449789870530367\n",
            "Epoch: 124, Len of Training loss: 36, Average loss: 2.403502775563134\n",
            "Len of Validation loss: 128, Average loss: 2.0682234363630414\n",
            "Epoch: 125, Len of Training loss: 36, Average loss: 2.390765623913871\n",
            "Len of Validation loss: 128, Average loss: 2.046432016417384\n",
            "Epoch: 126, Len of Training loss: 36, Average loss: 2.3284326924218073\n",
            "Len of Validation loss: 128, Average loss: 2.0376080218702555\n",
            "Epoch: 127, Len of Training loss: 36, Average loss: 2.3456953002346888\n",
            "Len of Validation loss: 128, Average loss: 2.084976594429463\n",
            "Epoch: 128, Len of Training loss: 36, Average loss: 2.483247776826223\n",
            "Len of Validation loss: 128, Average loss: 2.078303834423423\n",
            "Epoch: 129, Len of Training loss: 36, Average loss: 2.316535555654102\n",
            "Len of Validation loss: 128, Average loss: 2.0257332073524594\n",
            "Epoch: 130, Len of Training loss: 36, Average loss: 2.355779333247079\n",
            "Len of Validation loss: 128, Average loss: 2.1039901934564114\n",
            "Epoch: 131, Len of Training loss: 36, Average loss: 2.4974833826224008\n",
            "Len of Validation loss: 128, Average loss: 2.513760984642431\n",
            "Epoch: 132, Len of Training loss: 36, Average loss: 2.5328621400727167\n",
            "Len of Validation loss: 128, Average loss: 2.1038787569850683\n",
            "Epoch: 133, Len of Training loss: 36, Average loss: 2.713203751378589\n",
            "Len of Validation loss: 128, Average loss: 2.5707681849598885\n",
            "Epoch: 134, Len of Training loss: 36, Average loss: 2.5821072922812567\n",
            "Len of Validation loss: 128, Average loss: 2.067200814373791\n",
            "Epoch: 135, Len of Training loss: 36, Average loss: 2.8188896477222443\n",
            "Len of Validation loss: 128, Average loss: 2.900205288082361\n",
            "Epoch: 136, Len of Training loss: 36, Average loss: 3.59851121240192\n",
            "Len of Validation loss: 128, Average loss: 2.256156787276268\n",
            "Epoch: 137, Len of Training loss: 36, Average loss: 2.699287864896986\n",
            "Len of Validation loss: 128, Average loss: 2.226368608418852\n",
            "Epoch: 138, Len of Training loss: 36, Average loss: 2.5255306793583765\n",
            "Len of Validation loss: 128, Average loss: 2.115085263736546\n",
            "Epoch: 139, Len of Training loss: 36, Average loss: 2.356813993718889\n",
            "Len of Validation loss: 128, Average loss: 2.051069234497845\n",
            "Epoch: 140, Len of Training loss: 36, Average loss: 2.6036349866125317\n",
            "Len of Validation loss: 128, Average loss: 2.176930376328528\n",
            "Epoch: 141, Len of Training loss: 36, Average loss: 2.4959154460165234\n",
            "Len of Validation loss: 128, Average loss: 2.1061398941092193\n",
            "Epoch: 142, Len of Training loss: 36, Average loss: 2.410341398583518\n",
            "Len of Validation loss: 128, Average loss: 2.075716021936387\n",
            "Epoch: 143, Len of Training loss: 36, Average loss: 2.4545361697673798\n",
            "Len of Validation loss: 128, Average loss: 2.067356217186898\n",
            "Epoch: 144, Len of Training loss: 36, Average loss: 2.7412757641739316\n",
            "Len of Validation loss: 128, Average loss: 2.715843606274575\n",
            "Epoch: 145, Len of Training loss: 36, Average loss: 2.6838910049862332\n",
            "Len of Validation loss: 128, Average loss: 2.132648002356291\n",
            "Epoch: 146, Len of Training loss: 36, Average loss: 2.363094217247433\n",
            "Len of Validation loss: 128, Average loss: 2.237788976635784\n",
            "Epoch: 147, Len of Training loss: 36, Average loss: 2.447830852535036\n",
            "Len of Validation loss: 128, Average loss: 2.089826479088515\n",
            "Epoch: 148, Len of Training loss: 36, Average loss: 2.5138506988684335\n",
            "Len of Validation loss: 128, Average loss: 2.1398488925769925\n",
            "Epoch: 149, Len of Training loss: 36, Average loss: 2.505403223964903\n",
            "Len of Validation loss: 128, Average loss: 1.9829422617331147\n",
            "Epoch: 150, Len of Training loss: 36, Average loss: 2.5149474574459925\n",
            "Len of Validation loss: 128, Average loss: 2.2840594835579395\n",
            "Epoch: 151, Len of Training loss: 36, Average loss: 2.3035145699977875\n",
            "Len of Validation loss: 128, Average loss: 2.15575100062415\n",
            "Epoch: 152, Len of Training loss: 36, Average loss: 2.46665867500835\n",
            "Len of Validation loss: 128, Average loss: 1.973641908261925\n",
            "Epoch: 153, Len of Training loss: 36, Average loss: 2.284466567966673\n",
            "Len of Validation loss: 128, Average loss: 1.9992078659124672\n",
            "Epoch: 154, Len of Training loss: 36, Average loss: 2.369510422150294\n",
            "Len of Validation loss: 128, Average loss: 2.025502839125693\n",
            "Epoch: 155, Len of Training loss: 36, Average loss: 2.3688970141940646\n",
            "Len of Validation loss: 128, Average loss: 2.0215618601068854\n",
            "Epoch: 156, Len of Training loss: 36, Average loss: 2.308797392580244\n",
            "Len of Validation loss: 128, Average loss: 1.9140789308585227\n",
            "Epoch: 157, Len of Training loss: 36, Average loss: 2.2751057744026184\n",
            "Len of Validation loss: 128, Average loss: 2.0168433552607894\n",
            "Epoch: 158, Len of Training loss: 36, Average loss: 2.2926071849134235\n",
            "Len of Validation loss: 128, Average loss: 1.8906359607353806\n",
            "Epoch: 159, Len of Training loss: 36, Average loss: 2.322442955440945\n",
            "Len of Validation loss: 128, Average loss: 1.922768880147487\n",
            "Epoch: 160, Len of Training loss: 36, Average loss: 2.449008650249905\n",
            "Len of Validation loss: 128, Average loss: 1.9469679472967982\n",
            "Epoch: 161, Len of Training loss: 36, Average loss: 2.541312062078052\n",
            "Len of Validation loss: 128, Average loss: 1.9306954899802804\n",
            "Epoch: 162, Len of Training loss: 36, Average loss: 2.270327948861652\n",
            "Len of Validation loss: 128, Average loss: 1.8732957816682756\n",
            "Epoch: 163, Len of Training loss: 36, Average loss: 2.215636740128199\n",
            "Len of Validation loss: 128, Average loss: 1.9035528972744942\n",
            "Epoch: 164, Len of Training loss: 36, Average loss: 2.1915462944242687\n",
            "Len of Validation loss: 128, Average loss: 1.9008568262215704\n",
            "Epoch: 165, Len of Training loss: 36, Average loss: 2.3926152984301248\n",
            "Len of Validation loss: 128, Average loss: 2.2444879757240415\n",
            "Epoch: 166, Len of Training loss: 36, Average loss: 2.2663292619917126\n",
            "Len of Validation loss: 128, Average loss: 1.9499490279704332\n",
            "Epoch: 167, Len of Training loss: 36, Average loss: 2.269262390004264\n",
            "Len of Validation loss: 128, Average loss: 2.087619187310338\n",
            "Epoch: 168, Len of Training loss: 36, Average loss: 2.21484183271726\n",
            "Len of Validation loss: 128, Average loss: 2.1874183639883995\n",
            "Epoch: 169, Len of Training loss: 36, Average loss: 2.30178079340193\n",
            "Len of Validation loss: 128, Average loss: 2.2212570514529943\n",
            "Epoch: 170, Len of Training loss: 36, Average loss: 2.336199677652783\n",
            "Len of Validation loss: 128, Average loss: 1.9282320155762136\n",
            "Epoch: 171, Len of Training loss: 36, Average loss: 2.3967339429590435\n",
            "Len of Validation loss: 128, Average loss: 2.044794938992709\n",
            "Epoch: 172, Len of Training loss: 36, Average loss: 2.379643830988142\n",
            "Len of Validation loss: 128, Average loss: 1.8905568276531994\n",
            "Epoch: 173, Len of Training loss: 36, Average loss: 2.14415975411733\n",
            "Len of Validation loss: 128, Average loss: 1.9206332818139344\n",
            "Epoch: 174, Len of Training loss: 36, Average loss: 2.2063724994659424\n",
            "Len of Validation loss: 128, Average loss: 2.260329646524042\n",
            "Epoch: 175, Len of Training loss: 36, Average loss: 2.255752725733651\n",
            "Len of Validation loss: 128, Average loss: 2.032731498591602\n",
            "Epoch: 176, Len of Training loss: 36, Average loss: 2.410389910141627\n",
            "Len of Validation loss: 128, Average loss: 2.0774697214365005\n",
            "Epoch: 177, Len of Training loss: 36, Average loss: 2.2245019177595773\n",
            "Len of Validation loss: 128, Average loss: 1.8486062809824944\n",
            "Epoch: 178, Len of Training loss: 36, Average loss: 2.230892128414578\n",
            "Len of Validation loss: 128, Average loss: 2.1922746347263455\n",
            "Epoch: 179, Len of Training loss: 36, Average loss: 2.4230621059735618\n",
            "Len of Validation loss: 128, Average loss: 1.93459995649755\n",
            "Epoch: 180, Len of Training loss: 36, Average loss: 2.321060379346212\n",
            "Len of Validation loss: 128, Average loss: 2.098566475789994\n",
            "Epoch: 181, Len of Training loss: 36, Average loss: 2.1992861926555634\n",
            "Len of Validation loss: 128, Average loss: 1.9918578588403761\n",
            "Epoch: 182, Len of Training loss: 36, Average loss: 2.1452521118852825\n",
            "Len of Validation loss: 128, Average loss: 1.9260582732968032\n",
            "Epoch: 183, Len of Training loss: 36, Average loss: 2.1619582109981113\n",
            "Len of Validation loss: 128, Average loss: 1.7970906889531761\n",
            "Epoch: 184, Len of Training loss: 36, Average loss: 2.224924604098002\n",
            "Len of Validation loss: 128, Average loss: 1.8965407400391996\n",
            "Epoch: 185, Len of Training loss: 36, Average loss: 2.3409585191143885\n",
            "Len of Validation loss: 128, Average loss: 1.9314957661554217\n",
            "Epoch: 186, Len of Training loss: 36, Average loss: 2.2097285091876984\n",
            "Len of Validation loss: 128, Average loss: 2.1158351222984493\n",
            "Epoch: 187, Len of Training loss: 36, Average loss: 2.2577250036928387\n",
            "Len of Validation loss: 128, Average loss: 1.8555496581830084\n",
            "Epoch: 188, Len of Training loss: 36, Average loss: 2.134783628914091\n",
            "Len of Validation loss: 128, Average loss: 1.9286958584561944\n",
            "Epoch: 189, Len of Training loss: 36, Average loss: 2.2137313551372952\n",
            "Len of Validation loss: 128, Average loss: 2.2424267306923866\n",
            "Epoch: 190, Len of Training loss: 36, Average loss: 2.2164776656362744\n",
            "Len of Validation loss: 128, Average loss: 2.288674674462527\n",
            "Epoch: 191, Len of Training loss: 36, Average loss: 2.496639357672797\n",
            "Len of Validation loss: 128, Average loss: 1.961994365323335\n",
            "Epoch: 192, Len of Training loss: 36, Average loss: 2.245903319782681\n",
            "Len of Validation loss: 128, Average loss: 1.9239854020997882\n",
            "Epoch: 193, Len of Training loss: 36, Average loss: 2.137110951873991\n",
            "Len of Validation loss: 128, Average loss: 1.833300907863304\n",
            "Epoch: 194, Len of Training loss: 36, Average loss: 2.2000890970230103\n",
            "Len of Validation loss: 128, Average loss: 2.26338871428743\n",
            "Epoch: 195, Len of Training loss: 36, Average loss: 2.3695852723386555\n",
            "Len of Validation loss: 128, Average loss: 2.0509216017089784\n",
            "Epoch: 196, Len of Training loss: 36, Average loss: 2.129344777928458\n",
            "Len of Validation loss: 128, Average loss: 1.7853167890571058\n",
            "Epoch: 197, Len of Training loss: 36, Average loss: 2.2218881150086722\n",
            "Len of Validation loss: 128, Average loss: 1.8191518997773528\n",
            "Epoch: 198, Len of Training loss: 36, Average loss: 2.228200031651391\n",
            "Len of Validation loss: 128, Average loss: 1.908907282864675\n",
            "Epoch: 199, Len of Training loss: 36, Average loss: 2.1598411798477173\n",
            "Len of Validation loss: 128, Average loss: 1.779952206183225\n",
            "Epoch: 200, Len of Training loss: 36, Average loss: 2.230245610078176\n",
            "Len of Validation loss: 128, Average loss: 2.5678709615021944\n",
            "Epoch: 201, Len of Training loss: 36, Average loss: 2.269698586728838\n",
            "Len of Validation loss: 128, Average loss: 1.7637388873845339\n",
            "Epoch: 202, Len of Training loss: 36, Average loss: 2.1126015848583646\n",
            "Len of Validation loss: 128, Average loss: 1.822210215497762\n",
            "Epoch: 203, Len of Training loss: 36, Average loss: 2.1190286543634205\n",
            "Len of Validation loss: 128, Average loss: 1.9305885843932629\n",
            "Epoch: 204, Len of Training loss: 36, Average loss: 2.1796208951208325\n",
            "Len of Validation loss: 128, Average loss: 1.7557180230505764\n",
            "Epoch: 205, Len of Training loss: 36, Average loss: 2.1547045740816326\n",
            "Len of Validation loss: 128, Average loss: 1.862073527649045\n",
            "Epoch: 206, Len of Training loss: 36, Average loss: 2.31418106953303\n",
            "Len of Validation loss: 128, Average loss: 1.9719813312403858\n",
            "Epoch: 207, Len of Training loss: 36, Average loss: 2.2296197182602353\n",
            "Len of Validation loss: 128, Average loss: 1.9367155181244016\n",
            "Epoch: 208, Len of Training loss: 36, Average loss: 2.3087856769561768\n",
            "Len of Validation loss: 128, Average loss: 1.8731628418900073\n",
            "Epoch: 209, Len of Training loss: 36, Average loss: 2.110691166586346\n",
            "Len of Validation loss: 128, Average loss: 1.952124597504735\n",
            "Epoch: 210, Len of Training loss: 36, Average loss: 2.1529164711634317\n",
            "Len of Validation loss: 128, Average loss: 1.8679426163434982\n",
            "Epoch: 211, Len of Training loss: 36, Average loss: 2.181555211544037\n",
            "Len of Validation loss: 128, Average loss: 1.804967159871012\n",
            "Epoch: 212, Len of Training loss: 36, Average loss: 2.1225032574600644\n",
            "Len of Validation loss: 128, Average loss: 1.7477881936356425\n",
            "Epoch: 213, Len of Training loss: 36, Average loss: 2.187529510921902\n",
            "Len of Validation loss: 128, Average loss: 2.0317989229224622\n",
            "Epoch: 214, Len of Training loss: 36, Average loss: 2.17255566517512\n",
            "Len of Validation loss: 128, Average loss: 2.2596339965239167\n",
            "Epoch: 215, Len of Training loss: 36, Average loss: 2.184070279200872\n",
            "Len of Validation loss: 128, Average loss: 1.7678511417470872\n",
            "Epoch: 216, Len of Training loss: 36, Average loss: 2.1026537550820246\n",
            "Len of Validation loss: 128, Average loss: 1.7619441910646856\n",
            "Epoch: 217, Len of Training loss: 36, Average loss: 2.128435727622774\n",
            "Len of Validation loss: 128, Average loss: 1.9315946972928941\n",
            "Epoch: 218, Len of Training loss: 36, Average loss: 2.1774983273612127\n",
            "Len of Validation loss: 128, Average loss: 1.7629326444584876\n",
            "Epoch: 219, Len of Training loss: 36, Average loss: 2.3341357674863605\n",
            "Len of Validation loss: 128, Average loss: 1.8998521915636957\n",
            "Epoch: 220, Len of Training loss: 36, Average loss: 2.0897686117225223\n",
            "Len of Validation loss: 128, Average loss: 1.8132538525387645\n",
            "Epoch: 221, Len of Training loss: 36, Average loss: 2.095850987566842\n",
            "Len of Validation loss: 128, Average loss: 1.9234156706370413\n",
            "Epoch: 222, Len of Training loss: 36, Average loss: 2.1355557673507266\n",
            "Len of Validation loss: 128, Average loss: 1.761439437046647\n",
            "Epoch: 223, Len of Training loss: 36, Average loss: 2.060789773861567\n",
            "Len of Validation loss: 128, Average loss: 1.743198310956359\n",
            "Epoch: 224, Len of Training loss: 36, Average loss: 2.040909217463599\n",
            "Len of Validation loss: 128, Average loss: 1.7523340107873082\n",
            "Epoch: 225, Len of Training loss: 36, Average loss: 2.151120993826124\n",
            "Len of Validation loss: 128, Average loss: 1.8262889618054032\n",
            "Epoch: 226, Len of Training loss: 36, Average loss: 2.2362491885821023\n",
            "Len of Validation loss: 128, Average loss: 1.784605129621923\n",
            "Epoch: 227, Len of Training loss: 36, Average loss: 2.3968449764781528\n",
            "Len of Validation loss: 128, Average loss: 1.8454500222578645\n",
            "Epoch: 228, Len of Training loss: 36, Average loss: 2.1021889050801597\n",
            "Len of Validation loss: 128, Average loss: 1.8147620474919677\n",
            "Epoch: 229, Len of Training loss: 36, Average loss: 2.319804698228836\n",
            "Len of Validation loss: 128, Average loss: 2.463493020972237\n",
            "Epoch: 230, Len of Training loss: 36, Average loss: 2.2792669700251684\n",
            "Len of Validation loss: 128, Average loss: 1.811195588670671\n",
            "Epoch: 231, Len of Training loss: 36, Average loss: 2.0559196637736425\n",
            "Len of Validation loss: 128, Average loss: 1.6948752289172262\n",
            "Epoch: 232, Len of Training loss: 36, Average loss: 2.2385437024964228\n",
            "Len of Validation loss: 128, Average loss: 1.8396458309143782\n",
            "Epoch: 233, Len of Training loss: 36, Average loss: 2.051868955294291\n",
            "Len of Validation loss: 128, Average loss: 1.7592502457555383\n",
            "Epoch: 234, Len of Training loss: 36, Average loss: 2.099614401658376\n",
            "Len of Validation loss: 128, Average loss: 1.7484523521270603\n",
            "Epoch: 235, Len of Training loss: 36, Average loss: 2.056503630346722\n",
            "Len of Validation loss: 128, Average loss: 1.9694632356986403\n",
            "Epoch: 236, Len of Training loss: 36, Average loss: 2.106978439622455\n",
            "Len of Validation loss: 128, Average loss: 1.748280027648434\n",
            "Epoch: 237, Len of Training loss: 36, Average loss: 2.0998737116654715\n",
            "Len of Validation loss: 128, Average loss: 1.772694808896631\n",
            "Epoch: 238, Len of Training loss: 36, Average loss: 2.1283578508430057\n",
            "Len of Validation loss: 128, Average loss: 1.7809062213636935\n",
            "Epoch: 239, Len of Training loss: 36, Average loss: 2.081073476208581\n",
            "Len of Validation loss: 128, Average loss: 1.7955893357284367\n",
            "Epoch: 240, Len of Training loss: 36, Average loss: 2.1403650409645505\n",
            "Len of Validation loss: 128, Average loss: 2.3696991908364\n",
            "Epoch: 241, Len of Training loss: 36, Average loss: 2.139373845524258\n",
            "Len of Validation loss: 128, Average loss: 1.9036488677375019\n",
            "Epoch: 242, Len of Training loss: 36, Average loss: 2.107643332746294\n",
            "Len of Validation loss: 128, Average loss: 1.8339046873152256\n",
            "Epoch: 243, Len of Training loss: 36, Average loss: 2.1167286568217807\n",
            "Len of Validation loss: 128, Average loss: 1.7690587523393333\n",
            "Epoch: 244, Len of Training loss: 36, Average loss: 2.1041720608870187\n",
            "Len of Validation loss: 128, Average loss: 1.7733824055176228\n",
            "Epoch: 245, Len of Training loss: 36, Average loss: 2.1312332186434\n",
            "Len of Validation loss: 128, Average loss: 1.7143970634788275\n",
            "Epoch: 246, Len of Training loss: 36, Average loss: 2.0526397162013583\n",
            "Len of Validation loss: 128, Average loss: 2.2062502049375325\n",
            "Epoch: 247, Len of Training loss: 36, Average loss: 2.2443745732307434\n",
            "Len of Validation loss: 128, Average loss: 1.9000727087259293\n",
            "Epoch: 248, Len of Training loss: 36, Average loss: 2.1446599629190235\n",
            "Len of Validation loss: 128, Average loss: 1.8656048576813191\n",
            "Epoch: 249, Len of Training loss: 36, Average loss: 2.116812414593167\n",
            "Len of Validation loss: 128, Average loss: 1.8490634807385504\n",
            "Epoch: 250, Len of Training loss: 36, Average loss: 2.0798031124803753\n",
            "Len of Validation loss: 128, Average loss: 2.393766514956951\n",
            "Epoch: 251, Len of Training loss: 36, Average loss: 2.1650130848089852\n",
            "Len of Validation loss: 128, Average loss: 1.861218738835305\n",
            "Epoch: 252, Len of Training loss: 36, Average loss: 2.1038917435540094\n",
            "Len of Validation loss: 128, Average loss: 1.9121196565683931\n",
            "Epoch: 253, Len of Training loss: 36, Average loss: 2.0955210659239025\n",
            "Len of Validation loss: 128, Average loss: 1.725376213900745\n",
            "Epoch: 254, Len of Training loss: 36, Average loss: 2.1290576259295144\n",
            "Len of Validation loss: 128, Average loss: 1.7757308781147003\n",
            "Epoch: 255, Len of Training loss: 36, Average loss: 2.344403045045005\n",
            "Len of Validation loss: 128, Average loss: 1.8393114407081157\n",
            "Epoch: 256, Len of Training loss: 36, Average loss: 2.130577584107717\n",
            "Len of Validation loss: 128, Average loss: 1.7448677029460669\n",
            "Epoch: 257, Len of Training loss: 36, Average loss: 2.1026284330421023\n",
            "Len of Validation loss: 128, Average loss: 1.8582676593214273\n",
            "Epoch: 258, Len of Training loss: 36, Average loss: 2.0637257397174835\n",
            "Len of Validation loss: 128, Average loss: 1.7573831116314977\n",
            "Epoch: 259, Len of Training loss: 36, Average loss: 2.0793242818779416\n",
            "Len of Validation loss: 128, Average loss: 1.739337233826518\n",
            "Epoch: 260, Len of Training loss: 36, Average loss: 2.079239156511095\n",
            "Len of Validation loss: 128, Average loss: 2.1259781839326024\n",
            "Epoch: 261, Len of Training loss: 36, Average loss: 2.120725280708737\n",
            "Len of Validation loss: 128, Average loss: 1.8694275077432394\n",
            "Epoch: 262, Len of Training loss: 36, Average loss: 2.0866878065798016\n",
            "Len of Validation loss: 128, Average loss: 1.75000532111153\n",
            "Epoch: 263, Len of Training loss: 36, Average loss: 2.0338433020644717\n",
            "Len of Validation loss: 128, Average loss: 1.7020221261773258\n",
            "Epoch: 264, Len of Training loss: 36, Average loss: 2.124910546673669\n",
            "Len of Validation loss: 128, Average loss: 1.7285217675380409\n",
            "Epoch: 265, Len of Training loss: 36, Average loss: 2.0205015904373593\n",
            "Len of Validation loss: 128, Average loss: 1.8937305668368936\n",
            "Epoch: 266, Len of Training loss: 36, Average loss: 2.069249904818005\n",
            "Len of Validation loss: 128, Average loss: 1.6789157469756901\n",
            "Epoch: 267, Len of Training loss: 36, Average loss: 2.0096696284082203\n",
            "Len of Validation loss: 128, Average loss: 1.7442913777194917\n",
            "Epoch: 268, Len of Training loss: 36, Average loss: 2.054668621884452\n",
            "Len of Validation loss: 128, Average loss: 6.576821554452181\n",
            "Epoch: 269, Len of Training loss: 36, Average loss: 7.743352591991425\n",
            "Len of Validation loss: 128, Average loss: 6.487726183608174\n",
            "Epoch: 270, Len of Training loss: 36, Average loss: 13.94423515929116\n",
            "Len of Validation loss: 128, Average loss: 9.116169676184654\n",
            "Epoch: 271, Len of Training loss: 36, Average loss: 13.71133061250051\n",
            "Len of Validation loss: 128, Average loss: 6.37259186245501\n",
            "Epoch: 272, Len of Training loss: 36, Average loss: 4.844757629765405\n",
            "Len of Validation loss: 128, Average loss: 2.825702023226768\n",
            "Epoch: 273, Len of Training loss: 36, Average loss: 3.1565094192822776\n",
            "Len of Validation loss: 128, Average loss: 2.4019762631505728\n",
            "Epoch: 274, Len of Training loss: 36, Average loss: 2.921580867634879\n",
            "Len of Validation loss: 128, Average loss: 2.924969640560448\n",
            "Epoch: 275, Len of Training loss: 36, Average loss: 2.711464467975828\n",
            "Len of Validation loss: 128, Average loss: 2.2139866002835333\n",
            "Epoch: 276, Len of Training loss: 36, Average loss: 2.408894161383311\n",
            "Len of Validation loss: 128, Average loss: 1.9615813358686864\n",
            "Epoch: 277, Len of Training loss: 36, Average loss: 2.290894329547882\n",
            "Len of Validation loss: 128, Average loss: 2.024840864818543\n",
            "Epoch: 278, Len of Training loss: 36, Average loss: 2.4173068238629236\n",
            "Len of Validation loss: 128, Average loss: 2.170377698726952\n",
            "Epoch: 279, Len of Training loss: 36, Average loss: 2.2775300211376615\n",
            "Len of Validation loss: 128, Average loss: 1.8948618988506496\n",
            "Epoch: 280, Len of Training loss: 36, Average loss: 2.3811390002568564\n",
            "Len of Validation loss: 128, Average loss: 2.540407665539533\n",
            "Epoch: 281, Len of Training loss: 36, Average loss: 2.4269380602571697\n",
            "Len of Validation loss: 128, Average loss: 1.9274535074364394\n",
            "Epoch: 282, Len of Training loss: 36, Average loss: 2.197396993637085\n",
            "Len of Validation loss: 128, Average loss: 1.8845927962101996\n",
            "Epoch: 283, Len of Training loss: 36, Average loss: 2.2167648904853396\n",
            "Len of Validation loss: 128, Average loss: 1.9031850849278271\n",
            "Epoch: 284, Len of Training loss: 36, Average loss: 2.2034682167900934\n",
            "Len of Validation loss: 128, Average loss: 2.179301938507706\n",
            "Epoch: 285, Len of Training loss: 36, Average loss: 2.1670826839076147\n",
            "Len of Validation loss: 128, Average loss: 1.994353309739381\n",
            "Epoch: 286, Len of Training loss: 36, Average loss: 2.301661365562015\n",
            "Len of Validation loss: 128, Average loss: 1.9193957457318902\n",
            "Epoch: 287, Len of Training loss: 36, Average loss: 2.2096612552801767\n",
            "Len of Validation loss: 128, Average loss: 1.820255534723401\n",
            "Epoch: 288, Len of Training loss: 36, Average loss: 2.116984635591507\n",
            "Len of Validation loss: 128, Average loss: 1.8737868994940072\n",
            "Epoch: 289, Len of Training loss: 36, Average loss: 2.290155029959149\n",
            "Len of Validation loss: 128, Average loss: 2.626971796154976\n",
            "Epoch: 290, Len of Training loss: 36, Average loss: 2.298917406135135\n",
            "Len of Validation loss: 128, Average loss: 1.8011480900458992\n",
            "Epoch: 291, Len of Training loss: 36, Average loss: 2.108375304275089\n",
            "Len of Validation loss: 128, Average loss: 1.7757330788299441\n",
            "Epoch: 292, Len of Training loss: 36, Average loss: 2.106467452314165\n",
            "Len of Validation loss: 128, Average loss: 2.0601106914691627\n",
            "Epoch: 293, Len of Training loss: 36, Average loss: 2.3385698000590005\n",
            "Len of Validation loss: 128, Average loss: 1.8542542015202343\n",
            "Epoch: 294, Len of Training loss: 36, Average loss: 2.310571253299713\n",
            "Len of Validation loss: 128, Average loss: 1.9089705455116928\n",
            "Epoch: 295, Len of Training loss: 36, Average loss: 2.2230376336309643\n",
            "Len of Validation loss: 128, Average loss: 2.012173253344372\n",
            "Epoch: 296, Len of Training loss: 36, Average loss: 2.225939518875546\n",
            "Len of Validation loss: 128, Average loss: 1.799883515574038\n",
            "Epoch: 297, Len of Training loss: 36, Average loss: 2.087816913922628\n",
            "Len of Validation loss: 128, Average loss: 1.744753785431385\n",
            "Epoch: 298, Len of Training loss: 36, Average loss: 2.1253916521867118\n",
            "Len of Validation loss: 128, Average loss: 1.8084911298938096\n",
            "Epoch: 299, Len of Training loss: 36, Average loss: 2.129139416747623\n",
            "Len of Validation loss: 128, Average loss: 1.7436551116406918\n",
            "Epoch: 300, Len of Training loss: 36, Average loss: 2.178736196623908\n",
            "Len of Validation loss: 128, Average loss: 2.503005570732057\n",
            "Epoch: 301, Len of Training loss: 36, Average loss: 2.1973201400703855\n",
            "Len of Validation loss: 128, Average loss: 2.264635758008808\n",
            "Epoch: 302, Len of Training loss: 36, Average loss: 2.1835601958963604\n",
            "Len of Validation loss: 128, Average loss: 1.9133801641874015\n",
            "Epoch: 303, Len of Training loss: 36, Average loss: 2.108596682548523\n",
            "Len of Validation loss: 128, Average loss: 1.8701499681919813\n",
            "Epoch: 304, Len of Training loss: 36, Average loss: 2.0839799212084875\n",
            "Len of Validation loss: 128, Average loss: 1.6931366086937487\n",
            "Epoch: 305, Len of Training loss: 36, Average loss: 2.1704345842202506\n",
            "Len of Validation loss: 128, Average loss: 1.7952590924687684\n",
            "Epoch: 306, Len of Training loss: 36, Average loss: 2.1584746142228446\n",
            "Len of Validation loss: 128, Average loss: 1.8984069717116654\n",
            "Epoch: 307, Len of Training loss: 36, Average loss: 2.2768493956989713\n",
            "Len of Validation loss: 128, Average loss: 1.7652735391166061\n",
            "Epoch: 308, Len of Training loss: 36, Average loss: 2.313092533085081\n",
            "Len of Validation loss: 128, Average loss: 1.9178262879140675\n",
            "Epoch: 309, Len of Training loss: 36, Average loss: 2.18324222167333\n",
            "Len of Validation loss: 128, Average loss: 1.8704052513930947\n",
            "Epoch: 310, Len of Training loss: 36, Average loss: 2.122666322522693\n",
            "Len of Validation loss: 128, Average loss: 1.7255551042035222\n",
            "Epoch: 311, Len of Training loss: 36, Average loss: 2.098866595162286\n",
            "Len of Validation loss: 128, Average loss: 2.0581134045496583\n",
            "Epoch: 312, Len of Training loss: 36, Average loss: 2.1687834759553275\n",
            "Len of Validation loss: 128, Average loss: 1.766347434837371\n",
            "Epoch: 313, Len of Training loss: 36, Average loss: 2.200478265682856\n",
            "Len of Validation loss: 128, Average loss: 2.0990980961360037\n",
            "Epoch: 314, Len of Training loss: 36, Average loss: 2.1561248534255557\n",
            "Len of Validation loss: 128, Average loss: 2.0819371165707707\n",
            "Epoch: 315, Len of Training loss: 36, Average loss: 2.083747112088733\n",
            "Len of Validation loss: 128, Average loss: 1.9894221569411457\n",
            "Epoch: 316, Len of Training loss: 36, Average loss: 2.1007323033279843\n",
            "Len of Validation loss: 128, Average loss: 1.7991505218669772\n",
            "Epoch: 317, Len of Training loss: 36, Average loss: 2.094744407468372\n",
            "Len of Validation loss: 128, Average loss: 1.852361760335043\n",
            "Epoch: 318, Len of Training loss: 36, Average loss: 2.1042559179994793\n",
            "Len of Validation loss: 128, Average loss: 1.731492267921567\n",
            "Epoch: 319, Len of Training loss: 36, Average loss: 2.2299026317066617\n",
            "Len of Validation loss: 128, Average loss: 1.7780390987172723\n",
            "Epoch: 320, Len of Training loss: 36, Average loss: 2.1989192399713726\n",
            "Len of Validation loss: 128, Average loss: 1.7834328322205693\n",
            "Epoch: 321, Len of Training loss: 36, Average loss: 2.2002983258830175\n",
            "Len of Validation loss: 128, Average loss: 1.8170739938504994\n",
            "Epoch: 322, Len of Training loss: 36, Average loss: 2.10663174589475\n",
            "Len of Validation loss: 128, Average loss: 1.6934556069318205\n",
            "Epoch: 323, Len of Training loss: 36, Average loss: 2.075944082604514\n",
            "Len of Validation loss: 128, Average loss: 1.867705694399774\n",
            "Epoch: 324, Len of Training loss: 36, Average loss: 2.2439896232552\n",
            "Len of Validation loss: 128, Average loss: 1.8425919888541102\n",
            "Epoch: 325, Len of Training loss: 36, Average loss: 2.1067066888014474\n",
            "Len of Validation loss: 128, Average loss: 1.9245538874529302\n",
            "Epoch: 326, Len of Training loss: 36, Average loss: 2.1048422190878124\n",
            "Len of Validation loss: 128, Average loss: 1.7353192195296288\n",
            "Epoch: 327, Len of Training loss: 36, Average loss: 2.1610055367151895\n",
            "Len of Validation loss: 128, Average loss: 1.946636273059994\n",
            "Epoch: 328, Len of Training loss: 36, Average loss: 2.074726257059309\n",
            "Len of Validation loss: 128, Average loss: 1.7203937591984868\n",
            "Epoch: 329, Len of Training loss: 36, Average loss: 2.091328654024336\n",
            "Len of Validation loss: 128, Average loss: 1.7372279088012874\n",
            "Epoch: 330, Len of Training loss: 36, Average loss: 2.220929821332296\n",
            "Len of Validation loss: 128, Average loss: 1.7460225718095899\n",
            "Epoch: 331, Len of Training loss: 36, Average loss: 2.2018816239304013\n",
            "Len of Validation loss: 128, Average loss: 1.8863812086638063\n",
            "Epoch: 332, Len of Training loss: 36, Average loss: 2.093573513958189\n",
            "Len of Validation loss: 128, Average loss: 1.7969524571672082\n",
            "Epoch: 333, Len of Training loss: 36, Average loss: 2.20770600438118\n",
            "Len of Validation loss: 128, Average loss: 1.7468755301088095\n",
            "Epoch: 334, Len of Training loss: 36, Average loss: 2.087554656796985\n",
            "Len of Validation loss: 128, Average loss: 1.8785989335738122\n",
            "Epoch: 335, Len of Training loss: 36, Average loss: 2.065826998816596\n",
            "Len of Validation loss: 128, Average loss: 1.727440289221704\n",
            "Epoch: 336, Len of Training loss: 36, Average loss: 2.066243734624651\n",
            "Len of Validation loss: 128, Average loss: 1.769592343363911\n",
            "Epoch: 337, Len of Training loss: 36, Average loss: 2.209577096833123\n",
            "Len of Validation loss: 128, Average loss: 1.784454278415069\n",
            "Epoch: 338, Len of Training loss: 36, Average loss: 2.07577336496777\n",
            "Len of Validation loss: 128, Average loss: 2.0376835092902184\n",
            "Epoch: 339, Len of Training loss: 36, Average loss: 2.0566169089741178\n",
            "Len of Validation loss: 128, Average loss: 1.6936989065725356\n",
            "Epoch: 340, Len of Training loss: 36, Average loss: 2.06261079510053\n",
            "Len of Validation loss: 128, Average loss: 1.772570409346372\n",
            "Epoch: 341, Len of Training loss: 36, Average loss: 2.066880073812273\n",
            "Len of Validation loss: 128, Average loss: 1.9295150265097618\n",
            "Epoch: 342, Len of Training loss: 36, Average loss: 2.118071330918206\n",
            "Len of Validation loss: 128, Average loss: 1.825078853406012\n",
            "Epoch: 343, Len of Training loss: 36, Average loss: 2.1001309951146445\n",
            "Len of Validation loss: 128, Average loss: 1.7011995785869658\n",
            "Epoch: 344, Len of Training loss: 36, Average loss: 2.110628710852729\n",
            "Len of Validation loss: 128, Average loss: 1.858323663007468\n",
            "Epoch: 345, Len of Training loss: 36, Average loss: 2.079260915517807\n",
            "Len of Validation loss: 128, Average loss: 1.6964196884073317\n",
            "Epoch: 346, Len of Training loss: 36, Average loss: 2.0366205937332578\n",
            "Len of Validation loss: 128, Average loss: 1.7342820100020617\n",
            "Epoch: 347, Len of Training loss: 36, Average loss: 2.0265462862120733\n",
            "Len of Validation loss: 128, Average loss: 2.041694333544001\n",
            "Epoch: 348, Len of Training loss: 36, Average loss: 2.038099752532111\n",
            "Len of Validation loss: 128, Average loss: 1.8831974980421364\n",
            "Epoch: 349, Len of Training loss: 36, Average loss: 2.1403023534350925\n",
            "Len of Validation loss: 128, Average loss: 1.7431566410232335\n",
            "Epoch: 350, Len of Training loss: 36, Average loss: 2.277441746658749\n",
            "Len of Validation loss: 128, Average loss: 1.9198305814061314\n",
            "Epoch: 351, Len of Training loss: 36, Average loss: 2.2869299915101795\n",
            "Len of Validation loss: 128, Average loss: 1.8154748766683042\n",
            "Epoch: 352, Len of Training loss: 36, Average loss: 2.0510127345720925\n",
            "Len of Validation loss: 128, Average loss: 1.7260747093241662\n",
            "Epoch: 353, Len of Training loss: 36, Average loss: 2.206404341591729\n",
            "Len of Validation loss: 128, Average loss: 2.01765752537176\n",
            "Epoch: 354, Len of Training loss: 36, Average loss: 2.17445264922248\n",
            "Len of Validation loss: 128, Average loss: 1.7960073570720851\n",
            "Epoch: 355, Len of Training loss: 36, Average loss: 2.0223549471961126\n",
            "Len of Validation loss: 128, Average loss: 1.7428625093307346\n",
            "Epoch: 356, Len of Training loss: 36, Average loss: 2.098967989285787\n",
            "Len of Validation loss: 128, Average loss: 1.7231233809143305\n",
            "Epoch: 357, Len of Training loss: 36, Average loss: 2.0339388847351074\n",
            "Len of Validation loss: 128, Average loss: 1.7313508139923215\n",
            "Epoch: 358, Len of Training loss: 36, Average loss: 2.088771210776435\n",
            "Len of Validation loss: 128, Average loss: 1.9006674094125628\n",
            "Epoch: 359, Len of Training loss: 36, Average loss: 2.1027924617131553\n",
            "Len of Validation loss: 128, Average loss: 1.6879798059817404\n",
            "Epoch: 360, Len of Training loss: 36, Average loss: 2.1291158066855536\n",
            "Len of Validation loss: 128, Average loss: 1.9808403975330293\n",
            "Epoch: 361, Len of Training loss: 36, Average loss: 2.067063573333952\n",
            "Len of Validation loss: 128, Average loss: 1.7586268344894052\n",
            "Epoch: 362, Len of Training loss: 36, Average loss: 2.072539829545551\n",
            "Len of Validation loss: 128, Average loss: 1.6861526272259653\n",
            "Epoch: 363, Len of Training loss: 36, Average loss: 2.0902067720890045\n",
            "Len of Validation loss: 128, Average loss: 1.752758146263659\n",
            "Epoch: 364, Len of Training loss: 36, Average loss: 2.1204953723483615\n",
            "Len of Validation loss: 128, Average loss: 1.7606217686552554\n",
            "Epoch: 365, Len of Training loss: 36, Average loss: 2.065358257955975\n",
            "Len of Validation loss: 128, Average loss: 2.0470959078520536\n",
            "Epoch: 366, Len of Training loss: 36, Average loss: 2.1702222724755607\n",
            "Len of Validation loss: 128, Average loss: 1.9822529745288193\n",
            "Epoch: 367, Len of Training loss: 36, Average loss: 2.185894015762541\n",
            "Len of Validation loss: 128, Average loss: 1.7632085492368788\n",
            "Epoch: 368, Len of Training loss: 36, Average loss: 2.163498649994532\n",
            "Len of Validation loss: 128, Average loss: 1.7372183380648494\n",
            "Epoch: 369, Len of Training loss: 36, Average loss: 2.076009902689192\n",
            "Len of Validation loss: 128, Average loss: 1.949862202629447\n",
            "Epoch: 370, Len of Training loss: 36, Average loss: 2.1723204818036823\n",
            "Len of Validation loss: 128, Average loss: 2.099405821878463\n",
            "Epoch: 371, Len of Training loss: 36, Average loss: 2.167247792085012\n",
            "Len of Validation loss: 128, Average loss: 1.724166655447334\n",
            "Epoch: 372, Len of Training loss: 36, Average loss: 2.1310620076126523\n",
            "Len of Validation loss: 128, Average loss: 2.2689133705571294\n",
            "Epoch: 373, Len of Training loss: 36, Average loss: 2.187144501341714\n",
            "Len of Validation loss: 128, Average loss: 1.7426883922889829\n",
            "Epoch: 374, Len of Training loss: 36, Average loss: 2.0964423484272428\n",
            "Len of Validation loss: 128, Average loss: 1.7447985596954823\n",
            "Epoch: 375, Len of Training loss: 36, Average loss: 2.056502193212509\n",
            "Len of Validation loss: 128, Average loss: 1.8400410544127226\n",
            "Epoch: 376, Len of Training loss: 36, Average loss: 2.046499619881312\n",
            "Len of Validation loss: 128, Average loss: 1.7910699327476323\n",
            "Epoch: 377, Len of Training loss: 36, Average loss: 2.036628829108344\n",
            "Len of Validation loss: 128, Average loss: 1.7177612965460867\n",
            "Epoch: 378, Len of Training loss: 36, Average loss: 2.0183958212534585\n",
            "Len of Validation loss: 128, Average loss: 1.7139280845876783\n",
            "Epoch: 379, Len of Training loss: 36, Average loss: 2.0909443729453616\n",
            "Len of Validation loss: 128, Average loss: 1.9430494182743132\n",
            "Epoch: 380, Len of Training loss: 36, Average loss: 2.0488598710960813\n",
            "Len of Validation loss: 128, Average loss: 1.803764761891216\n",
            "Epoch: 381, Len of Training loss: 36, Average loss: 2.0667526688840656\n",
            "Len of Validation loss: 128, Average loss: 1.7739647917915136\n",
            "Epoch: 382, Len of Training loss: 36, Average loss: 2.071293678548601\n",
            "Len of Validation loss: 128, Average loss: 1.7126510844100267\n",
            "Epoch: 383, Len of Training loss: 36, Average loss: 2.034081760380003\n",
            "Len of Validation loss: 128, Average loss: 2.1036373311653733\n",
            "Epoch: 384, Len of Training loss: 36, Average loss: 2.1009071932898626\n",
            "Len of Validation loss: 128, Average loss: 1.6962156887166202\n",
            "Epoch: 385, Len of Training loss: 36, Average loss: 2.103922963142395\n",
            "Len of Validation loss: 128, Average loss: 1.8090127317700535\n",
            "Epoch: 386, Len of Training loss: 36, Average loss: 2.3024958504570856\n",
            "Len of Validation loss: 128, Average loss: 2.0531893470324576\n",
            "Epoch: 387, Len of Training loss: 36, Average loss: 2.0677275127834744\n",
            "Len of Validation loss: 128, Average loss: 1.7090377036947757\n",
            "Epoch: 388, Len of Training loss: 36, Average loss: 2.232359253697925\n",
            "Len of Validation loss: 128, Average loss: 1.8664627373218536\n",
            "Epoch: 389, Len of Training loss: 36, Average loss: 2.089568532175488\n",
            "Len of Validation loss: 128, Average loss: 1.7152624083682895\n",
            "Epoch: 390, Len of Training loss: 36, Average loss: 2.138707114590539\n",
            "Len of Validation loss: 128, Average loss: 1.8682886068709195\n",
            "Epoch: 391, Len of Training loss: 36, Average loss: 2.0972391631868152\n",
            "Len of Validation loss: 128, Average loss: 1.9627397917211056\n",
            "Epoch: 392, Len of Training loss: 36, Average loss: 2.0479178064399295\n",
            "Len of Validation loss: 128, Average loss: 1.7014785364735872\n",
            "Epoch: 393, Len of Training loss: 36, Average loss: 2.019909077220493\n",
            "Len of Validation loss: 128, Average loss: 1.724365513306111\n",
            "Epoch: 394, Len of Training loss: 36, Average loss: 2.1157032251358032\n",
            "Len of Validation loss: 128, Average loss: 1.7251025685109198\n",
            "Epoch: 395, Len of Training loss: 36, Average loss: 2.0085178083843656\n",
            "Len of Validation loss: 128, Average loss: 1.807700329925865\n",
            "Epoch: 396, Len of Training loss: 36, Average loss: 2.018551299969355\n",
            "Len of Validation loss: 128, Average loss: 1.6931003951467574\n",
            "Epoch: 397, Len of Training loss: 36, Average loss: 2.0352974931399026\n",
            "Len of Validation loss: 128, Average loss: 1.678747246740386\n",
            "Epoch: 398, Len of Training loss: 36, Average loss: 2.011752035882738\n",
            "Len of Validation loss: 128, Average loss: 1.7389878777321428\n",
            "Epoch: 399, Len of Training loss: 36, Average loss: 2.000077784061432\n",
            "Len of Validation loss: 128, Average loss: 1.691425392171368\n",
            "Epoch: 400, Len of Training loss: 36, Average loss: 2.0084230270650654\n",
            "Len of Validation loss: 128, Average loss: 1.7525810929946601\n",
            "Epoch: 401, Len of Training loss: 36, Average loss: 2.089198480049769\n",
            "Len of Validation loss: 128, Average loss: 1.785253117326647\n",
            "Epoch: 402, Len of Training loss: 36, Average loss: 2.0504514707459345\n",
            "Len of Validation loss: 128, Average loss: 1.9538921574130654\n",
            "Epoch: 403, Len of Training loss: 36, Average loss: 2.102864089939329\n",
            "Len of Validation loss: 128, Average loss: 1.7091012885794044\n",
            "Epoch: 404, Len of Training loss: 36, Average loss: 2.176239679257075\n",
            "Len of Validation loss: 128, Average loss: 2.123670664150268\n",
            "Epoch: 405, Len of Training loss: 36, Average loss: 2.1196311761935553\n",
            "Len of Validation loss: 128, Average loss: 1.6570460577495396\n",
            "Epoch: 406, Len of Training loss: 36, Average loss: 2.0139953825208874\n",
            "Len of Validation loss: 128, Average loss: 1.850574424257502\n",
            "Epoch: 407, Len of Training loss: 36, Average loss: 2.023920920160082\n",
            "Len of Validation loss: 128, Average loss: 1.8934390651993454\n",
            "Epoch: 408, Len of Training loss: 36, Average loss: 2.046518156925837\n",
            "Len of Validation loss: 128, Average loss: 1.6992882443591952\n",
            "Epoch: 409, Len of Training loss: 36, Average loss: 2.220883071422577\n",
            "Len of Validation loss: 128, Average loss: 2.1167172444984317\n",
            "Epoch: 410, Len of Training loss: 36, Average loss: 2.1315409309334226\n",
            "Len of Validation loss: 128, Average loss: 1.6837763786315918\n",
            "Epoch: 411, Len of Training loss: 36, Average loss: 2.087852183315489\n",
            "Len of Validation loss: 128, Average loss: 1.7728853931184858\n",
            "Epoch: 412, Len of Training loss: 36, Average loss: 2.062586943308512\n",
            "Len of Validation loss: 128, Average loss: 1.7822882970795035\n",
            "Epoch: 413, Len of Training loss: 36, Average loss: 2.0690642495950065\n",
            "Len of Validation loss: 128, Average loss: 1.7042784460354596\n",
            "Epoch: 414, Len of Training loss: 36, Average loss: 2.1200090183152094\n",
            "Len of Validation loss: 128, Average loss: 1.7910258690826595\n",
            "Epoch: 415, Len of Training loss: 36, Average loss: 2.0694733957449594\n",
            "Len of Validation loss: 128, Average loss: 1.7102557849138975\n",
            "Epoch: 416, Len of Training loss: 36, Average loss: 2.0813303788503013\n",
            "Len of Validation loss: 128, Average loss: 1.7639916634652764\n",
            "Epoch: 417, Len of Training loss: 36, Average loss: 2.0464394092559814\n",
            "Len of Validation loss: 128, Average loss: 1.7325116102583706\n",
            "Epoch: 418, Len of Training loss: 36, Average loss: 2.167471624082989\n",
            "Len of Validation loss: 128, Average loss: 2.1022799336351454\n",
            "Epoch: 419, Len of Training loss: 36, Average loss: 2.118177298042509\n",
            "Len of Validation loss: 128, Average loss: 1.8588929967954755\n",
            "Epoch: 420, Len of Training loss: 36, Average loss: 2.0155807501739926\n",
            "Len of Validation loss: 128, Average loss: 1.6544179739430547\n",
            "Epoch: 421, Len of Training loss: 36, Average loss: 2.0550122525956898\n",
            "Len of Validation loss: 128, Average loss: 1.7063610560726374\n",
            "Epoch: 422, Len of Training loss: 36, Average loss: 2.0210872292518616\n",
            "Len of Validation loss: 128, Average loss: 1.9011802386958152\n",
            "Epoch: 423, Len of Training loss: 36, Average loss: 2.004033694664637\n",
            "Len of Validation loss: 128, Average loss: 1.7096010281238705\n",
            "Epoch: 424, Len of Training loss: 36, Average loss: 2.0072958171367645\n",
            "Len of Validation loss: 128, Average loss: 1.7527783535188064\n",
            "Epoch: 425, Len of Training loss: 36, Average loss: 2.058064023653666\n",
            "Len of Validation loss: 128, Average loss: 1.6678260378539562\n",
            "Epoch: 426, Len of Training loss: 36, Average loss: 2.119153552585178\n",
            "Len of Validation loss: 128, Average loss: 1.94683084404096\n",
            "Epoch: 427, Len of Training loss: 36, Average loss: 2.0413873030079737\n",
            "Len of Validation loss: 128, Average loss: 1.6619438098277897\n",
            "Epoch: 428, Len of Training loss: 36, Average loss: 1.9878024922476873\n",
            "Len of Validation loss: 128, Average loss: 1.7147929414641112\n",
            "Epoch: 429, Len of Training loss: 36, Average loss: 2.059704042143292\n",
            "Len of Validation loss: 128, Average loss: 1.7207024698145688\n",
            "Epoch: 430, Len of Training loss: 36, Average loss: 2.025012989838918\n",
            "Len of Validation loss: 128, Average loss: 1.7237826599739492\n",
            "Epoch: 431, Len of Training loss: 36, Average loss: 2.0372017323970795\n",
            "Len of Validation loss: 128, Average loss: 1.6967132587451488\n",
            "Epoch: 432, Len of Training loss: 36, Average loss: 2.0560671786467233\n",
            "Len of Validation loss: 128, Average loss: 1.6621365672908723\n",
            "Epoch: 433, Len of Training loss: 36, Average loss: 1.9808850785096486\n",
            "Len of Validation loss: 128, Average loss: 1.665157362120226\n",
            "Epoch: 434, Len of Training loss: 36, Average loss: 2.076011015309228\n",
            "Len of Validation loss: 128, Average loss: 1.9124873157124966\n",
            "Epoch: 435, Len of Training loss: 36, Average loss: 2.064395487308502\n",
            "Len of Validation loss: 128, Average loss: 1.6639849375933409\n",
            "Epoch: 436, Len of Training loss: 36, Average loss: 2.019262499279446\n",
            "Len of Validation loss: 128, Average loss: 1.7725000232458115\n",
            "Epoch: 437, Len of Training loss: 36, Average loss: 2.1201230022642346\n",
            "Len of Validation loss: 128, Average loss: 1.6809411549475044\n",
            "Epoch: 438, Len of Training loss: 36, Average loss: 2.0439111987749734\n",
            "Len of Validation loss: 128, Average loss: 1.8282798849977553\n",
            "Epoch: 439, Len of Training loss: 36, Average loss: 2.105901367134518\n",
            "Len of Validation loss: 128, Average loss: 1.7572554708458483\n",
            "Epoch: 440, Len of Training loss: 36, Average loss: 2.025048534075419\n",
            "Len of Validation loss: 128, Average loss: 1.7594455280341208\n",
            "Epoch: 441, Len of Training loss: 36, Average loss: 2.0157132115628986\n",
            "Len of Validation loss: 128, Average loss: 1.7380722505040467\n",
            "Epoch: 442, Len of Training loss: 36, Average loss: 2.0224620865450964\n",
            "Len of Validation loss: 128, Average loss: 1.6980661700945348\n",
            "Epoch: 443, Len of Training loss: 36, Average loss: 1.9873997701538935\n",
            "Len of Validation loss: 128, Average loss: 1.791699334513396\n",
            "Epoch: 444, Len of Training loss: 36, Average loss: 2.0028930604457855\n",
            "Len of Validation loss: 128, Average loss: 1.6484944340772927\n",
            "Epoch: 445, Len of Training loss: 36, Average loss: 2.0241376393371158\n",
            "Len of Validation loss: 128, Average loss: 1.7029693848453462\n",
            "Epoch: 446, Len of Training loss: 36, Average loss: 1.981033831834793\n",
            "Len of Validation loss: 128, Average loss: 1.6634702808223665\n",
            "Epoch: 447, Len of Training loss: 36, Average loss: 2.1190266708532968\n",
            "Len of Validation loss: 128, Average loss: 3.508295362815261\n",
            "Epoch: 448, Len of Training loss: 36, Average loss: 3.976294540696674\n",
            "Len of Validation loss: 128, Average loss: 2.818382059922442\n",
            "Epoch: 449, Len of Training loss: 36, Average loss: 2.2740461793210773\n",
            "Len of Validation loss: 128, Average loss: 1.9376133447512984\n",
            "Epoch: 450, Len of Training loss: 36, Average loss: 2.2460649973816342\n",
            "Len of Validation loss: 128, Average loss: 1.8471354255452752\n",
            "Epoch: 451, Len of Training loss: 36, Average loss: 2.0812710192468433\n",
            "Len of Validation loss: 128, Average loss: 1.9301544823683798\n",
            "Epoch: 452, Len of Training loss: 36, Average loss: 2.065022256639269\n",
            "Len of Validation loss: 128, Average loss: 1.7359040598385036\n",
            "Epoch: 453, Len of Training loss: 36, Average loss: 2.069783889585071\n",
            "Len of Validation loss: 128, Average loss: 1.6949947457760572\n",
            "Epoch: 454, Len of Training loss: 36, Average loss: 2.2247195872995587\n",
            "Len of Validation loss: 128, Average loss: 1.9119124147109687\n",
            "Epoch: 455, Len of Training loss: 36, Average loss: 2.276695662074619\n",
            "Len of Validation loss: 128, Average loss: 1.6888529399875551\n",
            "Epoch: 456, Len of Training loss: 36, Average loss: 1.9994893206490412\n",
            "Len of Validation loss: 128, Average loss: 1.7085043592378497\n",
            "Epoch: 457, Len of Training loss: 36, Average loss: 2.2285823391543493\n",
            "Len of Validation loss: 128, Average loss: 1.74919494939968\n",
            "Epoch: 458, Len of Training loss: 36, Average loss: 2.142324987385008\n",
            "Len of Validation loss: 128, Average loss: 1.9555509318597615\n",
            "Epoch: 459, Len of Training loss: 36, Average loss: 2.0280469126171536\n",
            "Len of Validation loss: 128, Average loss: 1.7556500739883631\n",
            "Epoch: 460, Len of Training loss: 36, Average loss: 1.9852645794550579\n",
            "Len of Validation loss: 128, Average loss: 1.6873057805933058\n",
            "Epoch: 461, Len of Training loss: 36, Average loss: 1.975529773367776\n",
            "Len of Validation loss: 128, Average loss: 1.6557190949097276\n",
            "Epoch: 462, Len of Training loss: 36, Average loss: 2.0013485617107816\n",
            "Len of Validation loss: 128, Average loss: 1.731741314055398\n",
            "Epoch: 463, Len of Training loss: 36, Average loss: 2.024362477991316\n",
            "Len of Validation loss: 128, Average loss: 1.6938175044488162\n",
            "Epoch: 464, Len of Training loss: 36, Average loss: 2.103437387280994\n",
            "Len of Validation loss: 128, Average loss: 1.669563605217263\n",
            "Epoch: 465, Len of Training loss: 36, Average loss: 1.9749655160639021\n",
            "Len of Validation loss: 128, Average loss: 1.7014262601733208\n",
            "Epoch: 466, Len of Training loss: 36, Average loss: 2.03004209862815\n",
            "Len of Validation loss: 128, Average loss: 2.0149115016683936\n",
            "Epoch: 467, Len of Training loss: 36, Average loss: 2.0576409300168357\n",
            "Len of Validation loss: 128, Average loss: 1.7087176372297108\n",
            "Epoch: 468, Len of Training loss: 36, Average loss: 2.018642703692118\n",
            "Len of Validation loss: 128, Average loss: 1.6784034280572087\n",
            "Epoch: 469, Len of Training loss: 36, Average loss: 2.0141345924801297\n",
            "Len of Validation loss: 128, Average loss: 1.8027880359441042\n",
            "Epoch: 470, Len of Training loss: 36, Average loss: 2.0441036654843225\n",
            "Len of Validation loss: 128, Average loss: 1.8180991525296122\n",
            "Epoch: 471, Len of Training loss: 36, Average loss: 2.030143082141876\n",
            "Len of Validation loss: 128, Average loss: 1.7497978513129056\n",
            "Epoch: 472, Len of Training loss: 36, Average loss: 1.984302133321762\n",
            "Len of Validation loss: 128, Average loss: 1.7483020182698965\n",
            "Epoch: 473, Len of Training loss: 36, Average loss: 2.053822957807117\n",
            "Len of Validation loss: 128, Average loss: 1.8654333357699215\n",
            "Epoch: 474, Len of Training loss: 36, Average loss: 2.153478297922346\n",
            "Len of Validation loss: 128, Average loss: 2.1248028026893735\n",
            "Epoch: 475, Len of Training loss: 36, Average loss: 2.0444592469268374\n",
            "Len of Validation loss: 128, Average loss: 1.69892210024409\n",
            "Epoch: 476, Len of Training loss: 36, Average loss: 2.0359290374649897\n",
            "Len of Validation loss: 128, Average loss: 1.6552911505568773\n",
            "Epoch: 477, Len of Training loss: 36, Average loss: 2.02173540658421\n",
            "Len of Validation loss: 128, Average loss: 1.814169844146818\n",
            "Epoch: 478, Len of Training loss: 36, Average loss: 2.0555019511116877\n",
            "Len of Validation loss: 128, Average loss: 1.6730139469727874\n",
            "Epoch: 479, Len of Training loss: 36, Average loss: 1.9987513919671376\n",
            "Len of Validation loss: 128, Average loss: 1.6698774371761829\n",
            "Epoch: 480, Len of Training loss: 36, Average loss: 1.9524077822764714\n",
            "Len of Validation loss: 128, Average loss: 1.643468438880518\n",
            "Epoch: 481, Len of Training loss: 36, Average loss: 1.968510114484363\n",
            "Len of Validation loss: 128, Average loss: 1.686456284020096\n",
            "Epoch: 482, Len of Training loss: 36, Average loss: 1.985809365908305\n",
            "Len of Validation loss: 128, Average loss: 1.7188853309489787\n",
            "Epoch: 483, Len of Training loss: 36, Average loss: 2.0132370789845786\n",
            "Len of Validation loss: 128, Average loss: 1.746752994833514\n",
            "Epoch: 484, Len of Training loss: 36, Average loss: 1.9983449247148302\n",
            "Len of Validation loss: 128, Average loss: 1.6654200609773397\n",
            "Epoch: 485, Len of Training loss: 36, Average loss: 1.961518751250373\n",
            "Len of Validation loss: 128, Average loss: 1.6327012351248413\n",
            "Epoch: 486, Len of Training loss: 36, Average loss: 1.993930846452713\n",
            "Len of Validation loss: 128, Average loss: 1.8250413252972066\n",
            "Epoch: 487, Len of Training loss: 36, Average loss: 1.9786566860145993\n",
            "Len of Validation loss: 128, Average loss: 1.6506546658929437\n",
            "Epoch: 488, Len of Training loss: 36, Average loss: 1.9723215864764319\n",
            "Len of Validation loss: 128, Average loss: 1.7541427602991462\n",
            "Epoch: 489, Len of Training loss: 36, Average loss: 2.016256206565433\n",
            "Len of Validation loss: 128, Average loss: 1.6560565759427845\n",
            "Epoch: 490, Len of Training loss: 36, Average loss: 2.0250859359900155\n",
            "Len of Validation loss: 128, Average loss: 1.6282674495596439\n",
            "Epoch: 491, Len of Training loss: 36, Average loss: 1.9400887671444151\n",
            "Len of Validation loss: 128, Average loss: 1.7284214943647385\n",
            "Epoch: 492, Len of Training loss: 36, Average loss: 1.956384089257982\n",
            "Len of Validation loss: 128, Average loss: 1.693282404448837\n",
            "Epoch: 493, Len of Training loss: 36, Average loss: 1.9745978812376659\n",
            "Len of Validation loss: 128, Average loss: 1.6906628762371838\n",
            "Epoch: 494, Len of Training loss: 36, Average loss: 2.008505450354682\n",
            "Len of Validation loss: 128, Average loss: 1.6730226436629891\n",
            "Epoch: 495, Len of Training loss: 36, Average loss: 2.0202539463837943\n",
            "Len of Validation loss: 128, Average loss: 1.932897015940398\n",
            "Epoch: 496, Len of Training loss: 36, Average loss: 1.982587410344018\n",
            "Len of Validation loss: 128, Average loss: 1.7140444228425622\n",
            "Epoch: 497, Len of Training loss: 36, Average loss: 1.98235618074735\n",
            "Len of Validation loss: 128, Average loss: 1.643807340413332\n",
            "Epoch: 498, Len of Training loss: 36, Average loss: 1.9810374908977084\n",
            "Len of Validation loss: 128, Average loss: 1.693130895961076\n",
            "Epoch: 499, Len of Training loss: 36, Average loss: 2.0439554121759205\n",
            "Len of Validation loss: 128, Average loss: 1.9793434692546725\n",
            "Epoch: 500, Len of Training loss: 36, Average loss: 2.095620718267229\n",
            "Len of Validation loss: 128, Average loss: 1.7975795285310596\n",
            "Epoch: 501, Len of Training loss: 36, Average loss: 2.020323442088233\n",
            "Len of Validation loss: 128, Average loss: 1.8259352729655802\n",
            "Epoch: 502, Len of Training loss: 36, Average loss: 2.0011025534735785\n",
            "Len of Validation loss: 128, Average loss: 1.778125770855695\n",
            "Epoch: 503, Len of Training loss: 36, Average loss: 1.976445734500885\n",
            "Len of Validation loss: 128, Average loss: 1.7978087738156319\n",
            "Epoch: 504, Len of Training loss: 36, Average loss: 1.9512893027729459\n",
            "Len of Validation loss: 128, Average loss: 1.6667612462770194\n",
            "Epoch: 505, Len of Training loss: 36, Average loss: 2.0263129572073617\n",
            "Len of Validation loss: 128, Average loss: 1.694836502429098\n",
            "Epoch: 506, Len of Training loss: 36, Average loss: 2.063637610938814\n",
            "Len of Validation loss: 128, Average loss: 1.7190701956860721\n",
            "Epoch: 507, Len of Training loss: 36, Average loss: 2.07179057598114\n",
            "Len of Validation loss: 128, Average loss: 1.7017127224244177\n",
            "Epoch: 508, Len of Training loss: 36, Average loss: 2.0163826611306934\n",
            "Len of Validation loss: 128, Average loss: 1.7863182416185737\n",
            "Epoch: 509, Len of Training loss: 36, Average loss: 2.0100601745976343\n",
            "Len of Validation loss: 128, Average loss: 1.7334891536738724\n",
            "Epoch: 510, Len of Training loss: 36, Average loss: 2.0910765859815807\n",
            "Len of Validation loss: 128, Average loss: 1.7554531747009605\n",
            "Epoch: 511, Len of Training loss: 36, Average loss: 2.030678196085824\n",
            "Len of Validation loss: 128, Average loss: 1.6573650580830872\n",
            "Epoch: 512, Len of Training loss: 36, Average loss: 1.973932296037674\n",
            "Len of Validation loss: 128, Average loss: 1.6632786262780428\n",
            "Epoch: 513, Len of Training loss: 36, Average loss: 1.954278028673596\n",
            "Len of Validation loss: 128, Average loss: 1.6533478868659586\n",
            "Epoch: 514, Len of Training loss: 36, Average loss: 1.9658903910054102\n",
            "Len of Validation loss: 128, Average loss: 1.6756008339580148\n",
            "Epoch: 515, Len of Training loss: 36, Average loss: 1.9611504475275676\n",
            "Len of Validation loss: 128, Average loss: 1.7006995424162596\n",
            "Epoch: 516, Len of Training loss: 36, Average loss: 1.968680461247762\n",
            "Len of Validation loss: 128, Average loss: 1.6993463777471334\n",
            "Epoch: 517, Len of Training loss: 36, Average loss: 1.993168009652032\n",
            "Len of Validation loss: 128, Average loss: 1.751725752837956\n",
            "Epoch: 518, Len of Training loss: 36, Average loss: 1.982752200629976\n",
            "Len of Validation loss: 128, Average loss: 1.6428945867810398\n",
            "Epoch: 519, Len of Training loss: 36, Average loss: 1.949728075000975\n",
            "Len of Validation loss: 128, Average loss: 1.675135656259954\n",
            "Epoch: 520, Len of Training loss: 36, Average loss: 1.951105276743571\n",
            "Len of Validation loss: 128, Average loss: 1.6616229359060526\n",
            "Epoch: 521, Len of Training loss: 36, Average loss: 1.9671292006969452\n",
            "Len of Validation loss: 128, Average loss: 1.6719412226229906\n",
            "Epoch: 522, Len of Training loss: 36, Average loss: 2.020968029896418\n",
            "Len of Validation loss: 128, Average loss: 1.7135998797602952\n",
            "Epoch: 523, Len of Training loss: 36, Average loss: 1.946173154645496\n",
            "Len of Validation loss: 128, Average loss: 1.6693601160077378\n",
            "Epoch: 524, Len of Training loss: 36, Average loss: 1.9751350912782881\n",
            "Len of Validation loss: 128, Average loss: 1.7180787054821849\n",
            "Epoch: 525, Len of Training loss: 36, Average loss: 1.9709585275914934\n",
            "Len of Validation loss: 128, Average loss: 1.7828630842268467\n",
            "Epoch: 526, Len of Training loss: 36, Average loss: 2.0243619779745736\n",
            "Len of Validation loss: 128, Average loss: 1.7491543972864747\n",
            "Epoch: 527, Len of Training loss: 36, Average loss: 2.0711370044284396\n",
            "Len of Validation loss: 128, Average loss: 1.9460306507535279\n",
            "Epoch: 528, Len of Training loss: 36, Average loss: 2.161124312215381\n",
            "Len of Validation loss: 128, Average loss: 1.74802510952577\n",
            "Epoch: 529, Len of Training loss: 36, Average loss: 2.0442236529456244\n",
            "Len of Validation loss: 128, Average loss: 1.7294344929978251\n",
            "Epoch: 530, Len of Training loss: 36, Average loss: 2.0174640549553766\n",
            "Len of Validation loss: 128, Average loss: 1.734586094506085\n",
            "Epoch: 531, Len of Training loss: 36, Average loss: 2.0262173778480954\n",
            "Len of Validation loss: 128, Average loss: 1.712384530575946\n",
            "Epoch: 532, Len of Training loss: 36, Average loss: 2.026034947898653\n",
            "Len of Validation loss: 128, Average loss: 1.7924278676509857\n",
            "Epoch: 533, Len of Training loss: 36, Average loss: 1.9960759745703802\n",
            "Len of Validation loss: 128, Average loss: 1.710166469682008\n",
            "Epoch: 534, Len of Training loss: 36, Average loss: 1.9974299338128831\n",
            "Len of Validation loss: 128, Average loss: 1.6644184601027519\n",
            "Epoch: 535, Len of Training loss: 36, Average loss: 1.9524567226568859\n",
            "Len of Validation loss: 128, Average loss: 1.752756955451332\n",
            "Epoch: 536, Len of Training loss: 36, Average loss: 1.9829761584599812\n",
            "Len of Validation loss: 128, Average loss: 1.6446877343114465\n",
            "Epoch: 537, Len of Training loss: 36, Average loss: 2.035915265480677\n",
            "Len of Validation loss: 128, Average loss: 1.6296865986660123\n",
            "Epoch: 538, Len of Training loss: 36, Average loss: 1.957815448443095\n",
            "Len of Validation loss: 128, Average loss: 1.6433862880803645\n",
            "Epoch: 539, Len of Training loss: 36, Average loss: 1.9557035168011982\n",
            "Len of Validation loss: 128, Average loss: 1.7603240483440459\n",
            "Epoch: 540, Len of Training loss: 36, Average loss: 1.9801071915361617\n",
            "Len of Validation loss: 128, Average loss: 1.7444230315741152\n",
            "Epoch: 541, Len of Training loss: 36, Average loss: 2.039075563351313\n",
            "Len of Validation loss: 128, Average loss: 1.6760085253044963\n",
            "Epoch: 542, Len of Training loss: 36, Average loss: 1.962418857547972\n",
            "Len of Validation loss: 128, Average loss: 1.7266458065714687\n",
            "Epoch: 543, Len of Training loss: 36, Average loss: 1.9523612989319696\n",
            "Len of Validation loss: 128, Average loss: 1.632968850666657\n",
            "Epoch: 544, Len of Training loss: 36, Average loss: 2.0056970616181693\n",
            "Len of Validation loss: 128, Average loss: 1.7701021418906748\n",
            "Epoch: 545, Len of Training loss: 36, Average loss: 2.0730708837509155\n",
            "Len of Validation loss: 128, Average loss: 1.6816375574562699\n",
            "Epoch: 546, Len of Training loss: 36, Average loss: 2.0391021735138364\n",
            "Len of Validation loss: 128, Average loss: 1.7770808166824281\n",
            "Epoch: 547, Len of Training loss: 36, Average loss: 1.9710478219721053\n",
            "Len of Validation loss: 128, Average loss: 1.7169149823021144\n",
            "Epoch: 548, Len of Training loss: 36, Average loss: 1.9792435268561046\n",
            "Len of Validation loss: 128, Average loss: 1.6885009678080678\n",
            "Epoch: 549, Len of Training loss: 36, Average loss: 2.098656154341168\n",
            "Len of Validation loss: 128, Average loss: 2.313635236117989\n",
            "Epoch: 550, Len of Training loss: 36, Average loss: 2.1427445742819042\n",
            "Len of Validation loss: 128, Average loss: 1.7749579122755677\n",
            "Epoch: 551, Len of Training loss: 36, Average loss: 2.0510495983892016\n",
            "Len of Validation loss: 128, Average loss: 1.67823891248554\n",
            "Epoch: 552, Len of Training loss: 36, Average loss: 1.9965984092818365\n",
            "Len of Validation loss: 128, Average loss: 1.70604416471906\n",
            "Epoch: 553, Len of Training loss: 36, Average loss: 1.9610045817163255\n",
            "Len of Validation loss: 128, Average loss: 1.7317634266801178\n",
            "Epoch: 554, Len of Training loss: 36, Average loss: 2.0330558319886527\n",
            "Len of Validation loss: 128, Average loss: 1.8432929650880396\n",
            "Epoch: 555, Len of Training loss: 36, Average loss: 1.9901090893480513\n",
            "Len of Validation loss: 128, Average loss: 1.6345103313215077\n",
            "Epoch: 556, Len of Training loss: 36, Average loss: 1.9871468941370647\n",
            "Len of Validation loss: 128, Average loss: 1.6773388925939798\n",
            "Epoch: 557, Len of Training loss: 36, Average loss: 2.0042677919069924\n",
            "Len of Validation loss: 128, Average loss: 1.6552937291562557\n",
            "Epoch: 558, Len of Training loss: 36, Average loss: 1.9606942865583632\n",
            "Len of Validation loss: 128, Average loss: 1.6283566372003406\n",
            "Epoch: 559, Len of Training loss: 36, Average loss: 1.9026124311818018\n",
            "Len of Validation loss: 128, Average loss: 1.6952880714088678\n",
            "Epoch: 560, Len of Training loss: 36, Average loss: 1.9619061085912917\n",
            "Len of Validation loss: 128, Average loss: 1.665103980107233\n",
            "Epoch: 561, Len of Training loss: 36, Average loss: 1.9764009051852756\n",
            "Len of Validation loss: 128, Average loss: 1.6346583932172507\n",
            "Epoch: 562, Len of Training loss: 36, Average loss: 1.9334304200278387\n",
            "Len of Validation loss: 128, Average loss: 1.8146371264010668\n",
            "Epoch: 563, Len of Training loss: 36, Average loss: 2.046977354420556\n",
            "Len of Validation loss: 128, Average loss: 1.7277466889936477\n",
            "Epoch: 564, Len of Training loss: 36, Average loss: 1.9328100515736475\n",
            "Len of Validation loss: 128, Average loss: 1.6950793149881065\n",
            "Epoch: 565, Len of Training loss: 36, Average loss: 1.9616051216920216\n",
            "Len of Validation loss: 128, Average loss: 1.720567712560296\n",
            "Epoch: 566, Len of Training loss: 36, Average loss: 1.9875132805771298\n",
            "Len of Validation loss: 128, Average loss: 1.6431636225897819\n",
            "Epoch: 567, Len of Training loss: 36, Average loss: 2.0150605771276684\n",
            "Len of Validation loss: 128, Average loss: 1.7088428088463843\n",
            "Epoch: 568, Len of Training loss: 36, Average loss: 2.0632883409659066\n",
            "Len of Validation loss: 128, Average loss: 1.7485994342714548\n",
            "Epoch: 569, Len of Training loss: 36, Average loss: 1.9923308193683624\n",
            "Len of Validation loss: 128, Average loss: 1.83368233544752\n",
            "Epoch: 570, Len of Training loss: 36, Average loss: 1.9642245355579588\n",
            "Len of Validation loss: 128, Average loss: 1.6829537071753293\n",
            "Epoch: 571, Len of Training loss: 36, Average loss: 1.9364304973019495\n",
            "Len of Validation loss: 128, Average loss: 1.6800492310430855\n",
            "Epoch: 572, Len of Training loss: 36, Average loss: 2.092068291372723\n",
            "Len of Validation loss: 128, Average loss: 1.937832057941705\n",
            "Epoch: 573, Len of Training loss: 36, Average loss: 2.0881140298313565\n",
            "Len of Validation loss: 128, Average loss: 1.619666418991983\n",
            "Epoch: 574, Len of Training loss: 36, Average loss: 1.9805448651313782\n",
            "Len of Validation loss: 128, Average loss: 1.701551392674446\n",
            "Epoch: 575, Len of Training loss: 36, Average loss: 1.9707332419024572\n",
            "Len of Validation loss: 128, Average loss: 1.649932072032243\n",
            "Epoch: 576, Len of Training loss: 36, Average loss: 1.9456595447328355\n",
            "Len of Validation loss: 128, Average loss: 1.6553068021312356\n",
            "Epoch: 577, Len of Training loss: 36, Average loss: 1.9826304680771298\n",
            "Len of Validation loss: 128, Average loss: 1.6556873274967074\n",
            "Epoch: 578, Len of Training loss: 36, Average loss: 2.0336698624822827\n",
            "Len of Validation loss: 128, Average loss: 1.6920990166254342\n",
            "Epoch: 579, Len of Training loss: 36, Average loss: 1.9398892290062375\n",
            "Len of Validation loss: 128, Average loss: 1.6653386338148266\n",
            "Epoch: 580, Len of Training loss: 36, Average loss: 2.49689159128401\n",
            "Len of Validation loss: 128, Average loss: 5.583553915843368\n",
            "Epoch: 581, Len of Training loss: 36, Average loss: 7.884387744797601\n",
            "Len of Validation loss: 128, Average loss: 8.765419060364366\n",
            "Epoch: 582, Len of Training loss: 36, Average loss: 4.661272903283437\n",
            "Len of Validation loss: 128, Average loss: 2.388544208370149\n",
            "Epoch: 583, Len of Training loss: 36, Average loss: 2.473680701520708\n",
            "Len of Validation loss: 128, Average loss: 2.095510699786246\n",
            "Epoch: 584, Len of Training loss: 36, Average loss: 2.1725241574976177\n",
            "Len of Validation loss: 128, Average loss: 1.877567816292867\n",
            "Epoch: 585, Len of Training loss: 36, Average loss: 2.053677588701248\n",
            "Len of Validation loss: 128, Average loss: 1.7315037588123232\n",
            "Epoch: 586, Len of Training loss: 36, Average loss: 2.10404806666904\n",
            "Len of Validation loss: 128, Average loss: 1.855262657161802\n",
            "Epoch: 587, Len of Training loss: 36, Average loss: 2.1816515823205314\n",
            "Len of Validation loss: 128, Average loss: 1.868339785374701\n",
            "Epoch: 588, Len of Training loss: 36, Average loss: 2.0566573441028595\n",
            "Len of Validation loss: 128, Average loss: 1.7113703563809395\n",
            "Epoch: 589, Len of Training loss: 36, Average loss: 1.992306845055686\n",
            "Len of Validation loss: 128, Average loss: 1.726162129547447\n",
            "Epoch: 590, Len of Training loss: 36, Average loss: 2.0850289397769504\n",
            "Len of Validation loss: 128, Average loss: 1.8076029880903661\n",
            "Epoch: 591, Len of Training loss: 36, Average loss: 2.0123635795381336\n",
            "Len of Validation loss: 128, Average loss: 1.7141010221093893\n",
            "Epoch: 592, Len of Training loss: 36, Average loss: 2.014379951688978\n",
            "Len of Validation loss: 128, Average loss: 1.6763285968918353\n",
            "Epoch: 593, Len of Training loss: 36, Average loss: 2.069258060720232\n",
            "Len of Validation loss: 128, Average loss: 1.7218862874433398\n",
            "Epoch: 594, Len of Training loss: 36, Average loss: 2.0713938540882535\n",
            "Len of Validation loss: 128, Average loss: 1.6676771480124444\n",
            "Epoch: 595, Len of Training loss: 36, Average loss: 1.9946988059414759\n",
            "Len of Validation loss: 128, Average loss: 1.654867899371311\n",
            "Epoch: 596, Len of Training loss: 36, Average loss: 1.9541316131750743\n",
            "Len of Validation loss: 128, Average loss: 1.878151010721922\n",
            "Epoch: 597, Len of Training loss: 36, Average loss: 1.9899164901839361\n",
            "Len of Validation loss: 128, Average loss: 1.6713180562946945\n",
            "Epoch: 598, Len of Training loss: 36, Average loss: 1.9855908387237124\n",
            "Len of Validation loss: 128, Average loss: 1.80070685967803\n",
            "Epoch: 599, Len of Training loss: 36, Average loss: 2.0222579605049558\n",
            "Len of Validation loss: 128, Average loss: 1.7595173539593816\n",
            "Epoch: 600, Len of Training loss: 36, Average loss: 2.053488449917899\n",
            "Len of Validation loss: 128, Average loss: 1.7211133618839085\n",
            "Epoch: 601, Len of Training loss: 36, Average loss: 2.018748448954688\n",
            "Len of Validation loss: 128, Average loss: 1.8297356725670397\n",
            "Epoch: 602, Len of Training loss: 36, Average loss: 2.0057640108797283\n",
            "Len of Validation loss: 128, Average loss: 1.676713515771553\n",
            "Epoch: 603, Len of Training loss: 36, Average loss: 1.9789515170786116\n",
            "Len of Validation loss: 128, Average loss: 1.644971095956862\n",
            "Epoch: 604, Len of Training loss: 36, Average loss: 1.9909700353940327\n",
            "Len of Validation loss: 128, Average loss: 1.6569622227689251\n",
            "Epoch: 605, Len of Training loss: 36, Average loss: 1.9742750293678708\n",
            "Len of Validation loss: 128, Average loss: 1.688928799238056\n",
            "Epoch: 606, Len of Training loss: 36, Average loss: 1.9407424761189356\n",
            "Len of Validation loss: 128, Average loss: 1.652081892825663\n",
            "Epoch: 607, Len of Training loss: 36, Average loss: 2.0716019802623324\n",
            "Len of Validation loss: 128, Average loss: 1.8797989375889301\n",
            "Epoch: 608, Len of Training loss: 36, Average loss: 2.0180842545297413\n",
            "Len of Validation loss: 128, Average loss: 1.678128506289795\n",
            "Epoch: 609, Len of Training loss: 36, Average loss: 2.047362599107954\n",
            "Len of Validation loss: 128, Average loss: 1.6692950141150504\n",
            "Epoch: 610, Len of Training loss: 36, Average loss: 2.0535950793160334\n",
            "Len of Validation loss: 128, Average loss: 1.7488165621180087\n",
            "Epoch: 611, Len of Training loss: 36, Average loss: 2.0226679941018424\n",
            "Len of Validation loss: 128, Average loss: 1.652099176775664\n",
            "Epoch: 612, Len of Training loss: 36, Average loss: 2.0054387185308666\n",
            "Len of Validation loss: 128, Average loss: 1.6647702348418534\n",
            "Epoch: 613, Len of Training loss: 36, Average loss: 1.9837708705001407\n",
            "Len of Validation loss: 128, Average loss: 1.6577742164954543\n",
            "Epoch: 614, Len of Training loss: 36, Average loss: 2.042785210741891\n",
            "Len of Validation loss: 128, Average loss: 1.7141662898939103\n",
            "Epoch: 615, Len of Training loss: 36, Average loss: 1.9545611772272322\n",
            "Len of Validation loss: 128, Average loss: 1.6824597562663257\n",
            "Epoch: 616, Len of Training loss: 36, Average loss: 1.9813067151440515\n",
            "Len of Validation loss: 128, Average loss: 1.6749002309516072\n",
            "Epoch: 617, Len of Training loss: 36, Average loss: 1.9743160704771678\n",
            "Len of Validation loss: 128, Average loss: 1.615657641319558\n",
            "Epoch: 618, Len of Training loss: 36, Average loss: 1.9727615747186873\n",
            "Len of Validation loss: 128, Average loss: 1.6151350021827966\n",
            "Epoch: 619, Len of Training loss: 36, Average loss: 1.9394109414683447\n",
            "Len of Validation loss: 128, Average loss: 1.675435377517715\n",
            "Epoch: 620, Len of Training loss: 36, Average loss: 2.006633265150918\n",
            "Len of Validation loss: 128, Average loss: 1.783551182365045\n",
            "Epoch: 621, Len of Training loss: 36, Average loss: 2.039763023455938\n",
            "Len of Validation loss: 128, Average loss: 1.678915943019092\n",
            "Epoch: 622, Len of Training loss: 36, Average loss: 1.982415606578191\n",
            "Len of Validation loss: 128, Average loss: 1.7969257780350745\n",
            "Epoch: 623, Len of Training loss: 36, Average loss: 1.9634870489438374\n",
            "Len of Validation loss: 128, Average loss: 1.644154526060447\n",
            "Epoch: 624, Len of Training loss: 36, Average loss: 2.0140953924920826\n",
            "Len of Validation loss: 128, Average loss: 1.8464377068448812\n",
            "Epoch: 625, Len of Training loss: 36, Average loss: 2.049763732486301\n",
            "Len of Validation loss: 128, Average loss: 1.6753495163284242\n",
            "Epoch: 626, Len of Training loss: 36, Average loss: 2.054759257369571\n",
            "Len of Validation loss: 128, Average loss: 1.682321330998093\n",
            "Epoch: 627, Len of Training loss: 36, Average loss: 1.9610738191339705\n",
            "Len of Validation loss: 128, Average loss: 1.6878778752870858\n",
            "Epoch: 628, Len of Training loss: 36, Average loss: 1.981175833278232\n",
            "Len of Validation loss: 128, Average loss: 1.6286898478865623\n",
            "Epoch: 629, Len of Training loss: 36, Average loss: 1.9966320594151814\n",
            "Len of Validation loss: 128, Average loss: 1.7615996338427067\n",
            "Epoch: 630, Len of Training loss: 36, Average loss: 1.9721281826496124\n",
            "Len of Validation loss: 128, Average loss: 1.7574741190765053\n",
            "Epoch: 631, Len of Training loss: 36, Average loss: 1.9512385196155972\n",
            "Len of Validation loss: 128, Average loss: 1.6249985774047673\n",
            "Epoch: 632, Len of Training loss: 36, Average loss: 1.9917515615622203\n",
            "Len of Validation loss: 128, Average loss: 2.097843974363059\n",
            "Epoch: 633, Len of Training loss: 36, Average loss: 2.026904735300276\n",
            "Len of Validation loss: 128, Average loss: 1.6970271575264633\n",
            "Epoch: 634, Len of Training loss: 36, Average loss: 1.9058306614557903\n",
            "Len of Validation loss: 128, Average loss: 1.6413905597291887\n",
            "Epoch: 635, Len of Training loss: 36, Average loss: 1.9660131699509091\n",
            "Len of Validation loss: 128, Average loss: 1.6733917756937444\n",
            "Epoch: 636, Len of Training loss: 36, Average loss: 1.9581368366877239\n",
            "Len of Validation loss: 128, Average loss: 1.7995933855418116\n",
            "Epoch: 637, Len of Training loss: 36, Average loss: 1.9668102297517989\n",
            "Len of Validation loss: 128, Average loss: 1.6429872529115528\n",
            "Epoch: 638, Len of Training loss: 36, Average loss: 1.9887454973326788\n",
            "Len of Validation loss: 128, Average loss: 1.6418598340824246\n",
            "Epoch: 639, Len of Training loss: 36, Average loss: 1.994684014055464\n",
            "Len of Validation loss: 128, Average loss: 1.6390297603793442\n",
            "Epoch: 640, Len of Training loss: 36, Average loss: 1.91197583410475\n",
            "Len of Validation loss: 128, Average loss: 1.6966791464947164\n",
            "Epoch: 641, Len of Training loss: 36, Average loss: 1.9193488127655454\n",
            "Len of Validation loss: 128, Average loss: 1.6673744171857834\n",
            "Epoch: 642, Len of Training loss: 36, Average loss: 1.9440880318482716\n",
            "Len of Validation loss: 128, Average loss: 1.7218433753587306\n",
            "Epoch: 643, Len of Training loss: 36, Average loss: 2.039689974652396\n",
            "Len of Validation loss: 128, Average loss: 1.6605093530379236\n",
            "Epoch: 644, Len of Training loss: 36, Average loss: 1.9045658111572266\n",
            "Len of Validation loss: 128, Average loss: 1.701380557846278\n",
            "Epoch: 645, Len of Training loss: 36, Average loss: 1.9368020362324185\n",
            "Len of Validation loss: 128, Average loss: 1.6561413863673806\n",
            "Epoch: 646, Len of Training loss: 36, Average loss: 1.9104084504975214\n",
            "Len of Validation loss: 128, Average loss: 1.6544598443433642\n",
            "Epoch: 647, Len of Training loss: 36, Average loss: 1.9228405952453613\n",
            "Len of Validation loss: 128, Average loss: 1.6852516611106694\n",
            "Epoch: 648, Len of Training loss: 36, Average loss: 1.9961390064822302\n",
            "Len of Validation loss: 128, Average loss: 1.9701681709848344\n",
            "Epoch: 649, Len of Training loss: 36, Average loss: 2.0265897942913904\n",
            "Len of Validation loss: 128, Average loss: 1.6278719347901642\n",
            "Epoch: 650, Len of Training loss: 36, Average loss: 2.0314167969756656\n",
            "Len of Validation loss: 128, Average loss: 1.7249067327938974\n",
            "Epoch: 651, Len of Training loss: 36, Average loss: 2.036509338352415\n",
            "Len of Validation loss: 128, Average loss: 1.751780773513019\n",
            "Epoch: 652, Len of Training loss: 36, Average loss: 2.103490518199073\n",
            "Len of Validation loss: 128, Average loss: 1.7747640702873468\n",
            "Epoch: 653, Len of Training loss: 36, Average loss: 1.994602964984046\n",
            "Len of Validation loss: 128, Average loss: 1.6458810078911483\n",
            "Epoch: 654, Len of Training loss: 36, Average loss: 2.0122464862134724\n",
            "Len of Validation loss: 128, Average loss: 1.786007555667311\n",
            "Epoch: 655, Len of Training loss: 36, Average loss: 1.9232598675621881\n",
            "Len of Validation loss: 128, Average loss: 1.6880506025627255\n",
            "Epoch: 656, Len of Training loss: 36, Average loss: 1.9433752331468794\n",
            "Len of Validation loss: 128, Average loss: 1.820572582539171\n",
            "Epoch: 657, Len of Training loss: 36, Average loss: 1.9601419568061829\n",
            "Len of Validation loss: 128, Average loss: 1.638389000436291\n",
            "Epoch: 658, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 659, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 660, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 661, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 662, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 663, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 664, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 665, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 666, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 667, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 668, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 669, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 670, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 671, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 672, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 673, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 674, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 675, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 676, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 677, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 678, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 679, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 680, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 681, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 682, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 683, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 684, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 685, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 686, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 687, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 688, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 689, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 690, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 691, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 692, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 693, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 694, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 695, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 696, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 697, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 698, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 699, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 700, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 701, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n",
            "Epoch: 702, Len of Training loss: 36, Average loss: nan\n",
            "Len of Validation loss: 128, Average loss: nan\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Asus\\Documents\\ml\\PIL\\gnn_pil_versions\\pil_gnnv3.ipynb Cell 32\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X46sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X46sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m3000\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X46sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# Train.\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X46sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     train_epoch_losses \u001b[39m=\u001b[39m train(train_loader)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X46sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, Len of Training loss: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(train_epoch_losses)\u001b[39m}\u001b[39;00m\u001b[39m, Average loss: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mfloat\u001b[39m(np\u001b[39m.\u001b[39msum(train_epoch_losses))\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(train_epoch_losses)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X46sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     train_losses\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mmean(train_epoch_losses))\n",
            "\u001b[1;32mc:\\Users\\Asus\\Documents\\ml\\PIL\\gnn_pil_versions\\pil_gnnv3.ipynb Cell 32\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X46sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X46sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m i\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X46sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataset):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X46sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m#print(\"misaa\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X46sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# Training step.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X46sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     batch \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X46sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
            "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "\u001b[1;32mc:\\Users\\Asus\\Documents\\ml\\PIL\\gnn_pil_versions\\pil_gnnv3.ipynb Cell 32\u001b[0m in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X46sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X46sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m     \u001b[39m#print(\"Part: \", self.processed_file_names[1])\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X46sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m     \u001b[39mif\u001b[39;00m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X46sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m         data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocessed_dir, \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdata_train_\u001b[39;49m\u001b[39m{\u001b[39;49;00midx\u001b[39m}\u001b[39;49;00m\u001b[39m.pt\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X46sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m     \u001b[39melif\u001b[39;00m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Asus/Documents/ml/PIL/gnn_pil_versions/pil_gnnv3.ipynb#X46sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m         data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessed_dir, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata_valid_\u001b[39m\u001b[39m{\u001b[39;00midx\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m'\u001b[39m))\n",
            "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\serialization.py:789\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    787\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    788\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m--> 789\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[0;32m    790\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[0;32m    791\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\serialization.py:1131\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1129\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1130\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[1;32m-> 1131\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m   1133\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1135\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
            "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\serialization.py:1101\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m loaded_storages:\n\u001b[0;32m   1100\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1101\u001b[0m     load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[0;32m   1103\u001b[0m \u001b[39mreturn\u001b[39;00m loaded_storages[key]\n",
            "File \u001b[1;32mc:\\Users\\Asus\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\serialization.py:1079\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_tensor\u001b[39m(dtype, numel, key, location):\n\u001b[0;32m   1077\u001b[0m     name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata/\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m-> 1079\u001b[0m     storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39;49mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39;49mUntypedStorage)\u001b[39m.\u001b[39;49mstorage()\u001b[39m.\u001b[39muntyped()\n\u001b[0;32m   1080\u001b[0m     \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m     \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m   1082\u001b[0m     loaded_storages[key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[0;32m   1083\u001b[0m         wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[0;32m   1084\u001b[0m         dtype\u001b[39m=\u001b[39mdtype)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "# Metrics recorder per epoch.\n",
        "train_losses = []\n",
        "\n",
        "valid_losses = []\n",
        "valid_losses_corrected = []\n",
        "\n",
        "# Training loop.\n",
        "model.train()\n",
        "for epoch in range(3000):\n",
        "    # Train.\n",
        "    train_epoch_losses = train(train_loader)\n",
        "    print(f\"Epoch: {epoch}, Len of Training loss: {len(train_epoch_losses)}, Average loss: {float(np.sum(train_epoch_losses))/len(train_epoch_losses)}\")\n",
        "    train_losses.append(np.mean(train_epoch_losses))\n",
        "\n",
        "    valid_epoch_losses= evaluate(valid_loader)\n",
        "    print(f\"Len of Validation loss: {len(valid_epoch_losses)}, Average loss: {float(np.sum(valid_epoch_losses))/len(valid_epoch_losses)}\")\n",
        "    valid_losses.append(np.mean(valid_epoch_losses))\n",
        "    # save model at 205 epoch\n",
        "    if epoch == 205:\n",
        "        torch.save(model, \"model_205.pth\")\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 206 epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [],
      "source": [
        "#remove gpu usage\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "n7mjcXV3coC4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "fmZa5ypccoC5",
        "outputId": "bc1b7828-d5f1-4eb0-f32b-46d3b6b5273d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 500\n",
        "\n",
        "plt.plot(np.linspace(1, num_epochs, num_epochs).astype(int), train_losses[:500], label=\"Training loss\")\n",
        "plt.plot(np.linspace(1, num_epochs, num_epochs).astype(int), valid_losses[:500], label=\"Testing loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "MZ4PODuucoC6",
        "outputId": "324fc0ed-ff9a-4818-c2c3-8f7c30af1d77"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_epochs = 1600\n",
        "\n",
        "plt.plot(np.linspace(1, num_epochs, num_epochs).astype(int), train_losses[:1600], label=\"Training loss\")\n",
        "plt.plot(np.linspace(1, num_epochs, num_epochs).astype(int), valid_losses[:1600], label=\"Testing loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "iYfRwBEScoC7"
      },
      "outputs": [],
      "source": [
        "torch.save(model,\"GNN_edge_rssi_int_v3.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uy1xuS5ZcoC8",
        "outputId": "f8da4168-6859-431d-f324-5e9dd7c9a381"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MetaNet(\n",
              "  (input): MetaLayer(\n",
              "    edge_model=EdgeModel(\n",
              "    (edge_mlp): Sequential(\n",
              "      (0): Linear(in_features=29, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "  ),\n",
              "    node_model=NodeModel(\n",
              "    (node_mlp_1): Sequential(\n",
              "      (0): Linear(in_features=141, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "    (node_mlp_2): Sequential(\n",
              "      (0): Linear(in_features=141, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "  ),\n",
              "    global_model=None\n",
              "  )\n",
              "  (output): MetaLayer(\n",
              "    edge_model=EdgeModel(\n",
              "    (edge_mlp): Sequential(\n",
              "      (0): Linear(in_features=384, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "  ),\n",
              "    node_model=NodeModel(\n",
              "    (node_mlp_1): Sequential(\n",
              "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    )\n",
              "    (node_mlp_2): Sequential(\n",
              "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
              "    )\n",
              "  ),\n",
              "    global_model=None\n",
              "  )\n",
              "  (attention): MultiheadAttention(\n",
              "    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 160,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "modelv2 = torch.load(\"../RESULTS/model_500.pth\")\n",
        "modelv2.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(dataset):\n",
        "    # Monitor evaluation.\n",
        "    losses = []\n",
        "\n",
        "    # Validation (1)\n",
        "    modelv2.eval()\n",
        "    i = 0\n",
        "    for i, batch in enumerate(dataset):\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        # Calculate validation losses.\n",
        "        out = modelv2(batch)\n",
        "        loss = torch.sqrt(F.mse_loss(out.squeeze(), batch.y.squeeze()))\n",
        "\n",
        "        # Metric logging.\n",
        "        losses.append(loss.item())\n",
        "        if(i == 383): break\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3JS5k_jcoC8",
        "outputId": "c84a067a-6bd5-4b20-835c-d547ee6b42e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Len of Validation loss: 36, Average loss: 1056.3697408040364\n"
          ]
        }
      ],
      "source": [
        "#evaluate the model on the test set\n",
        "test_epoch_losses= evaluate(train_loader)\n",
        "print(f\"Len of Validation loss: {len(test_epoch_losses)}, Average loss: {float(np.sum(test_epoch_losses))/len(test_epoch_losses)}\")\n",
        "valid_losses.append(np.mean(test_epoch_losses))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYnsCHSpcoC-",
        "outputId": "4d2717c2-1ffa-41fe-eba7-017017d92d3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1059.477140814066\n",
            "29.779035935344098\n",
            "32.549610455642416\n"
          ]
        }
      ],
      "source": [
        "#calculate the mean squared error for the test set\n",
        "print(np.mean(test_epoch_losses))\n",
        "#calculate the mean absolute error for the test set\n",
        "print(np.mean(np.sqrt(test_epoch_losses)))\n",
        "#calculate the root mean squared error for the test set\n",
        "print(np.sqrt(np.mean(test_epoch_losses)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2aUkX4uMe7x",
        "outputId": "e3fcf0e2-1bd7-462f-e6ad-a7363a707f25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1724171.637259153\n",
            "1059.477140814066\n",
            "1313.077163482464\n"
          ]
        }
      ],
      "source": [
        "squared_loss = [i**2 for i in test_epoch_losses]\n",
        "print(np.mean(squared_loss))\n",
        "#calculate the mean absolute error for the test set\n",
        "print(np.mean(np.sqrt(squared_loss)))\n",
        "#calculate the root mean squared error for the test set\n",
        "print(np.sqrt(np.mean(squared_loss)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3CHUFo5coC_",
        "outputId": "43781cd3-2893-40c3-888f-acca22c810d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[    19.0897],\n",
            "        [     1.3709],\n",
            "        [     1.6571],\n",
            "        [     0.4226],\n",
            "        [     0.4225],\n",
            "        [     1.7116],\n",
            "        [     1.4826],\n",
            "        [     1.6775],\n",
            "        [     1.1024],\n",
            "        [     0.7719],\n",
            "        [     0.4895],\n",
            "        [     1.7993],\n",
            "        [     2.4787],\n",
            "        [    -0.1309],\n",
            "        [    -0.0699],\n",
            "        [    -0.1316],\n",
            "        [    -0.1238],\n",
            "        [     0.2801],\n",
            "        [    -0.0667],\n",
            "        [     0.2250],\n",
            "        [    -0.1557],\n",
            "        [     0.0033],\n",
            "        [    -0.1561],\n",
            "        [    -0.0098],\n",
            "        [     0.2874],\n",
            "        [    -0.1591],\n",
            "        [     2.3397],\n",
            "        [    -0.1331],\n",
            "        [     0.1385],\n",
            "        [    -0.1630],\n",
            "        [    -0.1776],\n",
            "        [     0.1865],\n",
            "        [    -0.1429],\n",
            "        [     0.0163],\n",
            "        [    -0.1523],\n",
            "        [     0.0230],\n",
            "        [    -0.1357],\n",
            "        [    -0.1590],\n",
            "        [    -0.1281],\n",
            "        [    -0.0112],\n",
            "        [    -0.1345],\n",
            "        [     0.1121],\n",
            "        [     4.0910],\n",
            "        [    -0.0855],\n",
            "        [    -0.0608],\n",
            "        [     0.4145],\n",
            "        [     0.7964],\n",
            "        [     0.3248],\n",
            "        [     0.2469],\n",
            "        [    -0.0164],\n",
            "        [     0.3411],\n",
            "        [    -0.0848],\n",
            "        [    -0.0892],\n",
            "        [     0.2798],\n",
            "        [     0.1017],\n",
            "        [     0.0046],\n",
            "        [    19.2722],\n",
            "        [     1.5737],\n",
            "        [     1.6029],\n",
            "        [     1.4785],\n",
            "        [     1.0414],\n",
            "        [    -0.2806],\n",
            "        [    -0.2510],\n",
            "        [     0.5660],\n",
            "        [     1.3635],\n",
            "        [     1.6734],\n",
            "        [     1.5281],\n",
            "        [     0.7579],\n",
            "        [     1.2004],\n",
            "        [     1.5386],\n",
            "        [     1.6408],\n",
            "        [    -0.2681],\n",
            "        [     9.6665],\n",
            "        [    -0.1622],\n",
            "        [    -0.1826],\n",
            "        [     1.7393],\n",
            "        [     1.5915],\n",
            "        [     0.3976],\n",
            "        [    -0.1716],\n",
            "        [    -0.1623],\n",
            "        [     0.4514],\n",
            "        [     0.0923],\n",
            "        [    -0.1454],\n",
            "        [     0.5921],\n",
            "        [    -0.1727],\n",
            "        [     0.2750],\n",
            "        [     0.3923],\n",
            "        [    -0.1481],\n",
            "        [    -0.1270],\n",
            "        [     0.2934],\n",
            "        [    -0.0424],\n",
            "        [    -0.1435],\n",
            "        [    -0.1359],\n",
            "        [     1.4213],\n",
            "        [    -0.0236],\n",
            "        [    -0.1761],\n",
            "        [    -0.1249],\n",
            "        [     0.2081],\n",
            "        [    -0.1785],\n",
            "        [     0.0186],\n",
            "        [    -0.1351],\n",
            "        [    -0.1203],\n",
            "        [    -0.1738],\n",
            "        [    -0.2052],\n",
            "        [    -0.1282],\n",
            "        [    -0.1031],\n",
            "        [    -0.1734],\n",
            "        [    -0.1300],\n",
            "        [    -0.1725],\n",
            "        [    -0.1776],\n",
            "        [    -0.2088],\n",
            "        [     0.2207],\n",
            "        [    -0.1374],\n",
            "        [    -0.1149],\n",
            "        [    -0.1029],\n",
            "        [    -0.1376],\n",
            "        [    -0.1280],\n",
            "        [    -0.0832],\n",
            "        [    -0.1156],\n",
            "        [    -0.1027],\n",
            "        [    -0.0764],\n",
            "        [    -0.1232]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor([14.9600,  1.4900,  4.5600,  0.7500,  0.3700,  1.5200,  0.3700,  2.2400,\n",
            "         2.0300,  0.7500,  0.3700,  0.5100,  2.0500,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000,  0.3700,  0.0000,  0.3700,  0.0000,  0.7500,  0.0000,  0.0000,\n",
            "         0.5600,  0.0000,  2.0500,  0.0000,  1.1200,  0.0000,  0.0000,  0.5600,\n",
            "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000,  0.3700,  4.4900,  0.0000,  0.0000,  0.0000,  2.1200,  0.0000,\n",
            "         0.5100,  0.0000,  0.7500,  0.0000,  0.0000,  0.3700,  0.7500,  0.0000,\n",
            "        17.7200,  0.7500,  3.1300,  1.1200,  0.7500,  0.0000,  0.0000,  0.7500,\n",
            "         0.3700,  2.8000,  0.5600,  0.3700,  2.2400,  1.1200,  3.7600,  0.0000,\n",
            "         6.8000,  0.0000,  0.0000,  0.7100,  0.7100,  0.5100,  0.0000,  0.0000,\n",
            "         0.7500,  0.3700,  0.0000,  2.5100,  0.0000,  0.5100,  0.3700,  0.0000,\n",
            "         0.0000,  0.3700,  0.0000,  0.0000,  0.0000,  1.0800,  0.3700,  0.0000,\n",
            "         0.0000,  0.7100,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000,  0.0000], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# #evaluate model and predict on the test set\n",
        "# np.set_printoptions(suppress=True)\n",
        "# #dont print tensor in scientific notation\n",
        "# torch.set_printoptions(sci_mode=False)\n",
        "# modelv2.eval()\n",
        "# for data in test_loader:\n",
        "#     # print(data.shape)\n",
        "#     out = modelv2(data.to(device))\n",
        "#     # #print the predicted values and the actual values side by side for comparison\n",
        "\n",
        "#     print(out)\n",
        "#     print(data.y)\n",
        "#     break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku68BCZicoDA",
        "outputId": "9c3a205a-cd40-4213-a6c4-94827501ecce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 9.6856, 24.9700],\n",
            "        [ 2.1046,  1.8800],\n",
            "        [ 1.1991,  1.1200],\n",
            "        [ 1.6514,  1.8700],\n",
            "        [ 2.1329,  2.2400],\n",
            "        [ 2.1750,  2.5300],\n",
            "        [ 1.5385,  1.8700],\n",
            "        [ 1.2824,  1.1200],\n",
            "        [ 1.6668,  1.5200],\n",
            "        [ 1.7268,  2.9900],\n",
            "        [ 1.2441,  1.4900],\n",
            "        [ 1.3285,  1.4900],\n",
            "        [ 2.0457,  3.7300],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 1.6040,  1.1200],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 9.6856, 12.7600],\n",
            "        [ 0.7811,  0.5100],\n",
            "        [ 0.9670,  0.5600],\n",
            "        [ 1.0741,  1.8800],\n",
            "        [ 1.0388,  0.3700],\n",
            "        [ 1.0045,  1.0100],\n",
            "        [ 0.6794,  0.3700],\n",
            "        [ 0.4616,  0.3700],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 1.1584,  1.0100],\n",
            "        [ 0.9271,  0.3700],\n",
            "        [ 1.0033,  1.4900],\n",
            "        [ 0.5110,  0.3700],\n",
            "        [ 1.3647,  1.6800],\n",
            "        [ 1.0970,  0.6300],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 1.7773,  2.1200],\n",
            "        [ 9.6856,  8.4500],\n",
            "        [ 1.4448,  0.6300],\n",
            "        [ 1.0510,  0.5100],\n",
            "        [ 0.8084,  0.3700],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 1.5747,  3.5300],\n",
            "        [ 1.2339,  1.0100],\n",
            "        [ 1.1247,  1.5200],\n",
            "        [ 0.7240,  0.3700],\n",
            "        [ 1.0850,  0.5100],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 9.6856, 17.4800],\n",
            "        [ 2.8231,  0.7100],\n",
            "        [ 2.6221,  1.4100],\n",
            "        [ 2.0504,  2.5300],\n",
            "        [ 1.7800,  0.3700],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 1.9060,  0.5100],\n",
            "        [ 2.1575,  2.5100],\n",
            "        [ 1.6154,  0.3700],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 2.6123,  0.7100],\n",
            "        [ 2.1301,  1.1200],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 2.0782,  0.5600],\n",
            "        [ 1.7335,  0.3700],\n",
            "        [ 2.7964,  0.7100],\n",
            "        [ 1.8415,  1.5200],\n",
            "        [ 1.6477,  1.4900],\n",
            "        [ 2.5224,  0.7100],\n",
            "        [ 2.5012,  1.8800],\n",
            "        [ 9.6856,  3.7300],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.7273,  0.7100],\n",
            "        [ 0.7409,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 1.1125,  0.5600],\n",
            "        [ 0.7283,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 1.1813,  0.7100],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.6239,  0.3700],\n",
            "        [ 0.8156,  1.0100],\n",
            "        [ 0.6072,  0.3700],\n",
            "        [ 9.6856,  3.0800],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.7372,  0.0000],\n",
            "        [ 0.6541,  0.3700],\n",
            "        [ 0.6630,  0.5100],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.5260,  0.3700],\n",
            "        [ 0.3775,  0.3700],\n",
            "        [ 0.5631,  0.7500],\n",
            "        [ 0.4017,  0.0000],\n",
            "        [ 1.0587,  0.7100],\n",
            "        [ 9.6856,  4.6900],\n",
            "        [ 0.2863,  0.0000],\n",
            "        [ 0.7808,  0.3700],\n",
            "        [ 0.7038,  0.3700],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 1.1314,  0.3700],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.6197,  0.3700],\n",
            "        [ 1.1043,  0.7100],\n",
            "        [ 0.4363,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.7534,  0.3700],\n",
            "        [ 1.6754,  2.1200],\n",
            "        [ 9.6856, 13.3200],\n",
            "        [ 1.1406,  1.0100],\n",
            "        [ 1.5194,  0.7500],\n",
            "        [ 2.0046,  1.0100],\n",
            "        [ 1.5217,  1.0100],\n",
            "        [ 2.5593,  1.4100],\n",
            "        [ 2.0319,  0.5600],\n",
            "        [ 2.5085,  2.1200],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.6586,  0.3700],\n",
            "        [ 1.8928,  1.4900],\n",
            "        [ 2.3528,  2.8300],\n",
            "        [ 1.7946,  0.3700],\n",
            "        [ 1.8677,  0.3700],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 1.1111,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 9.6856,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 9.6856,  4.6400],\n",
            "        [ 0.8900,  0.0000],\n",
            "        [ 0.5686,  0.5600],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 1.0717,  0.0000],\n",
            "        [ 0.8616,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.6499,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.6623,  2.5100],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.5664,  1.0100],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.6240,  0.5600],\n",
            "        [ 0.0000,  0.0000]], device='cuda:0', grad_fn=<CatBackward0>)\n"
          ]
        }
      ],
      "source": [
        "#evaluate model and predict on the test set\n",
        "np.set_printoptions(suppress=True)\n",
        "#dont print tensor in scientific notation\n",
        "torch.set_printoptions(sci_mode=False)\n",
        "modelv2.eval()\n",
        "for data in test_loader:\n",
        "    out = modelv2(data.to(device))\n",
        "    #print the predicted values and the actual values side by side for comparison\n",
        "    print(torch.cat((out.view(-1,1),data.y.view(-1,1)),1))\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4rY-r30coDB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SecW8YxzcoDC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRlb8Zs-coDC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxJbwLD4coDD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJkPXM-fcoDD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4koFooORcoDD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfGznCq1coDD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oo2nLp2WcoDH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
